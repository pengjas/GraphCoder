{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/LPJ/ICML25/GraphCoder/GraphGPT /data/LPJ/ICML25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/yiyao_yang/anaconda3/envs/graphgpt/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "curPath = os.path.abspath('/data/LPJ/ICML25/GraphCoder/GraphGPT')\n",
    "rootPath = os.path.split(os.path.split(curPath)[0])[0]\n",
    "print(curPath, rootPath)\n",
    "sys.path.append(rootPath)\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from graphgpt.model import *\n",
    "from graphgpt.model.GraphLlama_pl import GraphGPT_pl\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    version: Optional[str] = field(default=\"v0\")\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_graph_mlp_adapter: bool = field(default=False)\n",
    "    graph_tower: Optional[str] = field(default=None)\n",
    "    graph_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n",
    "    pretrain_graph_mlp_adapter: Optional[str] = field(default=None)\n",
    "    use_graph_start_end: bool = field(default=False)\n",
    "    model_save_name: Optional[str] = field(default=\"model_{epoch}-{step}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_graph: bool = False\n",
    "    sep_graph_conv_front: bool = False\n",
    "    graph_token_len: int = 0\n",
    "    graph_content: Optional[str] = field(default=None)\n",
    "    graph_data_path: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = 'square'\n",
    "    bert_path: Optional[str] = field(default='/data/LPJ/bert/bert-L12-H128-uncased')\n",
    "    bert_gpu: Optional[int] = field(default=3)\n",
    "    bert_tokenizer_max_length: Optional[int] = field(default=15)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    freeze_graph_mlp_adapter: bool = field(default=False)\n",
    "    force_fsdp: bool = field(default=False)\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    double_quant: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
    "    )\n",
    "    quant_type: str = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
    "    )\n",
    "    bits: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"How many bits to use.\"}\n",
    "    )\n",
    "    strategy: str = field(\n",
    "        default='fsdp'\n",
    "    )\n",
    "    real_batch_size: int = field(default=1)\n",
    "\n",
    "    lora_enable: bool = False\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_weight_path: str = \"\"\n",
    "    lora_bias: str = \"none\"\n",
    "    disable_tqdm: bool =False\n",
    "\n",
    "    gpus: Optional[str] = field(default='0,1')\n",
    "    resume: Optional[str] = field(default=None)\n",
    "\n",
    "    adam_epsilon: float = field(default=1e-8)\n",
    "    warmup_steps:int = field(default=1000)\n",
    "    num_workers:int = field(default=16)\n",
    "\n",
    "    bf16: bool = field(default=False) \n",
    "    fp16: bool = field(default=False) \n",
    "    output_dir: str = field(default='./checkpoints/graphchat-gt-graphmatch-7b') \n",
    "    num_train_epochs: int = field(default=3)\n",
    "    per_device_train_batch_size: int = field(default=1)\n",
    "    per_device_eval_batch_size: int = field(default=1)\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    evaluation_strategy: str = field(default='no')\n",
    "    save_strategy: str = field(default='steps')\n",
    "    save_steps: int = field(default=2400)\n",
    "    save_total_limit: int = field(default=1)\n",
    "    learning_rate: float = field(default=2e-5)\n",
    "    weight_decay: float = field(default=0.)\n",
    "    warmup_ratio: float = field(default=0.03)\n",
    "    lr_scheduler_type: str = field(default='cosine')\n",
    "    logging_steps: int = field(default=1)\n",
    "    tf32: bool = field(default=True) \n",
    "    gradient_checkpointing: bool = field(default=True)\n",
    "    report_to: str = field(default='wandb')\n",
    "    freeze_gnn: bool = field(default=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"/data/LPJ/Llama-2-7b-chat-hf\",\n",
    "    version=\"v1\",\n",
    "    graph_tower='clip_gt_arxiv',\n",
    "    tune_graph_mlp_adapter=True,\n",
    "    graph_select_layer=-2,\n",
    "    use_graph_start_end=True,\n",
    "    freeze_backbone=True,\n",
    ")\n",
    "data_args = DataArguments(\n",
    "    data_path='/data/LPJ/ICML25/graphgpt_dataset/gpt_dataset_construction/rtlcoder_gpt4_v1/import_for_graphgpt/conversations.json',\n",
    "    graph_data_path='/data/LPJ/ICML25/graphgpt_dataset/gpt_dataset_construction/rtlcoder_gpt4_v1/import_for_graphgpt/graph.jsonl',\n",
    "    lazy_preprocess=True,\n",
    "    bert_path='/data/LPJ/bert/bert-L12-H128-uncased',\n",
    "    bert_gpu=3,\n",
    "    bert_tokenizer_max_length=15,\n",
    "\n",
    ")\n",
    "train_args = TrainingArguments(\n",
    "    bf16=False,\n",
    "    output_dir='/data/LPJ/ICML25/GraphGPT/checkpoints/pretraining_stage/v0',\n",
    "    num_train_epochs=3,\n",
    "    gpus='0,1,2',\n",
    "    # lora_enable=True,\n",
    "    freeze_gnn=True,\n",
    "    lora_enable=False,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GraphGPT_pl(train_args, model_args, data_args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# prj = torch.load('/data/LPJ/GraphGPT/checkpoints/stage2_projector/stage2_projector.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens.weight': tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
       "          -6.5565e-06,  8.9407e-07],\n",
       "         [ 2.9659e-03, -2.9545e-03,  9.2316e-04,  ..., -8.8196e-03,\n",
       "           2.5730e-03, -2.9144e-03],\n",
       "         [ 1.0010e-02,  1.1124e-02, -6.3515e-03,  ...,  2.9354e-03,\n",
       "          -3.4833e-04, -4.8790e-03],\n",
       "         ...,\n",
       "         [-3.5670e-03, -6.4739e-03,  5.1549e-02,  ...,  1.9736e-02,\n",
       "          -3.9243e-03, -2.2117e-02],\n",
       "         [ 2.2690e-01, -3.3845e-01,  9.8375e-02,  ...,  5.1070e-02,\n",
       "           1.6382e-01, -2.3463e-01],\n",
       "         [ 1.5626e-01, -8.9884e-02,  3.7082e-01,  ...,  1.0706e-01,\n",
       "          -1.3957e-01,  2.3497e-01]]),\n",
       " 'model.graph_projector.bias': tensor([-0.1266, -0.1382, -0.0562,  ...,  0.0539,  0.1247, -0.1546]),\n",
       " 'model.graph_projector.weight': tensor([[-0.0174,  0.0082, -0.4908,  ...,  0.0561, -0.0531,  0.1414],\n",
       "         [ 0.2474, -0.1808, -0.2793,  ..., -0.1517,  0.1896,  0.1827],\n",
       "         [ 0.0906,  0.0196,  0.1289,  ...,  0.2189,  0.0118, -0.0049],\n",
       "         ...,\n",
       "         [ 0.3729,  0.2153,  0.1221,  ..., -0.0606,  0.0221,  0.0269],\n",
       "         [ 0.0264,  0.1642, -0.0825,  ...,  0.0907,  0.2621,  0.0207],\n",
       "         [ 0.0345,  0.1449,  0.3363,  ...,  0.0417, -0.0136,  0.1885]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type GraphLlama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph pre train model\n",
      "CLIP(\n",
      "  (gnn): graph_transformer(\n",
      "    (gtLayers): Sequential(\n",
      "      (0): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (W_P): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (inverW_P): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "************************** parameters: # 131612672\n",
      "['model.embed_tokens.weight', 'model.graph_projector.weight', 'model.graph_projector.bias']\n"
     ]
    }
   ],
   "source": [
    "model = GraphGPT_pl(training_args=train_args, model_args=model_args, data_args=data_args, tokenizer=tokenizer, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "graph_tower_path = '/data/LPJ/ICML25/GraphGPT/clip_gt_arxiv/clip_gt_arxiv_pub.pkl'\n",
    "graph = torch.load(graph_tower_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('positional_embedding',\n",
       "              tensor([[ 0.0058, -0.0168,  0.0063,  ..., -0.0139, -0.0081,  0.0112],\n",
       "                      [ 0.0232, -0.0001,  0.0147,  ...,  0.0132, -0.0007,  0.0020],\n",
       "                      [ 0.0133,  0.0132,  0.0058,  ...,  0.0047, -0.0026,  0.0081],\n",
       "                      ...,\n",
       "                      [-0.0152,  0.0029, -0.0169,  ..., -0.0090,  0.0010, -0.0025],\n",
       "                      [-0.0162, -0.0187,  0.0087,  ...,  0.0078,  0.0164, -0.0023],\n",
       "                      [-0.0097, -0.0164, -0.0059,  ..., -0.0043,  0.0037,  0.0037]])),\n",
       "             ('text_projection',\n",
       "              tensor([[-0.0410, -0.0377, -0.0631,  ..., -0.0065, -0.0226,  0.0741],\n",
       "                      [-0.0365, -0.0054, -0.0592,  ...,  0.0389, -0.0064,  0.0429],\n",
       "                      [ 0.0242, -0.0850, -0.0273,  ...,  0.0583,  0.0585,  0.0480],\n",
       "                      ...,\n",
       "                      [-0.0469, -0.0222,  0.0058,  ..., -0.0335, -0.0138,  0.0039],\n",
       "                      [ 0.0486, -0.0557,  0.0617,  ...,  0.0045, -0.0933, -0.0408],\n",
       "                      [-0.0480,  0.0018,  0.0018,  ...,  0.0597, -0.0146, -0.0121]])),\n",
       "             ('gnn.W_pos',\n",
       "              tensor([[ 5.9695e-03,  1.3920e-02, -1.9633e-04, -1.7928e-02,  9.3351e-03,\n",
       "                       -3.8910e-03, -2.8296e-03,  7.7119e-05,  1.6241e-02,  1.1422e-03,\n",
       "                       -7.6159e-03,  1.0688e-02, -1.3478e-02,  5.1847e-03,  1.4821e-02,\n",
       "                       -4.4356e-03, -4.7085e-03, -3.6886e-03, -1.5971e-02,  4.8877e-03,\n",
       "                       -9.7153e-03, -1.2940e-02, -1.2782e-02,  9.9297e-03, -1.5936e-02,\n",
       "                        8.7045e-03,  6.9003e-03,  1.2230e-03,  5.7747e-03, -3.0063e-03,\n",
       "                       -1.0129e-02,  9.7615e-03,  1.2311e-02, -1.4170e-03, -7.6393e-03,\n",
       "                       -1.1192e-02,  1.8476e-02,  8.1993e-03, -8.2276e-04,  7.7580e-03,\n",
       "                       -7.3658e-03, -1.9681e-02, -5.3578e-03,  1.9113e-02,  5.9419e-03,\n",
       "                       -1.6973e-02, -2.9029e-03, -1.0431e-02,  1.3012e-02,  4.8928e-03,\n",
       "                       -1.4325e-03, -1.4519e-02,  4.3928e-03, -4.6018e-03,  5.9175e-06,\n",
       "                       -5.9936e-03, -1.3447e-02, -7.8248e-03, -2.1619e-03,  1.1672e-02,\n",
       "                       -1.1645e-02, -4.0546e-03, -1.1098e-02, -1.6258e-03, -3.1101e-03,\n",
       "                        1.4682e-02,  3.4828e-03, -1.0307e-02, -1.0616e-02, -3.6176e-03,\n",
       "                       -1.3466e-02, -9.7250e-03,  1.5865e-02, -1.2434e-02, -4.7864e-03,\n",
       "                        1.0237e-02, -1.7147e-03, -5.1913e-04, -7.5332e-03, -4.6487e-03,\n",
       "                       -1.0430e-02,  1.1445e-02,  5.9446e-03,  1.1523e-02,  1.0772e-03,\n",
       "                       -1.0290e-02, -6.5755e-03,  1.1763e-02,  1.8842e-02,  1.6092e-02,\n",
       "                       -1.9552e-02, -1.9731e-02, -2.4895e-03, -1.0218e-03,  1.8953e-02,\n",
       "                       -8.5273e-03,  1.2600e-02,  3.6807e-03,  1.9064e-02,  6.2555e-04,\n",
       "                        9.8014e-03, -4.2738e-03,  5.9907e-03,  3.2355e-03, -8.6433e-03,\n",
       "                       -1.6310e-02,  4.4622e-03,  7.8263e-04,  6.4539e-03,  3.1145e-04,\n",
       "                        1.5624e-02, -2.8902e-03, -1.7908e-02,  5.5098e-03,  1.8304e-02,\n",
       "                        5.3510e-03,  1.9762e-02, -1.0069e-02, -1.1989e-02,  8.0916e-03,\n",
       "                        8.5878e-03, -8.4290e-03, -5.0578e-05, -4.0286e-03, -1.6813e-02,\n",
       "                       -3.1945e-03, -1.9965e-02, -3.3666e-03]])),\n",
       "             ('gnn.gtLayers.0.qTrans',\n",
       "              tensor([[ 0.0741, -0.0778, -0.0239,  ..., -0.0861, -0.1395, -0.0260],\n",
       "                      [ 0.0893,  0.1540, -0.1024,  ...,  0.0712,  0.0160, -0.0735],\n",
       "                      [ 0.0887,  0.0918, -0.0063,  ..., -0.1260,  0.1193, -0.0394],\n",
       "                      ...,\n",
       "                      [-0.0344, -0.0132, -0.0895,  ..., -0.0237, -0.0309,  0.0127],\n",
       "                      [-0.0968, -0.1065,  0.0725,  ..., -0.1153, -0.0287, -0.1241],\n",
       "                      [-0.1292, -0.0773,  0.0175,  ..., -0.1423, -0.0280, -0.1142]])),\n",
       "             ('gnn.gtLayers.0.kTrans',\n",
       "              tensor([[-0.0396, -0.0691, -0.1384,  ...,  0.1060,  0.0972,  0.0222],\n",
       "                      [ 0.0149,  0.1415, -0.0296,  ..., -0.0767,  0.1159, -0.0847],\n",
       "                      [ 0.1098, -0.1202, -0.0229,  ..., -0.1494, -0.0793, -0.0868],\n",
       "                      ...,\n",
       "                      [ 0.0653,  0.1458, -0.1069,  ...,  0.1378, -0.0967,  0.1485],\n",
       "                      [-0.0599, -0.0913,  0.1095,  ...,  0.1353, -0.0295, -0.1313],\n",
       "                      [ 0.0237, -0.0295,  0.1077,  ...,  0.0655, -0.1162, -0.0118]])),\n",
       "             ('gnn.gtLayers.0.vTrans',\n",
       "              tensor([[ 0.0383,  0.0674, -0.0410,  ...,  0.0179,  0.0723,  0.0730],\n",
       "                      [-0.1262, -0.0997,  0.0750,  ..., -0.0819,  0.0574, -0.0486],\n",
       "                      [ 0.0675,  0.0067,  0.0408,  ...,  0.1486,  0.1214,  0.0886],\n",
       "                      ...,\n",
       "                      [-0.0971,  0.1477,  0.1373,  ..., -0.0300,  0.0554, -0.1249],\n",
       "                      [ 0.0421,  0.1454, -0.0453,  ..., -0.0933, -0.0317, -0.1353],\n",
       "                      [ 0.1318,  0.0585, -0.1073,  ...,  0.0476,  0.0284, -0.0267]])),\n",
       "             ('gnn.gtLayers.0.norm.weight',\n",
       "              tensor([0.9992, 0.9929, 1.0030, 0.9945, 0.9964, 0.9992, 1.0046, 1.0022, 1.0078,\n",
       "                      0.9907, 0.9925, 1.0019, 0.9999, 1.0101, 1.0015, 0.9976, 0.9997, 0.9975,\n",
       "                      0.9917, 0.9960, 1.0073, 0.9955, 0.9973, 0.9981, 0.9915, 0.9894, 0.9909,\n",
       "                      1.0005, 0.9968, 0.9939, 1.0030, 1.0007, 1.0000, 0.9986, 0.9882, 0.9936,\n",
       "                      0.9996, 0.9997, 1.0015, 0.9996, 0.9936, 1.0009, 1.0004, 0.9982, 0.9926,\n",
       "                      1.0011, 0.9899, 0.9946, 0.9994, 0.9994, 0.9947, 0.9837, 1.0055, 0.9967,\n",
       "                      0.9968, 1.0018, 1.0035, 0.9917, 0.9940, 0.9982, 1.0023, 1.0033, 1.0018,\n",
       "                      0.9978, 0.9917, 1.0037, 0.9997, 0.9957, 1.0045, 1.0018, 0.9974, 0.9974,\n",
       "                      1.0022, 0.9986, 0.9927, 0.9965, 0.9986, 0.9958, 1.0060, 1.0039, 0.9909,\n",
       "                      0.9951, 0.9990, 0.9897, 0.9975, 0.9888, 1.0010, 0.9935, 0.9919, 1.0002,\n",
       "                      1.0049, 0.9929, 0.9959, 0.9933, 1.0026, 0.9983, 0.9993, 1.0017, 1.0053,\n",
       "                      0.9938, 1.0052, 0.9975, 0.9963, 1.0044, 0.9981, 0.9960, 0.9929, 1.0050,\n",
       "                      1.0004, 1.0007, 1.0059, 0.9937, 0.9959, 0.9956, 0.9976, 0.9962, 1.0043,\n",
       "                      0.9955, 0.9916, 0.9963, 0.9975, 0.9898, 1.0080, 0.9970, 0.9972, 0.9949,\n",
       "                      1.0016, 1.0034])),\n",
       "             ('gnn.gtLayers.0.norm.bias',\n",
       "              tensor([ 9.5526e-04,  3.6719e-02, -5.7622e-03,  1.6530e-02,  6.0306e-03,\n",
       "                      -1.1069e-02,  4.6048e-02,  1.7064e-02,  1.4878e-02, -2.8257e-02,\n",
       "                       1.3936e-02,  5.3416e-03,  2.5205e-02,  3.3243e-02,  6.1865e-03,\n",
       "                       1.9362e-02, -1.7830e-02, -9.2207e-03, -1.1298e-02, -9.5391e-03,\n",
       "                      -3.8267e-02,  2.8741e-02, -8.5987e-03,  8.0343e-05, -2.2523e-02,\n",
       "                       2.3067e-03, -7.8895e-04,  2.5888e-02,  2.4329e-02,  3.1887e-03,\n",
       "                       1.4177e-02, -1.9578e-02, -6.8368e-04,  3.7055e-03,  1.0169e-02,\n",
       "                      -2.1224e-03,  1.7878e-03,  2.5253e-02, -5.8213e-03, -2.1362e-02,\n",
       "                      -1.3963e-02,  6.7063e-03, -2.8163e-02,  6.4929e-03,  6.8272e-03,\n",
       "                       3.2531e-02, -1.0706e-03, -1.0601e-03,  1.6787e-02, -1.1569e-02,\n",
       "                      -1.2319e-02, -3.3079e-03, -2.8014e-02,  1.7754e-02, -2.1985e-02,\n",
       "                      -2.6705e-02, -2.3185e-02,  1.6248e-02,  1.3771e-03, -1.7500e-02,\n",
       "                      -2.4127e-02,  2.3612e-02, -1.8023e-02, -7.9868e-03,  5.3127e-03,\n",
       "                      -7.8145e-03,  3.1653e-02,  1.3105e-02, -1.6203e-02, -2.2066e-02,\n",
       "                      -1.4683e-02, -2.2922e-02,  1.1994e-02,  9.0641e-03,  4.1345e-02,\n",
       "                       1.0857e-02, -1.4437e-03,  3.3447e-02,  2.0323e-02, -1.6650e-03,\n",
       "                       7.8135e-03,  2.6693e-03,  1.2730e-02,  1.2045e-02,  7.9678e-03,\n",
       "                       1.9515e-02,  1.3226e-02,  2.6120e-02, -4.7656e-03, -1.2215e-02,\n",
       "                      -8.8401e-03,  2.0215e-02,  3.6830e-03, -1.2090e-02,  2.5264e-03,\n",
       "                      -1.7758e-02, -6.7931e-03, -2.6880e-02,  2.5605e-02, -1.4449e-03,\n",
       "                      -1.2975e-02, -3.7101e-03,  3.2416e-02, -2.7463e-03,  3.2609e-03,\n",
       "                       2.0656e-02,  2.5931e-03,  1.0902e-02,  4.6376e-03,  8.2565e-03,\n",
       "                      -3.0405e-02, -3.1872e-02,  1.9126e-02,  1.5396e-02, -4.1811e-03,\n",
       "                      -3.5136e-03, -4.7257e-02,  6.7599e-03, -2.5258e-02,  1.4545e-02,\n",
       "                      -1.4434e-03,  1.0288e-03, -2.8038e-02,  3.0253e-02,  3.5114e-03,\n",
       "                       6.6984e-03, -2.0336e-02,  3.0965e-02])),\n",
       "             ('gnn.gtLayers.1.qTrans',\n",
       "              tensor([[ 0.0648, -0.0177, -0.1365,  ..., -0.1397,  0.0273, -0.0657],\n",
       "                      [ 0.1189, -0.0059,  0.1248,  ...,  0.0204, -0.1198,  0.1089],\n",
       "                      [-0.0008, -0.1372,  0.0504,  ..., -0.0697, -0.1424,  0.0522],\n",
       "                      ...,\n",
       "                      [-0.0927, -0.0547,  0.1063,  ...,  0.1333,  0.0499,  0.0744],\n",
       "                      [-0.0117, -0.0571,  0.0289,  ..., -0.0011, -0.0233,  0.0378],\n",
       "                      [ 0.1261, -0.1461,  0.0477,  ...,  0.0303, -0.0527,  0.1027]])),\n",
       "             ('gnn.gtLayers.1.kTrans',\n",
       "              tensor([[-0.1244, -0.0564, -0.1420,  ..., -0.0994, -0.0779, -0.0005],\n",
       "                      [-0.0106, -0.0354, -0.1048,  ...,  0.0186, -0.1053, -0.0864],\n",
       "                      [ 0.0985, -0.1305, -0.1000,  ...,  0.0182, -0.0220, -0.0913],\n",
       "                      ...,\n",
       "                      [ 0.0955, -0.0884,  0.0584,  ..., -0.1380, -0.0822,  0.0615],\n",
       "                      [-0.1355,  0.1264, -0.0097,  ..., -0.1215,  0.0526, -0.0375],\n",
       "                      [-0.0518,  0.1395, -0.1023,  ...,  0.0601, -0.1130, -0.0410]])),\n",
       "             ('gnn.gtLayers.1.vTrans',\n",
       "              tensor([[-0.0470,  0.1331,  0.0249,  ...,  0.0138,  0.0837,  0.1119],\n",
       "                      [ 0.1404, -0.0442, -0.1323,  ...,  0.0056, -0.1046,  0.0060],\n",
       "                      [ 0.1014,  0.0036, -0.0771,  ..., -0.0099, -0.1518, -0.0468],\n",
       "                      ...,\n",
       "                      [-0.0546, -0.0113, -0.0577,  ..., -0.1282,  0.0383,  0.0831],\n",
       "                      [ 0.1260,  0.0352, -0.0573,  ..., -0.0973, -0.0064,  0.0132],\n",
       "                      [-0.0245,  0.1086, -0.0479,  ..., -0.1469, -0.0979, -0.0504]])),\n",
       "             ('gnn.gtLayers.1.norm.weight',\n",
       "              tensor([1.0004, 0.9977, 0.9930, 1.0021, 0.9944, 1.0000, 0.9926, 0.9928, 1.0091,\n",
       "                      0.9990, 1.0084, 1.0056, 0.9945, 0.9928, 1.0017, 1.0047, 0.9953, 0.9978,\n",
       "                      0.9923, 0.9866, 1.0034, 0.9958, 0.9952, 0.9932, 0.9942, 0.9985, 0.9960,\n",
       "                      0.9889, 0.9968, 0.9966, 1.0008, 0.9910, 0.9981, 0.9886, 0.9938, 0.9986,\n",
       "                      1.0023, 0.9990, 1.0132, 0.9989, 0.9990, 0.9999, 1.0028, 0.9924, 0.9891,\n",
       "                      1.0047, 0.9938, 0.9942, 1.0002, 1.0058, 0.9948, 0.9894, 1.0157, 1.0048,\n",
       "                      0.9947, 0.9942, 1.0123, 0.9877, 1.0063, 1.0171, 1.0017, 1.0075, 1.0037,\n",
       "                      0.9981, 0.9959, 0.9994, 0.9959, 0.9951, 0.9990, 0.9977, 0.9933, 1.0003,\n",
       "                      1.0007, 1.0010, 0.9919, 0.9939, 0.9972, 0.9936, 0.9984, 0.9985, 1.0028,\n",
       "                      0.9992, 1.0032, 0.9883, 0.9897, 1.0041, 1.0079, 0.9852, 0.9996, 0.9967,\n",
       "                      1.0020, 0.9925, 0.9910, 1.0045, 0.9971, 0.9997, 0.9902, 0.9906, 1.0002,\n",
       "                      0.9933, 0.9932, 1.0004, 0.9959, 1.0037, 1.0017, 0.9967, 0.9912, 0.9842,\n",
       "                      0.9988, 0.9942, 1.0030, 0.9948, 1.0003, 0.9983, 1.0061, 1.0019, 1.0051,\n",
       "                      1.0006, 0.9920, 1.0104, 0.9975, 0.9913, 0.9916, 0.9933, 0.9935, 1.0041,\n",
       "                      0.9989, 0.9980])),\n",
       "             ('gnn.gtLayers.1.norm.bias',\n",
       "              tensor([ 0.0290,  0.0421,  0.0050,  0.0471, -0.0005, -0.0025,  0.0332,  0.0451,\n",
       "                       0.0001, -0.0374,  0.0076,  0.0285,  0.0088,  0.0085,  0.0102,  0.0166,\n",
       "                      -0.0107, -0.0051, -0.0189, -0.0027, -0.0377,  0.0282, -0.0196,  0.0141,\n",
       "                      -0.0036, -0.0302,  0.0078,  0.0054,  0.0192, -0.0127,  0.0176, -0.0206,\n",
       "                      -0.0054, -0.0082, -0.0090,  0.0050, -0.0128,  0.0317, -0.0078, -0.0197,\n",
       "                      -0.0073,  0.0223, -0.0323,  0.0134,  0.0013,  0.0228, -0.0028, -0.0013,\n",
       "                       0.0136, -0.0024, -0.0265, -0.0057, -0.0223, -0.0002, -0.0252,  0.0034,\n",
       "                      -0.0217, -0.0039, -0.0350, -0.0129,  0.0022,  0.0078, -0.0320,  0.0010,\n",
       "                      -0.0060, -0.0082,  0.0209,  0.0046,  0.0011, -0.0188, -0.0040,  0.0095,\n",
       "                      -0.0188, -0.0018,  0.0345, -0.0010, -0.0085,  0.0222,  0.0222, -0.0184,\n",
       "                       0.0154, -0.0148,  0.0239, -0.0035,  0.0058,  0.0454,  0.0060,  0.0218,\n",
       "                      -0.0104,  0.0086, -0.0147,  0.0319,  0.0017, -0.0245,  0.0087, -0.0096,\n",
       "                      -0.0075,  0.0037,  0.0152,  0.0031, -0.0115, -0.0101,  0.0426,  0.0107,\n",
       "                       0.0011, -0.0134, -0.0024, -0.0208,  0.0202,  0.0002, -0.0338, -0.0434,\n",
       "                       0.0220, -0.0039,  0.0025,  0.0042, -0.0454,  0.0136, -0.0141,  0.0321,\n",
       "                       0.0134, -0.0094, -0.0150,  0.0205,  0.0007,  0.0151, -0.0185,  0.0249])),\n",
       "             ('gnn.gtLayers.2.qTrans',\n",
       "              tensor([[-0.0190, -0.0385,  0.0077,  ..., -0.0412,  0.0496,  0.0381],\n",
       "                      [-0.0683, -0.1257, -0.0287,  ...,  0.0841, -0.1447, -0.1406],\n",
       "                      [-0.0843,  0.0457,  0.0392,  ...,  0.0006, -0.0983, -0.0555],\n",
       "                      ...,\n",
       "                      [-0.0169, -0.1488, -0.0200,  ..., -0.0461,  0.1134, -0.0796],\n",
       "                      [ 0.0356, -0.0787, -0.1285,  ...,  0.0358, -0.0179,  0.0772],\n",
       "                      [ 0.0476, -0.0501,  0.1125,  ..., -0.0358, -0.0349,  0.0766]])),\n",
       "             ('gnn.gtLayers.2.kTrans',\n",
       "              tensor([[-0.1401,  0.1268,  0.0160,  ..., -0.0518, -0.0855,  0.0295],\n",
       "                      [-0.0830,  0.0573, -0.0262,  ..., -0.1187, -0.0871, -0.0507],\n",
       "                      [ 0.0607, -0.0408,  0.0978,  ..., -0.0227, -0.0207, -0.0192],\n",
       "                      ...,\n",
       "                      [-0.0730, -0.1331,  0.1262,  ..., -0.0431, -0.0304,  0.0218],\n",
       "                      [-0.1380, -0.1332, -0.1276,  ...,  0.0467, -0.0351,  0.0502],\n",
       "                      [-0.0354,  0.0889,  0.0746,  ...,  0.0785,  0.0459, -0.1493]])),\n",
       "             ('gnn.gtLayers.2.vTrans',\n",
       "              tensor([[-0.0408,  0.1459,  0.0831,  ...,  0.0664, -0.0802, -0.1026],\n",
       "                      [ 0.0173,  0.0370, -0.0439,  ...,  0.0182,  0.0017,  0.0673],\n",
       "                      [-0.0587,  0.0534,  0.0230,  ...,  0.1442,  0.0870, -0.0326],\n",
       "                      ...,\n",
       "                      [-0.1183, -0.0326, -0.1569,  ...,  0.0632,  0.0782, -0.1404],\n",
       "                      [-0.0947, -0.0042, -0.0049,  ...,  0.1286,  0.0206, -0.0892],\n",
       "                      [-0.1533, -0.0814,  0.1031,  ..., -0.0343,  0.0747,  0.0274]])),\n",
       "             ('gnn.gtLayers.2.norm.weight',\n",
       "              tensor([1.0013, 1.0012, 0.9917, 1.0019, 0.9958, 0.9966, 1.0060, 0.9982, 0.9931,\n",
       "                      1.0029, 1.0024, 1.0020, 0.9908, 0.9961, 1.0007, 1.0029, 0.9945, 0.9944,\n",
       "                      0.9996, 0.9918, 0.9990, 0.9947, 1.0032, 0.9975, 0.9991, 0.9959, 1.0000,\n",
       "                      0.9971, 0.9977, 1.0055, 0.9947, 0.9926, 1.0046, 1.0011, 0.9968, 0.9971,\n",
       "                      1.0034, 0.9957, 1.0019, 0.9975, 0.9994, 1.0046, 1.0018, 0.9953, 0.9914,\n",
       "                      0.9954, 0.9923, 1.0058, 1.0020, 0.9998, 1.0003, 0.9960, 1.0051, 1.0031,\n",
       "                      0.9936, 0.9949, 0.9974, 0.9953, 1.0075, 1.0041, 0.9970, 1.0026, 1.0017,\n",
       "                      1.0035, 1.0020, 0.9930, 1.0055, 0.9954, 0.9998, 0.9999, 1.0013, 0.9974,\n",
       "                      0.9984, 1.0018, 0.9982, 0.9979, 1.0063, 0.9995, 0.9997, 1.0009, 1.0018,\n",
       "                      1.0038, 1.0031, 0.9956, 1.0025, 0.9986, 0.9978, 1.0060, 1.0000, 0.9941,\n",
       "                      0.9998, 1.0007, 1.0007, 1.0053, 0.9965, 0.9995, 0.9942, 0.9986, 0.9974,\n",
       "                      0.9972, 0.9973, 0.9969, 1.0019, 0.9955, 1.0111, 0.9968, 1.0073, 0.9954,\n",
       "                      0.9958, 0.9986, 0.9995, 1.0015, 0.9968, 1.0012, 0.9989, 1.0019, 1.0059,\n",
       "                      0.9910, 0.9950, 1.0080, 0.9936, 1.0075, 0.9986, 1.0011, 0.9886, 0.9999,\n",
       "                      1.0036, 1.0002])),\n",
       "             ('gnn.gtLayers.2.norm.bias',\n",
       "              tensor([ 3.1587e-02,  4.0808e-02, -8.9973e-03,  5.4825e-02, -1.8816e-02,\n",
       "                      -6.2868e-03,  4.2358e-02,  2.9962e-02,  1.6284e-02, -6.0164e-02,\n",
       "                       1.5616e-02,  1.2748e-02,  2.7787e-02,  1.6403e-02,  2.6648e-02,\n",
       "                      -1.0175e-02, -1.8838e-02,  2.1434e-02,  5.2504e-03,  2.5470e-03,\n",
       "                      -2.4838e-02,  2.7269e-02, -2.6564e-02,  1.5282e-02, -1.4472e-03,\n",
       "                      -3.9610e-02, -3.5466e-02,  2.3879e-02,  1.9298e-02, -6.2727e-03,\n",
       "                       2.7902e-02, -1.9838e-02,  1.9641e-02, -2.0910e-05, -5.7066e-03,\n",
       "                      -1.7362e-03, -3.5724e-02,  2.2690e-02, -2.4467e-03, -2.0958e-02,\n",
       "                      -1.4046e-02,  1.6474e-02, -3.0445e-02, -6.5218e-03,  1.4064e-02,\n",
       "                       3.5703e-02, -3.6066e-02, -2.9727e-02,  3.7807e-02, -1.1798e-02,\n",
       "                      -4.5077e-02, -1.3048e-02, -4.4533e-02,  2.9109e-02, -1.4475e-02,\n",
       "                       1.6565e-02, -3.1731e-02, -3.1870e-02, -4.0659e-02, -1.5966e-02,\n",
       "                       3.2345e-03,  1.7765e-02, -4.7173e-02,  5.1644e-03,  9.7635e-03,\n",
       "                      -3.3474e-03,  3.4887e-02,  1.6275e-03, -1.6222e-03,  3.4943e-03,\n",
       "                       4.2314e-02, -5.3796e-03, -1.4866e-02, -2.7856e-02,  5.2443e-02,\n",
       "                       5.6123e-03, -3.4809e-02, -1.6626e-03,  1.4482e-02, -3.1177e-02,\n",
       "                       1.9710e-02, -3.4225e-02,  2.3427e-02, -2.4892e-02,  9.7473e-03,\n",
       "                       4.6729e-02, -2.6578e-04,  4.6219e-02, -6.1328e-03,  3.1001e-02,\n",
       "                       5.5353e-06,  2.5629e-02,  1.4581e-02, -2.8540e-02,  3.3441e-02,\n",
       "                      -1.3832e-02, -6.4792e-03,  1.2709e-02,  2.1843e-02,  8.7725e-03,\n",
       "                      -2.5315e-02, -2.3552e-02,  3.5604e-02,  8.4724e-03,  8.4452e-03,\n",
       "                      -3.8335e-02, -4.6164e-02, -5.1326e-02,  3.6177e-02, -4.5464e-03,\n",
       "                      -3.0572e-02, -6.5088e-02,  1.6926e-02, -3.5335e-02,  3.3994e-03,\n",
       "                       1.4689e-02, -6.2100e-02,  3.6186e-02, -4.6472e-02,  1.6351e-02,\n",
       "                       5.0639e-03,  4.5115e-04,  1.6955e-03,  1.2460e-02,  2.1843e-02,\n",
       "                       3.9354e-02, -3.2607e-02,  1.8911e-04])),\n",
       "             ('gnn.W_P.weight',\n",
       "              tensor([[-0.0854, -0.0825, -0.0147,  ..., -0.0303,  0.0900,  0.0180],\n",
       "                      [-0.0768,  0.0165,  0.0666,  ..., -0.0111, -0.0920, -0.0205],\n",
       "                      [-0.0228,  0.0610,  0.0138,  ...,  0.0817,  0.0719, -0.0205],\n",
       "                      ...,\n",
       "                      [-0.0172, -0.0145, -0.0516,  ...,  0.0710, -0.0230,  0.0663],\n",
       "                      [-0.0305,  0.0131,  0.0615,  ...,  0.0293,  0.0499,  0.0129],\n",
       "                      [-0.0751,  0.0848, -0.0541,  ..., -0.0175,  0.0694,  0.0865]])),\n",
       "             ('gnn.W_P.bias',\n",
       "              tensor([-8.7655e-02,  7.8680e-02,  1.3938e-02, -5.8576e-02,  9.4576e-02,\n",
       "                      -9.4593e-02,  1.1346e-01, -6.5674e-02,  7.2189e-02, -4.8012e-02,\n",
       "                       3.3784e-02, -2.7620e-03, -4.1281e-05,  9.1136e-02, -1.8032e-03,\n",
       "                      -5.6314e-02, -3.0182e-02, -1.0155e-01, -8.8021e-02,  2.5575e-03,\n",
       "                      -1.2610e-01, -6.5165e-02, -1.0351e-01,  1.0010e-01, -1.2968e-02,\n",
       "                       4.2197e-04,  1.4644e-02,  8.3570e-02, -8.5883e-03, -5.7709e-02,\n",
       "                       7.5260e-03, -3.0290e-02,  5.1715e-02,  3.4343e-02, -7.0819e-02,\n",
       "                      -3.9374e-02, -7.0462e-02,  6.4101e-02, -6.1254e-02, -9.1402e-02,\n",
       "                       5.2804e-02, -3.0192e-02, -1.0996e-01,  5.7273e-02, -6.1042e-02,\n",
       "                       5.9220e-02, -3.7308e-02,  8.1883e-03,  5.0842e-02, -4.7257e-02,\n",
       "                      -6.6437e-02,  8.8789e-02, -1.5162e-02,  7.8679e-02, -7.2773e-02,\n",
       "                      -3.4388e-02, -6.4134e-02, -4.8023e-02,  4.6626e-03,  7.1599e-02,\n",
       "                      -9.1222e-02,  3.8100e-02, -3.3988e-02, -5.3712e-02, -1.2653e-02,\n",
       "                      -4.7175e-02,  4.7928e-02,  4.2778e-02, -1.0639e-01, -8.3135e-02,\n",
       "                       2.0098e-03, -4.2490e-03, -2.1321e-03,  6.0602e-02,  8.1554e-02,\n",
       "                       5.6088e-02,  1.9894e-02, -9.7941e-03, -4.4532e-02, -5.4827e-02,\n",
       "                      -6.4218e-02,  4.2505e-02, -1.0349e-02,  5.9688e-02, -6.0179e-02,\n",
       "                       4.7464e-02, -3.0455e-02,  5.7658e-02,  3.4830e-02, -8.5584e-02,\n",
       "                      -8.4766e-02,  1.0681e-01,  2.9717e-03, -5.5121e-02, -9.4939e-02,\n",
       "                      -7.5192e-02, -3.5241e-02,  6.4260e-02,  1.2786e-01, -2.6551e-02,\n",
       "                       1.4718e-02, -4.5942e-02,  9.4168e-02, -8.2567e-03,  9.2575e-03,\n",
       "                       4.3849e-02, -1.4865e-02, -4.2334e-02,  7.6384e-02,  1.2236e-02,\n",
       "                      -9.7390e-02, -4.6814e-02,  6.0293e-02, -7.6254e-02, -6.4413e-02,\n",
       "                       1.7664e-02, -3.7057e-02, -3.3938e-02, -1.1180e-02,  8.3412e-02,\n",
       "                       4.0897e-02,  8.6419e-02, -1.1531e-01, -2.0190e-02, -1.0869e-02,\n",
       "                      -1.7238e-02, -9.9846e-02,  1.1949e-01])),\n",
       "             ('gnn.inverW_P.weight',\n",
       "              tensor([[ 7.3807e-03, -5.8127e-02,  5.9884e-02,  ...,  4.9166e-02,\n",
       "                       -6.9525e-02,  1.1893e-02],\n",
       "                      [ 6.3348e-02, -8.1143e-02,  3.7269e-02,  ..., -8.1960e-03,\n",
       "                       -3.3159e-03, -6.7866e-02],\n",
       "                      [-5.7288e-05,  6.9881e-02,  7.9479e-02,  ..., -3.2622e-02,\n",
       "                       -8.6070e-02,  8.4725e-02],\n",
       "                      ...,\n",
       "                      [-2.4912e-02, -6.7204e-02,  8.1664e-02,  ...,  6.3426e-02,\n",
       "                        1.2365e-02,  3.0331e-02],\n",
       "                      [ 4.8598e-02, -1.2044e-02,  4.3424e-03,  ...,  6.5452e-02,\n",
       "                       -6.1171e-02, -1.8918e-02],\n",
       "                      [ 5.8774e-02, -5.8837e-02, -3.0929e-02,  ..., -5.8053e-02,\n",
       "                       -6.7920e-02, -3.1544e-02]])),\n",
       "             ('gnn.inverW_P.bias',\n",
       "              tensor([-0.1079,  0.0271, -0.0525, -0.0501,  0.1035,  0.0651, -0.0278,  0.0166,\n",
       "                       0.0353, -0.0596,  0.0658, -0.0409,  0.0133, -0.0272,  0.0830,  0.0385,\n",
       "                      -0.0956,  0.0887,  0.0844, -0.0931,  0.0363,  0.0663, -0.0679,  0.0687,\n",
       "                       0.0920, -0.0603,  0.0639,  0.0197,  0.0883, -0.0607,  0.0263,  0.0498,\n",
       "                      -0.0425, -0.0195,  0.0371,  0.0225,  0.0549, -0.0989,  0.0160,  0.0828,\n",
       "                      -0.1219,  0.0553, -0.0298, -0.0245,  0.0424, -0.0586,  0.0746,  0.0790,\n",
       "                       0.0394,  0.0260,  0.0499, -0.0644, -0.0379,  0.0992,  0.0416,  0.0518,\n",
       "                      -0.0280,  0.0175, -0.1091,  0.0494, -0.1001, -0.0700, -0.0488, -0.0268,\n",
       "                       0.0175, -0.0251,  0.0503,  0.0350,  0.0436,  0.0008, -0.0887,  0.0838,\n",
       "                      -0.0804, -0.0301, -0.0782, -0.0803,  0.0079, -0.0719, -0.0189,  0.0776,\n",
       "                      -0.0497,  0.0999, -0.1057, -0.0182,  0.0137, -0.0360,  0.0490, -0.0326,\n",
       "                      -0.0394, -0.0918,  0.0907, -0.0205, -0.0093,  0.0317,  0.0742, -0.0601,\n",
       "                       0.0900,  0.0218, -0.0549, -0.0245,  0.0393,  0.0002,  0.0002, -0.0298,\n",
       "                       0.0096, -0.0078, -0.0736, -0.1073, -0.0014, -0.1011,  0.0966,  0.0738,\n",
       "                      -0.0875,  0.0360, -0.0618, -0.0241, -0.0349,  0.0946,  0.0980,  0.0398,\n",
       "                       0.0665, -0.0924,  0.0885, -0.0263,  0.0869, -0.0260,  0.0221, -0.0743])),\n",
       "             ('transformer.resblocks.0.attn.in_proj_weight',\n",
       "              tensor([[-0.0386, -0.0164,  0.0539,  ...,  0.0473, -0.0142,  0.0294],\n",
       "                      [-0.1036, -0.0155,  0.0263,  ...,  0.0997, -0.0942,  0.0818],\n",
       "                      [-0.0279, -0.0485, -0.0516,  ..., -0.0584, -0.0111, -0.0295],\n",
       "                      ...,\n",
       "                      [ 0.0253,  0.0657,  0.0075,  ...,  0.0373,  0.0687,  0.0145],\n",
       "                      [ 0.0336, -0.0426,  0.0012,  ...,  0.0046,  0.0415,  0.0268],\n",
       "                      [-0.0350, -0.1482,  0.0063,  ..., -0.0491,  0.0799, -0.0633]])),\n",
       "             ('transformer.resblocks.0.attn.in_proj_bias',\n",
       "              tensor([ 0.0040,  0.0027,  0.0017,  ..., -0.0003,  0.0015, -0.0008])),\n",
       "             ('transformer.resblocks.0.attn.out_proj.weight',\n",
       "              tensor([[ 0.0020, -0.0054,  0.0113,  ...,  0.0046,  0.0014, -0.0005],\n",
       "                      [-0.0046, -0.0104, -0.0114,  ...,  0.0081, -0.0010,  0.0026],\n",
       "                      [-0.0075, -0.0157, -0.0159,  ..., -0.0241, -0.0126,  0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0123,  0.0151,  0.0187,  ..., -0.0049,  0.0095,  0.0021],\n",
       "                      [-0.0165,  0.0040,  0.0126,  ..., -0.0096,  0.0125, -0.0073],\n",
       "                      [ 0.0067,  0.0091, -0.0133,  ...,  0.0202,  0.0022, -0.0105]])),\n",
       "             ('transformer.resblocks.0.attn.out_proj.bias',\n",
       "              tensor([ 8.9692e-04,  1.0220e-03, -2.0629e-04,  1.6931e-03,  8.9451e-04,\n",
       "                      -4.8473e-04,  5.6180e-04,  6.8366e-04, -7.0256e-04, -3.6743e-04,\n",
       "                      -7.4765e-04, -7.3229e-04, -4.4554e-04,  5.7694e-04,  9.9435e-04,\n",
       "                       4.0542e-04,  1.9019e-04,  6.2458e-04,  1.7963e-03, -4.4144e-04,\n",
       "                       7.2305e-04,  1.6039e-03,  1.1330e-03, -1.9102e-04,  2.2421e-03,\n",
       "                       8.5972e-04,  2.5603e-04, -7.0506e-04, -4.1549e-04,  1.0301e-03,\n",
       "                      -4.5542e-04, -9.2871e-04,  1.4805e-04, -1.7179e-03,  5.0727e-04,\n",
       "                       1.6532e-03,  5.7939e-04,  9.4943e-04, -5.2340e-04, -4.6331e-04,\n",
       "                       1.7569e-04, -2.7131e-04, -7.6733e-04, -3.0273e-04, -8.1773e-04,\n",
       "                       1.5065e-03,  9.7191e-05,  2.8187e-04,  1.0993e-04,  8.8506e-05,\n",
       "                      -7.5200e-05, -2.9801e-05,  1.5703e-03,  2.5936e-05,  5.7550e-04,\n",
       "                       7.3876e-04,  4.2168e-04,  8.5444e-04, -3.8489e-04,  2.7007e-04,\n",
       "                       3.2627e-04, -2.5476e-05, -4.5499e-04, -9.0727e-04, -1.2014e-03,\n",
       "                      -2.2536e-04, -4.5173e-04,  1.0406e-03, -9.6512e-04,  1.9338e-05,\n",
       "                      -1.1766e-03, -5.6536e-04, -1.4194e-03,  8.6014e-05, -3.5373e-04,\n",
       "                       3.4790e-04,  1.2651e-04, -4.5867e-04, -1.3185e-03,  6.7173e-04,\n",
       "                      -1.3936e-03, -1.1014e-03,  9.8046e-04,  5.0388e-04,  2.3145e-05,\n",
       "                      -3.2738e-04, -6.5935e-04,  2.2038e-04,  4.2638e-04, -3.6696e-04,\n",
       "                      -1.1963e-03,  2.0712e-03,  2.3618e-05,  6.7208e-04, -4.6925e-04,\n",
       "                       8.8791e-04, -7.3709e-04,  3.0648e-04, -4.8083e-06,  3.3078e-04,\n",
       "                      -1.1549e-03,  3.4765e-05, -6.1867e-04, -7.6668e-06,  6.9569e-04,\n",
       "                      -1.6938e-03, -2.6221e-04, -3.8009e-04,  2.8414e-04,  1.4151e-03,\n",
       "                       8.8072e-05,  5.6230e-05,  8.5553e-04,  1.1832e-05,  7.4261e-04,\n",
       "                       6.0962e-04, -4.5861e-05,  7.8123e-04,  2.8068e-04,  7.7428e-04,\n",
       "                       6.5349e-05,  1.1633e-03, -3.2152e-04,  2.9833e-04, -7.2344e-05,\n",
       "                       1.7998e-05, -5.4800e-04,  5.8205e-05, -8.3471e-04, -2.9114e-04,\n",
       "                       3.5986e-04, -9.6127e-04, -2.5775e-04,  2.0157e-04, -6.2095e-04,\n",
       "                       2.9256e-04, -1.9428e-03,  2.0363e-04,  4.7808e-05,  7.1667e-04,\n",
       "                       3.0340e-04, -8.5887e-04,  5.0873e-04,  5.3221e-04, -1.2960e-03,\n",
       "                      -7.9078e-05,  3.6400e-04, -2.4539e-03, -5.8637e-04, -5.0800e-04,\n",
       "                       1.2770e-03,  1.8153e-04,  1.0105e-03,  9.7379e-04,  2.0904e-04,\n",
       "                      -6.4616e-04, -7.9483e-04, -1.3308e-03, -8.3434e-04,  1.4739e-03,\n",
       "                      -3.3451e-04,  2.1672e-04,  1.4554e-03,  4.3463e-04,  2.8649e-04,\n",
       "                       5.5627e-04, -5.6611e-04,  1.2173e-03, -9.1645e-04,  2.7396e-04,\n",
       "                      -1.1560e-03, -1.4724e-03,  6.3512e-04,  2.0867e-03,  3.0078e-04,\n",
       "                      -7.8669e-04, -9.7139e-04, -9.6531e-05,  9.7628e-04, -4.5166e-04,\n",
       "                       3.9560e-04,  1.1330e-04,  7.8455e-04, -9.3623e-04, -4.6543e-04,\n",
       "                      -7.5056e-04, -1.8352e-04,  3.7556e-04,  6.6008e-04,  1.6193e-03,\n",
       "                       1.6577e-03,  2.1819e-04,  1.3395e-04, -6.9964e-04,  3.1236e-04,\n",
       "                       5.8553e-04,  6.9310e-05, -1.0884e-03, -5.2378e-04, -1.2921e-03,\n",
       "                      -1.3960e-04, -3.7702e-04, -1.0043e-04,  3.2043e-05, -1.1581e-04,\n",
       "                       4.9428e-04,  6.1434e-04, -1.3726e-03, -7.0124e-04, -1.7059e-03,\n",
       "                       5.1525e-04,  2.9169e-04,  7.3414e-04,  9.8541e-04,  1.1007e-04,\n",
       "                      -9.6796e-04,  2.6266e-04, -4.4513e-04, -7.2059e-04,  2.9118e-04,\n",
       "                       1.3963e-04,  2.3316e-04, -7.6686e-04,  7.2668e-04,  9.6914e-04,\n",
       "                       9.9880e-04, -6.6108e-04,  1.3415e-03, -3.0407e-04, -5.6631e-04,\n",
       "                      -3.9831e-05, -2.2778e-04,  6.1089e-04,  1.2600e-05,  5.2760e-04,\n",
       "                      -9.3144e-04,  1.2101e-03, -1.0319e-03,  7.4840e-04,  1.5777e-04,\n",
       "                       3.0675e-04,  6.1750e-04, -1.3482e-03, -3.5096e-04,  1.7628e-03,\n",
       "                      -1.1044e-03,  4.8034e-04,  1.2709e-04, -1.5143e-03, -2.0024e-03,\n",
       "                       1.9095e-03, -2.7955e-04, -2.8318e-04,  2.9668e-06, -6.8586e-04,\n",
       "                       2.2817e-05,  4.3179e-04, -8.5359e-05,  1.6266e-03, -4.5430e-04,\n",
       "                      -4.9097e-04, -6.2658e-04,  6.1421e-04,  5.5195e-04, -2.8340e-04,\n",
       "                      -1.9695e-04,  2.8017e-04,  9.0482e-05, -1.1321e-03, -9.0262e-04,\n",
       "                       5.7476e-04,  8.5642e-05,  8.9081e-04,  3.7390e-04, -7.6151e-04,\n",
       "                       3.8985e-04, -1.4566e-04, -1.1038e-03,  1.1649e-04,  1.6464e-04,\n",
       "                      -1.2837e-04,  9.5553e-04,  1.5119e-03, -1.2641e-03,  5.7717e-04,\n",
       "                       3.0475e-04, -4.4462e-04,  1.1369e-04,  4.0737e-04, -2.0025e-03,\n",
       "                      -1.3861e-03, -4.9279e-04, -7.4467e-04,  1.0534e-03,  2.3929e-04,\n",
       "                       3.6871e-04, -3.9376e-04,  9.7215e-04,  9.3228e-04,  3.9301e-04,\n",
       "                      -7.7462e-04, -9.0844e-04, -1.8968e-04,  2.0062e-03,  4.3904e-04,\n",
       "                      -7.0972e-05,  4.2055e-04,  3.1657e-04, -1.0156e-06, -5.9397e-04,\n",
       "                       5.2881e-04, -2.7621e-04, -3.7214e-06,  1.2336e-03,  1.0860e-04,\n",
       "                      -1.0385e-04, -3.6195e-04, -2.7200e-04,  5.6270e-04, -1.0502e-03,\n",
       "                      -6.3616e-04,  5.0637e-04, -9.7316e-04, -5.0709e-04,  1.3640e-04,\n",
       "                      -5.6896e-04,  6.1610e-04,  3.6764e-04,  3.4348e-04, -7.7506e-04,\n",
       "                      -6.3178e-04, -5.0729e-04, -9.3212e-04, -5.0660e-04, -3.2749e-04,\n",
       "                       5.8207e-04, -1.0015e-03,  6.4061e-04, -4.7054e-04, -3.4890e-05,\n",
       "                      -1.3166e-03,  4.6117e-04,  2.4520e-04,  9.1687e-04,  5.1324e-05,\n",
       "                      -1.1110e-03, -1.3344e-03,  2.0561e-04, -2.8262e-04,  9.8221e-04,\n",
       "                      -9.5720e-04,  3.5476e-04, -4.2815e-04,  8.5939e-04, -1.6153e-04,\n",
       "                       7.6452e-04, -4.2771e-04, -1.8201e-03,  4.8649e-04, -6.2209e-04,\n",
       "                       5.8773e-04,  9.9585e-04, -5.1314e-05,  2.5152e-04,  9.8854e-04,\n",
       "                       4.7113e-04, -8.9438e-04, -5.1264e-04, -9.8020e-04, -3.2317e-04,\n",
       "                      -3.7521e-04, -3.9020e-05,  8.0912e-04,  7.5333e-04, -5.4069e-04,\n",
       "                       1.9214e-04,  6.4748e-04, -1.5575e-03, -4.0697e-04, -7.8428e-04,\n",
       "                       3.5700e-04,  4.6012e-04, -5.3387e-04,  2.7970e-04,  2.2902e-04,\n",
       "                      -1.0648e-04,  9.0122e-04, -8.2274e-04, -9.8687e-04, -2.6759e-04,\n",
       "                      -1.5707e-04, -8.3946e-04, -6.0048e-05,  1.0207e-03, -5.4866e-05,\n",
       "                       8.5741e-04,  2.9920e-03, -3.2679e-04,  1.4097e-04,  2.5292e-04,\n",
       "                      -4.7795e-04,  3.0938e-04,  6.6889e-04, -8.0228e-04, -4.9483e-05,\n",
       "                      -2.4723e-04,  9.9546e-04, -1.5574e-04, -8.8235e-04, -2.0734e-04,\n",
       "                       7.9217e-04,  6.7424e-04, -2.4781e-05, -5.5328e-04, -8.6280e-05,\n",
       "                      -3.9576e-05, -5.3599e-05, -1.4427e-03,  4.6976e-04,  4.7721e-04,\n",
       "                       1.3103e-03, -1.7787e-03, -1.3363e-04, -7.9303e-04,  4.0881e-04,\n",
       "                       2.1592e-03, -1.9176e-03,  6.1342e-05, -6.6012e-04, -1.1078e-03,\n",
       "                      -1.7160e-05,  1.1319e-04,  2.8143e-04,  9.6312e-04, -2.5835e-05,\n",
       "                       4.6238e-05, -3.6271e-04, -5.8414e-05, -9.0819e-04,  5.5315e-04,\n",
       "                       8.2721e-05, -1.7111e-03, -1.7543e-03,  6.1506e-04,  6.6007e-04,\n",
       "                       1.2193e-03, -5.4004e-04,  4.4432e-04,  1.0531e-03, -5.6289e-04,\n",
       "                       9.6702e-04,  5.0796e-04, -1.8214e-03, -3.0902e-05,  2.9076e-04,\n",
       "                       2.3231e-04, -2.8966e-04,  3.1071e-04,  8.2733e-04, -1.2972e-03,\n",
       "                       2.4762e-04, -6.6570e-04,  5.2558e-04, -9.0817e-04,  1.5767e-03,\n",
       "                      -9.8624e-04, -6.8600e-04, -5.7675e-04,  2.0363e-05, -6.8184e-05,\n",
       "                      -1.3518e-03,  3.0835e-04, -7.3340e-04,  2.0453e-04,  7.8527e-04,\n",
       "                      -1.4559e-03,  1.1291e-03, -1.0113e-03,  1.2377e-03,  3.6430e-04,\n",
       "                      -6.6615e-04,  9.0753e-04, -4.1210e-04,  1.9458e-04, -1.2175e-03,\n",
       "                      -1.0939e-03,  6.6910e-05,  8.6588e-04, -1.7949e-03,  4.8105e-04,\n",
       "                      -4.4849e-04, -1.2561e-03, -5.4309e-05, -4.8665e-04,  1.1672e-04,\n",
       "                       3.0257e-04, -1.0169e-05, -9.1882e-05, -1.2537e-03, -6.7055e-05,\n",
       "                       1.3950e-04,  2.6962e-04,  4.0142e-04,  6.3397e-04,  6.3286e-04,\n",
       "                       1.3265e-03,  2.0296e-04,  4.3784e-04,  1.3011e-04, -2.4116e-04,\n",
       "                      -2.4917e-04, -3.7785e-04])),\n",
       "             ('transformer.resblocks.0.ln_1.weight',\n",
       "              tensor([0.9995, 1.0022, 1.0002, 0.9986, 1.0021, 0.9989, 0.9986, 1.0004, 1.0009,\n",
       "                      0.9940, 0.9996, 0.9994, 1.0000, 1.0024, 0.9969, 0.9965, 0.9979, 0.9983,\n",
       "                      1.0003, 0.9995, 0.9982, 1.0002, 0.9964, 0.9991, 1.0008, 0.9999, 1.0042,\n",
       "                      0.9987, 1.0008, 0.9991, 0.9994, 0.9986, 0.9920, 0.9992, 0.9978, 0.9962,\n",
       "                      0.9992, 0.9984, 0.9969, 1.0009, 0.9991, 1.0005, 0.9972, 1.0013, 1.0002,\n",
       "                      0.9969, 0.9969, 1.0021, 0.9977, 0.9978, 0.9987, 0.9990, 0.9972, 1.0025,\n",
       "                      0.9989, 0.9979, 0.9983, 1.0006, 1.0001, 1.0002, 0.9994, 0.9989, 1.0016,\n",
       "                      0.9990, 0.9961, 0.9991, 1.0012, 0.9991, 0.9992, 1.0022, 1.0011, 0.9988,\n",
       "                      1.0014, 0.9959, 1.0002, 0.9994, 0.9999, 1.0008, 1.0009, 0.9957, 0.9990,\n",
       "                      1.0010, 1.0001, 1.0010, 0.9977, 1.0002, 1.0019, 0.9990, 1.0000, 1.0007,\n",
       "                      1.0030, 0.9983, 1.0005, 1.0025, 0.9986, 1.0015, 0.9950, 1.0016, 0.9994,\n",
       "                      0.9995, 1.0002, 0.9998, 0.9980, 0.9971, 0.9985, 0.9950, 1.0029, 0.9965,\n",
       "                      1.0036, 0.9983, 0.9995, 1.0022, 1.0021, 0.9981, 1.0019, 1.0018, 1.0032,\n",
       "                      0.9997, 1.0003, 0.9985, 0.9972, 1.0024, 0.9979, 0.9984, 0.9997, 1.0026,\n",
       "                      0.9982, 1.0010, 0.9968, 1.0001, 0.9980, 1.0002, 0.9973, 0.9973, 1.0011,\n",
       "                      0.9972, 0.9994, 0.9974, 0.9998, 0.9984, 0.9977, 1.0027, 0.9963, 1.0000,\n",
       "                      0.9975, 0.9986, 1.0006, 0.9957, 1.0001, 1.0020, 1.0055, 0.9982, 0.9969,\n",
       "                      0.9972, 0.9982, 1.0010, 1.0013, 0.9981, 1.0008, 0.9983, 1.0002, 1.0005,\n",
       "                      1.0005, 0.9997, 1.0014, 1.0008, 0.9955, 1.0039, 0.9977, 0.9981, 1.0017,\n",
       "                      0.9993, 1.0025, 1.0019, 1.0006, 0.9973, 1.0033, 1.0007, 1.0012, 1.0021,\n",
       "                      0.9990, 1.0015, 1.0014, 0.9990, 1.0029, 0.9974, 1.0029, 1.0002, 1.0020,\n",
       "                      0.9981, 0.9986, 1.0004, 0.9942, 0.9971, 1.0026, 0.9999, 0.9975, 0.9993,\n",
       "                      0.9998, 0.9950, 1.0017, 0.9977, 1.0008, 0.9977, 1.0047, 1.0018, 0.9968,\n",
       "                      0.9958, 1.0008, 0.9981, 1.0019, 1.0008, 0.9938, 0.9990, 0.9986, 0.9960,\n",
       "                      0.9989, 0.9976, 0.9966, 1.0034, 0.9975, 1.0012, 1.0003, 1.0020, 0.9988,\n",
       "                      0.9992, 0.9975, 1.0052, 0.9966, 0.9990, 0.9988, 0.9997, 0.9991, 0.9960,\n",
       "                      1.0006, 0.9988, 0.9991, 0.9960, 1.0022, 0.9944, 1.0012, 0.9997, 0.9981,\n",
       "                      1.0010, 0.9973, 0.9993, 0.9998, 1.0038, 0.9965, 0.9997, 0.9980, 1.0036,\n",
       "                      0.9970, 0.9997, 0.9972, 0.9998, 0.9918, 1.0003, 1.0001, 0.9988, 1.0009,\n",
       "                      0.9969, 0.9999, 0.9992, 0.9974, 0.9983, 0.9954, 1.0016, 0.9978, 0.9981,\n",
       "                      1.0039, 1.0003, 0.9947, 0.9966, 1.0011, 0.9997, 1.0012, 1.0006, 0.9998,\n",
       "                      1.0025, 0.9956, 0.9985, 0.9993, 1.0012, 0.9984, 0.9974, 0.9979, 1.0005,\n",
       "                      1.0010, 0.9996, 1.0022, 0.9981, 1.0022, 1.0007, 1.0009, 1.0003, 0.9986,\n",
       "                      0.9962, 0.9985, 0.9984, 1.0022, 0.9978, 0.9980, 0.9995, 1.0009, 1.0005,\n",
       "                      1.0012, 1.0032, 1.0000, 1.0012, 0.9997, 1.0025, 1.0033, 1.0009, 0.9968,\n",
       "                      0.9990, 1.0013, 1.0002, 0.9977, 1.0001, 0.9988, 1.0010, 0.9969, 0.9957,\n",
       "                      0.9979, 1.0045, 0.9981, 0.9981, 1.0001, 0.9982, 1.0017, 1.0025, 1.0027,\n",
       "                      1.0014, 0.9999, 1.0022, 0.9991, 1.0006, 0.9972, 0.9996, 0.9980, 0.9997,\n",
       "                      1.0015, 1.0024, 0.9999, 0.9988, 1.0061, 0.9974, 1.0002, 0.9972, 0.9989,\n",
       "                      1.0005, 0.9973, 1.0007, 0.9995, 0.9971, 1.0047, 1.0017, 0.9983, 1.0042,\n",
       "                      0.9959, 0.9992, 0.9979, 0.9990, 0.9984, 0.9971, 1.0006, 0.9993, 1.0018,\n",
       "                      1.0007, 0.9991, 1.0011, 0.9998, 1.0005, 0.9978, 1.0028, 1.0025, 0.9992,\n",
       "                      0.9999, 1.0020, 0.9983, 0.9999, 0.9956, 1.0028, 1.0003, 1.0036, 0.9989,\n",
       "                      0.9989, 0.9989, 0.9987, 0.9983, 0.9992, 0.9993, 0.9958, 0.9961, 1.0009,\n",
       "                      0.9941, 1.0000, 0.9978, 0.9989, 0.9969, 0.9991, 0.9973, 0.9920, 0.9958,\n",
       "                      1.0027, 1.0005, 0.9972, 0.9977, 1.0022, 1.0002, 1.0002, 0.9970, 0.9975,\n",
       "                      0.9974, 1.0011, 0.9959, 0.9969, 0.9976, 0.9993, 0.9998, 0.9991, 0.9965,\n",
       "                      0.9961, 1.0001, 1.0001, 1.0005, 0.9986, 0.9992, 0.9970, 0.9998, 1.0020,\n",
       "                      0.9987, 0.9982, 0.9974, 0.9958, 0.9987, 0.9969, 0.9980, 1.0066, 0.9976,\n",
       "                      0.9991, 0.9979, 1.0031, 1.0023, 0.9989, 0.9960, 1.0021, 1.0019, 1.0012,\n",
       "                      1.0007, 0.9995, 1.0014, 0.9998, 0.9988, 1.0013, 0.9993, 1.0011, 1.0030,\n",
       "                      0.9999, 1.0021, 1.0012, 0.9965, 1.0025, 0.9968, 0.9988, 0.9999, 1.0013,\n",
       "                      1.0016, 0.9977, 1.0025, 1.0004, 1.0043, 1.0013, 0.9985, 0.9995, 0.9964,\n",
       "                      0.9969, 0.9993, 1.0000, 0.9983, 1.0025, 0.9992, 0.9984, 1.0005, 0.9990,\n",
       "                      0.9993, 1.0021, 0.9964, 1.0030, 0.9967, 1.0005, 1.0014, 0.9943, 1.0012,\n",
       "                      0.9994, 1.0001, 0.9984, 0.9987, 1.0006, 0.9957, 1.0015, 0.9985, 1.0028,\n",
       "                      0.9980, 0.9976, 1.0009, 1.0008, 0.9950, 0.9992, 0.9967, 0.9962])),\n",
       "             ('transformer.resblocks.0.ln_1.bias',\n",
       "              tensor([-5.4751e-04, -6.3205e-04,  2.4731e-04, -2.5029e-04,  9.3515e-04,\n",
       "                      -1.4771e-04,  7.9424e-04, -2.7111e-04, -7.5258e-04,  1.8667e-03,\n",
       "                       5.8203e-04,  1.1020e-03,  3.1211e-04, -4.2563e-04, -2.8050e-03,\n",
       "                       1.8054e-03,  4.9505e-04, -3.2166e-04, -1.4422e-03, -2.3268e-04,\n",
       "                      -4.4786e-04,  5.0874e-04,  7.1012e-04, -5.4490e-04,  2.0383e-03,\n",
       "                       1.1511e-03,  1.6098e-03, -8.7192e-04, -1.2520e-03,  4.2318e-04,\n",
       "                       1.2740e-03, -9.1562e-04,  4.4365e-04, -1.3052e-04, -1.1739e-03,\n",
       "                       1.8795e-03, -2.2700e-04,  6.2491e-04, -2.4644e-04, -2.8358e-04,\n",
       "                       4.2929e-04, -1.7085e-03,  1.6371e-03, -1.9492e-04,  2.1556e-03,\n",
       "                      -9.7771e-04,  1.1916e-03, -2.9786e-04, -5.9186e-04,  2.0103e-03,\n",
       "                       6.0074e-04,  3.3451e-04,  3.1776e-04, -6.4521e-04, -4.4765e-05,\n",
       "                      -6.8813e-04,  4.2328e-04, -1.2276e-04, -1.0339e-03,  7.5170e-05,\n",
       "                      -1.8452e-04, -2.0868e-04,  1.5871e-03,  7.0354e-04,  1.6007e-03,\n",
       "                      -1.4893e-04,  5.1100e-04,  8.4957e-04,  2.9422e-04, -1.9823e-03,\n",
       "                       2.9141e-04,  9.2138e-04,  2.3704e-04,  1.2817e-04, -4.9750e-04,\n",
       "                      -4.8842e-04,  5.4538e-04, -3.9898e-04, -9.4801e-04, -6.3521e-04,\n",
       "                       2.3519e-04,  5.5830e-04, -7.5273e-04, -5.9472e-04,  1.0329e-03,\n",
       "                       1.1925e-04, -6.8955e-05, -6.3308e-04,  7.7143e-04, -2.9183e-06,\n",
       "                       1.1040e-03, -1.6041e-04, -9.0543e-04, -1.5607e-04, -3.3894e-04,\n",
       "                      -4.2831e-04, -1.6782e-03,  8.8799e-04, -8.1726e-04, -7.2051e-06,\n",
       "                       5.1052e-04, -2.1384e-04,  1.5802e-04, -2.2532e-04, -4.7976e-04,\n",
       "                       9.1574e-04, -3.7162e-05, -1.1764e-03,  1.4123e-03, -7.0820e-04,\n",
       "                      -1.4037e-03,  1.2010e-04, -2.8382e-04,  5.2317e-04,  1.4617e-04,\n",
       "                      -8.2410e-04,  5.8531e-04, -6.7709e-04,  1.2631e-03, -1.5452e-03,\n",
       "                      -4.2793e-04,  7.5399e-04, -3.8328e-04, -1.2614e-03,  5.2640e-04,\n",
       "                      -1.0699e-03, -2.9101e-04,  9.7183e-04, -3.5844e-05, -1.6933e-03,\n",
       "                      -1.5612e-04,  6.6371e-04, -6.4428e-04,  3.1999e-03, -4.7080e-04,\n",
       "                      -1.1538e-03, -9.9621e-04, -1.4167e-03,  8.1094e-04, -1.3568e-04,\n",
       "                       2.1201e-03, -5.0259e-04, -4.8985e-04, -3.9589e-04,  1.6822e-03,\n",
       "                       1.4761e-04,  7.4229e-04,  5.8008e-04,  6.9285e-04, -5.9647e-04,\n",
       "                      -6.6239e-04, -8.9916e-04,  8.3800e-04, -6.8753e-04, -9.5630e-04,\n",
       "                       1.9257e-04,  1.1618e-03, -7.1904e-04, -8.1378e-05, -2.1677e-04,\n",
       "                       1.5376e-04,  2.0885e-03, -1.5145e-03,  4.9579e-04,  4.6564e-04,\n",
       "                      -4.6662e-04,  1.3407e-03, -3.4093e-04, -4.9467e-04, -2.0867e-03,\n",
       "                      -7.2311e-04, -1.2607e-05, -2.3145e-03,  3.4383e-04,  6.4175e-04,\n",
       "                      -8.4211e-04, -2.1518e-04, -9.5534e-05, -1.0080e-03, -1.3168e-03,\n",
       "                      -1.7182e-03, -9.9167e-04,  1.3056e-03,  8.3837e-04,  2.7253e-04,\n",
       "                       9.4831e-04, -1.1955e-03,  1.0804e-03, -9.1070e-04,  1.2740e-03,\n",
       "                       6.0381e-04,  1.0610e-03, -7.1478e-04, -2.0287e-03, -8.1867e-04,\n",
       "                       5.0587e-05, -8.8001e-04, -6.7607e-04,  6.7934e-05, -1.9803e-03,\n",
       "                      -1.2457e-03,  8.3991e-05,  1.3584e-03,  6.7928e-04,  3.6176e-04,\n",
       "                      -1.3673e-03, -4.2773e-04,  1.2768e-03, -6.7806e-04,  6.6743e-04,\n",
       "                      -1.8548e-03,  5.5036e-05, -1.2901e-03,  3.0876e-04,  1.6158e-04,\n",
       "                       7.4943e-04,  7.8373e-04, -4.4758e-05,  1.2602e-03, -1.4129e-04,\n",
       "                       1.3060e-03,  1.2375e-03, -8.1887e-04, -6.4267e-04,  9.2448e-04,\n",
       "                       1.6900e-03, -1.9577e-04,  2.4147e-04,  1.4096e-03, -7.4620e-04,\n",
       "                      -5.0680e-05,  5.8029e-04,  1.5810e-04, -1.6483e-03, -6.9959e-04,\n",
       "                       4.7785e-04, -9.0646e-04, -4.2626e-04,  9.9619e-04, -1.6812e-03,\n",
       "                      -1.5177e-03, -2.3788e-04,  3.5252e-04,  4.2822e-05,  3.7543e-05,\n",
       "                       1.1176e-03,  1.1316e-03, -7.5654e-04, -1.8871e-03, -1.0461e-03,\n",
       "                      -8.0501e-04, -5.1176e-04, -1.4165e-03,  9.8751e-05,  1.6705e-03,\n",
       "                      -7.6119e-04,  1.2396e-03,  1.0924e-03,  1.3067e-04,  1.1660e-03,\n",
       "                       4.9933e-05, -9.9141e-05,  1.6192e-03,  2.4641e-04,  1.4763e-03,\n",
       "                       2.0062e-04, -1.0280e-03,  1.1334e-03,  1.8847e-03,  1.7101e-04,\n",
       "                       3.4573e-04, -6.4573e-04, -9.2285e-04,  1.2433e-03,  1.2647e-03,\n",
       "                       1.2370e-04,  9.4361e-05, -2.5353e-04,  2.0876e-03,  3.1602e-04,\n",
       "                       1.4285e-03,  1.3261e-03, -1.5643e-03, -3.4570e-04, -2.3742e-05,\n",
       "                       5.8343e-04,  9.9129e-05,  1.6019e-03, -2.8607e-04, -9.4064e-04,\n",
       "                      -1.7512e-03, -6.3208e-04,  1.4633e-03,  1.6213e-04, -8.3521e-04,\n",
       "                       1.0014e-03,  2.7541e-03,  1.2333e-03, -2.9373e-03, -2.0969e-05,\n",
       "                       9.2416e-04,  2.2858e-04, -3.2426e-05,  8.7978e-04, -1.0958e-03,\n",
       "                      -1.9178e-04,  5.0769e-04, -1.1216e-04, -4.5908e-04, -1.2439e-03,\n",
       "                       8.9423e-04, -8.0550e-05,  6.0153e-04,  7.2287e-04, -1.9168e-03,\n",
       "                       6.3732e-04,  3.6415e-05,  1.8133e-03, -1.1423e-03,  1.4897e-03,\n",
       "                       1.0922e-03, -4.7427e-05,  7.2533e-04, -1.3933e-03,  6.0719e-04,\n",
       "                      -1.2789e-03, -1.6752e-03,  1.2111e-03, -2.7376e-03,  1.6756e-03,\n",
       "                      -8.4430e-04, -1.2605e-04,  4.4555e-04, -6.0973e-04,  4.7874e-04,\n",
       "                      -8.5913e-04, -1.1758e-03, -1.4299e-04,  2.6330e-04,  1.6569e-04,\n",
       "                       4.8088e-04, -1.8481e-04, -6.4562e-04, -6.1054e-04,  5.3066e-04,\n",
       "                       1.4467e-03, -1.5239e-03, -1.2992e-04, -1.0153e-03,  1.2774e-03,\n",
       "                      -1.7388e-04, -3.2916e-05, -1.4547e-03, -4.9248e-04,  6.6070e-04,\n",
       "                       5.7510e-05,  3.7327e-04, -1.0139e-03,  5.7490e-04, -8.7747e-04,\n",
       "                       1.1639e-03,  1.1691e-04,  1.7719e-03, -9.5812e-04, -5.3698e-04,\n",
       "                      -9.1345e-04, -7.4393e-04, -9.6320e-04, -1.8803e-05, -6.9145e-04,\n",
       "                       8.6969e-04, -2.3226e-04, -4.2062e-04,  4.0347e-04, -1.9000e-03,\n",
       "                      -7.5886e-04,  8.6211e-05,  3.4179e-04,  1.9096e-04,  1.3308e-03,\n",
       "                       1.8810e-04, -8.4474e-04,  2.6152e-03,  7.5242e-04, -1.3709e-03,\n",
       "                       1.1048e-03,  3.4841e-04, -2.1902e-04, -6.0064e-04,  3.1104e-04,\n",
       "                      -5.6785e-04,  7.3023e-05, -4.2189e-04, -2.2720e-04,  5.0906e-04,\n",
       "                       3.2464e-04, -9.6423e-04,  3.8380e-04,  3.1409e-04,  2.5591e-04,\n",
       "                      -4.2938e-04, -9.9443e-04, -4.3689e-04, -5.0597e-05, -5.5932e-04,\n",
       "                      -5.8075e-04, -6.5576e-04,  1.9603e-04, -7.0269e-04,  2.2896e-04,\n",
       "                      -3.9596e-04, -1.5449e-03,  8.5928e-04, -8.8209e-04, -8.3056e-04,\n",
       "                      -2.5425e-04, -4.8838e-04,  7.4449e-04,  5.4131e-04, -1.2240e-05,\n",
       "                      -4.3031e-04, -1.4784e-03, -4.9087e-04, -1.1400e-03,  2.6109e-04,\n",
       "                       5.7867e-04, -1.4400e-03,  1.1436e-03,  1.7917e-05, -5.3836e-04,\n",
       "                       7.1042e-04, -4.0966e-05, -8.9320e-04, -1.2206e-05,  4.5473e-04,\n",
       "                       4.0509e-05,  9.3379e-04,  5.7039e-04, -2.4544e-04, -1.1600e-03,\n",
       "                      -2.6492e-03,  3.8176e-04,  9.9591e-04, -1.2813e-03,  1.4187e-04,\n",
       "                       4.7575e-04, -8.9878e-04,  5.8561e-04, -8.1034e-05,  9.2909e-05,\n",
       "                       1.9626e-04, -4.3689e-04, -5.8935e-05,  3.3462e-04,  1.0309e-03,\n",
       "                      -5.4731e-04,  2.5183e-04,  1.0651e-03, -1.4338e-03, -4.2308e-05,\n",
       "                      -3.2292e-04,  7.8388e-04,  2.4041e-05, -2.3490e-04, -7.3837e-04,\n",
       "                       1.2757e-03,  9.4370e-04,  5.8075e-05, -3.9978e-04, -1.0236e-04,\n",
       "                      -7.9390e-04, -8.8893e-04,  3.3053e-04, -1.2809e-03,  1.7651e-03,\n",
       "                       6.9574e-05,  1.2843e-03,  1.1798e-03,  3.9029e-04, -5.2935e-04,\n",
       "                      -1.1238e-03,  3.6029e-05, -7.4881e-04, -3.4884e-04, -6.8033e-04,\n",
       "                       8.0448e-04, -3.8789e-04,  1.3864e-04, -5.9311e-04, -1.1513e-04,\n",
       "                       1.1144e-03,  4.8031e-04,  5.5154e-04,  3.5446e-04, -5.5865e-04,\n",
       "                       3.2409e-04,  6.0854e-06,  9.7965e-04, -1.4917e-03, -8.3525e-04,\n",
       "                      -7.4206e-06, -2.6484e-04,  1.4139e-03,  1.1752e-03, -1.8416e-03,\n",
       "                      -5.6810e-04, -2.2367e-04, -5.9258e-04,  1.6745e-03, -1.4960e-03,\n",
       "                      -1.7515e-03, -9.5662e-04])),\n",
       "             ('transformer.resblocks.0.mlp.c_fc.weight',\n",
       "              tensor([[-0.0359,  0.0661,  0.0342,  ...,  0.0385,  0.0177, -0.0454],\n",
       "                      [ 0.0107, -0.0155, -0.0448,  ...,  0.0338, -0.0019,  0.0206],\n",
       "                      [-0.0245, -0.0037,  0.0214,  ...,  0.0491,  0.0357,  0.0146],\n",
       "                      ...,\n",
       "                      [-0.0290, -0.0237,  0.0343,  ...,  0.0106, -0.0111,  0.0119],\n",
       "                      [ 0.0049,  0.0364, -0.0086,  ..., -0.0130, -0.0504,  0.0180],\n",
       "                      [-0.0190, -0.0314,  0.0071,  ...,  0.0198, -0.0189, -0.0213]])),\n",
       "             ('transformer.resblocks.0.mlp.c_fc.bias',\n",
       "              tensor([-0.0141,  0.0244, -0.0103,  ...,  0.0170,  0.0081, -0.0238])),\n",
       "             ('transformer.resblocks.0.mlp.c_proj.weight',\n",
       "              tensor([[-0.0002,  0.0121,  0.0159,  ...,  0.0038, -0.0046, -0.0163],\n",
       "                      [-0.0149,  0.0068,  0.0001,  ..., -0.0207, -0.0084, -0.0029],\n",
       "                      [ 0.0108,  0.0002,  0.0077,  ..., -0.0021, -0.0158, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0033, -0.0118,  0.0077,  ..., -0.0051,  0.0148,  0.0063],\n",
       "                      [-0.0106,  0.0052,  0.0140,  ..., -0.0012,  0.0049, -0.0104],\n",
       "                      [ 0.0013,  0.0026, -0.0051,  ...,  0.0163,  0.0134,  0.0004]])),\n",
       "             ('transformer.resblocks.0.mlp.c_proj.bias',\n",
       "              tensor([ 7.2207e-03,  7.0620e-04,  7.3272e-03, -1.9631e-02,  9.2378e-04,\n",
       "                       1.6839e-02,  1.6696e-02,  1.3302e-02,  7.3129e-03,  1.9667e-02,\n",
       "                       1.2005e-02, -3.1761e-03, -1.4670e-03, -1.0488e-03,  5.5552e-03,\n",
       "                       2.2032e-02, -2.8537e-04,  7.4668e-03, -1.4519e-02, -1.5367e-02,\n",
       "                      -1.4335e-03, -1.0426e-03, -1.3584e-02, -3.7857e-04,  5.9556e-03,\n",
       "                      -9.2862e-03, -1.6773e-02, -6.3463e-03,  2.2184e-02,  4.1932e-03,\n",
       "                      -2.1609e-02, -1.0560e-02, -8.5329e-03, -8.8175e-03,  1.0353e-02,\n",
       "                       6.0896e-03,  1.3533e-02,  1.5808e-02, -4.7661e-03,  2.0424e-02,\n",
       "                       1.3330e-02,  1.7033e-02,  1.3163e-02,  1.2422e-02, -1.3651e-02,\n",
       "                      -1.9497e-02, -6.0461e-03,  1.7404e-02,  6.4414e-03, -7.0163e-03,\n",
       "                       2.1560e-02,  1.6599e-02,  2.3045e-03, -6.9857e-03,  1.6309e-02,\n",
       "                       6.7711e-03,  8.7089e-03, -5.5778e-03,  3.3965e-03,  1.4899e-02,\n",
       "                       4.9925e-03,  1.2006e-02, -1.2858e-02,  5.1845e-04, -5.3306e-03,\n",
       "                      -5.6828e-03, -4.8649e-03, -1.5420e-02, -2.1049e-02,  1.5184e-02,\n",
       "                       5.3192e-03,  1.9760e-02, -1.4244e-02,  1.3180e-02, -9.2257e-04,\n",
       "                       1.5334e-03, -1.6974e-03,  2.1403e-02,  7.9118e-03,  2.4889e-03,\n",
       "                      -1.2258e-02, -1.9011e-02, -3.9968e-03,  3.0647e-03, -3.9830e-03,\n",
       "                       1.8063e-02, -1.3944e-02,  1.1742e-02, -2.0356e-02,  1.2612e-02,\n",
       "                      -1.8373e-02, -1.4095e-02,  1.8039e-02,  1.0695e-02,  8.1946e-03,\n",
       "                       8.1278e-03, -6.2443e-03,  6.7737e-03, -1.3897e-04, -2.0368e-02,\n",
       "                      -1.6097e-02,  1.4575e-02,  2.8098e-03, -1.1195e-03,  9.6635e-03,\n",
       "                      -3.2161e-03, -1.2853e-03,  1.4689e-02,  1.9965e-02,  1.9891e-02,\n",
       "                       2.5545e-04,  1.2330e-02, -1.8914e-02,  4.2655e-03,  1.7515e-02,\n",
       "                      -9.7167e-03, -1.1418e-02,  2.4581e-03,  1.5573e-02, -1.9548e-02,\n",
       "                      -1.0486e-02,  1.6981e-02,  1.5960e-02, -1.6400e-02,  1.2553e-02,\n",
       "                      -2.1639e-02, -2.0060e-02,  5.9889e-04,  1.9173e-02, -6.7214e-03,\n",
       "                      -5.0513e-03, -1.1987e-02,  3.5219e-03, -1.8555e-02,  1.7163e-02,\n",
       "                       1.8073e-02, -3.5548e-03, -1.8542e-02,  6.4161e-03, -1.3931e-02,\n",
       "                      -1.4134e-02, -5.5467e-03,  6.7812e-03,  5.4703e-03, -1.4593e-02,\n",
       "                       1.5974e-02, -3.5024e-03, -2.1150e-02, -1.8211e-02, -1.2226e-02,\n",
       "                      -8.6529e-03,  4.4564e-03, -1.5282e-02, -1.9344e-02, -2.1603e-02,\n",
       "                       3.2161e-03,  2.7551e-03, -8.3407e-03, -1.5545e-02,  8.1392e-03,\n",
       "                       2.0939e-02,  1.1274e-02, -7.8433e-03,  3.6509e-03, -9.5915e-03,\n",
       "                       2.3781e-03, -6.2474e-05, -5.9122e-03,  6.7563e-04,  1.0878e-02,\n",
       "                      -1.0919e-02, -8.2868e-03, -1.3629e-02,  1.9804e-02, -8.3799e-03,\n",
       "                      -1.4535e-02,  8.9886e-03,  1.2905e-02,  4.3568e-03,  2.0987e-02,\n",
       "                       1.6997e-02,  1.6835e-02,  4.5416e-03, -1.5500e-02,  2.9985e-03,\n",
       "                       1.9033e-02, -8.9178e-04,  3.9554e-03,  1.4787e-02,  9.4070e-03,\n",
       "                      -1.3238e-02,  1.8726e-03,  6.8654e-03,  6.2252e-03, -1.9372e-02,\n",
       "                      -9.1580e-03, -3.9882e-03,  1.9133e-02, -1.1506e-02,  1.5946e-02,\n",
       "                      -1.5601e-03,  3.1210e-03, -3.8600e-03,  1.6233e-02,  1.7407e-02,\n",
       "                       2.0733e-03, -2.5077e-03, -1.6050e-02,  1.5100e-02, -1.0315e-02,\n",
       "                       1.2988e-03,  9.0751e-03, -1.3956e-02,  1.7573e-02, -5.1375e-03,\n",
       "                      -1.8220e-02, -6.7211e-03,  6.3529e-03,  9.9898e-03,  9.8469e-03,\n",
       "                       6.0387e-03, -1.0132e-05, -1.1346e-02, -1.9190e-02,  1.6172e-02,\n",
       "                      -4.5991e-03,  1.7746e-02, -3.9752e-03,  2.0148e-02,  1.5940e-02,\n",
       "                      -1.4625e-02,  1.7676e-02,  3.4034e-03,  8.5695e-04, -9.8230e-03,\n",
       "                      -1.6755e-02, -1.3493e-02,  1.0931e-02, -1.9621e-03, -3.9915e-03,\n",
       "                       1.0899e-02, -2.0201e-04,  2.0950e-02, -1.2365e-02,  1.8094e-03,\n",
       "                      -8.1404e-03, -1.0113e-02,  2.9354e-03,  3.5886e-03,  1.9085e-02,\n",
       "                      -4.1159e-03, -8.2959e-03,  7.4783e-03,  1.8343e-02, -8.8452e-03,\n",
       "                      -2.0784e-02,  1.9906e-02, -1.3878e-02,  1.0271e-02, -4.2954e-03,\n",
       "                      -1.8187e-02, -8.8326e-04, -1.8707e-02, -9.7680e-03,  1.1452e-03,\n",
       "                      -5.3221e-03,  1.4428e-03,  1.3252e-02, -1.7347e-02, -1.2227e-02,\n",
       "                       1.2144e-02, -8.8869e-03,  9.9531e-03, -2.1589e-02, -1.1998e-02,\n",
       "                      -9.2310e-03, -2.7346e-03,  1.6668e-02, -6.1619e-03,  7.0895e-03,\n",
       "                      -1.1378e-02,  1.0639e-02, -6.1479e-03,  3.0756e-03, -1.2246e-02,\n",
       "                      -9.8790e-03,  1.4231e-02,  9.3705e-04, -6.4303e-03, -6.0419e-03,\n",
       "                       1.2228e-02, -1.7026e-02, -1.1299e-02,  6.5989e-03,  1.8022e-02,\n",
       "                       1.8650e-02,  4.8607e-04, -1.2267e-02,  4.1052e-03,  2.2371e-02,\n",
       "                      -8.3488e-03, -1.1308e-02,  1.1254e-02, -2.4858e-03, -1.6502e-02,\n",
       "                      -2.0400e-02,  7.3094e-03,  2.0393e-02, -1.1072e-02,  1.7949e-02,\n",
       "                      -2.1197e-02, -1.9588e-02,  1.9413e-02,  1.9341e-02,  2.1540e-02,\n",
       "                      -8.7008e-03, -1.9456e-02, -1.3362e-02,  5.8306e-03, -1.7109e-02,\n",
       "                       3.1295e-03,  2.0644e-02, -1.2378e-02,  7.3432e-03, -1.9111e-02,\n",
       "                      -7.5158e-03,  8.5229e-03, -2.0925e-02, -1.7603e-02,  8.8000e-03,\n",
       "                       5.9470e-03, -1.8201e-02,  4.5629e-03, -1.1167e-02, -1.9085e-03,\n",
       "                      -1.3983e-02,  3.9556e-03,  1.0437e-02,  1.9501e-02, -4.6114e-04,\n",
       "                      -5.4119e-03,  1.2890e-02,  2.1684e-07,  2.7218e-03,  1.3256e-02,\n",
       "                       9.5864e-03,  2.9975e-03, -1.8665e-02,  1.0767e-02, -3.5088e-03,\n",
       "                      -1.5140e-02, -1.2344e-02, -2.3043e-02, -1.6030e-02,  1.0534e-02,\n",
       "                       2.1310e-02, -1.0388e-02, -1.0218e-02,  1.7451e-03, -4.6040e-03,\n",
       "                      -3.2498e-03, -7.0064e-03, -1.6388e-02, -7.4716e-03, -1.7249e-02,\n",
       "                       1.7766e-02, -1.5467e-02,  1.6683e-02, -6.1399e-03, -1.5028e-02,\n",
       "                      -1.2503e-02, -1.0907e-02, -1.3351e-02,  1.9914e-02, -1.0891e-03,\n",
       "                       1.4321e-02,  1.5763e-02,  2.8837e-03, -3.4586e-03,  1.8424e-02,\n",
       "                       6.8106e-03,  3.9128e-03, -1.9220e-02, -1.3973e-02, -3.0981e-03,\n",
       "                      -1.8909e-02,  2.2215e-02, -2.0379e-02, -1.4219e-03, -1.0782e-02,\n",
       "                      -1.6361e-02,  2.0082e-02, -2.1159e-02,  1.4662e-02,  1.9757e-02,\n",
       "                      -1.4131e-02, -4.8634e-04, -1.2539e-02, -1.1837e-02, -5.3688e-03,\n",
       "                       1.4233e-02, -6.4317e-04,  7.5841e-03, -2.9773e-03,  1.6259e-02,\n",
       "                       1.5935e-03,  1.6137e-02, -1.5701e-02,  1.5742e-02, -7.5596e-03,\n",
       "                      -6.1406e-03,  5.7377e-03, -1.5881e-02,  7.0901e-03, -5.5137e-03,\n",
       "                      -3.8385e-03, -6.5413e-03, -7.4370e-03,  1.3644e-02,  6.2876e-03,\n",
       "                       2.1727e-02, -1.7081e-02, -1.1268e-02,  1.2542e-02, -1.1952e-02,\n",
       "                       8.1027e-03, -1.0288e-02,  1.0803e-02, -9.1592e-03,  1.3981e-03,\n",
       "                       1.3204e-02, -7.5699e-03,  1.1708e-02, -1.1636e-02,  1.9146e-02,\n",
       "                       2.1737e-02, -1.3548e-02,  1.7269e-02, -6.0336e-03,  1.8076e-02,\n",
       "                       2.0619e-02, -1.8411e-02, -2.2402e-02, -9.3753e-03,  5.0485e-03,\n",
       "                       2.1244e-02,  8.4447e-03,  1.0778e-02,  2.4914e-03,  1.8134e-03,\n",
       "                      -8.3484e-03, -1.0609e-02,  6.5596e-03, -4.9852e-03,  1.1028e-02,\n",
       "                       1.2404e-02, -1.6705e-02, -1.2807e-02,  1.9169e-02, -1.3017e-02,\n",
       "                      -4.4374e-03,  1.7131e-03, -1.5701e-03, -1.1530e-02, -1.1059e-02,\n",
       "                      -2.8568e-04,  1.3386e-02,  1.3440e-02, -1.1498e-02, -1.4431e-02,\n",
       "                      -1.0302e-03,  4.6806e-03,  1.6207e-02,  1.8796e-03,  1.1543e-02,\n",
       "                       1.2308e-02,  2.1544e-02,  4.2095e-03, -1.1749e-02, -7.3108e-03,\n",
       "                      -9.0701e-03,  2.0581e-03, -1.3348e-02,  2.0722e-02,  6.2828e-03,\n",
       "                      -1.3011e-02,  1.4758e-02,  1.9482e-02, -1.6900e-02, -2.0595e-02,\n",
       "                       3.7032e-04,  6.1473e-03,  2.6948e-03,  6.1192e-05,  1.5237e-02,\n",
       "                      -2.1110e-02,  1.2804e-02, -9.0139e-03,  1.4740e-02, -3.8695e-03,\n",
       "                       1.5604e-02,  2.9332e-04,  1.6603e-02, -4.4222e-03, -1.3833e-02,\n",
       "                       1.9074e-02,  4.6578e-03,  2.1044e-02, -1.7052e-02, -1.7813e-02,\n",
       "                       1.9240e-02,  1.4541e-02])),\n",
       "             ('transformer.resblocks.0.ln_2.weight',\n",
       "              tensor([1.0054, 0.9997, 1.0031, 0.9985, 1.0018, 1.0021, 1.0011, 1.0021, 1.0021,\n",
       "                      0.9998, 1.0031, 1.0000, 1.0022, 1.0052, 0.9997, 1.0014, 1.0021, 1.0017,\n",
       "                      1.0017, 1.0008, 1.0026, 0.9997, 1.0006, 1.0001, 1.0023, 1.0029, 1.0010,\n",
       "                      1.0006, 1.0003, 1.0033, 0.9996, 1.0002, 1.0049, 1.0011, 1.0026, 1.0012,\n",
       "                      1.0038, 1.0012, 1.0027, 1.0004, 1.0017, 1.0030, 0.9992, 1.0006, 0.9972,\n",
       "                      1.0011, 1.0026, 1.0027, 1.0032, 1.0023, 1.0025, 1.0016, 0.9984, 0.9981,\n",
       "                      1.0024, 1.0035, 1.0012, 1.0004, 1.0044, 1.0022, 1.0033, 0.9984, 1.0000,\n",
       "                      1.0044, 1.0007, 1.0033, 1.0009, 1.0006, 1.0018, 0.9992, 1.0002, 0.9992,\n",
       "                      1.0026, 1.0038, 1.0020, 1.0005, 0.9996, 1.0021, 0.9984, 1.0005, 1.0004,\n",
       "                      0.9998, 1.0054, 0.9997, 1.0005, 1.0027, 1.0000, 1.0042, 1.0039, 1.0045,\n",
       "                      1.0029, 1.0015, 1.0015, 1.0028, 1.0015, 1.0030, 1.0052, 0.9967, 0.9989,\n",
       "                      1.0023, 1.0020, 1.0018, 1.0006, 0.9995, 1.0005, 1.0028, 0.9999, 1.0017,\n",
       "                      1.0008, 1.0015, 0.9992, 1.0015, 1.0031, 1.0024, 1.0021, 1.0007, 1.0022,\n",
       "                      1.0011, 1.0020, 1.0001, 1.0010, 1.0024, 1.0022, 1.0039, 1.0049, 1.0009,\n",
       "                      1.0027, 1.0013, 1.0018, 1.0020, 1.0018, 1.0025, 0.9987, 1.0021, 1.0040,\n",
       "                      0.9985, 0.9999, 0.9992, 1.0016, 1.0001, 1.0000, 0.9977, 1.0017, 1.0034,\n",
       "                      1.0038, 1.0028, 1.0026, 1.0001, 1.0021, 1.0038, 1.0024, 1.0015, 1.0029,\n",
       "                      0.9994, 0.9990, 1.0012, 1.0004, 0.9989, 1.0014, 1.0046, 1.0042, 0.9986,\n",
       "                      1.0013, 0.9990, 1.0010, 1.0045, 1.0032, 1.0026, 1.0004, 0.9990, 1.0035,\n",
       "                      0.9997, 1.0028, 0.9966, 1.0011, 1.0026, 1.0006, 1.0044, 1.0030, 1.0031,\n",
       "                      1.0035, 1.0011, 1.0018, 1.0005, 1.0026, 1.0015, 1.0009, 1.0014, 1.0004,\n",
       "                      0.9985, 1.0029, 1.0015, 1.0042, 1.0050, 1.0024, 1.0032, 1.0025, 1.0051,\n",
       "                      1.0013, 1.0008, 1.0015, 1.0019, 1.0012, 0.9999, 1.0009, 1.0017, 1.0000,\n",
       "                      1.0017, 1.0020, 1.0002, 1.0042, 1.0027, 1.0037, 1.0034, 1.0008, 0.9995,\n",
       "                      1.0033, 1.0018, 1.0031, 1.0014, 1.0032, 1.0021, 1.0023, 1.0020, 0.9994,\n",
       "                      1.0040, 1.0021, 1.0000, 1.0003, 1.0016, 1.0051, 1.0023, 0.9987, 1.0018,\n",
       "                      1.0024, 1.0024, 1.0006, 1.0016, 1.0032, 1.0027, 1.0019, 1.0026, 1.0006,\n",
       "                      1.0022, 0.9992, 1.0026, 1.0032, 1.0009, 0.9987, 1.0012, 1.0007, 1.0016,\n",
       "                      1.0009, 1.0026, 1.0013, 0.9971, 1.0044, 0.9988, 0.9987, 1.0024, 1.0044,\n",
       "                      1.0038, 1.0021, 1.0006, 1.0010, 1.0012, 1.0003, 1.0010, 1.0019, 1.0026,\n",
       "                      1.0030, 0.9999, 1.0027, 1.0045, 1.0014, 0.9999, 1.0044, 1.0042, 1.0048,\n",
       "                      0.9991, 1.0015, 0.9999, 0.9987, 1.0032, 1.0010, 1.0018, 1.0005, 0.9999,\n",
       "                      1.0009, 1.0019, 1.0016, 1.0016, 1.0024, 1.0015, 1.0009, 1.0019, 1.0012,\n",
       "                      1.0027, 1.0032, 1.0021, 1.0019, 1.0009, 1.0017, 1.0030, 0.9980, 1.0029,\n",
       "                      1.0039, 0.9994, 1.0018, 1.0044, 0.9979, 1.0009, 0.9999, 1.0048, 1.0016,\n",
       "                      1.0034, 1.0023, 1.0030, 1.0030, 1.0041, 1.0006, 1.0024, 1.0017, 1.0005,\n",
       "                      1.0003, 1.0011, 0.9979, 1.0011, 1.0027, 1.0026, 1.0005, 0.9997, 1.0004,\n",
       "                      0.9980, 1.0035, 1.0009, 1.0028, 0.9990, 1.0026, 0.9997, 1.0021, 0.9999,\n",
       "                      1.0003, 1.0022, 0.9982, 0.9996, 0.9998, 1.0042, 1.0042, 1.0015, 1.0018,\n",
       "                      1.0012, 1.0020, 1.0048, 1.0031, 1.0016, 1.0026, 1.0018, 1.0012, 1.0033,\n",
       "                      1.0015, 1.0011, 1.0040, 1.0028, 0.9985, 1.0010, 1.0017, 1.0038, 0.9989,\n",
       "                      1.0030, 1.0052, 1.0036, 1.0009, 1.0024, 1.0018, 1.0001, 0.9980, 1.0020,\n",
       "                      1.0014, 1.0004, 1.0004, 1.0032, 1.0012, 0.9999, 1.0000, 1.0042, 1.0028,\n",
       "                      1.0024, 1.0022, 1.0042, 1.0022, 1.0016, 1.0011, 0.9999, 1.0024, 0.9994,\n",
       "                      1.0006, 1.0016, 1.0030, 1.0017, 1.0016, 1.0040, 1.0044, 1.0007, 1.0006,\n",
       "                      1.0013, 1.0024, 0.9985, 1.0031, 1.0015, 1.0050, 1.0007, 0.9996, 0.9990,\n",
       "                      1.0019, 1.0005, 1.0028, 1.0005, 1.0034, 1.0038, 1.0038, 1.0038, 1.0044,\n",
       "                      1.0035, 1.0002, 1.0005, 1.0008, 1.0012, 1.0000, 1.0001, 1.0020, 1.0016,\n",
       "                      1.0019, 1.0027, 1.0031, 1.0038, 1.0015, 1.0034, 1.0011, 1.0034, 1.0022,\n",
       "                      1.0020, 0.9970, 1.0046, 1.0001, 1.0019, 1.0016, 1.0019, 1.0026, 1.0018,\n",
       "                      0.9997, 1.0004, 0.9986, 1.0008, 1.0038, 1.0020, 1.0012, 0.9987, 1.0014,\n",
       "                      1.0073, 1.0014, 1.0039, 1.0024, 1.0023, 1.0007, 1.0025, 1.0022, 1.0022,\n",
       "                      1.0025, 1.0044, 0.9971, 1.0013, 1.0014, 1.0008, 1.0021, 0.9995, 1.0015,\n",
       "                      0.9996, 1.0008, 1.0020, 0.9979, 1.0016, 1.0029, 1.0000, 1.0002, 1.0001,\n",
       "                      1.0024, 1.0044, 1.0010, 1.0028, 1.0052, 1.0011, 1.0033, 1.0014, 0.9997,\n",
       "                      1.0032, 1.0012, 1.0033, 1.0030, 1.0047, 1.0002, 1.0030, 1.0005, 1.0003,\n",
       "                      0.9984, 1.0004, 0.9986, 1.0018, 1.0046, 1.0034, 1.0035, 1.0051])),\n",
       "             ('transformer.resblocks.0.ln_2.bias',\n",
       "              tensor([ 7.4647e-04,  1.0557e-03, -1.5635e-05,  1.9297e-03,  3.9092e-04,\n",
       "                       7.2086e-05,  1.1176e-03,  3.0559e-04, -4.3475e-04, -3.8109e-04,\n",
       "                      -2.5239e-04, -2.2653e-04, -3.9979e-04, -5.4019e-04, -2.3611e-04,\n",
       "                      -5.1506e-04,  1.3634e-03,  1.4026e-03,  9.2857e-04, -5.4396e-04,\n",
       "                       7.4458e-04,  2.0645e-03,  7.7347e-04,  5.0741e-04,  1.4401e-03,\n",
       "                       9.4586e-04, -2.0964e-05, -1.1220e-04, -8.5382e-04,  1.2012e-03,\n",
       "                      -3.7650e-04, -6.3698e-04, -1.0370e-03, -1.2347e-03,  1.1867e-03,\n",
       "                       1.8296e-03, -1.8656e-04,  3.5024e-04,  1.0154e-03, -7.5326e-04,\n",
       "                       5.6149e-04,  2.2174e-04, -9.9872e-04, -5.7211e-04, -9.6418e-06,\n",
       "                       3.9277e-04, -4.2970e-04,  7.8828e-04,  2.4397e-05,  1.0354e-03,\n",
       "                      -5.2518e-04, -3.4078e-04,  1.2099e-03, -8.9934e-04,  9.8530e-04,\n",
       "                       8.6576e-04,  3.9017e-04,  5.9374e-04, -5.5099e-04, -9.7114e-05,\n",
       "                       3.3525e-04, -3.6922e-04, -1.2361e-03, -6.7222e-04, -1.4511e-03,\n",
       "                      -1.0097e-03,  1.5597e-04,  1.6299e-04, -8.3220e-04,  2.6716e-05,\n",
       "                      -1.4822e-04, -1.0887e-03, -9.7998e-04, -6.4691e-05,  8.0753e-05,\n",
       "                      -2.9962e-04, -3.5611e-04, -2.7400e-04, -9.3792e-04,  4.6763e-04,\n",
       "                      -9.5150e-04, -5.5524e-04,  2.0619e-04,  5.0534e-04, -1.2465e-03,\n",
       "                       3.4640e-04, -9.8893e-04,  6.0752e-04,  2.8378e-04, -1.1258e-03,\n",
       "                      -1.0782e-03,  6.5810e-04, -1.7564e-04,  1.6457e-04, -9.1057e-05,\n",
       "                       3.6818e-04, -5.7068e-04,  6.9802e-04, -2.5397e-04, -2.5286e-05,\n",
       "                      -1.0651e-03,  6.4650e-05, -3.6961e-04,  1.1076e-03,  1.4685e-03,\n",
       "                      -9.4514e-04, -1.6235e-03, -3.2903e-04, -1.5857e-04,  1.2200e-03,\n",
       "                      -2.4702e-04, -2.5925e-04,  1.3007e-04, -2.9594e-04, -2.3967e-05,\n",
       "                       7.2063e-05,  4.3811e-04,  1.1376e-05,  5.1287e-04,  1.1695e-03,\n",
       "                       2.3695e-04,  1.7088e-04, -8.0467e-04, -4.3272e-05, -4.6062e-04,\n",
       "                       1.2716e-04, -5.2372e-04,  9.2247e-04,  5.8748e-05, -7.9394e-04,\n",
       "                       6.3126e-04, -4.9539e-04, -7.5031e-04, -7.8104e-04, -3.6285e-05,\n",
       "                      -4.9551e-04, -1.3514e-03,  4.0917e-04,  4.4030e-04,  1.4770e-03,\n",
       "                       1.1259e-04,  2.2008e-04, -6.7289e-04,  4.3932e-04, -1.2237e-03,\n",
       "                      -6.3431e-04,  1.3445e-04, -2.0907e-03, -5.7570e-04, -7.3988e-04,\n",
       "                       1.1449e-03, -3.8937e-04,  8.1403e-04,  1.8288e-04, -4.1718e-04,\n",
       "                      -4.0757e-04,  1.4481e-04, -1.0438e-03, -4.5608e-05,  6.9886e-04,\n",
       "                       1.4510e-04,  4.5770e-04,  1.0915e-03,  6.5790e-04, -9.9259e-05,\n",
       "                       1.6932e-03,  1.4867e-04,  1.2962e-03, -4.1148e-04,  5.2249e-04,\n",
       "                      -3.7678e-04, -1.2029e-03,  8.0948e-04,  1.8392e-03, -5.1726e-04,\n",
       "                      -4.4200e-04, -1.8118e-03,  4.5643e-04,  1.2138e-03,  7.5822e-05,\n",
       "                       5.4669e-04,  5.6028e-04,  1.0576e-03, -4.1723e-04, -7.5170e-04,\n",
       "                      -1.7064e-04, -2.6822e-04,  3.1883e-04,  9.6700e-04,  2.0745e-03,\n",
       "                       8.1444e-04, -5.5409e-05, -4.5794e-05,  5.2131e-05, -8.6547e-04,\n",
       "                       5.9942e-05, -7.1140e-04, -4.9541e-04, -2.0509e-04, -8.6433e-04,\n",
       "                      -5.4577e-04, -3.1480e-04, -5.3878e-05,  1.8506e-04, -7.1211e-04,\n",
       "                       3.8275e-04, -5.2027e-05, -8.1062e-04, -9.2703e-05, -2.1119e-04,\n",
       "                       6.3292e-04, -7.9076e-04, -6.2347e-04,  1.1419e-03,  2.1932e-04,\n",
       "                       1.9534e-04,  5.8341e-04, -1.9895e-04, -1.3395e-03,  8.8627e-06,\n",
       "                      -2.6801e-05, -9.2326e-04, -1.7047e-04,  4.4346e-04,  9.0408e-04,\n",
       "                       6.9046e-04, -6.3836e-04,  1.2625e-03, -6.2081e-04, -6.5297e-04,\n",
       "                       1.1019e-04, -8.0510e-06, -7.8183e-05,  4.1650e-04,  3.7296e-05,\n",
       "                      -1.1018e-03,  7.7554e-04, -1.0508e-03,  6.5695e-04,  7.3508e-04,\n",
       "                       3.0773e-04,  5.9150e-04, -6.9202e-04, -9.0096e-04,  1.1301e-03,\n",
       "                      -2.1322e-03,  1.7216e-04, -1.6400e-04, -1.7816e-03, -1.6013e-03,\n",
       "                       6.7041e-04, -3.1617e-04, -6.4954e-04, -2.7078e-04, -1.5073e-03,\n",
       "                      -7.3666e-04, -2.9272e-04,  5.3770e-04,  1.4357e-04,  5.9831e-04,\n",
       "                      -8.7619e-04, -2.9664e-05,  9.8934e-04, -7.5947e-04, -1.1957e-03,\n",
       "                      -5.1284e-05, -5.2494e-04,  5.1394e-04, -4.0699e-04, -6.0896e-04,\n",
       "                       2.6462e-04, -6.8929e-04,  1.2983e-03,  1.2234e-03, -7.2371e-04,\n",
       "                       7.0965e-04,  9.1013e-05, -1.2538e-03,  8.1124e-04, -2.0524e-04,\n",
       "                      -6.7497e-05,  4.1960e-04,  6.7336e-04, -6.0755e-04, -1.3339e-04,\n",
       "                      -1.3702e-04,  2.1783e-04,  1.4983e-04,  6.8922e-04, -1.6094e-03,\n",
       "                      -1.2838e-03, -9.0465e-04, -6.9029e-04,  3.0028e-04, -9.9102e-05,\n",
       "                       4.6423e-04, -6.5015e-04,  7.9616e-04,  2.8565e-05,  7.6609e-05,\n",
       "                      -3.4084e-04, -1.0389e-03, -2.5791e-04,  1.4793e-03, -1.4785e-04,\n",
       "                      -6.2543e-04, -2.6956e-04, -1.6445e-04,  2.8691e-05, -1.7995e-04,\n",
       "                       4.7813e-05, -1.0160e-04, -2.4113e-04,  8.8937e-04,  2.1180e-04,\n",
       "                      -6.2256e-04,  5.9421e-04,  4.2427e-04,  5.4475e-04, -1.3738e-03,\n",
       "                      -2.7111e-04,  2.2765e-04, -2.0052e-03, -1.1306e-05, -4.6809e-04,\n",
       "                      -3.1379e-04,  3.2302e-04,  1.5051e-03,  3.6393e-04, -1.1681e-04,\n",
       "                      -5.0376e-04,  1.6749e-04, -4.8635e-04,  1.3298e-04, -8.3420e-05,\n",
       "                       6.9548e-04, -4.9221e-04,  5.9193e-04, -8.1806e-05, -2.6152e-04,\n",
       "                      -3.3420e-04,  1.0156e-03,  5.2809e-05,  8.0571e-04,  4.7198e-04,\n",
       "                       8.6140e-05, -1.1324e-03, -2.2691e-04,  2.7394e-04, -3.1905e-05,\n",
       "                      -1.2116e-03,  8.5600e-04,  1.1413e-04,  6.7877e-04,  3.0773e-04,\n",
       "                       1.3553e-03, -3.5754e-04, -1.0672e-03,  5.6350e-04, -9.4885e-04,\n",
       "                       6.8292e-04,  1.0028e-03, -8.4046e-04,  6.6224e-04,  1.0876e-04,\n",
       "                       3.2958e-05, -9.6555e-04,  4.8662e-04, -8.3416e-04,  2.8414e-06,\n",
       "                      -8.8772e-04,  5.0459e-04,  4.1091e-04, -5.1385e-04, -1.0416e-03,\n",
       "                       5.1432e-04, -4.8644e-04, -1.1087e-03, -4.2717e-04, -1.6047e-04,\n",
       "                       1.0668e-03,  6.2022e-04, -7.2998e-04,  5.2533e-05, -7.0137e-04,\n",
       "                       7.6529e-05,  3.9422e-04, -3.7672e-04, -1.5258e-03, -9.5454e-04,\n",
       "                      -1.6603e-04,  1.0066e-04, -6.7393e-04,  5.1202e-04, -3.6541e-04,\n",
       "                       1.0055e-03,  1.2547e-03, -6.1500e-04,  6.2897e-04, -1.2510e-04,\n",
       "                      -1.6528e-04, -3.3754e-04, -1.5333e-04, -1.5244e-04, -4.7832e-04,\n",
       "                      -1.6998e-04,  1.0089e-03, -1.1636e-06, -9.0903e-04, -3.6681e-04,\n",
       "                       9.0982e-04, -1.3382e-04,  6.4636e-05,  2.4537e-04, -6.4064e-04,\n",
       "                       2.1280e-04, -8.3309e-04, -1.2755e-03,  1.5150e-04, -1.3939e-04,\n",
       "                       1.5243e-03, -1.9171e-03,  1.2643e-04, -9.6938e-04, -1.6114e-04,\n",
       "                       3.8143e-04, -1.8748e-03,  9.5399e-05, -2.6614e-04, -1.4337e-04,\n",
       "                      -9.2896e-05,  5.9097e-04,  6.6117e-04,  1.6326e-03,  2.1265e-04,\n",
       "                      -3.0114e-04,  6.1347e-04, -3.7957e-04, -3.4589e-04,  6.7724e-04,\n",
       "                       3.3814e-04, -1.4609e-03, -2.2049e-03,  5.5038e-06,  4.6795e-04,\n",
       "                       1.1483e-03, -8.1720e-04, -1.1727e-04,  7.2167e-04, -6.7244e-05,\n",
       "                       9.9075e-04, -3.3671e-04, -1.7607e-03,  5.2160e-04,  1.7278e-04,\n",
       "                      -3.5763e-04, -8.2190e-04,  7.4950e-04,  5.1791e-04, -4.1582e-04,\n",
       "                       5.3585e-04, -3.4772e-04,  1.7818e-03, -1.7912e-03,  6.4504e-04,\n",
       "                      -4.4585e-04, -9.9812e-04, -1.3150e-04, -1.0656e-03, -2.1290e-04,\n",
       "                      -1.4338e-03,  1.0230e-03, -8.6562e-04,  1.1053e-03,  9.5361e-04,\n",
       "                      -9.9644e-04,  1.1059e-03, -7.9882e-04,  2.3603e-04,  4.7932e-05,\n",
       "                      -1.2076e-03, -2.4408e-04,  7.3018e-05,  2.1585e-04, -4.5027e-04,\n",
       "                      -2.2340e-04, -2.8532e-04,  8.7791e-04, -1.0026e-03, -2.1607e-04,\n",
       "                      -6.0459e-04, -1.2777e-03,  1.7015e-04,  4.3244e-04, -4.6378e-04,\n",
       "                       1.0579e-03,  1.4600e-04,  2.8669e-04, -8.1613e-04,  2.8949e-04,\n",
       "                       1.1244e-04,  6.9714e-04,  1.6726e-04,  5.7441e-04, -5.0338e-04,\n",
       "                       1.5250e-03, -6.9851e-05,  1.0738e-03,  6.1064e-04, -9.6976e-05,\n",
       "                       6.0606e-04, -1.8134e-04])),\n",
       "             ('transformer.resblocks.1.attn.in_proj_weight',\n",
       "              tensor([[ 0.0343, -0.0361,  0.0056,  ...,  0.0081,  0.0311, -0.0538],\n",
       "                      [ 0.0598, -0.0050,  0.0264,  ...,  0.0198, -0.0013,  0.0044],\n",
       "                      [ 0.0621, -0.0384, -0.0414,  ..., -0.0209,  0.0109, -0.1039],\n",
       "                      ...,\n",
       "                      [-0.0405,  0.1010, -0.0057,  ..., -0.0620, -0.0283, -0.0215],\n",
       "                      [ 0.0045, -0.0111, -0.0103,  ..., -0.0077, -0.0472,  0.0444],\n",
       "                      [ 0.0693,  0.0695,  0.0275,  ..., -0.0405,  0.0203,  0.0274]])),\n",
       "             ('transformer.resblocks.1.attn.in_proj_bias',\n",
       "              tensor([ 6.5028e-05, -4.4116e-03,  4.5259e-03,  ...,  7.8695e-04,\n",
       "                       2.6951e-04,  7.3296e-04])),\n",
       "             ('transformer.resblocks.1.attn.out_proj.weight',\n",
       "              tensor([[ 7.5292e-03,  6.2797e-03, -1.4232e-02,  ...,  1.0955e-02,\n",
       "                        1.2070e-02, -2.6318e-03],\n",
       "                      [ 3.9137e-03,  1.2300e-02,  8.6482e-03,  ..., -8.3133e-03,\n",
       "                        9.7519e-03,  5.4085e-03],\n",
       "                      [-4.2132e-03, -1.2602e-02, -2.7509e-03,  ...,  1.3563e-04,\n",
       "                       -3.2271e-03, -1.2637e-02],\n",
       "                      ...,\n",
       "                      [-7.2892e-03,  7.1092e-03, -5.0384e-03,  ..., -9.0821e-03,\n",
       "                        1.1353e-02,  4.8089e-03],\n",
       "                      [ 7.9822e-03,  7.7361e-03,  1.0798e-02,  ...,  7.9541e-03,\n",
       "                        2.3363e-05, -8.0400e-03],\n",
       "                      [ 1.3715e-03, -1.1132e-03, -1.7193e-03,  ..., -1.8136e-03,\n",
       "                        5.3000e-03,  1.8606e-03]])),\n",
       "             ('transformer.resblocks.1.attn.out_proj.bias',\n",
       "              tensor([ 4.2207e-04,  5.9482e-04, -1.3804e-04, -2.4173e-05,  1.0069e-04,\n",
       "                       7.5809e-04,  1.7478e-04,  8.9945e-04, -9.7875e-04, -4.0496e-04,\n",
       "                       1.0269e-03, -9.2491e-04,  1.0614e-03,  1.3702e-04, -6.9678e-04,\n",
       "                      -8.4398e-04, -1.1482e-03,  9.3758e-05,  6.8151e-04,  6.4762e-04,\n",
       "                       1.6128e-04, -4.0688e-04, -4.1826e-04, -9.6739e-04,  6.7642e-04,\n",
       "                       2.0117e-04, -3.3474e-05,  4.6531e-04,  1.6544e-03, -9.8876e-04,\n",
       "                       6.5115e-04, -6.2753e-04, -6.6538e-04, -4.1957e-04, -5.9487e-04,\n",
       "                       1.0107e-03, -9.0763e-04, -5.1338e-04,  2.5297e-04, -1.3693e-03,\n",
       "                      -2.4599e-04, -6.2417e-04, -7.5137e-04,  1.7110e-03, -2.0287e-03,\n",
       "                       1.0965e-03,  7.2397e-04,  1.3392e-04,  1.9986e-04, -1.5059e-03,\n",
       "                      -2.7897e-04, -1.1740e-03, -1.8420e-03, -9.4526e-04,  1.3321e-03,\n",
       "                       7.0618e-04,  1.2298e-03,  1.8279e-03,  6.9629e-04,  5.6759e-04,\n",
       "                       3.7784e-04, -2.8927e-04, -5.2789e-04,  4.3498e-04,  2.2239e-04,\n",
       "                      -1.3636e-04, -8.1286e-04, -6.7444e-04, -9.2958e-05,  8.0763e-04,\n",
       "                      -4.8793e-04,  1.4640e-03, -7.9184e-04, -6.7715e-05,  7.9212e-04,\n",
       "                      -1.8859e-04, -3.4793e-04,  6.0356e-04, -1.5799e-04, -3.3628e-04,\n",
       "                      -1.2349e-04,  8.4303e-04,  4.3496e-04,  5.7033e-04,  3.5908e-04,\n",
       "                      -1.1258e-03, -3.7184e-04, -1.2667e-03, -7.4906e-04, -9.7727e-04,\n",
       "                       1.3550e-04, -4.8218e-04, -1.1098e-04, -7.7249e-04,  1.0671e-03,\n",
       "                       1.4325e-04, -1.3003e-04,  1.1918e-04,  9.0555e-04, -9.7398e-05,\n",
       "                       6.4523e-04, -1.6114e-04,  1.2668e-03,  6.5963e-04,  5.7633e-04,\n",
       "                      -3.9466e-05,  1.2536e-03, -2.6724e-05, -1.5549e-04,  4.4624e-04,\n",
       "                      -5.8809e-04, -1.0292e-03, -6.8816e-04, -3.9276e-04,  1.0089e-03,\n",
       "                      -1.5794e-03,  3.2095e-04,  1.3261e-04, -5.0586e-04, -1.3979e-03,\n",
       "                      -3.6436e-05,  6.5892e-04, -2.3206e-04,  3.8227e-04,  5.1119e-04,\n",
       "                      -5.2662e-04,  5.8077e-05,  1.5363e-04,  5.5897e-04,  1.5778e-04,\n",
       "                       1.2870e-03, -3.1923e-04, -1.9642e-04, -1.9429e-03,  2.8019e-04,\n",
       "                       2.3585e-04, -1.2425e-03, -5.3101e-04, -6.9356e-04, -2.5621e-04,\n",
       "                      -7.8006e-04, -6.4217e-05,  1.9343e-04,  5.5124e-04,  6.9079e-05,\n",
       "                      -5.5420e-04,  7.5108e-04, -6.5991e-05,  1.3400e-03, -2.4312e-05,\n",
       "                       5.7033e-04, -2.1149e-04, -1.9603e-04,  1.3623e-03,  4.4836e-04,\n",
       "                       1.1972e-03,  1.3751e-04,  2.5535e-04,  1.2410e-03,  8.4908e-04,\n",
       "                      -8.1091e-04,  7.2184e-04,  8.4753e-04, -5.0800e-04,  1.0352e-04,\n",
       "                       4.8247e-05,  8.1394e-05,  2.0316e-04, -6.4953e-04,  3.7543e-04,\n",
       "                      -3.0553e-04,  3.1837e-04,  2.6164e-04, -3.7246e-05, -2.3254e-04,\n",
       "                      -1.1952e-03,  2.2944e-04, -2.9401e-04, -8.1035e-05,  3.8096e-04,\n",
       "                      -7.2322e-04,  1.4235e-04, -5.0597e-05,  6.0897e-04, -7.3677e-04,\n",
       "                      -4.8604e-05,  6.1957e-04, -5.3849e-05, -2.2309e-03, -2.0926e-04,\n",
       "                       1.5279e-03,  7.6426e-04, -3.5743e-04, -1.3062e-03, -1.7903e-05,\n",
       "                      -9.9922e-04,  4.9754e-04,  8.1403e-05,  6.8694e-04,  1.3580e-04,\n",
       "                       6.7742e-04, -1.2586e-03, -3.6853e-04,  1.2987e-03, -6.6745e-04,\n",
       "                       5.5158e-04, -4.3895e-04,  2.3167e-04,  5.8675e-04,  2.6429e-04,\n",
       "                       2.7038e-04, -1.7901e-03,  4.8022e-05,  7.1756e-04,  2.0249e-04,\n",
       "                      -1.5908e-03,  8.9479e-04,  9.2792e-04,  9.1996e-04,  8.4281e-04,\n",
       "                       6.1710e-04,  1.1558e-03,  2.0288e-05, -7.1253e-04, -1.2792e-03,\n",
       "                       1.4605e-03, -8.4250e-04,  3.4713e-04, -5.1910e-05,  2.3476e-04,\n",
       "                      -2.2314e-04, -1.0003e-03, -1.2892e-04, -6.1734e-04, -3.2950e-04,\n",
       "                       4.0165e-04,  1.1637e-04,  3.1058e-06,  4.5949e-04,  1.2583e-03,\n",
       "                      -1.3025e-03,  7.2172e-05,  6.5306e-04,  1.5785e-03, -4.4538e-04,\n",
       "                       4.0680e-04,  9.3226e-04, -1.9534e-04,  4.1834e-04,  5.7744e-04,\n",
       "                      -5.6714e-04, -8.4446e-05, -2.6932e-04,  1.8279e-04, -1.3329e-04,\n",
       "                       1.0872e-03, -1.0100e-03,  7.3224e-04,  6.1025e-04, -1.1460e-03,\n",
       "                       3.2876e-04, -6.5620e-04,  1.0706e-03, -7.7553e-04, -2.7093e-04,\n",
       "                       8.4196e-04, -1.3258e-03,  7.1273e-04,  1.3507e-04, -1.3592e-03,\n",
       "                      -5.6831e-05,  8.2173e-04, -7.8866e-04,  1.5439e-03, -1.4099e-03,\n",
       "                      -4.3447e-04, -2.0427e-04,  6.7479e-04,  3.2638e-04, -8.5399e-04,\n",
       "                       7.4594e-05,  9.0323e-04, -1.5098e-05,  5.2700e-04, -4.7706e-04,\n",
       "                       1.0632e-03,  9.1166e-04,  2.0917e-03,  3.5908e-04,  3.7218e-04,\n",
       "                       6.9631e-04, -4.5619e-04, -1.8139e-04,  9.5960e-04,  3.8350e-04,\n",
       "                       8.6880e-05, -1.8742e-05, -4.1829e-04,  1.0841e-03,  2.6442e-04,\n",
       "                       9.1673e-04,  7.9619e-04,  2.1840e-04,  1.3015e-03,  7.0227e-04,\n",
       "                       7.0186e-04, -2.0687e-04, -7.4703e-04, -3.2702e-04, -7.2957e-04,\n",
       "                       1.3421e-03,  6.7977e-05,  2.7181e-04, -4.4912e-06, -4.4922e-04,\n",
       "                      -6.5312e-04,  2.3912e-04,  6.1900e-04, -8.8884e-04,  2.8374e-04,\n",
       "                      -3.0761e-04, -1.5532e-03,  8.2068e-04, -7.3194e-04,  4.6527e-04,\n",
       "                       4.1667e-04, -1.7607e-04, -1.1090e-03,  1.2000e-04, -3.3708e-04,\n",
       "                      -1.3504e-03, -2.7047e-04,  6.5228e-05,  7.1382e-04,  4.1380e-04,\n",
       "                      -8.7489e-04, -9.0296e-04, -1.2223e-03, -8.4464e-04,  8.9385e-04,\n",
       "                      -8.8267e-04,  3.5387e-05, -3.7731e-04, -2.1966e-04, -4.9647e-04,\n",
       "                      -4.1372e-04, -5.6462e-04, -4.0466e-04, -7.0373e-04, -3.5127e-05,\n",
       "                      -2.3559e-04,  3.2125e-04, -1.0309e-03,  7.3264e-04,  6.7204e-04,\n",
       "                      -9.7392e-04, -1.4915e-03,  3.2139e-04,  2.5929e-04,  9.7745e-05,\n",
       "                      -1.6555e-04, -3.8292e-04, -2.1123e-04,  1.2693e-03, -2.6279e-04,\n",
       "                       4.6657e-04, -6.4830e-05, -1.4545e-04, -1.9174e-03, -4.2756e-04,\n",
       "                      -3.9727e-04, -7.3876e-04,  5.5474e-05, -1.0293e-04, -3.4245e-04,\n",
       "                      -5.1097e-04,  1.3053e-04,  2.1279e-04, -9.2556e-04, -5.7658e-04,\n",
       "                      -1.0098e-03, -1.6727e-03,  5.9875e-05, -6.7845e-04, -7.2156e-04,\n",
       "                      -7.9899e-04,  8.8845e-04,  4.6655e-04, -7.9180e-05, -1.1921e-03,\n",
       "                       2.6180e-04, -1.0050e-03,  1.0159e-04, -2.0174e-04,  2.4170e-04,\n",
       "                      -1.8664e-05,  5.4935e-04, -2.0788e-04, -8.9495e-05,  1.1413e-03,\n",
       "                       5.7335e-04,  2.9020e-05, -1.0915e-03,  1.2753e-03,  3.7467e-04,\n",
       "                      -5.7608e-04, -4.7631e-04,  6.3283e-04,  7.3182e-04, -5.8975e-04,\n",
       "                      -6.7256e-04, -6.5000e-04,  4.6836e-04,  2.1565e-04, -1.1884e-04,\n",
       "                      -4.5613e-05, -1.0522e-03,  1.4065e-03, -3.4673e-04,  9.1401e-04,\n",
       "                      -5.3963e-04,  3.2495e-04, -1.8496e-04, -3.5440e-04, -5.3652e-04,\n",
       "                       1.1731e-03,  4.6869e-04,  5.2978e-05, -3.8036e-04, -9.0752e-04,\n",
       "                      -5.3709e-04,  1.3828e-03, -6.2401e-04, -2.5071e-04, -1.0468e-03,\n",
       "                      -4.1277e-04,  1.2892e-03, -1.0668e-03, -5.0647e-04, -1.8107e-04,\n",
       "                      -1.2714e-04, -4.8547e-04,  6.4900e-04, -2.5868e-04, -9.7693e-04,\n",
       "                       2.4540e-06, -1.0958e-03,  6.1819e-05,  6.8470e-04,  1.0745e-03,\n",
       "                      -1.2694e-03,  4.4143e-04, -3.0956e-04,  3.6485e-04, -5.6330e-04,\n",
       "                      -7.1612e-04, -7.0977e-04,  1.5660e-04, -1.5002e-03,  4.4592e-04,\n",
       "                      -2.8502e-04, -1.1168e-03,  1.8970e-05,  2.7814e-04,  1.5484e-03,\n",
       "                       3.4298e-04,  7.4382e-04, -1.2277e-03,  9.5325e-04,  3.6420e-04,\n",
       "                      -8.0097e-04, -9.8498e-05,  1.1936e-03,  6.1123e-04,  2.0385e-04,\n",
       "                       7.3572e-04, -7.4903e-04, -1.7093e-04, -1.3822e-03,  2.7139e-04,\n",
       "                      -4.5825e-04,  3.4927e-05,  2.3176e-04,  1.0734e-03,  1.2674e-03,\n",
       "                      -3.7664e-04,  1.9435e-03, -8.8220e-04,  1.0310e-03, -7.7865e-05,\n",
       "                       1.2554e-03,  1.1072e-03,  1.1751e-04, -1.6405e-04, -8.1201e-04,\n",
       "                      -9.0235e-04,  9.2547e-05, -5.7771e-04, -3.1717e-04,  2.5763e-04,\n",
       "                       6.7480e-05,  8.1625e-04,  1.1566e-03, -4.4597e-04, -9.5578e-04,\n",
       "                      -8.4381e-04,  4.6470e-04,  2.7711e-05,  1.5125e-03,  2.2702e-05,\n",
       "                       4.1230e-04,  5.3662e-04])),\n",
       "             ('transformer.resblocks.1.ln_1.weight',\n",
       "              tensor([0.9967, 1.0000, 0.9997, 1.0014, 0.9985, 0.9996, 1.0016, 0.9990, 0.9998,\n",
       "                      1.0000, 1.0002, 1.0017, 1.0005, 0.9994, 0.9999, 0.9989, 1.0002, 1.0002,\n",
       "                      0.9991, 0.9988, 1.0009, 0.9999, 0.9993, 1.0016, 0.9989, 1.0013, 1.0007,\n",
       "                      1.0019, 1.0029, 0.9985, 1.0009, 1.0010, 0.9983, 1.0010, 1.0017, 1.0006,\n",
       "                      1.0007, 0.9998, 0.9972, 0.9990, 0.9989, 1.0002, 1.0012, 1.0002, 1.0012,\n",
       "                      1.0001, 1.0024, 1.0006, 1.0029, 1.0016, 1.0015, 0.9977, 1.0016, 0.9992,\n",
       "                      1.0009, 1.0019, 0.9990, 1.0013, 0.9998, 0.9999, 1.0001, 1.0011, 0.9978,\n",
       "                      1.0002, 1.0008, 0.9992, 1.0004, 1.0024, 1.0006, 1.0002, 0.9977, 0.9997,\n",
       "                      1.0003, 0.9985, 0.9989, 1.0013, 1.0021, 0.9997, 1.0011, 0.9999, 0.9990,\n",
       "                      0.9995, 0.9992, 1.0035, 1.0020, 0.9979, 1.0016, 1.0020, 0.9961, 1.0026,\n",
       "                      1.0001, 1.0015, 0.9979, 0.9993, 0.9996, 1.0000, 1.0009, 1.0006, 0.9993,\n",
       "                      1.0014, 1.0000, 1.0001, 1.0004, 1.0020, 0.9998, 1.0012, 0.9997, 0.9985,\n",
       "                      0.9993, 1.0016, 1.0003, 1.0016, 1.0006, 1.0009, 0.9990, 1.0014, 0.9996,\n",
       "                      1.0011, 1.0008, 1.0022, 1.0016, 0.9995, 0.9976, 0.9995, 0.9991, 1.0005,\n",
       "                      1.0002, 1.0004, 0.9968, 1.0006, 0.9976, 1.0017, 1.0016, 1.0025, 1.0016,\n",
       "                      0.9997, 0.9988, 1.0013, 0.9996, 1.0006, 0.9969, 0.9995, 1.0002, 1.0015,\n",
       "                      0.9989, 1.0010, 0.9989, 1.0001, 1.0018, 0.9992, 0.9999, 1.0009, 1.0011,\n",
       "                      1.0012, 0.9998, 1.0014, 0.9992, 1.0011, 0.9982, 1.0045, 1.0003, 1.0002,\n",
       "                      1.0048, 0.9972, 1.0015, 1.0034, 0.9996, 0.9999, 0.9986, 1.0022, 0.9995,\n",
       "                      1.0001, 1.0023, 1.0004, 0.9993, 1.0014, 1.0006, 0.9997, 0.9998, 1.0011,\n",
       "                      0.9985, 0.9987, 0.9991, 0.9996, 1.0006, 0.9998, 1.0005, 0.9995, 1.0001,\n",
       "                      0.9979, 1.0059, 0.9991, 0.9994, 0.9986, 1.0007, 1.0028, 1.0007, 1.0004,\n",
       "                      0.9994, 1.0014, 1.0006, 0.9991, 1.0000, 1.0016, 1.0013, 0.9996, 1.0005,\n",
       "                      1.0000, 1.0023, 1.0020, 0.9985, 1.0009, 1.0022, 1.0026, 0.9980, 1.0003,\n",
       "                      0.9991, 0.9993, 1.0002, 0.9991, 1.0006, 1.0001, 0.9987, 1.0003, 1.0000,\n",
       "                      0.9995, 1.0007, 0.9991, 1.0011, 1.0026, 0.9998, 1.0007, 1.0018, 0.9936,\n",
       "                      0.9992, 1.0031, 1.0018, 0.9982, 1.0009, 1.0018, 0.9971, 1.0009, 1.0014,\n",
       "                      0.9981, 1.0009, 1.0011, 0.9988, 0.9993, 0.9986, 0.9987, 1.0024, 1.0007,\n",
       "                      0.9999, 1.0003, 0.9973, 1.0022, 1.0038, 1.0003, 0.9988, 0.9991, 1.0010,\n",
       "                      0.9989, 1.0004, 1.0021, 1.0005, 1.0003, 0.9998, 0.9973, 0.9988, 1.0003,\n",
       "                      1.0016, 1.0030, 0.9983, 1.0009, 1.0010, 0.9988, 1.0005, 1.0006, 0.9994,\n",
       "                      1.0012, 1.0013, 1.0007, 0.9985, 1.0009, 0.9982, 1.0007, 1.0019, 1.0006,\n",
       "                      0.9988, 0.9990, 0.9989, 0.9993, 1.0009, 1.0026, 1.0014, 1.0001, 1.0015,\n",
       "                      0.9986, 1.0009, 1.0025, 0.9988, 0.9949, 0.9989, 0.9999, 0.9996, 0.9984,\n",
       "                      1.0002, 0.9997, 1.0000, 1.0032, 0.9993, 1.0008, 0.9979, 0.9992, 1.0014,\n",
       "                      0.9985, 1.0018, 1.0009, 1.0003, 1.0000, 0.9973, 1.0001, 0.9983, 1.0005,\n",
       "                      1.0040, 0.9994, 0.9993, 0.9991, 0.9987, 1.0021, 0.9992, 0.9993, 1.0010,\n",
       "                      1.0032, 1.0019, 0.9954, 1.0003, 0.9996, 0.9997, 1.0009, 1.0010, 0.9993,\n",
       "                      1.0004, 1.0022, 0.9981, 1.0003, 0.9970, 0.9983, 1.0006, 0.9978, 1.0010,\n",
       "                      0.9986, 0.9980, 1.0003, 1.0017, 1.0012, 1.0019, 0.9984, 0.9985, 0.9976,\n",
       "                      1.0012, 1.0001, 0.9996, 1.0035, 1.0004, 1.0003, 1.0017, 1.0011, 1.0013,\n",
       "                      0.9999, 1.0005, 0.9967, 1.0000, 1.0026, 1.0025, 0.9997, 0.9987, 1.0006,\n",
       "                      1.0010, 1.0019, 1.0012, 0.9997, 0.9987, 1.0004, 1.0031, 0.9997, 0.9997,\n",
       "                      1.0019, 1.0010, 1.0001, 1.0009, 1.0023, 0.9978, 1.0001, 1.0011, 0.9996,\n",
       "                      1.0007, 1.0026, 0.9970, 1.0012, 0.9972, 1.0009, 1.0000, 1.0033, 1.0006,\n",
       "                      1.0025, 1.0002, 0.9973, 0.9998, 1.0001, 1.0011, 1.0026, 1.0011, 1.0003,\n",
       "                      0.9997, 1.0007, 1.0006, 1.0018, 1.0004, 0.9994, 1.0003, 0.9998, 1.0015,\n",
       "                      1.0012, 0.9964, 1.0016, 1.0011, 1.0002, 1.0012, 1.0002, 0.9995, 1.0027,\n",
       "                      1.0002, 1.0016, 0.9995, 0.9990, 1.0018, 0.9990, 1.0008, 1.0009, 1.0012,\n",
       "                      0.9986, 1.0001, 0.9994, 0.9993, 1.0011, 0.9991, 0.9990, 1.0001, 0.9989,\n",
       "                      0.9983, 1.0000, 1.0001, 1.0022, 1.0015, 0.9993, 0.9996, 1.0028, 0.9999,\n",
       "                      0.9997, 1.0015, 0.9993, 0.9994, 1.0007, 0.9991, 1.0011, 1.0013, 0.9995,\n",
       "                      0.9976, 1.0012, 1.0044, 1.0025, 0.9989, 0.9992, 1.0000, 0.9999, 1.0028,\n",
       "                      0.9993, 0.9977, 1.0018, 0.9992, 1.0004, 0.9997, 0.9975, 1.0003, 1.0005,\n",
       "                      1.0003, 1.0031, 0.9999, 1.0013, 1.0000, 0.9996, 1.0002, 0.9993, 0.9996,\n",
       "                      0.9988, 0.9937, 0.9996, 0.9997, 1.0004, 1.0010, 1.0007, 0.9999, 0.9998,\n",
       "                      1.0004, 1.0011, 0.9994, 1.0010, 0.9968, 1.0013, 0.9993, 0.9984])),\n",
       "             ('transformer.resblocks.1.ln_1.bias',\n",
       "              tensor([-1.1046e-03, -3.1054e-03, -6.6932e-04,  4.5530e-04, -1.1702e-04,\n",
       "                      -2.0908e-03, -7.7018e-04, -5.0672e-04,  2.0244e-03,  7.8007e-04,\n",
       "                      -3.6512e-04, -4.8029e-04, -3.0810e-03, -1.1465e-04, -5.1986e-04,\n",
       "                       1.8198e-03,  1.5890e-03,  2.2555e-04, -1.2895e-04,  5.3938e-04,\n",
       "                      -1.1830e-03, -5.7388e-05,  5.1957e-04, -3.7690e-04, -1.2176e-03,\n",
       "                      -2.8478e-04,  3.2485e-04, -1.1839e-03,  5.4946e-04,  1.0431e-03,\n",
       "                      -1.3429e-04,  1.2417e-03,  2.1433e-03,  1.5648e-03, -1.3753e-03,\n",
       "                      -5.2844e-04,  1.2163e-03,  5.1388e-04, -5.9141e-04,  1.5163e-03,\n",
       "                       9.3875e-04,  8.2068e-04,  6.4773e-04, -1.1880e-03,  9.7963e-04,\n",
       "                      -1.4660e-03, -4.6368e-04,  6.4859e-04, -4.1412e-04,  1.2170e-03,\n",
       "                      -3.4726e-04,  5.7276e-04,  5.3979e-04,  7.8049e-04,  1.7615e-04,\n",
       "                      -3.9187e-04, -2.0501e-03, -1.3484e-03, -1.0493e-03, -1.1369e-03,\n",
       "                      -1.6068e-03, -9.4048e-04,  9.4362e-04, -1.2171e-03, -7.5002e-04,\n",
       "                      -1.3821e-03,  2.6425e-04, -8.7452e-04, -3.7438e-06, -1.6616e-03,\n",
       "                      -2.5639e-04,  9.9614e-04, -5.1156e-04, -7.4260e-05,  2.3105e-04,\n",
       "                       4.5966e-04, -2.8476e-04, -8.1361e-04,  1.6494e-03,  7.9005e-04,\n",
       "                       8.3056e-04, -1.6519e-04,  7.4502e-04,  2.4768e-04,  7.9063e-04,\n",
       "                       3.6859e-04,  2.6522e-03,  1.3504e-03,  7.1521e-04,  1.8554e-03,\n",
       "                      -9.7081e-04, -2.7821e-04,  6.3400e-04,  3.4775e-04, -1.6589e-03,\n",
       "                      -2.2290e-04, -1.1160e-03, -6.4676e-04, -2.4373e-03, -6.9192e-04,\n",
       "                      -4.0484e-04, -4.4606e-04, -9.9687e-04, -1.5560e-04,  5.9203e-04,\n",
       "                      -2.2738e-03, -1.2503e-03, -2.0714e-04,  7.8123e-04,  3.8348e-04,\n",
       "                       1.3394e-04,  1.4068e-03,  7.1620e-04,  1.1818e-03, -2.0303e-03,\n",
       "                       2.7344e-04,  2.5474e-04,  8.6790e-04, -1.0103e-03,  2.8260e-03,\n",
       "                       2.5937e-03,  1.9817e-04, -4.0565e-04, -5.0743e-04, -2.7203e-03,\n",
       "                       3.5816e-05,  9.8082e-04,  6.2154e-05, -2.6693e-04, -6.4006e-04,\n",
       "                      -1.2398e-03,  4.0824e-04, -3.1707e-04,  6.5160e-04, -1.8630e-03,\n",
       "                      -2.4076e-03,  1.4212e-03, -1.5048e-03,  5.2906e-04,  9.7016e-04,\n",
       "                      -1.2702e-04, -1.7248e-07,  7.9880e-04,  8.2787e-04,  2.7429e-04,\n",
       "                       2.4488e-04,  7.8006e-04, -4.4065e-04, -5.7814e-04,  6.7771e-05,\n",
       "                      -1.6224e-03,  1.2827e-03, -1.1840e-03,  8.2220e-04, -8.3254e-04,\n",
       "                      -3.2138e-04,  9.2355e-04,  2.6418e-04,  1.3454e-04, -1.4224e-03,\n",
       "                      -5.9561e-04, -4.5271e-04, -4.2212e-04,  2.7519e-04, -1.3478e-03,\n",
       "                      -9.7182e-05, -9.1131e-05, -5.4034e-04,  7.8112e-04,  6.3276e-06,\n",
       "                       3.2584e-04, -9.2576e-04,  9.0612e-04, -7.3965e-04,  1.1144e-03,\n",
       "                       1.8505e-03,  3.1877e-04, -6.0852e-04,  4.4617e-04, -1.2671e-03,\n",
       "                       1.0286e-03,  1.8037e-04,  1.1537e-03,  6.2178e-04,  1.1204e-03,\n",
       "                       1.3291e-03, -8.5909e-05, -7.0596e-04,  1.3475e-03, -2.9936e-04,\n",
       "                      -1.7502e-03, -2.8569e-04, -1.3905e-03,  1.4310e-03, -1.8005e-04,\n",
       "                       1.0229e-03, -7.6908e-04, -6.9239e-04, -1.3111e-03,  1.0798e-04,\n",
       "                       1.4656e-03,  4.6243e-04,  9.7533e-04, -6.6932e-04, -3.4913e-04,\n",
       "                      -3.7348e-04, -6.2349e-04, -6.0680e-04, -6.9110e-04, -1.0667e-03,\n",
       "                       1.3844e-03,  1.2398e-03,  2.4380e-05, -1.3949e-03, -1.3027e-04,\n",
       "                       1.3500e-03, -2.6697e-04, -3.0181e-04, -1.1084e-03,  8.7702e-05,\n",
       "                      -5.1227e-05, -8.7122e-04, -3.9561e-04,  1.0321e-03, -1.7078e-05,\n",
       "                      -4.1396e-04,  8.0909e-04,  4.2218e-04,  5.2829e-04, -1.0767e-03,\n",
       "                      -3.1343e-04,  3.0145e-03,  6.6299e-04, -1.2023e-04,  9.4666e-04,\n",
       "                       1.2106e-03,  1.6888e-03,  1.2782e-03, -4.7497e-04, -1.9197e-03,\n",
       "                       9.1823e-04,  5.8141e-04,  1.9240e-04,  2.0661e-04,  2.1830e-03,\n",
       "                      -4.4051e-04, -7.2977e-04,  1.9252e-03, -7.9451e-04, -5.8106e-04,\n",
       "                       1.3277e-03,  1.7498e-04, -4.5632e-04,  4.6932e-05, -1.4923e-03,\n",
       "                       6.3882e-05,  2.5686e-03, -1.5367e-03,  2.8462e-04,  1.4223e-03,\n",
       "                       1.5309e-04,  1.1930e-04, -2.1085e-04,  1.0138e-03, -2.1614e-03,\n",
       "                      -2.7899e-04,  1.6737e-03, -1.5012e-03,  4.5826e-05,  8.4009e-04,\n",
       "                       1.2021e-03, -1.2082e-03,  3.9659e-04, -6.9616e-04,  2.2211e-04,\n",
       "                      -2.6008e-03, -1.7633e-03, -1.6673e-03, -1.0106e-03,  1.0561e-03,\n",
       "                       6.0682e-04,  9.6671e-04, -3.5161e-04, -3.0367e-04,  2.0035e-03,\n",
       "                       6.1572e-05, -1.1929e-04, -1.1398e-03, -1.5061e-03, -7.4692e-04,\n",
       "                      -2.4590e-05,  1.0710e-03,  8.9456e-05,  1.8068e-03,  2.0307e-04,\n",
       "                      -8.8116e-04, -6.0159e-04,  5.8491e-04, -1.8746e-03,  1.2827e-03,\n",
       "                      -5.0117e-04, -9.9861e-04, -7.0812e-04, -1.6646e-04, -1.7091e-03,\n",
       "                       3.0853e-04, -1.0811e-03,  9.2810e-04, -9.2750e-04,  1.4872e-03,\n",
       "                      -2.6892e-03,  7.1342e-04, -1.5711e-03,  6.3471e-04,  1.5700e-03,\n",
       "                       2.0587e-03, -2.1495e-04,  1.3122e-03,  7.6859e-04,  1.3097e-03,\n",
       "                       4.0069e-04,  1.3089e-03, -4.1580e-04, -9.2977e-04, -5.3965e-04,\n",
       "                       1.8715e-04,  1.0572e-03,  3.5791e-04, -2.5673e-04, -7.2434e-04,\n",
       "                       2.1634e-03, -1.1418e-03, -1.4714e-04, -6.4496e-04, -7.3982e-04,\n",
       "                       1.1302e-03,  8.9157e-04,  1.5177e-03,  1.3970e-03, -7.1988e-04,\n",
       "                       6.8486e-04,  5.8826e-04,  8.7287e-05, -1.8633e-05,  1.2269e-04,\n",
       "                       1.1819e-03,  2.0959e-04,  1.3513e-03,  1.0171e-03,  8.5133e-04,\n",
       "                       7.9019e-04, -1.8992e-03,  6.9188e-05, -2.0090e-04, -1.4947e-05,\n",
       "                       8.2296e-04,  1.7394e-03, -3.9870e-04, -1.0389e-03, -4.6902e-04,\n",
       "                       1.0807e-04,  7.1329e-04,  2.0870e-04, -9.9298e-04,  9.6324e-04,\n",
       "                       2.5057e-05, -1.5838e-03, -2.0620e-03,  1.1021e-03, -1.1191e-03,\n",
       "                       5.8174e-05, -7.2935e-05, -3.1161e-04,  2.1455e-03, -1.0849e-03,\n",
       "                       9.3084e-04,  5.1715e-04, -1.4936e-03,  1.1985e-03,  9.1074e-04,\n",
       "                       1.3503e-03,  2.6016e-03,  1.3006e-03,  1.1834e-03,  1.3247e-03,\n",
       "                       2.9021e-04, -9.3421e-05, -2.0477e-03,  7.0757e-04,  1.9188e-03,\n",
       "                      -7.4537e-04,  2.7545e-03,  2.0402e-04,  1.8916e-03, -1.4743e-04,\n",
       "                       1.1998e-04, -6.0622e-04, -2.9626e-04,  9.7106e-04, -1.2750e-03,\n",
       "                      -8.1409e-04, -4.2229e-04,  4.1747e-04, -3.8903e-03, -6.7353e-04,\n",
       "                      -6.1084e-05,  4.4722e-04,  1.5943e-04, -6.0266e-04,  7.7707e-04,\n",
       "                      -4.7805e-06,  1.7994e-03,  4.2888e-05,  5.9057e-04, -1.9053e-04,\n",
       "                      -1.3595e-03,  1.0693e-03, -2.5960e-05,  9.2632e-04,  3.4613e-05,\n",
       "                       4.1768e-04,  1.8472e-04,  1.1580e-04, -1.0089e-03,  1.7734e-03,\n",
       "                      -1.2855e-03, -1.2566e-03,  1.1434e-03, -2.9101e-04, -3.0624e-04,\n",
       "                      -6.6592e-04, -6.2216e-04,  8.8469e-04,  8.8327e-05, -6.9409e-05,\n",
       "                      -1.6552e-04, -7.3188e-04,  4.2729e-04, -1.0077e-03,  7.0889e-04,\n",
       "                       5.2830e-04,  1.4502e-03,  6.2777e-04, -2.9698e-04,  6.1219e-04,\n",
       "                       4.9949e-04, -2.1342e-04,  2.6530e-03, -1.7964e-03, -1.5057e-03,\n",
       "                      -1.1545e-03,  8.4841e-04,  1.2475e-03,  8.4561e-04, -1.0021e-04,\n",
       "                      -2.1901e-04,  2.2175e-03, -7.0654e-04,  4.0203e-04, -2.2336e-04,\n",
       "                       1.2369e-03, -6.6010e-04,  4.5980e-04, -9.2627e-05, -1.9773e-03,\n",
       "                      -1.6814e-04, -5.0174e-04,  2.1548e-03, -5.1006e-04,  1.7838e-04,\n",
       "                      -4.4361e-04, -1.4091e-03, -9.7264e-04, -6.4662e-04,  8.1110e-04,\n",
       "                      -5.1362e-04,  1.9722e-03,  4.3724e-04, -5.4441e-04, -1.5291e-04,\n",
       "                       7.8483e-05,  1.0694e-03, -4.7156e-04, -6.5073e-04, -1.0428e-03,\n",
       "                       8.3004e-04, -1.1166e-03,  3.3179e-04,  1.0235e-03,  8.4541e-04,\n",
       "                      -2.9949e-03, -4.8472e-04,  1.0419e-03, -9.0853e-04,  7.7575e-04,\n",
       "                       1.1226e-03,  1.0638e-03,  7.0678e-04,  1.3690e-03,  4.5288e-05,\n",
       "                       8.6883e-04, -1.9236e-04, -2.0240e-03,  7.8285e-04,  2.5963e-04,\n",
       "                       1.4235e-03, -8.2377e-04, -1.2681e-03, -1.4955e-03,  6.9451e-05,\n",
       "                      -2.0471e-03,  3.0484e-04])),\n",
       "             ('transformer.resblocks.1.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0288,  0.0415, -0.0304,  ..., -0.0044, -0.0503, -0.0120],\n",
       "                      [-0.0554, -0.0274, -0.0496,  ...,  0.0449,  0.0816,  0.0139],\n",
       "                      [ 0.0336, -0.0324, -0.0257,  ...,  0.0171, -0.0412, -0.0335],\n",
       "                      ...,\n",
       "                      [-0.0750,  0.0535,  0.0232,  ..., -0.0054, -0.0533, -0.0473],\n",
       "                      [-0.0116,  0.0088,  0.0338,  ..., -0.0057, -0.0123,  0.0154],\n",
       "                      [-0.0250, -0.0119,  0.0244,  ...,  0.0419, -0.0143,  0.0039]])),\n",
       "             ('transformer.resblocks.1.mlp.c_fc.bias',\n",
       "              tensor([-0.0230,  0.0081, -0.0118,  ...,  0.0264, -0.0137,  0.0386])),\n",
       "             ('transformer.resblocks.1.mlp.c_proj.weight',\n",
       "              tensor([[ 9.8953e-04,  5.9639e-05,  7.3232e-03,  ...,  4.4759e-05,\n",
       "                        3.5091e-03,  8.1685e-03],\n",
       "                      [-4.3447e-03, -8.7134e-03, -1.1988e-02,  ...,  3.7066e-03,\n",
       "                       -3.8676e-03,  9.9153e-03],\n",
       "                      [ 2.6333e-03,  3.2195e-03,  9.1827e-03,  ...,  2.1357e-03,\n",
       "                       -5.8614e-03,  8.9763e-03],\n",
       "                      ...,\n",
       "                      [ 1.2765e-02, -1.3748e-02,  8.5348e-04,  ..., -1.7411e-02,\n",
       "                        2.0333e-02,  9.5142e-04],\n",
       "                      [ 5.9059e-03, -1.8698e-02, -6.7441e-03,  ...,  7.2327e-03,\n",
       "                        8.7717e-03, -4.5567e-03],\n",
       "                      [-5.2666e-03,  3.9976e-03,  2.1196e-03,  ...,  1.4854e-02,\n",
       "                        2.0703e-02, -5.3053e-03]])),\n",
       "             ('transformer.resblocks.1.mlp.c_proj.bias',\n",
       "              tensor([-1.2552e-02,  1.5178e-03,  4.7129e-03, -1.3544e-02,  1.4004e-02,\n",
       "                       1.1505e-02, -3.9763e-03, -4.7296e-03, -2.1417e-02,  1.1577e-02,\n",
       "                       1.5547e-02, -5.3428e-03, -1.9047e-02, -1.1564e-05,  1.2991e-02,\n",
       "                       8.8063e-03, -1.1849e-02,  1.5420e-02, -1.8830e-02,  4.3607e-03,\n",
       "                       6.0272e-03, -1.4898e-02, -1.5738e-02, -1.5821e-02, -5.7050e-03,\n",
       "                      -9.6674e-03, -8.6855e-03,  1.3227e-03, -1.8688e-02, -2.0295e-03,\n",
       "                       8.8019e-03,  9.6632e-03,  2.1485e-02, -1.7951e-02, -9.4753e-03,\n",
       "                      -2.0934e-02,  1.2740e-03, -6.5856e-03,  1.7475e-02,  1.8781e-02,\n",
       "                      -2.2495e-02, -3.0251e-03, -1.2490e-02,  8.0612e-03,  8.3916e-03,\n",
       "                      -1.1011e-02,  1.5231e-03, -1.7024e-02, -2.1721e-03, -1.2556e-02,\n",
       "                      -2.1806e-03,  9.0133e-03, -1.8912e-02, -1.4266e-02,  1.1383e-02,\n",
       "                      -2.3261e-04,  7.8287e-04, -5.9371e-03, -1.8131e-02,  1.0878e-02,\n",
       "                       1.2276e-02, -1.3105e-02, -1.0729e-02,  1.9080e-03,  4.8878e-03,\n",
       "                       1.0750e-02, -1.8828e-02,  8.0655e-03,  1.0157e-02, -1.8427e-02,\n",
       "                       3.8381e-03, -1.1786e-02, -1.2972e-02,  1.8793e-02, -2.4444e-03,\n",
       "                       1.3157e-02,  7.1091e-04, -1.3650e-02,  2.2730e-02,  5.8246e-03,\n",
       "                      -8.0756e-03, -2.2136e-03, -1.7172e-02,  1.1444e-02, -1.4251e-02,\n",
       "                      -1.2396e-02, -3.5397e-03,  2.5896e-03,  3.8783e-03, -1.8975e-02,\n",
       "                       1.4311e-02, -1.8547e-02, -7.7287e-03,  1.2525e-02,  3.5587e-03,\n",
       "                      -5.3268e-03,  5.3298e-03, -1.6876e-02,  1.4006e-02, -1.6565e-02,\n",
       "                       1.0538e-02,  1.3293e-02,  1.2022e-02,  1.3728e-02,  2.0026e-02,\n",
       "                      -4.0816e-03, -1.6818e-02,  3.8781e-03, -1.1276e-02,  1.0549e-02,\n",
       "                      -3.7817e-03,  7.3295e-03, -9.5695e-03, -6.9872e-03,  2.0771e-02,\n",
       "                      -2.2632e-03,  3.3699e-03,  2.1698e-03,  1.4791e-02,  1.9799e-02,\n",
       "                       1.9343e-02,  5.8462e-03, -1.0707e-02,  1.7260e-02,  1.8500e-02,\n",
       "                       1.0030e-02,  1.1501e-02, -1.1773e-02, -6.2760e-03,  6.6463e-03,\n",
       "                      -1.8152e-02,  1.0965e-02, -1.7286e-02, -1.5873e-02, -2.1405e-02,\n",
       "                       5.0413e-04, -4.2615e-03,  8.9802e-04,  1.4593e-02,  1.0389e-02,\n",
       "                      -3.3529e-03, -1.4475e-02,  9.2309e-03, -3.1365e-03,  6.3959e-03,\n",
       "                      -9.5858e-03,  2.9811e-03,  2.0824e-02,  2.2679e-02, -1.1187e-02,\n",
       "                      -2.0116e-02,  1.5652e-02,  9.8173e-03, -3.4304e-03,  7.8777e-03,\n",
       "                      -6.4079e-03,  1.8038e-03,  1.7989e-02, -2.2355e-03,  1.4973e-02,\n",
       "                       2.0088e-02,  1.0047e-02,  1.2108e-02, -1.4833e-02,  1.3458e-02,\n",
       "                      -3.8972e-03,  1.0305e-02,  1.8020e-03, -1.5366e-02, -2.1792e-02,\n",
       "                      -3.4767e-03, -9.7686e-04, -1.9731e-02,  1.7126e-02, -1.8181e-03,\n",
       "                       1.4809e-02,  6.9518e-03,  1.8649e-02, -4.4772e-03,  1.3954e-02,\n",
       "                      -5.5424e-04,  8.5287e-03, -9.8983e-03, -1.7911e-02, -2.1258e-02,\n",
       "                       1.4199e-02, -6.0674e-03,  1.3409e-02, -1.1570e-02,  2.1307e-03,\n",
       "                       3.3063e-03, -9.4288e-03, -1.6165e-03, -9.2380e-03, -5.4340e-03,\n",
       "                      -8.8310e-03, -6.9176e-03, -1.9024e-02,  1.8324e-02,  3.2193e-03,\n",
       "                       7.5730e-03,  1.9773e-02,  4.2848e-03, -1.5599e-02,  4.9172e-03,\n",
       "                      -1.2600e-02, -8.9543e-03, -6.6536e-03, -6.5685e-03, -2.3068e-04,\n",
       "                      -1.6960e-02,  1.8786e-02,  4.4473e-03, -7.9454e-03, -1.9205e-02,\n",
       "                      -6.4650e-03, -4.6060e-03,  8.6991e-03, -8.2429e-03,  1.6875e-02,\n",
       "                       1.4933e-02,  1.8862e-02,  7.3228e-03, -1.6638e-02,  2.6916e-03,\n",
       "                      -6.0206e-03, -5.2247e-03,  1.3736e-03,  6.0272e-03, -7.9295e-03,\n",
       "                      -1.4232e-02,  1.9796e-02,  6.0285e-03,  1.8486e-02,  9.7183e-03,\n",
       "                      -1.9438e-02,  1.7219e-02,  4.0359e-03,  2.0426e-02,  1.0735e-02,\n",
       "                       7.0179e-03, -7.2895e-03,  9.1735e-03, -1.9936e-02,  1.7245e-02,\n",
       "                       1.2122e-02, -1.6198e-02,  1.4168e-02, -4.2986e-03, -1.4570e-02,\n",
       "                       2.0438e-02,  1.2803e-02, -1.3804e-02, -1.5284e-02,  1.6333e-02,\n",
       "                      -5.5530e-03,  1.2190e-02,  6.5724e-03,  1.0041e-03, -6.6043e-03,\n",
       "                       2.6850e-03,  2.0771e-02, -1.7656e-02, -2.3717e-02,  1.0856e-02,\n",
       "                      -9.7858e-03, -9.8193e-03, -1.6320e-02,  1.3040e-02,  6.5482e-03,\n",
       "                      -1.6223e-02, -3.6275e-03,  2.0853e-04,  1.6793e-02,  1.3024e-03,\n",
       "                       2.8403e-03, -6.3195e-03, -1.5445e-02, -1.8172e-02,  6.0922e-03,\n",
       "                       2.0878e-02,  1.9200e-02, -1.9502e-02, -1.9266e-02,  7.6684e-04,\n",
       "                       1.8109e-02, -1.8796e-02,  9.6665e-03, -1.5045e-02,  2.0296e-02,\n",
       "                      -1.4725e-02, -6.0194e-03,  1.8594e-02,  1.0848e-02, -1.6777e-02,\n",
       "                      -1.7512e-02,  1.4560e-03, -9.9024e-03, -9.2415e-03,  4.3573e-04,\n",
       "                       5.3215e-03,  3.2930e-03,  2.0380e-02,  1.6871e-02, -1.2925e-02,\n",
       "                       3.2790e-03,  1.5025e-02,  7.5235e-03, -3.2110e-03, -4.1795e-03,\n",
       "                      -3.8863e-03, -1.8779e-02, -1.4355e-02,  5.8031e-03,  1.4101e-02,\n",
       "                       1.4760e-02, -1.5045e-03,  6.7505e-03,  8.7784e-03, -4.2731e-03,\n",
       "                       2.0939e-02, -1.9507e-02, -1.3038e-02, -1.2368e-02,  2.0300e-02,\n",
       "                       9.3385e-03, -2.7551e-03,  1.0820e-02,  1.1191e-02, -2.2257e-03,\n",
       "                      -1.5723e-02, -7.5772e-03,  1.5513e-02,  2.1418e-02,  1.4977e-02,\n",
       "                      -8.5432e-03,  1.6973e-02, -5.3634e-03,  1.2726e-03,  1.6480e-02,\n",
       "                       1.1066e-02, -4.1585e-03, -4.8507e-03, -9.7299e-03, -1.3292e-02,\n",
       "                       1.9633e-02, -6.7923e-03, -1.1923e-02, -2.0964e-02,  9.8989e-03,\n",
       "                       1.0553e-02, -2.0967e-02, -1.0707e-02, -1.3329e-02, -1.5607e-02,\n",
       "                       1.9256e-03, -2.0054e-02,  1.9554e-02,  2.7420e-03, -9.1376e-03,\n",
       "                       1.9369e-02,  2.0149e-02, -4.5931e-03, -1.1046e-02,  1.0219e-02,\n",
       "                      -8.2711e-03, -8.1176e-04,  1.5270e-03, -6.7804e-03,  1.7041e-03,\n",
       "                       1.7638e-02, -2.2655e-03,  1.7384e-02, -4.4960e-03,  1.4367e-02,\n",
       "                      -1.0011e-02,  3.5169e-03,  9.3199e-03, -6.2088e-03, -1.1552e-02,\n",
       "                       1.2052e-02, -3.1756e-03,  2.7337e-03, -1.0527e-02, -2.1189e-02,\n",
       "                       1.7900e-02, -1.2960e-02, -7.1024e-04, -9.1810e-03, -9.6799e-03,\n",
       "                      -1.0655e-02, -2.2082e-02,  1.4847e-02, -1.0552e-02, -2.9725e-03,\n",
       "                      -1.6496e-02, -7.7401e-03, -2.0055e-02,  5.7734e-03,  1.0101e-03,\n",
       "                       1.2736e-02, -1.1875e-02, -1.4376e-02, -1.7306e-02,  3.5090e-03,\n",
       "                      -2.0516e-02, -1.7520e-02, -1.2008e-02,  8.2425e-03,  8.4726e-03,\n",
       "                       2.8114e-04,  1.9298e-02, -6.9560e-03, -1.4020e-02,  2.0484e-02,\n",
       "                      -2.0304e-03, -8.7688e-03, -6.5122e-03,  3.4936e-03,  1.5862e-03,\n",
       "                       1.1313e-02,  9.9250e-03,  1.1659e-02, -1.1217e-02, -2.0840e-02,\n",
       "                      -1.1784e-03,  1.3519e-02, -8.4638e-03, -1.4876e-02, -1.0111e-02,\n",
       "                       5.3046e-03,  8.6559e-03,  7.6758e-03,  2.1061e-02,  1.8709e-02,\n",
       "                       1.9401e-02, -2.1122e-02, -6.4294e-03, -1.0811e-02, -2.0815e-02,\n",
       "                       4.0658e-03,  4.1013e-03,  1.9165e-02, -8.5052e-03, -1.3964e-02,\n",
       "                      -6.8839e-03,  7.4628e-03, -9.9590e-03, -8.8546e-03,  1.9545e-03,\n",
       "                       1.2598e-02,  1.8478e-02,  6.9762e-03,  1.8813e-03,  2.0747e-02,\n",
       "                      -2.1401e-02,  5.3541e-03, -8.6192e-03,  1.8894e-02,  1.8465e-03,\n",
       "                       9.7852e-03,  1.7565e-02, -2.8449e-03,  1.6523e-02, -1.9129e-02,\n",
       "                      -1.1034e-02,  2.0254e-02, -9.3505e-03,  4.8876e-03, -1.5838e-02,\n",
       "                      -1.6048e-04,  1.4641e-02, -1.4339e-02,  1.4102e-02, -1.5941e-02,\n",
       "                       2.0720e-04, -1.9605e-02, -1.8442e-02, -1.8927e-02, -8.9952e-03,\n",
       "                       4.9509e-04, -1.0405e-02, -8.8046e-03,  1.7796e-02,  7.1814e-03,\n",
       "                       6.5564e-03,  1.3150e-02,  9.2907e-03, -3.5211e-03, -1.8482e-02,\n",
       "                      -1.8094e-02,  3.5503e-03, -1.5616e-02,  4.7652e-03,  8.8828e-03,\n",
       "                       1.9963e-03, -7.2376e-03, -1.3153e-02,  9.0163e-03, -1.1443e-02,\n",
       "                      -4.6033e-03, -1.5321e-04, -4.3359e-03,  2.0287e-02, -7.7023e-03,\n",
       "                      -1.5479e-02, -1.7830e-02, -2.9146e-03, -6.0447e-03, -7.3886e-05,\n",
       "                      -7.9130e-03, -1.1641e-02])),\n",
       "             ('transformer.resblocks.1.ln_2.weight',\n",
       "              tensor([1.0023, 1.0012, 1.0015, 1.0025, 1.0028, 0.9997, 1.0014, 1.0015, 1.0028,\n",
       "                      1.0030, 0.9994, 1.0026, 1.0029, 1.0005, 1.0055, 1.0018, 0.9988, 1.0035,\n",
       "                      1.0010, 1.0025, 0.9986, 1.0021, 0.9994, 1.0023, 1.0008, 1.0016, 1.0017,\n",
       "                      1.0023, 1.0025, 1.0005, 0.9992, 0.9996, 1.0041, 1.0018, 0.9986, 1.0004,\n",
       "                      1.0036, 1.0033, 1.0025, 1.0023, 1.0024, 1.0002, 1.0033, 1.0003, 1.0010,\n",
       "                      1.0017, 1.0014, 1.0007, 1.0033, 1.0009, 1.0018, 1.0032, 1.0017, 0.9980,\n",
       "                      1.0015, 1.0035, 1.0030, 1.0020, 1.0009, 1.0056, 1.0032, 1.0031, 0.9998,\n",
       "                      1.0025, 0.9998, 1.0019, 1.0019, 1.0013, 1.0006, 1.0035, 0.9998, 1.0037,\n",
       "                      1.0029, 1.0010, 1.0025, 1.0028, 1.0023, 1.0030, 1.0002, 1.0017, 1.0022,\n",
       "                      1.0018, 1.0018, 1.0018, 1.0044, 0.9985, 1.0005, 1.0039, 0.9999, 1.0007,\n",
       "                      1.0008, 1.0019, 1.0022, 1.0004, 1.0030, 1.0029, 1.0032, 0.9989, 1.0028,\n",
       "                      1.0014, 1.0005, 1.0041, 1.0005, 1.0030, 1.0036, 1.0019, 0.9997, 0.9976,\n",
       "                      1.0019, 1.0032, 1.0026, 1.0008, 1.0038, 1.0026, 1.0016, 1.0018, 1.0018,\n",
       "                      1.0035, 1.0021, 1.0034, 1.0008, 1.0004, 0.9973, 1.0006, 1.0018, 1.0002,\n",
       "                      0.9996, 1.0016, 1.0013, 0.9994, 1.0031, 1.0013, 1.0018, 1.0036, 1.0019,\n",
       "                      1.0003, 1.0014, 1.0010, 0.9985, 1.0007, 1.0021, 1.0047, 0.9983, 0.9996,\n",
       "                      1.0020, 1.0017, 1.0012, 1.0027, 1.0051, 1.0029, 1.0020, 1.0014, 1.0042,\n",
       "                      0.9986, 1.0027, 1.0025, 0.9993, 0.9995, 1.0004, 1.0026, 1.0038, 0.9998,\n",
       "                      0.9998, 0.9993, 1.0024, 1.0003, 1.0009, 1.0021, 1.0022, 1.0028, 1.0015,\n",
       "                      1.0033, 1.0029, 1.0020, 1.0007, 1.0015, 1.0025, 1.0001, 1.0032, 1.0026,\n",
       "                      1.0025, 1.0029, 1.0012, 1.0015, 1.0025, 1.0048, 1.0023, 1.0037, 1.0008,\n",
       "                      1.0033, 1.0028, 1.0019, 1.0018, 1.0033, 1.0033, 0.9997, 1.0011, 1.0037,\n",
       "                      1.0030, 1.0020, 1.0023, 1.0023, 0.9983, 1.0051, 1.0010, 1.0043, 1.0015,\n",
       "                      1.0008, 0.9992, 1.0055, 1.0028, 1.0012, 1.0007, 1.0013, 1.0018, 1.0022,\n",
       "                      1.0024, 1.0021, 1.0049, 1.0025, 1.0032, 1.0006, 1.0026, 1.0031, 1.0018,\n",
       "                      1.0016, 0.9993, 1.0016, 1.0043, 1.0021, 1.0026, 0.9999, 1.0007, 1.0029,\n",
       "                      1.0033, 1.0002, 1.0024, 0.9992, 1.0042, 0.9974, 1.0019, 1.0029, 1.0023,\n",
       "                      1.0022, 0.9977, 1.0033, 1.0023, 1.0013, 1.0033, 1.0020, 1.0021, 1.0013,\n",
       "                      1.0022, 1.0002, 0.9969, 1.0028, 1.0001, 1.0022, 1.0023, 1.0032, 1.0009,\n",
       "                      1.0039, 1.0035, 1.0014, 1.0028, 1.0026, 1.0044, 1.0002, 1.0023, 1.0027,\n",
       "                      1.0018, 1.0014, 1.0029, 1.0006, 1.0010, 1.0031, 1.0018, 0.9993, 1.0045,\n",
       "                      1.0036, 1.0004, 1.0026, 1.0030, 1.0012, 1.0029, 1.0042, 1.0017, 1.0010,\n",
       "                      1.0040, 0.9994, 1.0023, 0.9989, 1.0015, 1.0024, 1.0026, 1.0012, 1.0016,\n",
       "                      1.0051, 1.0029, 1.0014, 1.0012, 0.9993, 1.0036, 1.0044, 1.0019, 1.0026,\n",
       "                      1.0012, 1.0038, 1.0007, 1.0029, 1.0006, 1.0004, 1.0030, 0.9983, 1.0028,\n",
       "                      1.0034, 1.0023, 1.0025, 1.0033, 1.0034, 1.0019, 1.0039, 1.0025, 1.0023,\n",
       "                      1.0038, 1.0004, 1.0021, 0.9991, 1.0005, 1.0034, 0.9999, 1.0033, 0.9997,\n",
       "                      0.9990, 1.0032, 1.0015, 1.0047, 0.9974, 1.0047, 1.0048, 1.0058, 1.0013,\n",
       "                      0.9995, 1.0014, 1.0035, 1.0001, 1.0013, 1.0032, 1.0025, 1.0044, 1.0038,\n",
       "                      1.0008, 1.0019, 1.0018, 1.0001, 1.0019, 1.0002, 1.0016, 1.0021, 1.0031,\n",
       "                      1.0031, 1.0009, 1.0009, 0.9983, 1.0031, 1.0059, 1.0008, 1.0059, 1.0042,\n",
       "                      1.0006, 1.0019, 1.0016, 1.0032, 1.0002, 1.0035, 1.0018, 1.0019, 1.0013,\n",
       "                      1.0012, 1.0039, 1.0037, 1.0003, 1.0010, 1.0004, 1.0028, 1.0042, 1.0022,\n",
       "                      1.0037, 1.0035, 1.0010, 1.0001, 1.0003, 0.9987, 0.9994, 0.9962, 1.0019,\n",
       "                      1.0025, 1.0016, 1.0017, 1.0026, 1.0008, 1.0005, 1.0003, 1.0012, 0.9997,\n",
       "                      1.0002, 1.0022, 0.9989, 1.0005, 1.0000, 1.0040, 1.0026, 1.0018, 1.0046,\n",
       "                      1.0032, 1.0006, 1.0018, 1.0035, 1.0012, 0.9994, 1.0018, 1.0016, 0.9989,\n",
       "                      1.0037, 1.0025, 1.0006, 1.0024, 1.0018, 1.0023, 1.0004, 1.0020, 1.0021,\n",
       "                      1.0023, 1.0008, 1.0040, 1.0021, 1.0028, 1.0039, 1.0028, 1.0030, 1.0027,\n",
       "                      1.0026, 1.0032, 1.0007, 1.0041, 1.0002, 1.0014, 1.0041, 1.0022, 1.0020,\n",
       "                      0.9990, 1.0022, 1.0017, 1.0020, 1.0010, 1.0013, 1.0014, 0.9991, 1.0020,\n",
       "                      1.0016, 1.0037, 1.0013, 1.0015, 1.0016, 1.0018, 1.0030, 0.9986, 1.0012,\n",
       "                      1.0020, 1.0001, 0.9988, 1.0040, 1.0022, 1.0047, 1.0013, 1.0014, 1.0037,\n",
       "                      1.0045, 1.0017, 1.0014, 1.0034, 1.0040, 1.0049, 1.0038, 1.0035, 1.0037,\n",
       "                      0.9999, 1.0000, 1.0045, 1.0018, 1.0007, 1.0012, 1.0005, 1.0037, 1.0040,\n",
       "                      1.0003, 1.0036, 1.0033, 1.0014, 1.0049, 0.9982, 1.0013, 1.0010, 0.9995,\n",
       "                      0.9997, 0.9998, 1.0009, 1.0025, 1.0035, 1.0045, 1.0043, 1.0007])),\n",
       "             ('transformer.resblocks.1.ln_2.bias',\n",
       "              tensor([-1.7531e-04,  1.1325e-03,  8.5695e-05, -4.9151e-05, -1.9804e-04,\n",
       "                       9.4031e-04,  7.3279e-04,  1.3407e-03,  7.8689e-04, -8.5933e-04,\n",
       "                       6.8870e-05, -3.5730e-04,  4.5939e-04, -3.1089e-04,  7.7989e-04,\n",
       "                      -8.0758e-04,  1.0036e-03, -5.9625e-05,  7.9493e-04,  1.2474e-03,\n",
       "                      -1.8400e-04,  5.5826e-04, -1.9515e-03, -9.1973e-04, -7.4602e-05,\n",
       "                      -2.1512e-04, -8.6247e-05,  6.8410e-04,  1.0594e-03, -8.9426e-04,\n",
       "                      -1.2118e-03,  2.1502e-04, -1.9932e-03, -4.5484e-05, -4.6130e-06,\n",
       "                       1.2479e-03, -5.9894e-04,  3.3832e-04,  1.0601e-03, -6.8471e-05,\n",
       "                       1.9417e-04,  2.2735e-03, -5.0904e-04,  1.5236e-03, -1.5296e-03,\n",
       "                       6.9803e-04,  6.4963e-04, -5.9010e-05,  1.7989e-04, -2.8333e-03,\n",
       "                      -4.2725e-04, -1.0492e-03, -2.1238e-03, -2.9892e-04,  1.0041e-03,\n",
       "                      -3.8980e-04,  2.9573e-05,  5.4246e-04,  2.8305e-04,  1.4142e-03,\n",
       "                       4.5401e-04,  9.5024e-04, -1.1101e-03,  1.6300e-04, -3.2568e-05,\n",
       "                       1.0209e-03, -5.1718e-04, -1.0093e-03, -1.7314e-03, -3.0794e-04,\n",
       "                       4.9329e-04,  2.2417e-03,  3.8224e-04, -9.2650e-04,  1.8855e-03,\n",
       "                      -7.1570e-04,  2.6282e-04, -3.8030e-04, -6.1093e-04, -9.3620e-04,\n",
       "                      -1.7670e-04, -2.7358e-04, -2.0462e-03, -3.4203e-04, -3.0499e-04,\n",
       "                      -1.3133e-03, -5.8583e-04, -9.5613e-04, -5.3008e-04, -1.2988e-03,\n",
       "                      -1.6157e-04, -4.7129e-05, -5.3731e-04, -6.2493e-04,  6.7010e-04,\n",
       "                      -6.6985e-05, -8.6038e-04, -3.4460e-04, -5.2382e-04, -5.8637e-04,\n",
       "                       1.9360e-05, -4.2330e-04,  1.9784e-03,  5.9714e-05,  1.0100e-03,\n",
       "                      -2.6553e-05,  2.3213e-03,  7.2481e-04, -1.4372e-03, -9.5178e-04,\n",
       "                      -8.8428e-04, -1.9324e-03, -2.1890e-03, -1.3751e-03,  8.3426e-04,\n",
       "                      -1.3851e-03, -1.1388e-06, -7.7323e-04, -1.0272e-03, -2.0599e-03,\n",
       "                      -5.3378e-04, -1.3899e-03,  9.6581e-05, -1.0406e-03, -9.3266e-04,\n",
       "                      -4.4846e-05,  1.1228e-04, -1.5734e-04,  1.8069e-03, -1.5400e-03,\n",
       "                       5.9841e-04, -1.6487e-03,  1.4709e-04, -1.3561e-03,  1.5764e-03,\n",
       "                       3.3601e-04, -4.8169e-04, -3.2868e-04, -8.6204e-04, -2.7726e-04,\n",
       "                      -2.3913e-03, -1.5025e-03,  1.0094e-03, -1.8545e-03,  4.5471e-04,\n",
       "                      -1.5764e-03,  1.9878e-03,  5.7371e-04,  1.3585e-03, -4.7827e-04,\n",
       "                       7.1912e-04, -2.0861e-03,  8.2077e-04,  8.3292e-04,  8.5135e-04,\n",
       "                      -6.6636e-04, -5.6541e-04,  1.4785e-03,  7.3206e-04,  6.0834e-04,\n",
       "                      -1.4826e-03, -7.8589e-04, -2.9711e-04,  2.6691e-04,  1.3190e-03,\n",
       "                       1.0852e-03,  2.4316e-04,  2.0735e-05, -6.8169e-04,  2.7380e-04,\n",
       "                      -1.2401e-03, -1.8042e-04, -1.8289e-03, -9.9907e-04, -3.3674e-04,\n",
       "                      -1.8691e-03, -6.6203e-04,  1.0401e-03, -1.0595e-03,  9.5027e-04,\n",
       "                      -6.9710e-04, -4.2813e-04,  8.1192e-04,  2.1216e-03, -2.2067e-03,\n",
       "                       5.3782e-04, -9.8133e-05,  2.7927e-04, -2.4901e-03,  8.2238e-04,\n",
       "                       1.5682e-03,  6.9675e-04,  6.3293e-05, -6.3719e-04, -7.7844e-04,\n",
       "                      -7.8678e-05,  8.7726e-04,  1.5670e-05, -6.7790e-05,  1.4307e-03,\n",
       "                       1.8813e-03, -1.6860e-03,  9.3880e-04,  1.8345e-03, -4.6774e-04,\n",
       "                       1.3847e-04, -7.9204e-04,  1.4519e-03,  1.1647e-03,  8.6700e-04,\n",
       "                      -9.2026e-04, -1.5102e-03, -1.3366e-03,  2.4530e-03,  1.9541e-04,\n",
       "                      -6.2207e-04,  5.9372e-04, -1.0828e-03,  8.5876e-04,  1.3112e-03,\n",
       "                       6.2531e-04,  4.7379e-05, -2.4044e-04, -7.5248e-04, -2.1958e-03,\n",
       "                       1.4836e-03,  1.8066e-03,  1.2544e-03, -5.2126e-04,  4.3538e-04,\n",
       "                      -1.5960e-03, -9.2726e-04, -1.0406e-03, -5.3068e-04, -4.7262e-04,\n",
       "                       6.6950e-04, -1.2520e-04,  1.3417e-03, -9.1301e-04,  2.1527e-03,\n",
       "                      -1.7645e-03,  5.4476e-04, -2.9223e-05,  4.8417e-04, -9.4653e-05,\n",
       "                      -6.1643e-04,  1.0906e-03, -9.5584e-04, -2.9231e-04,  6.8181e-04,\n",
       "                      -2.3615e-03,  1.4820e-04, -6.1740e-04, -3.9502e-04,  1.2951e-03,\n",
       "                       3.9347e-03,  4.2468e-04,  4.6001e-04,  1.9090e-03,  1.5321e-04,\n",
       "                       1.7961e-03, -2.4236e-04,  3.4199e-04,  1.4368e-03,  4.4133e-04,\n",
       "                       1.3114e-03, -1.9766e-03, -7.0770e-04,  5.5797e-04, -1.3865e-03,\n",
       "                       2.1378e-04,  1.4856e-03, -5.1598e-04,  1.1652e-03, -9.3211e-04,\n",
       "                      -1.4044e-04, -3.1224e-05,  1.3215e-03, -1.0012e-03, -3.9408e-04,\n",
       "                      -1.1850e-03,  4.8692e-04,  3.7978e-04,  9.7262e-04, -1.5960e-03,\n",
       "                       1.6402e-03,  1.2064e-03, -8.4783e-05,  2.8199e-04,  1.9107e-03,\n",
       "                       8.7505e-04, -1.7858e-03,  5.5013e-04,  2.6085e-04, -5.9738e-04,\n",
       "                       4.9359e-04, -1.6162e-03, -1.8104e-03,  6.4926e-05,  7.2889e-04,\n",
       "                       2.5399e-03,  2.3860e-03,  3.1295e-04,  1.2445e-03,  7.0804e-04,\n",
       "                       1.7478e-03,  6.2246e-04, -6.3554e-06,  1.2558e-03, -5.3389e-04,\n",
       "                       1.3527e-03, -1.0884e-03, -1.3425e-03,  1.1917e-03, -1.3737e-03,\n",
       "                      -1.0408e-03,  3.8343e-04,  1.3584e-03, -6.5066e-04, -2.9019e-05,\n",
       "                      -4.7903e-04, -1.1613e-03, -2.0707e-04,  2.2595e-04,  2.2027e-03,\n",
       "                       7.6569e-04, -8.2208e-04, -1.9935e-04, -1.4450e-04, -2.4240e-04,\n",
       "                      -2.3028e-03,  1.0964e-03, -1.2918e-04,  1.4632e-03,  4.1980e-04,\n",
       "                      -3.6541e-04, -1.1103e-03, -1.4534e-03, -2.0827e-03,  6.1462e-05,\n",
       "                      -2.0315e-03, -3.6778e-04, -7.1758e-04, -7.0943e-05, -8.7207e-04,\n",
       "                      -6.5221e-04, -1.0429e-05, -6.3917e-04, -1.4582e-04, -6.3740e-04,\n",
       "                      -1.5393e-03, -2.6705e-04,  7.8633e-04,  1.0681e-03,  9.8650e-04,\n",
       "                      -3.8191e-04, -1.7546e-03,  1.2735e-03, -7.1221e-04,  5.6516e-04,\n",
       "                      -7.3274e-04,  2.5629e-04, -9.1867e-04,  9.5285e-05, -7.0983e-04,\n",
       "                      -8.9380e-04, -7.4184e-04, -1.0367e-03, -2.7533e-03,  8.3803e-04,\n",
       "                       9.6746e-04, -2.3838e-03, -4.7251e-04,  1.1507e-03, -7.8580e-04,\n",
       "                      -2.8550e-04, -6.0086e-04,  1.0170e-03, -5.4699e-04, -1.2185e-04,\n",
       "                      -1.2701e-03,  1.8962e-04, -2.0373e-05, -5.3270e-04, -8.9792e-04,\n",
       "                      -1.0434e-03, -1.0353e-04,  5.4145e-04, -8.4580e-04, -4.2091e-04,\n",
       "                      -1.9959e-04, -1.2182e-03, -9.5107e-04,  1.6642e-03,  1.4110e-03,\n",
       "                      -1.3678e-03,  6.2536e-04,  5.0817e-04, -7.3478e-04, -1.0014e-03,\n",
       "                       1.6463e-04, -1.2005e-04, -1.7800e-03,  1.8123e-03, -1.6538e-04,\n",
       "                      -1.0378e-03, -9.3386e-04,  5.4809e-04,  5.4639e-04, -9.4138e-04,\n",
       "                      -1.8992e-03, -1.5581e-03, -5.2702e-04, -7.7125e-04,  2.9691e-04,\n",
       "                      -2.0104e-05, -3.2568e-03,  7.0274e-04,  6.8795e-05, -1.2523e-03,\n",
       "                      -1.2092e-03,  8.5016e-04, -1.7662e-03,  6.4849e-04,  1.2488e-03,\n",
       "                       1.8173e-03, -9.3038e-04,  1.0152e-03, -5.6534e-04, -9.8509e-04,\n",
       "                       5.5720e-04,  9.0832e-05,  6.2413e-04, -3.5534e-04, -5.1995e-04,\n",
       "                      -7.3027e-05,  2.9697e-03,  3.6348e-04, -9.6433e-05, -1.3858e-03,\n",
       "                      -1.0298e-03, -2.1538e-03, -8.8415e-04,  2.2685e-04, -3.8461e-04,\n",
       "                       8.3534e-04, -3.7923e-04,  6.5326e-04, -6.9336e-04, -2.2569e-04,\n",
       "                      -1.3832e-03, -6.8627e-04,  1.1389e-03, -4.4038e-04, -3.3680e-04,\n",
       "                       4.4599e-04,  1.3368e-03,  1.3626e-03, -1.3768e-03, -7.2915e-05,\n",
       "                      -1.3246e-05, -1.1867e-03,  2.1866e-03,  1.8890e-04,  1.8691e-03,\n",
       "                       8.9357e-04, -8.4880e-04, -5.5885e-04, -1.0045e-03,  6.5294e-04,\n",
       "                      -3.0596e-04,  4.2054e-04,  1.0185e-03, -1.5039e-03,  1.5547e-03,\n",
       "                       2.2426e-03,  8.0436e-04,  8.4806e-04, -1.3863e-03, -9.5023e-05,\n",
       "                       1.6360e-03,  7.7671e-04,  9.6099e-04,  2.8871e-03,  1.5773e-03,\n",
       "                      -1.3453e-03,  1.9321e-03, -1.4288e-04,  1.3516e-03,  1.0959e-03,\n",
       "                      -2.6969e-05,  2.0077e-03,  1.3502e-03,  3.1677e-04, -1.3286e-04,\n",
       "                      -1.0545e-03,  1.0882e-03,  8.9698e-04, -2.1083e-04,  1.7430e-03,\n",
       "                       5.0498e-05,  2.0618e-03,  2.3635e-03, -1.7848e-03, -1.7961e-03,\n",
       "                       3.7661e-04,  1.0689e-03, -6.1276e-04,  2.4085e-03, -6.6575e-04,\n",
       "                       1.8265e-04,  9.9780e-04])),\n",
       "             ('transformer.resblocks.2.attn.in_proj_weight',\n",
       "              tensor([[-0.0007, -0.0477, -0.0097,  ...,  0.0352,  0.1438, -0.0686],\n",
       "                      [-0.0065,  0.0114,  0.0644,  ...,  0.0504,  0.0361, -0.0444],\n",
       "                      [ 0.0259,  0.0060,  0.0076,  ...,  0.0322,  0.0029,  0.0257],\n",
       "                      ...,\n",
       "                      [-0.0206, -0.0454, -0.0010,  ..., -0.1015, -0.0443, -0.0271],\n",
       "                      [ 0.0135,  0.0225,  0.0503,  ...,  0.0383, -0.0516,  0.0661],\n",
       "                      [-0.0427, -0.0005,  0.0876,  ..., -0.1104,  0.0324,  0.0689]])),\n",
       "             ('transformer.resblocks.2.attn.in_proj_bias',\n",
       "              tensor([ 1.3126e-03,  9.2339e-04,  2.2762e-05,  ..., -5.1400e-04,\n",
       "                       7.3806e-05, -3.3662e-04])),\n",
       "             ('transformer.resblocks.2.attn.out_proj.weight',\n",
       "              tensor([[ 0.0004, -0.0007,  0.0074,  ..., -0.0078, -0.0100, -0.0011],\n",
       "                      [-0.0084,  0.0010, -0.0126,  ...,  0.0052, -0.0084, -0.0007],\n",
       "                      [-0.0017, -0.0163,  0.0077,  ..., -0.0095, -0.0064,  0.0080],\n",
       "                      ...,\n",
       "                      [ 0.0053, -0.0154,  0.0037,  ..., -0.0040, -0.0200,  0.0065],\n",
       "                      [-0.0140,  0.0058,  0.0051,  ..., -0.0073,  0.0108, -0.0085],\n",
       "                      [-0.0044, -0.0054,  0.0134,  ...,  0.0011, -0.0104, -0.0049]])),\n",
       "             ('transformer.resblocks.2.attn.out_proj.bias',\n",
       "              tensor([ 1.0788e-03, -7.0583e-05,  3.2976e-04,  5.5309e-05,  1.8382e-03,\n",
       "                      -6.0728e-04,  2.2773e-04, -2.1526e-04, -4.8404e-04, -3.0273e-04,\n",
       "                       5.1810e-04, -8.3277e-04,  1.0824e-04, -4.5048e-04, -2.3310e-04,\n",
       "                       9.2724e-04,  4.1237e-04,  4.7027e-04, -1.8701e-04,  2.0805e-04,\n",
       "                       2.7908e-04, -3.2525e-04,  1.2453e-03,  6.9027e-04,  6.4053e-04,\n",
       "                      -2.1366e-04, -1.1909e-03, -7.1214e-04,  6.3259e-04, -1.2775e-03,\n",
       "                       1.5964e-03,  2.4853e-04, -2.0982e-04, -1.3061e-03, -7.4067e-04,\n",
       "                       5.6419e-04, -3.4099e-04,  7.0211e-04,  6.7352e-04, -1.3441e-03,\n",
       "                      -4.7957e-04, -9.3063e-04, -5.5904e-04,  4.2345e-04, -1.3661e-03,\n",
       "                       2.5806e-04, -4.6396e-04,  5.5423e-04,  6.6091e-06, -9.1748e-04,\n",
       "                      -4.0955e-04, -2.3621e-04, -5.5982e-04, -2.2214e-04,  1.5596e-03,\n",
       "                       6.4546e-04,  1.7707e-04,  8.9775e-04,  1.1972e-04,  2.4123e-04,\n",
       "                      -2.3887e-04, -1.1946e-03, -4.2026e-04,  1.7873e-04, -3.0884e-04,\n",
       "                      -3.0755e-05,  2.5013e-04, -5.9692e-04, -1.0264e-04,  2.4260e-04,\n",
       "                      -7.2011e-04,  1.9058e-05, -2.9902e-04,  9.6719e-05, -1.2158e-03,\n",
       "                      -8.0117e-05, -7.8910e-04,  3.9300e-04,  1.9539e-04, -2.3336e-04,\n",
       "                      -5.0787e-04,  1.3985e-03,  1.6600e-03,  1.0836e-03,  3.0494e-06,\n",
       "                       2.4455e-04, -4.3761e-04, -1.7686e-03,  6.4135e-04,  2.2536e-04,\n",
       "                       6.9802e-04, -1.0627e-04,  8.9352e-05, -1.5041e-03,  4.5531e-04,\n",
       "                      -5.7663e-04,  2.2210e-04, -2.9919e-04, -4.4846e-04,  4.2794e-04,\n",
       "                       9.4707e-04, -3.4071e-04,  6.9957e-04, -5.7645e-04,  3.6512e-04,\n",
       "                      -2.1415e-04,  5.4139e-04,  4.7210e-04,  9.5310e-05,  5.1223e-04,\n",
       "                      -6.4295e-04,  3.8983e-04, -5.1591e-04, -3.2967e-04, -3.4366e-04,\n",
       "                      -9.5598e-04,  6.0265e-04,  1.0413e-04, -2.3267e-04, -1.2103e-03,\n",
       "                      -5.4584e-04,  2.0285e-04, -6.1499e-04,  6.0621e-04,  9.8089e-04,\n",
       "                       2.8191e-04, -2.9952e-04, -4.5847e-05,  6.0392e-04,  1.0396e-04,\n",
       "                       4.3177e-04,  6.7270e-04, -7.2846e-04, -9.5107e-04, -3.5862e-04,\n",
       "                       3.3390e-05, -8.4610e-04, -6.0319e-05,  2.7791e-04,  1.5646e-04,\n",
       "                      -3.9709e-04,  7.4023e-05,  3.4979e-04,  1.9138e-03, -1.3593e-03,\n",
       "                       4.7592e-04, -4.7401e-04,  7.7366e-05,  3.0934e-05,  8.1610e-04,\n",
       "                       1.4155e-03,  1.9201e-03,  2.3244e-04,  1.3913e-03, -7.5672e-04,\n",
       "                       1.4882e-03, -5.1688e-04, -8.8179e-05,  6.6690e-04,  2.9419e-04,\n",
       "                       4.8436e-05,  7.1962e-04,  2.6451e-04,  5.0492e-05, -1.0588e-03,\n",
       "                      -5.4774e-05, -5.2343e-05,  1.0464e-04, -2.7150e-04, -4.1247e-04,\n",
       "                       2.0257e-04, -3.7790e-04,  6.5057e-04, -9.6552e-04, -7.5995e-05,\n",
       "                      -1.0471e-03, -6.1579e-04, -6.4189e-04, -4.3193e-04, -3.0548e-04,\n",
       "                      -1.6856e-04,  1.4702e-04, -3.9176e-04,  1.9407e-04,  2.8180e-06,\n",
       "                      -1.0685e-03,  1.9275e-04,  7.0346e-04, -1.6894e-04, -8.9843e-04,\n",
       "                       5.3915e-04,  7.0277e-04, -8.0556e-04, -4.4410e-04,  1.8077e-04,\n",
       "                      -5.3031e-04,  5.1932e-04,  1.9905e-04,  7.8995e-04,  9.8020e-05,\n",
       "                       3.6028e-04,  1.7358e-05, -9.3899e-05,  1.4030e-03, -4.1572e-04,\n",
       "                       1.5576e-03,  1.0847e-04, -1.8568e-04,  3.6438e-04,  1.7861e-04,\n",
       "                       1.8369e-03, -1.2600e-03,  2.7639e-04,  9.2959e-04, -4.7694e-04,\n",
       "                      -4.9693e-04,  1.0255e-03,  7.3612e-04, -6.3738e-04, -3.2830e-04,\n",
       "                      -1.7013e-03, -8.8892e-04,  6.6115e-04,  1.5598e-03,  1.0114e-04,\n",
       "                       3.3394e-04, -9.2118e-04, -1.4201e-03, -3.6957e-04,  6.3859e-04,\n",
       "                       1.3185e-03,  5.3853e-04, -2.0258e-04, -4.5248e-04,  1.0791e-03,\n",
       "                       5.2073e-04,  6.1999e-04, -3.4162e-04,  1.5875e-03,  5.9377e-04,\n",
       "                       1.4183e-04, -8.8916e-04,  5.1975e-04,  1.2660e-04, -9.2949e-04,\n",
       "                       2.2943e-04, -7.3619e-04,  6.3960e-04, -7.2609e-04,  1.6218e-04,\n",
       "                       5.3813e-04, -4.1680e-04, -3.9633e-04, -1.5390e-04, -1.1866e-03,\n",
       "                       4.0020e-04, -1.2720e-03,  7.8017e-04,  1.6455e-04, -1.2908e-03,\n",
       "                       1.2463e-04, -1.5802e-05,  8.9100e-04, -1.1256e-03, -3.0600e-05,\n",
       "                       1.2639e-04,  1.4033e-04,  1.4958e-03,  2.4708e-04, -3.1140e-04,\n",
       "                      -3.8112e-04, -1.4629e-04, -6.0581e-04,  3.6817e-04, -5.5054e-04,\n",
       "                       3.9778e-05, -1.2278e-04, -2.1709e-04,  1.0626e-03, -6.0697e-04,\n",
       "                       6.7491e-04,  1.5049e-03,  2.9085e-05, -1.4303e-03,  8.0451e-05,\n",
       "                       6.7944e-04,  1.4171e-03,  5.6739e-04,  1.0099e-04, -1.4261e-03,\n",
       "                       4.8575e-04, -8.0646e-04, -2.4492e-04,  1.0417e-03,  9.4370e-04,\n",
       "                      -3.8685e-05, -6.2090e-04,  8.4426e-04,  9.4159e-04, -8.0211e-04,\n",
       "                      -1.0534e-04, -1.0362e-05,  1.1322e-03, -2.6193e-04, -7.9493e-04,\n",
       "                      -1.8730e-04, -3.1713e-04, -7.8827e-05, -7.0631e-04, -7.6681e-04,\n",
       "                      -2.4357e-05,  4.7177e-04,  4.0871e-04, -3.6200e-05, -7.3406e-04,\n",
       "                      -1.0590e-04, -2.0656e-04,  6.1089e-04, -3.9895e-05,  4.1355e-04,\n",
       "                       8.4839e-04, -2.1535e-03,  1.8341e-04,  1.3777e-04, -6.8699e-04,\n",
       "                      -9.9359e-06, -1.4356e-04, -9.9310e-04, -6.1075e-04, -4.3136e-04,\n",
       "                      -1.7524e-03, -5.9916e-04, -4.4398e-05, -8.3048e-05,  1.0870e-03,\n",
       "                      -3.2931e-04, -8.0827e-04, -7.9985e-05, -2.0494e-04,  9.9773e-04,\n",
       "                      -6.4604e-04,  8.5203e-04, -5.3238e-04,  3.9169e-04, -6.6036e-04,\n",
       "                       4.2416e-04, -4.1965e-04, -1.8193e-04, -1.5361e-03,  2.8673e-04,\n",
       "                      -3.0903e-04, -6.8094e-04, -1.1882e-03, -1.1868e-03,  7.7630e-04,\n",
       "                      -2.0596e-03,  1.3206e-04,  1.0915e-03, -1.5177e-04, -5.3107e-04,\n",
       "                      -1.2611e-04, -2.4533e-04,  1.2457e-03,  1.1482e-03, -6.4767e-04,\n",
       "                       7.4927e-04, -3.6312e-04, -2.5870e-05,  1.5398e-03,  4.6609e-04,\n",
       "                      -4.3740e-04, -6.5781e-05, -9.1245e-04, -4.0970e-04, -2.1774e-04,\n",
       "                       9.3327e-04,  2.4129e-04,  1.5777e-04, -8.7031e-04, -8.4288e-04,\n",
       "                       2.8838e-04, -1.2539e-03, -5.2089e-04, -3.4935e-04,  3.5709e-04,\n",
       "                       3.5417e-04,  9.5692e-04,  9.8879e-04,  4.3006e-04, -1.0650e-04,\n",
       "                      -7.0912e-04,  7.0298e-05,  5.8975e-04,  5.7852e-04, -5.7818e-04,\n",
       "                       2.7570e-04, -1.2301e-04,  1.5596e-04, -6.6864e-05,  4.8282e-04,\n",
       "                      -3.0710e-04, -1.0350e-03, -6.6080e-04,  1.3351e-04,  1.3865e-03,\n",
       "                       4.1617e-04,  4.5625e-04, -7.8825e-04,  4.9521e-04,  2.5923e-05,\n",
       "                       4.0810e-04, -8.1699e-04,  4.2258e-04,  4.8234e-04,  8.6421e-04,\n",
       "                      -1.3123e-03,  7.7346e-04, -3.6454e-04, -5.8584e-04,  1.0098e-03,\n",
       "                      -1.1580e-04, -5.4598e-04,  1.4967e-03, -2.0328e-04, -7.3160e-04,\n",
       "                       2.3529e-06, -4.3107e-05, -4.1007e-04,  7.0130e-04, -1.7926e-04,\n",
       "                      -1.1366e-03,  2.5910e-03,  5.5488e-04,  4.9055e-04, -6.2487e-04,\n",
       "                      -1.6388e-03,  2.5095e-04, -9.9801e-04, -5.1282e-04,  4.7347e-04,\n",
       "                      -9.4108e-04,  8.2245e-04,  3.7064e-04,  5.6703e-04, -1.0893e-03,\n",
       "                      -7.5438e-04, -9.6756e-04, -1.1849e-03,  1.2655e-04,  1.9076e-03,\n",
       "                       1.2258e-03,  1.2572e-03, -2.5650e-04,  8.5367e-04,  1.1700e-04,\n",
       "                       1.6480e-04,  1.0927e-04, -3.8082e-04, -2.7805e-04, -1.7614e-04,\n",
       "                      -1.9891e-04, -2.3209e-04, -5.8167e-04,  1.0508e-03,  8.4568e-04,\n",
       "                      -2.1456e-04,  8.7826e-04, -5.1724e-04, -3.8735e-04, -2.5477e-04,\n",
       "                      -1.3751e-03, -5.0870e-04,  6.1244e-05,  6.8089e-04, -7.9311e-04,\n",
       "                       5.1918e-04, -1.2979e-03, -1.2205e-04, -1.3645e-04,  7.5354e-04,\n",
       "                      -1.0016e-03,  1.2364e-04,  4.1173e-04,  9.0910e-04, -2.2905e-04,\n",
       "                       5.5227e-04, -6.8065e-04,  2.2228e-04,  6.9378e-04, -3.5166e-04,\n",
       "                       1.8163e-03,  2.0868e-04, -7.3755e-04,  2.4698e-04, -8.1239e-04,\n",
       "                      -7.2992e-04, -6.6769e-06, -8.2464e-06,  7.7072e-04, -2.9898e-04,\n",
       "                      -6.9937e-05,  8.6123e-05, -1.2876e-04,  2.9448e-04, -3.1574e-04,\n",
       "                      -6.7214e-04, -3.5098e-04, -3.2520e-04,  1.6787e-04,  5.2162e-04,\n",
       "                       8.8198e-04, -4.5967e-04])),\n",
       "             ('transformer.resblocks.2.ln_1.weight',\n",
       "              tensor([1.0004, 0.9991, 0.9978, 0.9982, 1.0014, 1.0031, 0.9967, 1.0000, 0.9982,\n",
       "                      0.9991, 0.9963, 1.0004, 0.9974, 1.0000, 0.9984, 1.0007, 0.9971, 1.0001,\n",
       "                      1.0000, 1.0011, 1.0020, 0.9995, 1.0003, 1.0030, 0.9996, 0.9976, 0.9998,\n",
       "                      0.9995, 0.9986, 0.9963, 0.9984, 1.0001, 0.9995, 0.9987, 0.9995, 0.9991,\n",
       "                      1.0021, 0.9981, 1.0002, 1.0002, 1.0020, 0.9998, 1.0011, 0.9973, 0.9977,\n",
       "                      1.0003, 0.9995, 0.9978, 1.0004, 0.9988, 1.0016, 1.0000, 0.9988, 0.9990,\n",
       "                      0.9972, 0.9954, 1.0003, 0.9969, 0.9992, 0.9991, 0.9986, 0.9996, 0.9987,\n",
       "                      0.9982, 0.9990, 0.9989, 0.9984, 1.0006, 0.9996, 1.0007, 0.9986, 0.9998,\n",
       "                      0.9969, 0.9963, 0.9977, 1.0002, 0.9982, 0.9982, 0.9985, 1.0024, 0.9995,\n",
       "                      0.9991, 0.9993, 0.9989, 0.9980, 0.9999, 1.0009, 0.9985, 0.9980, 0.9977,\n",
       "                      0.9997, 0.9972, 1.0002, 0.9952, 0.9985, 0.9991, 0.9988, 0.9974, 0.9987,\n",
       "                      0.9989, 0.9965, 0.9990, 0.9997, 0.9994, 0.9979, 0.9981, 0.9972, 0.9960,\n",
       "                      0.9997, 0.9991, 0.9988, 1.0005, 1.0013, 0.9995, 0.9988, 0.9971, 0.9998,\n",
       "                      0.9982, 1.0012, 1.0007, 1.0009, 0.9977, 0.9991, 0.9981, 0.9999, 1.0000,\n",
       "                      1.0015, 0.9974, 1.0001, 1.0003, 0.9987, 0.9991, 1.0017, 0.9998, 0.9987,\n",
       "                      0.9974, 1.0019, 0.9990, 1.0002, 1.0019, 0.9984, 1.0028, 0.9996, 0.9968,\n",
       "                      0.9969, 0.9992, 0.9975, 0.9977, 1.0007, 1.0015, 0.9997, 1.0028, 1.0003,\n",
       "                      0.9983, 0.9998, 0.9995, 0.9991, 1.0000, 0.9993, 0.9986, 0.9975, 0.9991,\n",
       "                      0.9982, 0.9973, 1.0001, 0.9992, 0.9981, 0.9983, 0.9991, 1.0000, 1.0016,\n",
       "                      0.9969, 0.9988, 0.9987, 0.9952, 0.9981, 0.9998, 0.9984, 1.0005, 0.9993,\n",
       "                      1.0002, 0.9960, 1.0021, 0.9974, 1.0012, 0.9990, 0.9973, 0.9994, 0.9995,\n",
       "                      0.9985, 0.9987, 1.0000, 0.9975, 0.9999, 0.9982, 1.0003, 0.9976, 0.9986,\n",
       "                      1.0003, 0.9982, 0.9983, 1.0005, 0.9972, 0.9981, 1.0039, 0.9968, 0.9982,\n",
       "                      0.9992, 0.9980, 1.0013, 1.0000, 1.0003, 0.9994, 1.0001, 0.9979, 0.9990,\n",
       "                      0.9989, 0.9994, 1.0000, 0.9987, 0.9999, 1.0001, 0.9999, 1.0002, 0.9985,\n",
       "                      1.0012, 0.9963, 0.9975, 0.9978, 0.9997, 0.9967, 0.9955, 1.0000, 0.9987,\n",
       "                      0.9977, 0.9971, 1.0008, 0.9952, 0.9981, 1.0002, 1.0022, 1.0021, 0.9961,\n",
       "                      1.0002, 0.9997, 1.0013, 0.9980, 0.9970, 1.0000, 1.0020, 0.9979, 0.9990,\n",
       "                      0.9996, 0.9996, 0.9987, 0.9979, 0.9997, 1.0012, 0.9975, 0.9989, 0.9989,\n",
       "                      0.9980, 0.9972, 0.9992, 0.9981, 1.0020, 1.0011, 0.9988, 0.9948, 0.9977,\n",
       "                      0.9964, 0.9995, 1.0010, 0.9979, 0.9983, 0.9960, 0.9962, 1.0008, 0.9984,\n",
       "                      0.9988, 1.0002, 1.0000, 0.9984, 0.9989, 0.9976, 0.9963, 0.9958, 1.0018,\n",
       "                      1.0001, 0.9983, 0.9989, 1.0006, 0.9983, 1.0001, 1.0015, 0.9999, 0.9979,\n",
       "                      0.9987, 0.9992, 0.9983, 1.0015, 1.0020, 1.0020, 1.0005, 1.0003, 0.9987,\n",
       "                      0.9952, 0.9988, 0.9979, 0.9988, 0.9968, 0.9986, 0.9968, 1.0009, 0.9994,\n",
       "                      1.0000, 1.0000, 0.9980, 0.9998, 0.9985, 0.9996, 0.9987, 0.9970, 0.9968,\n",
       "                      1.0026, 0.9972, 1.0002, 0.9990, 0.9991, 0.9996, 0.9991, 0.9990, 0.9997,\n",
       "                      1.0007, 0.9988, 0.9980, 0.9985, 0.9961, 0.9985, 1.0005, 1.0003, 0.9992,\n",
       "                      0.9982, 1.0001, 0.9983, 1.0036, 0.9974, 0.9977, 1.0008, 0.9980, 0.9998,\n",
       "                      1.0016, 1.0004, 1.0004, 0.9982, 0.9977, 0.9973, 1.0007, 0.9978, 0.9989,\n",
       "                      0.9997, 0.9994, 0.9989, 0.9990, 0.9980, 1.0016, 0.9995, 0.9996, 0.9989,\n",
       "                      1.0021, 0.9985, 1.0001, 0.9986, 0.9993, 0.9996, 0.9970, 0.9976, 0.9996,\n",
       "                      0.9998, 0.9987, 0.9990, 0.9994, 0.9981, 0.9995, 1.0017, 0.9993, 1.0011,\n",
       "                      0.9993, 0.9974, 0.9990, 0.9981, 0.9958, 0.9995, 0.9982, 1.0009, 0.9958,\n",
       "                      0.9994, 0.9995, 1.0005, 1.0005, 0.9974, 0.9969, 1.0001, 1.0002, 0.9969,\n",
       "                      0.9978, 1.0001, 1.0011, 1.0019, 1.0004, 0.9978, 1.0001, 0.9978, 1.0004,\n",
       "                      1.0021, 0.9998, 1.0000, 0.9997, 0.9997, 0.9983, 0.9992, 0.9972, 0.9983,\n",
       "                      1.0033, 1.0004, 0.9985, 0.9974, 0.9989, 1.0011, 0.9953, 0.9996, 0.9973,\n",
       "                      0.9990, 0.9994, 0.9967, 1.0007, 0.9979, 1.0011, 0.9991, 1.0045, 0.9989,\n",
       "                      0.9989, 0.9998, 0.9966, 0.9985, 0.9981, 0.9982, 0.9964, 0.9972, 0.9988,\n",
       "                      1.0004, 0.9977, 0.9972, 0.9976, 1.0000, 1.0015, 1.0007, 0.9960, 0.9992,\n",
       "                      0.9984, 0.9984, 1.0004, 0.9999, 0.9993, 0.9974, 1.0036, 0.9998, 1.0001,\n",
       "                      0.9986, 0.9976, 0.9985, 0.9980, 0.9970, 0.9991, 0.9994, 1.0010, 0.9985,\n",
       "                      0.9977, 0.9974, 0.9989, 0.9997, 1.0001, 0.9991, 0.9997, 0.9989, 0.9980,\n",
       "                      0.9976, 0.9977, 1.0017, 0.9981, 0.9995, 0.9949, 1.0018, 0.9984, 0.9986,\n",
       "                      0.9992, 1.0011, 1.0005, 1.0006, 0.9997, 0.9992, 1.0018, 0.9990, 0.9994,\n",
       "                      1.0002, 0.9971, 0.9973, 0.9981, 0.9992, 0.9990, 0.9987, 0.9984])),\n",
       "             ('transformer.resblocks.2.ln_1.bias',\n",
       "              tensor([-4.4546e-04,  8.8843e-04, -4.1047e-04,  7.2467e-06, -8.9200e-04,\n",
       "                      -1.3068e-03, -6.1239e-04, -4.0535e-04,  9.7893e-04,  8.9550e-04,\n",
       "                       1.2836e-03,  1.3907e-04,  2.6542e-03,  1.6187e-04, -4.4291e-04,\n",
       "                      -1.1143e-03, -2.2917e-04, -1.7242e-03, -6.6833e-05, -1.1338e-03,\n",
       "                       7.2650e-04,  3.7882e-04, -7.4557e-04, -1.5618e-04, -1.3321e-03,\n",
       "                      -7.9174e-04,  6.0709e-04,  1.0031e-03,  3.6306e-05,  7.4209e-04,\n",
       "                       1.7536e-03,  2.4772e-03, -7.0597e-04,  2.3468e-03,  1.4676e-04,\n",
       "                       1.3034e-03, -1.9029e-03, -1.8199e-03, -8.4511e-04, -1.4899e-03,\n",
       "                      -1.5577e-03, -4.1813e-04,  3.7513e-05,  1.5307e-04, -9.2085e-05,\n",
       "                      -1.0511e-03, -4.9811e-04, -5.4862e-04, -5.7123e-04,  7.7654e-04,\n",
       "                       2.7024e-04,  2.1958e-04,  3.5945e-04,  9.6344e-05, -1.3560e-03,\n",
       "                      -1.9566e-05,  1.6102e-03, -1.3056e-03,  2.6139e-04, -1.8275e-03,\n",
       "                       1.1387e-03, -4.1119e-04,  5.0858e-04,  4.1987e-04,  9.8437e-04,\n",
       "                      -2.4829e-03, -1.0152e-03,  9.5354e-04, -3.3328e-04,  8.7416e-04,\n",
       "                      -8.7421e-04,  2.1722e-03, -3.9623e-04,  1.3034e-03,  6.2277e-04,\n",
       "                       1.2159e-03,  1.2921e-03,  9.8481e-04, -2.7996e-04, -7.1582e-04,\n",
       "                      -1.9334e-03, -1.8301e-04, -1.0663e-04, -9.3536e-04, -2.1396e-03,\n",
       "                      -2.0761e-06, -2.1454e-04,  1.9664e-03, -2.0798e-04, -3.9662e-04,\n",
       "                       9.2449e-04,  1.1670e-03,  9.3299e-04,  9.9641e-04, -1.1698e-03,\n",
       "                      -9.1854e-04,  4.7713e-04,  9.6114e-04, -8.9039e-04,  6.0679e-04,\n",
       "                      -6.4519e-05, -1.7151e-03,  5.4813e-04, -3.3829e-04, -2.2382e-05,\n",
       "                      -6.0529e-04,  1.7633e-03,  1.3646e-03,  4.5924e-05,  8.9560e-04,\n",
       "                      -1.7415e-03,  1.4536e-03,  1.6059e-03, -5.1923e-04, -1.1207e-03,\n",
       "                       8.1851e-04,  1.8413e-04,  2.2221e-04, -3.3718e-03,  1.8738e-03,\n",
       "                       1.0960e-03,  1.5343e-03,  1.0596e-04,  3.3406e-05, -1.3515e-03,\n",
       "                       1.2108e-03, -1.3200e-03, -1.6234e-03, -2.2934e-04, -3.1028e-04,\n",
       "                      -7.7774e-04,  1.1608e-03,  8.3825e-04, -6.9018e-04, -3.9324e-04,\n",
       "                      -3.1901e-04,  1.9514e-03,  7.3752e-04,  1.7079e-03,  7.5587e-04,\n",
       "                      -1.3583e-03,  1.2732e-03, -4.9160e-04, -2.0733e-03, -1.5459e-04,\n",
       "                      -1.3045e-03,  8.9162e-04,  9.3904e-05,  8.3076e-04,  2.8708e-03,\n",
       "                      -1.7893e-03, -1.4453e-03, -2.3089e-03, -7.4743e-04, -1.6937e-03,\n",
       "                       6.2819e-04,  1.2114e-03, -1.1239e-03, -4.3046e-04,  1.5366e-03,\n",
       "                      -1.1924e-03, -3.3064e-04,  5.1816e-04, -8.5037e-04, -1.6528e-03,\n",
       "                       4.9459e-04, -1.3531e-03, -2.6030e-04,  4.1690e-04,  2.2740e-04,\n",
       "                      -7.0553e-04, -3.4369e-04, -5.4524e-04,  6.7023e-04,  7.4304e-04,\n",
       "                       1.1398e-03,  2.5175e-03, -1.4857e-03,  3.3549e-04, -5.6069e-04,\n",
       "                      -1.7609e-04,  1.0665e-03,  1.3541e-03, -2.0515e-03,  1.7404e-03,\n",
       "                       1.0866e-03,  2.4746e-03, -2.5226e-04, -3.2820e-03,  5.0599e-04,\n",
       "                      -1.2273e-03,  1.4449e-03, -2.4038e-04, -5.0655e-04, -6.2203e-04,\n",
       "                      -4.4052e-04, -4.9622e-04, -7.9033e-04, -4.9222e-04, -1.3202e-03,\n",
       "                       2.2881e-04, -4.6825e-04,  1.5993e-04, -1.8070e-03, -1.1941e-03,\n",
       "                      -1.5269e-03,  4.2193e-05, -2.1027e-05, -3.8297e-04, -2.6729e-03,\n",
       "                      -6.5022e-04,  4.8625e-04, -2.2257e-04, -2.3433e-03, -2.9355e-04,\n",
       "                       7.6358e-05, -8.6654e-04, -4.0577e-04, -4.1698e-04, -3.1332e-04,\n",
       "                       2.0004e-03,  1.8206e-03, -3.7466e-04,  8.6883e-04, -1.9567e-03,\n",
       "                       4.5798e-04,  1.3840e-04, -5.7103e-05,  1.6430e-04,  5.3611e-04,\n",
       "                       2.2972e-04, -1.9354e-03, -3.1615e-05,  1.1869e-03, -1.3409e-03,\n",
       "                       2.8250e-03,  1.5549e-05, -1.0998e-03, -1.1555e-05, -1.0116e-03,\n",
       "                       3.2253e-04, -8.6895e-04,  1.7572e-03, -1.2006e-04,  1.2308e-03,\n",
       "                      -1.0552e-03,  1.1761e-03,  7.2239e-04,  3.0958e-03, -1.1152e-03,\n",
       "                       1.5802e-05,  5.5150e-04,  1.0869e-03,  1.2856e-03, -3.9723e-04,\n",
       "                       2.9019e-04, -6.2024e-04, -8.6106e-04, -2.7091e-03,  1.1470e-03,\n",
       "                       4.4069e-04, -1.1085e-03,  5.9756e-04, -1.9856e-03,  2.5649e-04,\n",
       "                      -1.0478e-03,  5.4950e-05, -5.8322e-04, -9.1375e-04,  9.5585e-04,\n",
       "                       1.7710e-03, -1.0876e-03,  1.9927e-04,  6.3013e-04, -5.0361e-04,\n",
       "                       1.9279e-04, -8.8037e-04, -3.7732e-04, -1.3201e-03, -1.3113e-03,\n",
       "                      -8.9387e-04,  6.6140e-04, -8.2554e-04, -4.4002e-04,  1.9433e-03,\n",
       "                      -6.3226e-04, -4.5870e-04, -4.4024e-04, -2.5860e-04,  2.5323e-03,\n",
       "                       8.2420e-04,  1.7760e-03, -2.7166e-04, -3.4893e-04,  1.3207e-04,\n",
       "                      -6.9796e-05,  2.4471e-04,  2.0193e-03,  3.9278e-04, -9.5148e-04,\n",
       "                      -7.9983e-05,  1.2764e-03,  2.5291e-04, -4.1086e-04,  1.0854e-03,\n",
       "                      -9.0034e-05, -1.6609e-03, -2.8514e-04,  3.3949e-04,  6.2351e-05,\n",
       "                       3.2613e-03, -1.5729e-03, -1.8290e-03,  9.3268e-04, -1.4300e-03,\n",
       "                      -1.9674e-03, -1.3958e-03,  2.0416e-04, -7.6786e-04,  8.0886e-04,\n",
       "                      -1.0515e-03,  1.2177e-03, -2.2295e-04, -2.8953e-03,  4.7717e-04,\n",
       "                       1.2154e-03,  9.3861e-04, -4.7353e-04,  8.9754e-05, -7.5558e-06,\n",
       "                       1.0443e-03,  1.4789e-03,  1.0364e-03, -4.9387e-05, -6.6088e-04,\n",
       "                      -8.7420e-04,  1.4396e-03, -2.0869e-03, -4.3213e-04, -4.6639e-04,\n",
       "                       4.4111e-04, -3.6654e-04, -8.9418e-04,  1.0059e-03,  1.5750e-03,\n",
       "                      -1.8889e-04, -3.3461e-04, -3.2549e-05, -6.0426e-04, -8.8984e-04,\n",
       "                      -2.8571e-04, -1.3165e-03,  6.0368e-04, -9.0067e-04, -1.0202e-03,\n",
       "                       8.4934e-04,  5.1094e-05, -1.3577e-05,  2.5093e-04,  6.9483e-04,\n",
       "                       1.7383e-03,  2.8034e-03,  1.7691e-04, -5.8064e-04, -4.3861e-04,\n",
       "                      -6.3327e-04, -5.9792e-04, -1.6169e-04,  3.8607e-04,  1.2399e-05,\n",
       "                       1.0813e-03,  2.3725e-04, -8.7465e-05, -9.7921e-05, -4.0188e-04,\n",
       "                       1.3986e-03,  3.5458e-04,  9.9887e-05,  2.7956e-03,  1.6032e-03,\n",
       "                       1.4489e-03, -3.1322e-04, -2.5792e-03,  2.4761e-04,  1.1484e-03,\n",
       "                       5.3593e-04, -1.2585e-03, -7.5779e-04, -1.5434e-03, -1.1433e-03,\n",
       "                      -5.3709e-04, -5.0934e-04, -5.3021e-04,  8.5810e-05, -3.1659e-04,\n",
       "                      -7.7686e-04,  5.1303e-04, -2.7524e-04, -8.7304e-04,  1.8904e-03,\n",
       "                       6.9433e-04,  1.6577e-03, -2.7056e-03, -1.5481e-03, -1.2301e-03,\n",
       "                      -1.2571e-03, -2.6307e-03,  1.3578e-03,  5.0100e-05, -1.1657e-03,\n",
       "                      -7.6775e-04, -1.4533e-03, -9.9939e-04,  3.9229e-04, -5.2965e-04,\n",
       "                      -1.0414e-03, -1.8961e-03,  1.4649e-03,  6.7364e-04,  1.1151e-03,\n",
       "                      -7.8906e-04,  1.0227e-03, -1.4181e-03, -1.6619e-04,  2.8335e-06,\n",
       "                      -6.4676e-04,  1.8603e-03, -5.2451e-05,  2.3614e-03,  5.7129e-04,\n",
       "                       1.0840e-04, -7.9552e-04, -1.6576e-03, -1.4790e-03,  3.0347e-04,\n",
       "                       1.4614e-03,  9.3177e-04, -3.9483e-04,  1.0347e-03,  8.6136e-04,\n",
       "                       5.9201e-04,  5.0072e-04,  3.4097e-04,  4.4190e-04, -5.3450e-04,\n",
       "                      -7.9252e-04, -7.1761e-04,  2.4509e-03,  8.9944e-04, -1.6533e-03,\n",
       "                      -1.0112e-03, -1.1529e-03,  4.0781e-04,  3.9858e-04,  1.1631e-03,\n",
       "                      -2.1696e-03, -2.1243e-03, -1.1696e-03, -8.7088e-04, -2.0510e-03,\n",
       "                      -5.9170e-04,  9.7465e-04,  5.9890e-04,  6.9382e-04,  1.2656e-03,\n",
       "                       1.1558e-03,  2.4040e-05, -1.3017e-03,  2.4537e-04, -2.5250e-03,\n",
       "                       8.0209e-04,  3.9157e-03,  1.7916e-03, -1.0685e-03, -2.8837e-04,\n",
       "                      -1.6408e-03,  4.7082e-04, -1.8125e-03, -1.1593e-03,  1.0252e-03,\n",
       "                      -1.1276e-03, -1.8781e-03,  3.2961e-05, -5.7781e-04,  5.0489e-04,\n",
       "                       5.9771e-04,  1.9168e-03, -7.9666e-04, -1.8934e-03, -3.0352e-03,\n",
       "                       9.0288e-04,  7.6004e-04,  2.3590e-03,  1.0824e-03, -1.6476e-03,\n",
       "                      -6.3389e-04,  4.7478e-06,  9.7434e-04, -1.1579e-03, -1.0707e-03,\n",
       "                      -1.6228e-04,  6.0308e-04, -1.2332e-03,  1.2992e-03, -3.9006e-04,\n",
       "                      -1.3635e-03,  2.8378e-04,  1.8723e-03, -8.0753e-04,  5.0518e-05,\n",
       "                      -1.3136e-03,  5.3384e-04])),\n",
       "             ('transformer.resblocks.2.mlp.c_fc.weight',\n",
       "              tensor([[-0.0184,  0.0025, -0.0093,  ..., -0.0033,  0.0117, -0.0083],\n",
       "                      [-0.0119, -0.0238,  0.0075,  ...,  0.0202, -0.0287,  0.0171],\n",
       "                      [ 0.0161,  0.0217,  0.0151,  ..., -0.0035, -0.0074, -0.0144],\n",
       "                      ...,\n",
       "                      [-0.0707, -0.0287,  0.0065,  ...,  0.0163,  0.0445, -0.0531],\n",
       "                      [-0.0027,  0.0056, -0.0641,  ...,  0.0288,  0.0100,  0.0623],\n",
       "                      [ 0.0897,  0.0072, -0.0267,  ..., -0.0459,  0.0264, -0.0346]])),\n",
       "             ('transformer.resblocks.2.mlp.c_fc.bias',\n",
       "              tensor([-0.0443, -0.0097,  0.0345,  ..., -0.0305, -0.0269,  0.0379])),\n",
       "             ('transformer.resblocks.2.mlp.c_proj.weight',\n",
       "              tensor([[-0.0005,  0.0064,  0.0142,  ...,  0.0188, -0.0069, -0.0109],\n",
       "                      [-0.0145,  0.0039, -0.0093,  ..., -0.0056, -0.0110,  0.0004],\n",
       "                      [ 0.0017,  0.0026,  0.0132,  ...,  0.0007, -0.0021, -0.0010],\n",
       "                      ...,\n",
       "                      [-0.0176,  0.0069, -0.0146,  ..., -0.0106,  0.0040, -0.0057],\n",
       "                      [-0.0076,  0.0014,  0.0013,  ..., -0.0064, -0.0194,  0.0022],\n",
       "                      [ 0.0012, -0.0116, -0.0108,  ...,  0.0043, -0.0086, -0.0009]])),\n",
       "             ('transformer.resblocks.2.mlp.c_proj.bias',\n",
       "              tensor([-1.1358e-02,  1.6665e-02, -9.8716e-03, -3.3284e-03, -7.9953e-03,\n",
       "                      -8.8469e-03,  1.0394e-02, -1.4970e-02, -3.2713e-05,  7.7888e-03,\n",
       "                      -2.0841e-02,  1.0335e-02,  1.7064e-02,  1.7698e-02,  1.0023e-02,\n",
       "                       1.7507e-02, -5.7339e-03, -3.3850e-03,  7.8721e-03,  1.1352e-02,\n",
       "                      -1.3457e-02, -4.9709e-03,  1.0983e-02, -1.5058e-02, -2.4549e-03,\n",
       "                      -4.8717e-03,  4.0386e-03, -1.8235e-02, -1.7343e-02,  4.0737e-03,\n",
       "                      -1.2172e-03, -2.2402e-03, -8.5369e-03, -1.7702e-02,  1.2314e-02,\n",
       "                      -1.8754e-02, -1.1743e-02,  1.5475e-02, -1.4310e-02, -1.1048e-03,\n",
       "                      -5.0330e-03, -4.2174e-03,  1.0398e-02, -4.7339e-03, -4.7212e-03,\n",
       "                       3.7604e-04, -3.5973e-03, -5.3992e-03, -8.8087e-03, -8.0324e-04,\n",
       "                      -7.0642e-03, -9.3759e-04,  1.1978e-02,  1.5125e-02,  1.6144e-02,\n",
       "                       1.9529e-02,  1.0500e-02,  3.8431e-04, -4.1249e-03,  9.6950e-03,\n",
       "                      -3.0037e-03,  1.6204e-02,  1.1624e-02,  9.5642e-03, -1.7405e-03,\n",
       "                      -3.1775e-03, -7.1509e-03, -1.0279e-02, -6.9883e-03,  8.2077e-03,\n",
       "                      -2.0755e-02, -4.1031e-04,  1.9247e-02,  1.8352e-03,  1.3280e-02,\n",
       "                       1.9351e-04, -1.9460e-02,  1.4148e-02, -1.5767e-02, -2.1631e-02,\n",
       "                       1.1708e-02, -4.6858e-03,  1.6424e-02, -5.5765e-03, -7.1241e-04,\n",
       "                       5.2721e-03, -1.0672e-02, -1.3879e-02, -2.1268e-02, -1.9410e-02,\n",
       "                      -1.1485e-02, -1.5675e-02,  1.1498e-02, -1.1412e-02, -1.7453e-02,\n",
       "                       1.7243e-02,  3.6769e-03, -2.0523e-02,  2.1181e-02,  1.1290e-02,\n",
       "                       7.9846e-03, -5.2080e-03, -4.6519e-03, -1.2635e-03,  7.3146e-03,\n",
       "                      -1.0997e-02,  1.8071e-02, -1.3564e-02,  1.4146e-02,  3.8101e-03,\n",
       "                      -9.8616e-03,  9.1994e-03, -8.4997e-03,  2.1784e-02, -9.4519e-03,\n",
       "                      -1.2689e-02, -5.2905e-03, -1.6947e-02, -1.4504e-02,  8.9473e-03,\n",
       "                      -7.2387e-03, -8.2120e-03, -1.2232e-02, -2.0607e-02, -2.1910e-03,\n",
       "                       1.3008e-02,  1.0269e-02, -1.9651e-02,  1.6709e-02, -1.8992e-02,\n",
       "                       9.4309e-03,  1.3942e-02,  1.8639e-02,  1.4576e-02, -4.7102e-04,\n",
       "                      -9.5657e-03,  6.2176e-03,  1.6900e-02, -1.0441e-02,  1.5837e-02,\n",
       "                      -1.1029e-03, -1.0196e-02,  5.6936e-03, -1.8939e-02, -1.9352e-02,\n",
       "                       1.8685e-02, -1.2535e-02,  1.5237e-03, -2.0125e-02, -6.7442e-03,\n",
       "                      -1.4241e-02,  1.5870e-02, -6.0302e-03, -1.3794e-02, -4.3689e-03,\n",
       "                      -1.6035e-02,  1.5747e-03, -1.1690e-02, -1.4908e-02,  6.6484e-03,\n",
       "                      -1.7606e-02, -7.1918e-03, -1.7160e-02, -8.1126e-03, -5.2905e-03,\n",
       "                       2.0023e-02,  2.5999e-03,  1.7084e-02,  5.3595e-03,  9.3157e-03,\n",
       "                      -5.2936e-03, -1.2346e-02, -9.2797e-03, -1.6128e-02, -1.4866e-02,\n",
       "                       2.1763e-03, -1.2338e-02,  1.6545e-02, -2.1438e-02, -1.7131e-02,\n",
       "                      -2.2705e-02, -3.5126e-03, -1.9162e-02, -1.9080e-03,  5.9786e-03,\n",
       "                      -1.2120e-02, -1.9374e-02, -6.9554e-03, -1.8953e-02,  9.6957e-03,\n",
       "                      -1.6281e-02,  1.1890e-02, -2.2407e-03,  1.4673e-02, -2.7592e-04,\n",
       "                      -1.1958e-02, -2.0825e-03,  1.3089e-02, -2.0084e-02, -7.1828e-03,\n",
       "                      -1.4101e-02, -1.9796e-03, -1.4368e-02,  1.3972e-02,  1.9137e-02,\n",
       "                       5.1512e-03,  1.5218e-02,  1.7198e-02, -1.1232e-02, -2.0947e-02,\n",
       "                      -8.3671e-03, -5.1537e-03, -3.7251e-03,  2.1775e-02,  6.8582e-03,\n",
       "                       8.9409e-03,  2.2042e-02,  2.0083e-02, -5.2449e-03, -3.5118e-03,\n",
       "                      -1.2641e-02,  1.6523e-02, -1.3691e-02, -1.3327e-02, -2.0639e-02,\n",
       "                       1.3046e-02,  2.9010e-03, -5.9822e-04, -1.1983e-02, -9.4485e-03,\n",
       "                       2.1736e-02, -1.2472e-02,  1.3686e-02,  1.6145e-02, -1.3766e-02,\n",
       "                      -7.3519e-03, -1.6260e-02, -8.9438e-03, -2.0378e-02,  2.1754e-02,\n",
       "                      -3.0694e-03,  9.0052e-03,  1.2026e-02, -1.7939e-02, -6.0594e-04,\n",
       "                       1.5539e-02,  2.0124e-02,  1.8901e-02,  1.7361e-02,  1.8182e-02,\n",
       "                       4.8452e-04,  1.2052e-02,  1.2909e-02, -8.9946e-04, -1.4988e-02,\n",
       "                       1.0262e-02,  2.0550e-04,  1.0781e-02,  3.3126e-03, -1.8020e-02,\n",
       "                      -2.1534e-02, -1.3788e-02,  1.0778e-02, -1.8575e-02, -1.3458e-03,\n",
       "                       1.5679e-02, -3.0968e-03,  4.9653e-04, -1.2143e-02, -7.6490e-03,\n",
       "                       7.9093e-04, -1.6596e-02,  1.7121e-02,  1.4869e-02,  1.5057e-02,\n",
       "                       1.8272e-02, -3.4053e-03,  6.4328e-03,  4.3557e-03,  1.1960e-02,\n",
       "                       2.8147e-03,  6.6270e-03, -1.2905e-02,  1.6300e-02, -2.0053e-02,\n",
       "                       3.5751e-03,  1.7020e-02, -1.8774e-03,  2.1545e-02, -1.8825e-02,\n",
       "                      -1.6106e-02, -1.4759e-02, -1.5563e-02,  2.2336e-02, -1.3144e-02,\n",
       "                       1.7416e-03, -8.6378e-03,  1.3188e-02, -4.5008e-03, -1.3372e-02,\n",
       "                      -1.7961e-02, -1.3025e-02, -1.8146e-02,  3.4078e-03,  8.9923e-03,\n",
       "                      -1.0706e-02,  2.3066e-03, -1.0892e-03,  1.5363e-02, -8.5036e-03,\n",
       "                       2.2004e-02, -1.6681e-02, -1.1885e-02,  1.4349e-02, -5.0074e-03,\n",
       "                      -1.6082e-02,  1.8667e-02, -4.0502e-03, -1.2102e-02,  1.9299e-02,\n",
       "                      -1.6409e-02,  1.0850e-02,  1.5311e-02,  1.7330e-02, -1.6980e-02,\n",
       "                      -1.1759e-02, -1.4861e-02,  2.1662e-02, -8.7827e-03,  1.9150e-02,\n",
       "                      -1.2108e-02, -2.1370e-02,  6.1153e-03,  1.3930e-02, -1.3670e-02,\n",
       "                       1.1802e-02, -1.9444e-02,  6.1098e-03, -4.6323e-03,  3.7608e-03,\n",
       "                      -3.8465e-03, -9.1787e-03,  2.0497e-02, -2.0917e-02,  1.0092e-02,\n",
       "                      -1.2018e-03,  1.1661e-02, -1.7251e-02, -2.0667e-02, -1.5093e-03,\n",
       "                      -1.0754e-02, -5.2275e-03,  6.4626e-03, -1.5276e-02,  9.4500e-03,\n",
       "                       6.0789e-03, -1.4474e-02,  1.8884e-02, -1.2774e-02,  1.7839e-02,\n",
       "                       5.6105e-03, -1.2791e-02, -7.4086e-03, -1.4415e-03,  1.9464e-02,\n",
       "                       2.1726e-02, -1.5070e-02,  7.8849e-03, -1.5026e-02,  1.7511e-02,\n",
       "                      -1.1754e-02, -1.9662e-02,  7.9313e-03,  1.9456e-02, -1.0469e-02,\n",
       "                      -1.6985e-02, -9.0128e-03,  2.2394e-03, -1.8231e-02, -1.2322e-02,\n",
       "                       2.0788e-02, -7.6048e-03,  6.2258e-03,  1.1665e-02, -1.6942e-02,\n",
       "                       5.9329e-04, -1.0751e-02,  1.5584e-02, -1.8749e-02, -9.5851e-03,\n",
       "                      -1.2636e-02, -7.0228e-03,  2.1575e-02,  1.4070e-03,  1.8672e-02,\n",
       "                      -1.7308e-02, -1.7013e-02, -1.8334e-02,  5.3994e-03,  1.8766e-02,\n",
       "                       3.9621e-03, -2.0104e-02, -1.7730e-02,  1.0281e-02, -6.1198e-03,\n",
       "                       1.1802e-02, -4.1950e-03, -1.5977e-03,  1.7160e-02, -7.3725e-03,\n",
       "                      -1.9198e-02, -3.3684e-03,  1.5646e-02,  3.3937e-04,  1.5110e-02,\n",
       "                       8.9554e-04, -1.2173e-02,  2.1333e-02,  9.8638e-03,  2.0609e-02,\n",
       "                      -1.5700e-02,  1.9667e-02, -2.1471e-02,  1.1301e-02, -2.0013e-02,\n",
       "                       1.7330e-03, -1.9439e-02, -1.4968e-02, -2.6490e-03,  1.8445e-02,\n",
       "                       1.2353e-02,  6.3921e-03,  9.9192e-03,  8.2863e-03, -2.0878e-02,\n",
       "                      -2.2753e-02, -2.4875e-03,  1.7436e-02,  9.6849e-03,  1.3678e-02,\n",
       "                       1.3411e-02,  1.2884e-02,  1.8005e-02,  8.7254e-03,  1.4011e-02,\n",
       "                       7.3733e-03, -1.3896e-02,  1.2485e-02,  1.4408e-02, -1.9013e-02,\n",
       "                       1.0726e-03, -1.2986e-02,  1.2524e-02, -1.6764e-02, -1.9910e-02,\n",
       "                       5.7418e-03, -5.2724e-03, -4.0596e-04, -1.7184e-02,  1.5067e-02,\n",
       "                       9.3770e-03, -9.4317e-03,  1.3349e-02, -1.4187e-02, -6.3902e-04,\n",
       "                      -2.4382e-03, -1.3622e-02,  8.5342e-03, -2.2565e-02,  1.9866e-02,\n",
       "                      -4.6369e-03,  5.3922e-03, -1.6225e-02,  1.2661e-02,  5.2068e-03,\n",
       "                      -5.7568e-03,  9.3254e-03,  8.3909e-03, -8.2053e-03,  7.6527e-03,\n",
       "                      -1.7501e-02,  1.7117e-02, -1.7327e-02,  1.6929e-02, -1.7197e-03,\n",
       "                      -1.0311e-02, -1.3931e-03, -3.2504e-03,  1.3912e-02, -2.2089e-02,\n",
       "                      -2.1703e-02,  2.1129e-02,  2.0952e-02,  2.1682e-02,  4.8102e-03,\n",
       "                      -4.7879e-03,  2.0135e-02, -2.6767e-04,  1.2846e-02, -4.0949e-03,\n",
       "                      -1.4024e-02, -6.7281e-03, -1.1861e-02, -2.0639e-02, -5.0866e-03,\n",
       "                      -8.0060e-03,  1.6898e-02, -1.6653e-02,  5.2644e-03,  1.0062e-02,\n",
       "                      -1.9718e-02,  1.5639e-02])),\n",
       "             ('transformer.resblocks.2.ln_2.weight',\n",
       "              tensor([1.0017, 1.0015, 1.0036, 1.0019, 1.0027, 0.9997, 1.0015, 1.0020, 1.0010,\n",
       "                      1.0033, 0.9993, 1.0018, 1.0027, 1.0013, 1.0009, 1.0036, 1.0044, 1.0025,\n",
       "                      1.0014, 1.0010, 0.9983, 0.9994, 1.0029, 1.0030, 1.0002, 1.0008, 0.9996,\n",
       "                      1.0016, 1.0032, 1.0035, 1.0009, 1.0001, 1.0007, 1.0048, 0.9985, 1.0026,\n",
       "                      0.9994, 1.0045, 1.0015, 1.0023, 1.0022, 1.0005, 1.0017, 0.9990, 1.0028,\n",
       "                      1.0019, 1.0021, 1.0023, 1.0024, 1.0020, 1.0029, 1.0040, 1.0019, 1.0011,\n",
       "                      0.9986, 1.0005, 1.0009, 1.0024, 1.0013, 1.0025, 1.0026, 1.0019, 1.0001,\n",
       "                      1.0002, 1.0048, 1.0032, 1.0013, 1.0018, 1.0018, 1.0031, 1.0025, 1.0017,\n",
       "                      1.0022, 1.0033, 1.0019, 0.9998, 1.0003, 1.0029, 1.0041, 1.0003, 1.0001,\n",
       "                      1.0011, 1.0024, 1.0022, 1.0029, 1.0018, 1.0005, 0.9989, 1.0013, 1.0000,\n",
       "                      1.0021, 1.0017, 1.0032, 1.0017, 1.0025, 1.0032, 1.0036, 0.9995, 1.0036,\n",
       "                      0.9997, 1.0021, 1.0007, 1.0017, 1.0014, 1.0019, 1.0039, 1.0018, 1.0049,\n",
       "                      1.0030, 1.0021, 1.0021, 1.0029, 1.0025, 1.0030, 1.0029, 1.0015, 1.0043,\n",
       "                      0.9981, 1.0012, 0.9992, 0.9989, 1.0042, 0.9997, 1.0006, 1.0013, 1.0033,\n",
       "                      1.0012, 0.9977, 1.0021, 1.0026, 1.0001, 1.0038, 1.0014, 1.0024, 1.0017,\n",
       "                      1.0034, 0.9992, 1.0021, 1.0035, 1.0010, 1.0044, 1.0008, 1.0009, 1.0010,\n",
       "                      1.0023, 1.0004, 1.0003, 0.9974, 1.0022, 1.0015, 1.0030, 1.0041, 1.0023,\n",
       "                      1.0032, 1.0013, 1.0050, 0.9996, 1.0040, 1.0012, 1.0022, 1.0031, 1.0026,\n",
       "                      1.0023, 1.0008, 1.0036, 1.0016, 1.0029, 1.0010, 1.0006, 1.0031, 1.0007,\n",
       "                      1.0057, 1.0024, 1.0040, 1.0028, 0.9995, 1.0018, 1.0020, 1.0012, 1.0037,\n",
       "                      1.0013, 1.0033, 1.0036, 1.0007, 1.0010, 1.0015, 1.0006, 1.0022, 1.0009,\n",
       "                      1.0018, 1.0016, 0.9992, 1.0028, 1.0033, 1.0032, 1.0036, 1.0009, 1.0018,\n",
       "                      1.0040, 1.0024, 1.0005, 0.9991, 1.0027, 1.0032, 1.0004, 1.0043, 0.9989,\n",
       "                      1.0015, 1.0035, 1.0016, 1.0035, 1.0023, 1.0018, 0.9995, 1.0014, 1.0044,\n",
       "                      0.9985, 1.0015, 0.9974, 0.9981, 1.0034, 0.9997, 1.0011, 0.9995, 1.0029,\n",
       "                      1.0032, 1.0010, 1.0003, 1.0029, 0.9982, 1.0002, 0.9993, 0.9999, 1.0015,\n",
       "                      1.0017, 0.9999, 1.0007, 1.0044, 1.0020, 1.0001, 1.0009, 0.9994, 1.0032,\n",
       "                      1.0028, 1.0008, 1.0049, 1.0017, 1.0033, 1.0002, 0.9987, 1.0009, 1.0032,\n",
       "                      0.9977, 1.0007, 1.0032, 0.9999, 1.0039, 1.0003, 1.0023, 1.0006, 1.0037,\n",
       "                      1.0016, 1.0018, 1.0014, 1.0048, 1.0007, 1.0022, 1.0056, 1.0021, 1.0026,\n",
       "                      1.0030, 1.0054, 1.0030, 1.0015, 1.0013, 1.0002, 1.0019, 1.0016, 1.0022,\n",
       "                      1.0012, 1.0016, 1.0006, 1.0021, 1.0025, 1.0030, 1.0023, 1.0005, 1.0020,\n",
       "                      1.0037, 1.0021, 1.0012, 1.0005, 0.9993, 1.0001, 1.0015, 1.0023, 1.0006,\n",
       "                      1.0029, 1.0024, 0.9986, 1.0017, 1.0033, 1.0029, 1.0015, 1.0006, 1.0020,\n",
       "                      1.0045, 1.0010, 1.0015, 1.0027, 1.0024, 1.0009, 1.0020, 1.0035, 1.0008,\n",
       "                      1.0020, 1.0031, 1.0011, 1.0029, 0.9990, 1.0008, 1.0035, 1.0013, 1.0020,\n",
       "                      1.0032, 1.0017, 1.0031, 1.0018, 1.0021, 1.0029, 0.9990, 1.0012, 1.0010,\n",
       "                      1.0026, 1.0041, 1.0046, 1.0029, 1.0006, 0.9994, 1.0002, 1.0001, 1.0004,\n",
       "                      1.0013, 1.0018, 1.0021, 1.0036, 1.0001, 0.9997, 1.0029, 1.0019, 1.0040,\n",
       "                      1.0024, 0.9978, 1.0029, 1.0008, 1.0022, 1.0019, 0.9980, 0.9999, 1.0014,\n",
       "                      1.0006, 1.0014, 1.0007, 1.0005, 1.0020, 1.0038, 1.0033, 1.0037, 0.9993,\n",
       "                      1.0017, 1.0010, 0.9977, 1.0011, 1.0026, 1.0037, 1.0037, 1.0005, 1.0010,\n",
       "                      1.0028, 1.0023, 1.0040, 1.0014, 0.9994, 1.0012, 1.0020, 0.9990, 1.0049,\n",
       "                      1.0025, 1.0051, 1.0004, 1.0016, 1.0025, 1.0011, 1.0023, 1.0021, 1.0000,\n",
       "                      1.0026, 1.0006, 0.9995, 1.0051, 1.0031, 1.0012, 1.0013, 1.0013, 1.0036,\n",
       "                      1.0044, 0.9997, 1.0005, 1.0012, 1.0001, 1.0015, 1.0033, 1.0030, 1.0031,\n",
       "                      1.0013, 0.9995, 0.9996, 1.0028, 1.0021, 1.0017, 1.0029, 1.0016, 1.0008,\n",
       "                      1.0018, 1.0009, 1.0018, 1.0047, 1.0024, 0.9991, 1.0023, 1.0030, 1.0004,\n",
       "                      1.0013, 1.0008, 1.0044, 1.0012, 1.0006, 1.0024, 1.0037, 1.0007, 0.9992,\n",
       "                      1.0004, 1.0011, 0.9998, 1.0027, 1.0019, 1.0005, 1.0028, 0.9998, 1.0025,\n",
       "                      0.9989, 1.0032, 1.0028, 1.0039, 1.0001, 1.0039, 1.0006, 0.9976, 1.0006,\n",
       "                      1.0016, 0.9991, 1.0011, 1.0024, 1.0030, 1.0007, 1.0024, 0.9982, 0.9984,\n",
       "                      1.0009, 1.0004, 1.0002, 1.0013, 1.0024, 1.0017, 1.0023, 1.0012, 1.0019,\n",
       "                      1.0010, 1.0036, 1.0035, 1.0015, 1.0025, 1.0031, 1.0055, 1.0001, 1.0012,\n",
       "                      0.9990, 1.0029, 1.0059, 1.0015, 1.0021, 1.0007, 1.0015, 1.0019, 1.0030,\n",
       "                      0.9990, 1.0013, 1.0022, 1.0021, 1.0022, 0.9993, 1.0025, 1.0003, 1.0018,\n",
       "                      1.0005, 1.0016, 1.0032, 1.0009, 1.0037, 1.0037, 1.0007, 1.0026])),\n",
       "             ('transformer.resblocks.2.ln_2.bias',\n",
       "              tensor([ 9.6036e-04, -5.5686e-04,  5.1491e-04, -2.3627e-03,  1.5546e-03,\n",
       "                      -1.0721e-03, -7.9342e-04, -9.5401e-04,  1.7343e-03, -1.0483e-03,\n",
       "                       3.6719e-03, -8.5162e-04,  4.6825e-04,  6.8710e-04,  1.2538e-03,\n",
       "                      -1.0737e-03,  1.9568e-03,  2.1247e-04, -8.7972e-04, -6.9864e-04,\n",
       "                      -1.0814e-03, -1.9967e-03,  4.5694e-04,  1.3153e-03,  2.5881e-03,\n",
       "                      -3.1177e-04, -1.4079e-03,  2.2728e-03,  1.6887e-03,  1.7027e-03,\n",
       "                      -2.7296e-04, -7.0640e-04,  3.1903e-04, -1.1301e-03, -4.3833e-04,\n",
       "                      -1.1563e-05, -1.3032e-03,  1.2124e-03,  5.7198e-04, -6.6166e-04,\n",
       "                      -2.7743e-03,  2.2294e-04,  1.6027e-03,  1.1529e-03, -1.1566e-04,\n",
       "                       1.4296e-03,  4.3116e-04,  5.0315e-04, -2.0903e-03,  8.3096e-05,\n",
       "                       9.2212e-04, -2.0101e-04, -2.1918e-03,  1.2798e-03,  4.7629e-03,\n",
       "                       1.0251e-03,  9.2637e-04,  3.1349e-03, -5.8664e-04,  1.8150e-03,\n",
       "                       7.4137e-04, -1.3332e-03,  8.8133e-04, -7.0228e-04,  2.0083e-04,\n",
       "                       1.4939e-04,  1.2702e-03, -1.9764e-03, -1.1765e-03,  9.8065e-04,\n",
       "                       4.5193e-05, -2.6606e-03, -4.9362e-04, -1.5944e-03,  2.5023e-03,\n",
       "                      -1.7781e-03,  1.1934e-03, -8.9991e-05,  1.3139e-03, -1.7968e-03,\n",
       "                      -9.6552e-04,  3.7701e-03,  2.7256e-03, -2.7840e-04, -1.5532e-03,\n",
       "                       7.5135e-04, -2.3470e-03, -1.0811e-03,  1.5311e-03,  1.7424e-03,\n",
       "                       2.1970e-03, -1.5876e-03,  2.8857e-04, -8.5501e-04, -1.2918e-03,\n",
       "                       1.8935e-03, -4.8391e-04, -9.6690e-05, -9.2241e-04, -7.5884e-04,\n",
       "                       2.8214e-04,  4.9544e-04, -3.7123e-04, -1.8933e-03,  2.1390e-03,\n",
       "                       5.2118e-04,  1.7296e-03,  2.2571e-03,  3.2415e-04,  1.2744e-03,\n",
       "                      -9.2447e-04,  1.0312e-03, -1.9136e-03,  3.1393e-04, -7.0964e-04,\n",
       "                       1.0598e-03,  2.6238e-03,  7.0327e-04, -1.0784e-03, -1.7698e-03,\n",
       "                      -1.3468e-03,  8.3846e-04, -1.5271e-03, -1.8079e-04,  2.8792e-03,\n",
       "                       1.8043e-03,  2.6025e-04,  2.2267e-03,  2.4429e-03,  1.8193e-03,\n",
       "                      -3.7372e-06,  1.9908e-04,  5.2142e-04, -2.0503e-03,  2.5437e-03,\n",
       "                      -7.1719e-04, -1.9009e-03, -1.3668e-04,  7.7856e-04,  3.8749e-04,\n",
       "                       3.9504e-04, -2.5191e-03, -2.0639e-04,  3.6486e-04, -3.1881e-04,\n",
       "                       1.1559e-03, -3.9358e-04, -8.1601e-05,  1.2746e-04, -1.8014e-04,\n",
       "                       2.5576e-03,  2.3151e-03, -1.7276e-04,  9.4496e-05,  3.7654e-04,\n",
       "                       9.2347e-04, -1.9163e-03,  2.8564e-03, -1.2782e-03,  3.3517e-04,\n",
       "                       2.3834e-03,  1.4023e-03, -5.1615e-05,  4.0447e-04, -7.3634e-04,\n",
       "                      -1.9770e-04, -3.4948e-04,  5.8188e-04, -2.3576e-03,  4.6603e-04,\n",
       "                       7.9387e-04, -2.5870e-03,  2.7028e-04, -1.7998e-03, -2.5819e-03,\n",
       "                      -7.7725e-04,  7.8235e-04, -1.9233e-03, -6.6900e-04, -9.0536e-04,\n",
       "                       2.2278e-04,  8.1287e-04,  9.5900e-04,  3.5805e-03,  4.5147e-05,\n",
       "                      -2.9736e-03, -2.5015e-04,  1.1322e-03,  1.6711e-04,  3.9709e-04,\n",
       "                      -1.2462e-03, -1.3678e-03, -1.5381e-03, -2.4091e-03,  1.4203e-03,\n",
       "                      -4.8219e-04, -4.9122e-04, -1.4540e-03,  2.9819e-05, -1.3505e-03,\n",
       "                       7.4738e-04,  3.9912e-03,  1.9542e-04,  2.0235e-03, -4.1164e-04,\n",
       "                       1.0931e-03, -1.2867e-03, -1.5176e-03,  9.8335e-04,  3.0250e-03,\n",
       "                       4.7075e-04, -2.0575e-03,  1.7414e-03,  9.6168e-05,  2.3070e-04,\n",
       "                       4.7563e-04,  1.1301e-03,  1.2266e-04, -4.7097e-04,  1.1789e-03,\n",
       "                      -2.1233e-03, -1.3371e-03,  3.3766e-04, -5.3108e-04, -1.5088e-03,\n",
       "                       7.8605e-04,  1.2916e-03, -1.9587e-03, -1.3695e-04,  1.7508e-03,\n",
       "                       4.2078e-03, -6.0515e-04, -2.1308e-03,  2.1383e-03,  7.4593e-05,\n",
       "                       9.1606e-04, -1.3195e-03,  9.2175e-04,  1.8367e-03,  2.2186e-03,\n",
       "                       4.8414e-04, -1.0804e-03,  1.6001e-04,  3.6883e-04, -1.5874e-03,\n",
       "                      -1.1511e-03, -5.6079e-04,  3.0869e-03, -5.5352e-04,  9.6025e-04,\n",
       "                       6.7800e-05,  1.1954e-03, -3.5784e-04,  6.8722e-04, -2.5602e-03,\n",
       "                       8.2967e-04, -1.6470e-03,  7.7879e-04,  6.3187e-04, -3.1758e-04,\n",
       "                       1.4541e-03, -1.2118e-03,  3.6547e-04, -9.7687e-04,  2.7097e-03,\n",
       "                      -4.4467e-04, -1.7786e-03,  1.1590e-03,  1.6243e-05,  1.2969e-03,\n",
       "                      -9.3034e-04, -3.6795e-04, -4.5187e-04, -5.5089e-04, -7.7353e-04,\n",
       "                       1.0512e-03,  4.6909e-04,  1.7174e-03,  9.8037e-04,  1.9934e-03,\n",
       "                       2.4868e-03, -2.7199e-03,  1.3936e-03, -2.5735e-03,  2.5853e-03,\n",
       "                       7.5870e-04,  2.4877e-03,  1.5328e-03, -2.9404e-03,  7.7285e-04,\n",
       "                       5.5241e-04, -8.8980e-04,  1.2134e-03,  2.5291e-03,  6.0783e-04,\n",
       "                       7.1562e-04, -1.9041e-03,  2.1805e-03, -1.2733e-05,  4.0275e-04,\n",
       "                       2.0434e-03,  1.0208e-03,  3.3948e-04,  5.3178e-04, -5.5920e-04,\n",
       "                       6.2152e-04, -8.4364e-05,  2.2554e-04,  8.1837e-05, -7.1517e-04,\n",
       "                       3.7523e-04,  1.6851e-03,  6.5777e-04,  8.4397e-04, -1.7491e-03,\n",
       "                      -6.9004e-05, -1.2867e-03,  2.3481e-04, -2.3063e-04,  2.3344e-04,\n",
       "                       2.1264e-04, -2.1577e-03,  1.6304e-04,  2.5500e-03,  2.5870e-03,\n",
       "                      -3.2757e-04, -5.3066e-04, -2.2426e-03,  4.0972e-04,  1.1742e-03,\n",
       "                      -8.8545e-04, -2.7238e-03, -2.1051e-03,  4.1600e-04,  2.5871e-03,\n",
       "                      -8.1362e-04, -7.0827e-04,  5.3345e-04,  2.6090e-04,  9.2796e-04,\n",
       "                       4.7837e-04, -1.0978e-03, -1.4581e-03,  9.0868e-04,  2.6720e-06,\n",
       "                       9.2143e-04,  7.8322e-04, -3.2395e-03, -2.2707e-03, -1.6847e-04,\n",
       "                       1.4857e-03,  1.2316e-03, -1.0708e-03, -2.9717e-04,  7.6947e-04,\n",
       "                      -2.9018e-03, -1.9575e-03,  1.4303e-03, -2.4194e-03,  5.9708e-05,\n",
       "                      -1.8624e-04, -1.3808e-04, -1.5041e-03,  2.8829e-03, -2.5314e-04,\n",
       "                       8.8265e-04, -2.6715e-03, -2.0868e-04,  2.2024e-03,  8.1313e-04,\n",
       "                      -3.8304e-04, -2.2214e-03, -1.4958e-03, -6.4811e-04, -1.6366e-03,\n",
       "                      -1.0401e-03,  1.3264e-03, -7.8210e-04, -5.2994e-05,  2.8493e-03,\n",
       "                       1.2239e-03,  6.6392e-05, -6.7804e-04,  1.6265e-03,  2.0180e-03,\n",
       "                       1.7708e-03,  1.7087e-03,  2.3881e-03, -2.3148e-03,  1.2658e-05,\n",
       "                      -2.0781e-03, -1.3105e-04,  3.5067e-04, -1.0786e-03,  1.0297e-03,\n",
       "                      -1.6367e-05,  1.7033e-03, -9.6938e-04, -1.0537e-03, -1.0197e-03,\n",
       "                      -6.6334e-05, -2.4906e-03, -6.7434e-04,  5.6243e-04,  3.3497e-03,\n",
       "                       7.9156e-04,  1.0765e-03, -1.3600e-04, -5.1377e-04, -8.4126e-04,\n",
       "                       1.0880e-03, -2.1945e-03, -1.4300e-03, -2.1416e-03, -1.3192e-04,\n",
       "                       7.6611e-04, -7.1508e-04,  2.5955e-04, -1.0631e-03, -3.9108e-04,\n",
       "                       3.6419e-03, -1.3243e-03,  1.9678e-03, -2.5880e-04,  1.2530e-03,\n",
       "                       9.2748e-04,  9.8708e-04,  1.6815e-03,  1.7524e-03,  2.6324e-03,\n",
       "                       4.6496e-04,  2.6325e-03,  9.2720e-04, -1.1844e-03,  1.9958e-03,\n",
       "                      -2.7506e-03, -1.5969e-03,  1.1272e-03, -2.6340e-03, -1.5733e-04,\n",
       "                      -2.0889e-03,  1.7880e-03, -9.6916e-04,  1.9068e-04, -1.2977e-04,\n",
       "                       1.9977e-03,  1.8005e-03, -2.2072e-03, -1.2077e-03,  3.6453e-03,\n",
       "                       1.7621e-03,  1.7448e-03, -6.4222e-04, -1.5561e-03, -7.2467e-04,\n",
       "                       2.4648e-03,  1.2332e-04, -4.4748e-04, -8.4531e-05,  6.7919e-05,\n",
       "                       6.3510e-04, -5.2366e-04, -1.9447e-03,  8.6605e-04,  4.5338e-04,\n",
       "                       2.2409e-04,  1.5899e-04, -7.5691e-04, -5.3770e-04,  3.3785e-03,\n",
       "                      -9.7621e-04,  1.1709e-03,  1.0616e-03,  8.3273e-04, -3.0788e-04,\n",
       "                       1.7569e-03,  1.7982e-04, -1.3010e-03,  3.0246e-04,  2.1820e-03,\n",
       "                       1.4556e-04, -1.1976e-03,  4.6039e-04,  2.1461e-03, -3.9154e-04,\n",
       "                      -1.3102e-04, -1.4051e-04,  3.5775e-03,  9.8829e-04, -1.3237e-03,\n",
       "                       1.6038e-03,  1.3733e-04,  8.8791e-04,  6.1196e-04, -7.4076e-04,\n",
       "                      -2.0579e-03, -1.4424e-03,  3.2579e-04, -6.3689e-04,  1.1592e-03,\n",
       "                       7.7510e-04,  9.0274e-04,  8.0486e-04,  1.0745e-03,  1.4727e-03,\n",
       "                      -1.9081e-03,  2.1803e-04,  8.7186e-04, -8.3855e-04, -1.0766e-03,\n",
       "                       2.1053e-03, -6.4145e-04])),\n",
       "             ('transformer.resblocks.3.attn.in_proj_weight',\n",
       "              tensor([[-1.9890e-02,  7.9917e-04, -7.8904e-03,  ...,  1.3954e-02,\n",
       "                        3.7170e-02, -3.1203e-02],\n",
       "                      [ 1.1332e-01, -1.7356e-03,  2.6082e-02,  ..., -1.0695e-03,\n",
       "                       -8.6503e-03, -9.9977e-05],\n",
       "                      [-7.5998e-02, -3.1627e-03, -2.6613e-02,  ..., -2.7176e-02,\n",
       "                       -6.9952e-03, -7.2870e-02],\n",
       "                      ...,\n",
       "                      [ 1.9832e-02, -2.1329e-02,  2.3266e-02,  ...,  4.5910e-02,\n",
       "                        2.7750e-02,  1.2606e-02],\n",
       "                      [-6.0293e-02,  2.6826e-02, -5.0036e-02,  ...,  4.1613e-02,\n",
       "                       -5.9368e-02,  7.4325e-02],\n",
       "                      [ 2.5464e-02, -1.3515e-02,  1.9036e-02,  ..., -7.1347e-03,\n",
       "                        4.7304e-02, -3.7281e-03]])),\n",
       "             ('transformer.resblocks.3.attn.in_proj_bias',\n",
       "              tensor([-0.0055,  0.0003,  0.0025,  ...,  0.0008,  0.0004, -0.0018])),\n",
       "             ('transformer.resblocks.3.attn.out_proj.weight',\n",
       "              tensor([[ 1.2138e-03, -8.6110e-03, -5.4841e-03,  ...,  1.7725e-02,\n",
       "                        1.6979e-02,  1.3019e-02],\n",
       "                      [-6.9294e-03, -6.9068e-03, -2.0699e-02,  ..., -1.7289e-02,\n",
       "                        6.1705e-05,  1.0792e-03],\n",
       "                      [-8.9462e-03,  1.5370e-02,  1.3009e-02,  ...,  1.0167e-02,\n",
       "                       -5.7642e-03,  6.6167e-03],\n",
       "                      ...,\n",
       "                      [ 2.6358e-03, -7.2706e-03,  1.7091e-02,  ...,  3.2101e-03,\n",
       "                       -6.9854e-03, -6.6105e-03],\n",
       "                      [-5.8642e-03,  6.0897e-03,  2.4875e-03,  ..., -9.1316e-03,\n",
       "                       -1.0531e-02,  9.6797e-03],\n",
       "                      [-2.0460e-03,  1.6571e-02, -6.0884e-03,  ..., -1.4865e-02,\n",
       "                       -2.0749e-02, -1.0724e-02]])),\n",
       "             ('transformer.resblocks.3.attn.out_proj.bias',\n",
       "              tensor([ 7.8802e-04, -1.0959e-03, -4.2048e-04,  1.4813e-04,  6.0576e-04,\n",
       "                      -5.3310e-04,  5.4590e-04,  9.0415e-04, -1.4848e-03,  1.1802e-04,\n",
       "                      -8.0188e-04,  1.0137e-04, -8.4586e-04, -5.1828e-04, -1.2619e-03,\n",
       "                       1.4398e-03, -6.9278e-04,  9.8153e-04,  2.3402e-04, -2.4516e-07,\n",
       "                       8.6155e-04,  1.2917e-04,  3.5875e-04, -1.9857e-04, -1.2062e-03,\n",
       "                      -6.9480e-04, -5.5211e-04, -6.0590e-04, -5.9502e-04, -1.3321e-03,\n",
       "                       1.0205e-03,  3.3011e-04,  9.5217e-05, -1.0696e-03, -4.3874e-04,\n",
       "                       1.4174e-03, -3.8197e-04,  4.3887e-04,  4.3105e-04, -1.0742e-03,\n",
       "                       6.3918e-04, -2.8960e-04,  1.5397e-06,  8.2915e-04, -9.4773e-04,\n",
       "                      -2.5488e-05, -8.3063e-04, -3.2671e-04,  4.0846e-04, -2.6176e-04,\n",
       "                      -4.4150e-04, -1.8788e-04, -4.3656e-04, -4.8919e-04,  2.5166e-04,\n",
       "                       3.9613e-04,  2.1661e-04, -8.4374e-04,  4.2055e-04, -1.0261e-03,\n",
       "                      -1.9355e-04, -5.8355e-04,  1.2540e-04,  8.2405e-04, -3.0776e-04,\n",
       "                      -8.7003e-04, -8.2438e-06,  1.3459e-03, -1.7467e-05,  8.9075e-04,\n",
       "                      -7.6448e-04,  9.5889e-04,  1.7319e-04, -1.0553e-04, -1.3001e-03,\n",
       "                       1.0777e-03,  2.4758e-04, -1.0296e-04, -1.5317e-04,  4.1763e-04,\n",
       "                      -2.7092e-04, -2.1349e-04,  1.0770e-04,  1.0083e-03,  6.3577e-04,\n",
       "                      -1.2707e-03, -4.3986e-04, -2.7567e-04, -1.5546e-04, -3.9017e-04,\n",
       "                       6.0790e-05,  6.9696e-04,  3.6312e-04, -1.0002e-03, -8.4289e-04,\n",
       "                      -1.3749e-03, -5.6882e-05, -1.7781e-04, -6.1451e-04,  6.5963e-04,\n",
       "                       6.1513e-04, -7.5487e-04,  4.5378e-04,  1.4616e-04,  5.3374e-05,\n",
       "                      -5.8909e-04,  4.8549e-05,  7.4830e-04,  1.4821e-03, -1.0714e-03,\n",
       "                       6.9931e-04, -1.3285e-04,  1.1482e-03,  7.3617e-04, -9.6476e-04,\n",
       "                      -1.4727e-03,  9.4538e-04, -2.8883e-04,  3.0555e-04, -1.3034e-03,\n",
       "                       2.6022e-04, -6.8250e-04,  8.3749e-04, -5.0996e-04, -4.1121e-04,\n",
       "                       3.3096e-04, -7.9333e-04, -6.2958e-04, -4.4812e-04, -3.8888e-04,\n",
       "                       3.7361e-04,  6.1388e-04, -6.4968e-04, -5.1006e-04, -1.5977e-03,\n",
       "                       5.9844e-04, -2.9833e-07,  3.8871e-04, -9.0284e-04, -1.5753e-04,\n",
       "                      -3.4038e-04,  1.1600e-03,  9.5851e-04,  1.8910e-03,  8.0115e-04,\n",
       "                       3.1665e-04, -2.6407e-04,  3.5776e-04,  5.4981e-04,  8.2429e-04,\n",
       "                       1.0509e-03,  1.5045e-03,  7.0476e-04,  1.2283e-03,  5.7058e-05,\n",
       "                       1.4599e-03,  2.7068e-04, -2.7271e-04,  8.9926e-04,  4.9774e-05,\n",
       "                      -1.0894e-03,  5.3381e-05,  3.4414e-04, -2.8551e-05, -3.2911e-04,\n",
       "                       3.6158e-04,  3.0408e-04, -1.9796e-04,  2.0800e-05, -4.2019e-04,\n",
       "                       6.4472e-05, -3.1430e-04, -4.5353e-04, -1.5587e-04,  1.2014e-03,\n",
       "                      -6.6050e-04, -2.3075e-05,  1.1487e-03, -1.8113e-04, -1.0363e-04,\n",
       "                      -8.4471e-04, -7.0410e-04, -4.3648e-04, -1.3517e-03,  1.2999e-03,\n",
       "                      -1.5741e-04, -6.4317e-05,  1.5858e-04,  1.2971e-04, -5.3209e-04,\n",
       "                       7.6090e-04,  1.2330e-03,  2.1183e-04,  8.8640e-04,  3.6444e-04,\n",
       "                       9.7396e-04,  1.2760e-03,  9.1501e-04,  1.2344e-03,  1.3314e-03,\n",
       "                       1.2384e-05, -1.3890e-03, -8.3275e-05,  1.7686e-03,  5.3634e-04,\n",
       "                       1.0630e-03, -7.9051e-04,  3.1074e-04,  1.7452e-05,  1.3248e-04,\n",
       "                       6.4428e-04,  6.0721e-04, -5.1045e-04,  7.8799e-04, -1.3709e-03,\n",
       "                       2.1905e-05,  9.2415e-04, -1.2904e-03, -7.2438e-04, -3.1965e-04,\n",
       "                      -3.6496e-04, -1.0768e-03,  6.5583e-04,  1.2268e-03,  7.7026e-04,\n",
       "                       4.9016e-04, -1.2054e-03, -1.7441e-04, -2.2835e-04,  4.7594e-04,\n",
       "                      -6.4864e-04,  7.6710e-04,  3.9312e-04, -1.5368e-04,  7.8510e-04,\n",
       "                      -5.4224e-05,  7.9357e-04, -1.3349e-03,  3.6998e-04, -2.6502e-04,\n",
       "                       5.2317e-04,  4.7961e-04,  4.1234e-04,  8.1329e-04, -5.9499e-04,\n",
       "                      -3.5871e-04,  7.7752e-04, -8.5179e-04, -3.2383e-04, -8.1965e-04,\n",
       "                       8.2194e-04, -3.9534e-04,  1.2912e-04,  3.2043e-05, -9.0936e-04,\n",
       "                       4.3199e-05, -1.2230e-04, -4.5630e-04, -5.9366e-04, -8.3725e-04,\n",
       "                       6.0340e-04, -9.8721e-05,  7.4366e-04, -5.0440e-04, -5.9440e-04,\n",
       "                       1.0165e-03,  4.4093e-04,  8.7921e-04, -3.2764e-04, -1.0831e-03,\n",
       "                       1.5001e-04, -1.4628e-04, -7.8241e-04,  3.5347e-04,  1.1361e-03,\n",
       "                      -6.3424e-04, -2.3917e-04, -4.2841e-04,  1.4266e-03, -1.7132e-03,\n",
       "                      -2.0740e-04,  2.2647e-03, -7.2318e-04, -8.4101e-04, -1.3343e-03,\n",
       "                       4.9310e-04,  7.7356e-04,  5.7242e-04,  1.5091e-03, -5.4748e-04,\n",
       "                       3.4816e-04, -2.9374e-04, -1.1992e-03,  6.9843e-04, -4.3892e-04,\n",
       "                       1.1672e-03, -4.0814e-04,  4.8874e-04, -9.5569e-04, -1.0131e-03,\n",
       "                      -6.8439e-04,  1.3148e-04,  7.5449e-04, -6.2787e-04, -6.2198e-05,\n",
       "                      -5.2232e-04, -6.5035e-05,  4.2281e-04, -1.2790e-03, -1.2987e-03,\n",
       "                      -8.5242e-05,  8.0088e-04, -1.0703e-03, -9.2119e-04,  4.1333e-04,\n",
       "                       2.6160e-04,  1.7740e-03,  1.0124e-03,  1.0885e-04,  9.2491e-04,\n",
       "                       1.2716e-03, -7.0258e-04, -4.4747e-04, -1.0666e-04, -1.5210e-03,\n",
       "                      -9.3008e-05,  7.0121e-04,  1.4860e-04, -1.1332e-03, -1.7892e-03,\n",
       "                      -1.2459e-03,  5.0255e-04,  9.4984e-04,  2.1389e-04,  9.9234e-04,\n",
       "                       6.6279e-05, -9.5051e-04, -3.1415e-04,  8.7414e-04,  1.5654e-03,\n",
       "                      -9.2401e-04,  1.8031e-03,  4.4282e-04, -5.9303e-04, -1.5326e-03,\n",
       "                       5.7764e-04, -3.7566e-04,  8.1183e-04, -2.9159e-04,  1.2006e-03,\n",
       "                      -5.3101e-04, -1.6982e-03, -1.1380e-03, -9.8595e-04, -8.1732e-05,\n",
       "                      -8.9956e-04,  1.9590e-03,  8.7024e-04,  1.1465e-03, -1.0403e-03,\n",
       "                       5.4723e-04, -4.5247e-04,  1.4954e-03,  1.5580e-03, -6.1752e-04,\n",
       "                       1.3165e-03,  7.3467e-04, -9.9260e-04,  1.7953e-03,  1.0783e-03,\n",
       "                      -3.5182e-04,  9.3690e-04, -2.8399e-04, -3.0742e-04,  1.6531e-04,\n",
       "                       2.2229e-03, -7.9666e-04,  1.2598e-03,  2.9490e-04, -2.3668e-04,\n",
       "                       5.2772e-04, -2.4313e-04, -1.3510e-03, -6.8196e-04, -1.8706e-04,\n",
       "                      -5.1962e-04,  3.9069e-04,  8.8417e-04,  8.5028e-04,  4.9849e-04,\n",
       "                       1.8812e-04, -6.0347e-05,  1.2807e-04,  1.4597e-03, -1.5506e-05,\n",
       "                       3.7962e-04, -3.9221e-04,  2.8316e-04,  2.6325e-04,  1.1370e-03,\n",
       "                      -2.5141e-04, -6.4522e-04, -7.0129e-04,  4.6410e-04, -6.0807e-05,\n",
       "                       3.5090e-04,  1.0198e-04, -4.3816e-04,  6.2294e-04,  1.9892e-04,\n",
       "                      -6.2974e-04, -3.8650e-04,  6.2637e-04,  2.1897e-04,  3.8680e-04,\n",
       "                      -1.3793e-03,  2.3594e-04,  7.5515e-05,  6.8007e-04,  6.5702e-04,\n",
       "                      -1.6208e-03,  5.1776e-05,  1.6230e-04, -3.3797e-05, -1.0200e-03,\n",
       "                       3.2066e-05, -8.5971e-04,  8.3477e-05,  2.7172e-04, -6.7379e-04,\n",
       "                      -1.1178e-03,  2.0455e-03,  3.4878e-05,  7.3972e-04, -1.2518e-03,\n",
       "                      -4.7098e-04,  5.4639e-04, -6.1397e-04,  8.2873e-04,  2.4208e-04,\n",
       "                      -5.2396e-04,  3.4206e-04,  4.8837e-04,  1.5775e-04, -1.2023e-03,\n",
       "                      -9.9014e-04, -9.8919e-04,  8.0634e-04,  9.5122e-04, -1.8510e-04,\n",
       "                      -6.6679e-05,  7.6840e-05, -7.0688e-04,  2.0569e-03,  8.8566e-04,\n",
       "                      -2.0983e-04, -1.3996e-04,  6.3445e-04,  2.8372e-04, -5.5682e-04,\n",
       "                      -2.5763e-04, -5.1818e-04,  2.6674e-04,  6.9409e-04,  2.2938e-04,\n",
       "                      -4.9832e-04, -4.8772e-04, -4.2784e-04, -5.1990e-04, -2.2592e-03,\n",
       "                      -9.4148e-04, -4.5198e-04, -2.9995e-04,  1.6443e-04, -6.9892e-04,\n",
       "                       7.2194e-04, -1.4327e-03, -4.9201e-04,  1.3065e-04, -6.5380e-04,\n",
       "                      -8.9832e-04, -2.5321e-04,  3.8051e-04,  1.3319e-04, -9.0289e-05,\n",
       "                       6.4482e-04, -1.2881e-03, -1.4111e-03,  3.8473e-04,  9.0052e-05,\n",
       "                       7.3723e-04, -6.9619e-04, -4.8719e-04, -8.1045e-06,  2.8656e-04,\n",
       "                       1.9117e-04,  1.8277e-03, -7.4678e-04, -3.6882e-04, -1.1045e-03,\n",
       "                      -7.9170e-04, -4.4762e-04,  1.4845e-05,  4.6272e-04, -9.0748e-04,\n",
       "                      -1.3037e-04, -3.4248e-04, -4.4875e-04,  2.2371e-04,  4.8383e-04,\n",
       "                      -3.1138e-04,  1.9673e-04])),\n",
       "             ('transformer.resblocks.3.ln_1.weight',\n",
       "              tensor([1.0009, 0.9997, 0.9979, 0.9977, 0.9980, 0.9988, 0.9997, 1.0002, 0.9987,\n",
       "                      1.0013, 0.9989, 0.9987, 0.9993, 0.9967, 0.9982, 0.9991, 0.9961, 1.0003,\n",
       "                      1.0005, 1.0015, 0.9991, 0.9973, 0.9993, 0.9975, 0.9998, 0.9987, 0.9993,\n",
       "                      0.9987, 0.9991, 1.0011, 0.9988, 0.9963, 0.9986, 1.0006, 0.9977, 0.9983,\n",
       "                      1.0012, 0.9965, 0.9944, 1.0017, 1.0017, 0.9981, 0.9968, 1.0018, 0.9980,\n",
       "                      0.9984, 1.0010, 0.9990, 0.9984, 0.9983, 1.0005, 0.9994, 1.0020, 0.9986,\n",
       "                      0.9996, 1.0008, 1.0015, 0.9979, 1.0026, 0.9988, 0.9988, 1.0006, 1.0014,\n",
       "                      1.0006, 1.0003, 0.9999, 0.9991, 0.9984, 0.9986, 0.9987, 1.0009, 1.0004,\n",
       "                      0.9971, 0.9990, 0.9988, 0.9973, 0.9981, 0.9988, 0.9989, 1.0019, 0.9985,\n",
       "                      0.9986, 0.9954, 0.9998, 0.9976, 0.9977, 1.0003, 0.9990, 0.9986, 0.9975,\n",
       "                      0.9981, 0.9987, 0.9991, 0.9982, 1.0012, 0.9990, 0.9995, 1.0009, 1.0006,\n",
       "                      0.9981, 0.9989, 1.0006, 0.9992, 0.9998, 1.0005, 0.9975, 0.9997, 1.0010,\n",
       "                      0.9992, 0.9972, 0.9997, 0.9989, 0.9998, 0.9996, 0.9992, 1.0008, 0.9982,\n",
       "                      0.9988, 1.0006, 0.9994, 0.9978, 1.0000, 0.9989, 0.9981, 0.9972, 0.9982,\n",
       "                      1.0010, 0.9970, 0.9998, 0.9992, 0.9960, 0.9979, 0.9997, 0.9987, 1.0004,\n",
       "                      0.9984, 1.0016, 0.9999, 0.9993, 0.9991, 0.9990, 0.9996, 1.0010, 0.9971,\n",
       "                      0.9991, 1.0016, 1.0005, 0.9972, 0.9987, 1.0003, 1.0018, 0.9975, 0.9988,\n",
       "                      0.9985, 1.0020, 1.0005, 1.0014, 0.9972, 1.0003, 1.0023, 0.9977, 0.9986,\n",
       "                      0.9999, 0.9985, 1.0017, 0.9987, 0.9993, 0.9977, 0.9988, 0.9999, 1.0016,\n",
       "                      1.0007, 1.0018, 0.9964, 0.9990, 0.9992, 0.9980, 1.0019, 0.9990, 0.9981,\n",
       "                      0.9973, 0.9978, 0.9980, 1.0012, 0.9974, 0.9955, 0.9987, 0.9974, 1.0037,\n",
       "                      0.9993, 0.9980, 0.9986, 0.9981, 0.9982, 1.0011, 0.9999, 0.9979, 1.0000,\n",
       "                      1.0005, 0.9998, 1.0017, 1.0032, 0.9992, 1.0002, 1.0001, 0.9986, 0.9997,\n",
       "                      0.9993, 0.9997, 1.0011, 0.9984, 0.9991, 1.0000, 1.0019, 0.9952, 1.0012,\n",
       "                      0.9999, 0.9988, 0.9992, 0.9991, 0.9998, 1.0015, 0.9968, 0.9994, 0.9991,\n",
       "                      1.0007, 0.9995, 0.9983, 0.9992, 0.9990, 0.9978, 0.9999, 0.9986, 0.9985,\n",
       "                      0.9973, 0.9966, 1.0007, 1.0012, 0.9989, 0.9972, 1.0010, 0.9977, 0.9991,\n",
       "                      1.0007, 1.0014, 1.0014, 1.0000, 0.9983, 0.9996, 0.9987, 1.0006, 0.9999,\n",
       "                      0.9975, 0.9996, 1.0015, 0.9980, 1.0002, 0.9947, 0.9987, 1.0001, 0.9990,\n",
       "                      0.9998, 0.9950, 0.9998, 0.9993, 1.0001, 0.9974, 0.9987, 1.0005, 0.9989,\n",
       "                      0.9988, 1.0006, 1.0019, 0.9993, 0.9983, 0.9998, 0.9982, 0.9970, 0.9994,\n",
       "                      0.9983, 1.0018, 0.9982, 0.9990, 0.9980, 0.9996, 1.0005, 0.9978, 0.9979,\n",
       "                      0.9979, 0.9997, 0.9974, 1.0003, 0.9989, 0.9993, 0.9996, 1.0003, 1.0009,\n",
       "                      1.0004, 1.0022, 0.9995, 0.9990, 1.0001, 1.0019, 1.0011, 1.0022, 1.0012,\n",
       "                      0.9981, 0.9975, 0.9996, 1.0014, 0.9994, 0.9961, 0.9991, 0.9977, 0.9953,\n",
       "                      1.0028, 1.0002, 0.9990, 0.9958, 0.9983, 1.0000, 0.9991, 0.9972, 0.9988,\n",
       "                      1.0009, 0.9964, 1.0013, 0.9963, 0.9990, 1.0020, 0.9958, 0.9992, 0.9951,\n",
       "                      1.0002, 1.0011, 0.9998, 0.9989, 0.9988, 1.0006, 0.9974, 0.9974, 0.9971,\n",
       "                      0.9987, 0.9960, 0.9990, 1.0035, 0.9992, 1.0004, 1.0005, 0.9977, 0.9971,\n",
       "                      1.0012, 1.0024, 0.9966, 1.0004, 0.9990, 1.0029, 1.0000, 0.9977, 1.0011,\n",
       "                      0.9992, 0.9991, 0.9985, 0.9994, 1.0026, 0.9989, 0.9982, 0.9953, 0.9963,\n",
       "                      1.0016, 0.9983, 1.0014, 0.9992, 1.0016, 1.0003, 0.9999, 1.0004, 0.9994,\n",
       "                      0.9982, 0.9994, 0.9987, 0.9999, 0.9996, 0.9974, 1.0021, 1.0006, 0.9982,\n",
       "                      0.9985, 0.9981, 1.0005, 0.9992, 0.9984, 0.9998, 0.9971, 0.9990, 1.0007,\n",
       "                      0.9977, 0.9990, 0.9991, 0.9963, 1.0004, 1.0018, 1.0004, 0.9975, 1.0005,\n",
       "                      1.0000, 0.9961, 0.9986, 0.9964, 0.9955, 0.9967, 1.0000, 0.9976, 0.9984,\n",
       "                      1.0000, 0.9999, 0.9970, 0.9981, 0.9996, 0.9962, 0.9997, 0.9967, 1.0029,\n",
       "                      1.0003, 0.9987, 0.9999, 0.9988, 1.0021, 0.9986, 0.9978, 0.9959, 0.9976,\n",
       "                      1.0019, 0.9992, 1.0001, 0.9987, 0.9980, 0.9992, 0.9980, 0.9991, 0.9984,\n",
       "                      1.0013, 0.9992, 1.0006, 0.9984, 1.0035, 0.9990, 0.9995, 0.9980, 0.9993,\n",
       "                      0.9987, 0.9978, 1.0000, 1.0004, 0.9997, 0.9967, 0.9995, 0.9979, 0.9989,\n",
       "                      1.0011, 0.9997, 0.9981, 1.0017, 0.9966, 0.9983, 0.9961, 0.9991, 0.9985,\n",
       "                      1.0010, 0.9989, 0.9979, 0.9991, 0.9994, 0.9985, 0.9985, 0.9998, 1.0017,\n",
       "                      0.9989, 0.9984, 0.9994, 0.9992, 1.0014, 1.0012, 0.9974, 1.0004, 0.9966,\n",
       "                      1.0011, 0.9976, 1.0017, 0.9992, 0.9992, 0.9997, 1.0021, 1.0002, 1.0006,\n",
       "                      0.9987, 0.9967, 0.9994, 1.0004, 1.0000, 1.0002, 0.9981, 0.9976, 1.0004,\n",
       "                      1.0001, 1.0011, 1.0013, 1.0009, 0.9980, 1.0006, 1.0003, 1.0000])),\n",
       "             ('transformer.resblocks.3.ln_1.bias',\n",
       "              tensor([-2.4861e-03,  2.1519e-03,  9.7229e-04, -8.2207e-04,  1.9397e-03,\n",
       "                      -5.7950e-04,  1.7623e-03,  7.8550e-04,  1.5230e-03, -2.3798e-04,\n",
       "                       2.6095e-04, -1.2852e-03, -9.9785e-04,  2.0135e-04,  9.7510e-04,\n",
       "                       6.5948e-04,  3.6901e-03, -2.6710e-03,  1.0632e-03,  1.1073e-04,\n",
       "                       2.3453e-04,  2.0244e-03, -1.5344e-03,  8.7341e-04,  9.0135e-04,\n",
       "                       1.1691e-03,  8.4032e-04,  5.3118e-05,  1.1185e-03,  8.7272e-04,\n",
       "                       1.4120e-03,  2.1370e-03, -3.5364e-04,  1.0282e-03, -1.2979e-03,\n",
       "                      -3.0660e-04,  3.0068e-04,  2.3198e-04, -5.8566e-04, -1.8514e-03,\n",
       "                      -3.1058e-04, -1.8899e-03, -3.3776e-03,  1.0938e-03,  1.3189e-04,\n",
       "                       6.7407e-05, -5.0654e-04,  2.8079e-03,  4.3597e-04, -8.6556e-04,\n",
       "                      -1.4609e-03, -8.2352e-04, -1.4101e-03,  2.0268e-03,  1.2993e-04,\n",
       "                       7.4745e-04, -7.6546e-04, -8.0416e-04,  1.5982e-03, -8.4997e-04,\n",
       "                       8.6487e-04, -1.4640e-03, -2.3641e-04, -6.8628e-04,  1.7921e-03,\n",
       "                      -1.7826e-04, -7.3164e-04, -1.9462e-03, -8.8355e-04, -4.7341e-04,\n",
       "                      -2.2678e-03,  1.0454e-03,  4.7874e-04,  1.7989e-03, -7.2877e-04,\n",
       "                       9.1259e-04, -3.0678e-03,  3.9214e-04,  6.4268e-04,  9.3604e-04,\n",
       "                      -4.7752e-04, -8.3416e-05, -2.8326e-04,  6.7544e-04, -3.7350e-03,\n",
       "                       3.0877e-03, -5.7715e-05, -9.7318e-06,  1.8807e-03,  1.9440e-03,\n",
       "                       1.0586e-03,  6.3622e-05, -2.7303e-03, -1.0423e-03, -1.7722e-03,\n",
       "                       2.2969e-04,  1.7374e-03,  6.8741e-05, -1.8007e-03, -1.1366e-03,\n",
       "                      -2.4168e-04, -7.2631e-04,  1.7491e-03, -3.2704e-04, -4.7461e-04,\n",
       "                      -6.3503e-05,  8.0527e-06,  6.5138e-04, -3.5380e-03, -2.5722e-03,\n",
       "                      -3.0947e-03, -3.1301e-04, -2.3492e-03, -1.2072e-03, -2.8192e-04,\n",
       "                       1.9629e-04, -4.9416e-04,  9.5982e-04, -7.1303e-04,  1.9661e-03,\n",
       "                      -9.6088e-04, -3.5951e-04, -9.9225e-04,  2.2256e-03, -7.1311e-04,\n",
       "                       5.8773e-04,  9.9984e-04,  6.3036e-04, -3.9246e-04, -2.8844e-03,\n",
       "                       8.4830e-04, -4.7269e-04, -1.5437e-03,  2.4670e-03,  2.5073e-03,\n",
       "                      -9.4972e-04,  3.4181e-03,  2.0117e-03,  1.4560e-03,  8.2751e-04,\n",
       "                      -7.5708e-04, -2.3647e-03, -2.1695e-03, -2.6505e-04, -2.7102e-03,\n",
       "                      -1.6129e-03,  8.3203e-04, -2.5683e-03, -2.6866e-03,  3.7137e-04,\n",
       "                      -5.5472e-04, -3.3747e-03, -1.5703e-03, -9.3306e-04,  2.9553e-04,\n",
       "                      -2.3470e-03, -5.1577e-04, -1.4862e-03, -7.9288e-06, -1.8964e-03,\n",
       "                       2.6141e-04, -9.8872e-04, -9.7121e-04,  7.2890e-04,  3.8181e-04,\n",
       "                      -1.0526e-04, -2.0979e-05, -1.5735e-04, -4.7011e-04,  6.2466e-04,\n",
       "                       6.1811e-04, -1.4126e-03,  2.7676e-03, -1.3061e-03,  8.2033e-04,\n",
       "                      -3.0907e-04, -2.4613e-03, -2.5244e-05,  1.9685e-03, -1.3228e-03,\n",
       "                       8.0125e-04, -9.9313e-04, -6.0108e-04, -4.6919e-04, -2.6027e-03,\n",
       "                       9.3714e-04,  6.6765e-05, -1.7160e-03, -7.4901e-04,  1.4437e-03,\n",
       "                      -3.5534e-03,  2.1199e-03,  8.4368e-04, -1.2631e-03, -2.5512e-03,\n",
       "                       2.3911e-04, -4.2770e-04, -6.0184e-04, -3.2896e-04, -1.6497e-03,\n",
       "                      -2.7171e-04,  8.3445e-04,  3.3881e-07, -3.4866e-05, -1.8079e-03,\n",
       "                      -2.2399e-03,  1.3657e-03,  5.3544e-04,  1.2633e-03,  1.9445e-04,\n",
       "                       2.1160e-03, -1.2828e-03, -7.4616e-04,  7.9448e-04, -2.1268e-04,\n",
       "                      -1.1219e-03,  1.3731e-04,  3.1699e-04, -3.4861e-04, -6.3299e-05,\n",
       "                       1.8001e-03, -1.3548e-03, -4.3483e-06, -8.1971e-05, -2.9227e-04,\n",
       "                       7.7679e-04,  1.0852e-03, -8.5305e-04,  1.4293e-03, -1.0087e-03,\n",
       "                       3.1116e-03,  3.7148e-04, -1.3865e-03,  8.4609e-04,  9.6548e-04,\n",
       "                       8.8937e-04,  1.1158e-03,  1.0259e-03,  2.1917e-03,  5.7237e-04,\n",
       "                      -1.4249e-03, -1.5993e-03,  1.3254e-03, -9.0606e-04, -1.0346e-03,\n",
       "                      -1.6165e-04, -5.2510e-04, -9.7268e-04, -5.8891e-04,  2.2560e-04,\n",
       "                       2.1825e-04, -1.0245e-03, -5.2008e-04, -4.9444e-04, -1.9286e-03,\n",
       "                       2.5580e-04, -6.4158e-04,  1.8161e-03, -1.3493e-03,  1.5419e-03,\n",
       "                      -5.7512e-05, -2.2385e-03, -1.1651e-03,  7.5593e-04,  3.4512e-03,\n",
       "                       9.0335e-04, -4.9301e-04, -7.1101e-04, -1.1111e-03,  1.5330e-03,\n",
       "                       1.6582e-03, -9.1689e-04,  9.2732e-04,  2.4467e-03, -7.5715e-04,\n",
       "                       6.4746e-04, -2.4591e-04,  4.3311e-04, -2.5962e-03,  1.5322e-03,\n",
       "                       9.7632e-05,  2.5124e-04, -1.5479e-03, -1.5255e-03,  1.3828e-03,\n",
       "                       8.1151e-04,  3.4778e-04, -7.0227e-04, -1.7366e-03, -1.2809e-03,\n",
       "                       1.5503e-03, -1.5131e-03,  3.8606e-05, -1.3071e-04,  1.2714e-03,\n",
       "                      -6.3026e-04,  2.6723e-03,  1.6381e-03,  2.1374e-03,  5.2557e-04,\n",
       "                       5.1346e-04, -3.0362e-04,  5.5191e-04, -9.7745e-04,  8.5926e-04,\n",
       "                      -5.2288e-04,  6.5873e-04, -2.0439e-04,  1.9774e-04,  2.4136e-03,\n",
       "                       1.7973e-03, -2.7468e-03, -1.0975e-03, -3.6401e-04, -3.0035e-03,\n",
       "                      -9.5805e-04, -2.6023e-03, -7.4680e-04, -2.1841e-04, -1.9792e-04,\n",
       "                      -3.4181e-04, -1.7563e-04, -1.6091e-03, -3.1568e-03,  4.1580e-04,\n",
       "                       6.9771e-04, -5.1127e-04, -5.2842e-04, -5.5596e-04,  2.8105e-03,\n",
       "                      -1.0495e-03,  2.0213e-03,  2.6398e-03, -2.9614e-04,  3.3223e-04,\n",
       "                      -8.6724e-04,  1.2733e-04, -1.1350e-03, -5.1567e-04, -3.4727e-03,\n",
       "                       5.0180e-04,  9.7363e-04, -1.7177e-03,  1.9143e-03,  1.6385e-03,\n",
       "                       1.5545e-03,  6.0507e-04, -5.8635e-05,  1.0609e-03, -2.8843e-03,\n",
       "                      -2.8438e-04, -2.2922e-04,  5.6299e-04, -6.5997e-04,  2.5060e-03,\n",
       "                      -1.3296e-04,  1.0116e-03, -1.8706e-03, -9.8979e-04,  1.6247e-03,\n",
       "                       1.2741e-03,  1.6384e-03, -7.0952e-04, -3.6662e-03,  1.4126e-05,\n",
       "                      -1.7497e-03, -2.9646e-05, -9.7027e-04, -1.3352e-04, -5.3253e-04,\n",
       "                      -1.3089e-04, -1.6869e-03,  1.5269e-03,  1.9191e-03,  7.8044e-04,\n",
       "                       1.6729e-03,  3.1086e-03,  7.0140e-04,  2.0133e-04, -7.7823e-04,\n",
       "                      -8.3517e-04, -7.5951e-04, -1.0518e-03, -2.1049e-03, -3.9793e-05,\n",
       "                       9.4935e-04, -7.0567e-04, -7.2115e-04, -1.7236e-03,  3.0323e-04,\n",
       "                       3.5925e-05, -6.9075e-04, -7.6012e-04,  1.2679e-03, -4.1781e-03,\n",
       "                       2.9346e-04, -5.5198e-05, -1.6073e-03, -4.1123e-04,  1.3823e-03,\n",
       "                      -1.7144e-03,  1.3097e-03, -1.0224e-03, -5.0773e-04, -6.2890e-04,\n",
       "                      -5.4880e-05, -2.4537e-03,  1.0118e-04, -2.7251e-03, -2.2817e-03,\n",
       "                       1.5745e-03,  7.1879e-04, -2.2625e-03, -6.9641e-04,  1.3881e-03,\n",
       "                      -1.3256e-03,  7.9782e-05,  8.9470e-04, -1.7018e-04,  1.6953e-03,\n",
       "                      -1.7410e-03, -4.3192e-04, -5.7634e-04,  1.6050e-03,  5.4330e-04,\n",
       "                      -3.5487e-03,  4.5887e-04, -3.8822e-04,  1.5548e-03, -1.5873e-03,\n",
       "                      -9.2297e-04,  3.9693e-04,  3.4501e-03, -8.8302e-04,  1.3541e-03,\n",
       "                      -2.2177e-03,  5.6708e-04, -1.4917e-03, -2.2527e-03,  7.4489e-04,\n",
       "                       1.2083e-03,  2.6950e-04, -2.0043e-03,  1.9499e-03, -4.5519e-04,\n",
       "                       3.6194e-04, -1.5669e-03, -7.1123e-04,  7.7738e-04,  5.0382e-04,\n",
       "                       1.5472e-03, -7.0284e-04, -8.5162e-05,  3.4250e-04, -1.2897e-03,\n",
       "                      -1.6889e-03,  2.2665e-03, -7.4750e-04, -9.7723e-04,  7.9580e-04,\n",
       "                       9.5360e-04,  7.0800e-04,  1.3720e-03,  1.3730e-03,  6.0985e-04,\n",
       "                      -9.5625e-05,  4.5984e-03, -1.5460e-05, -1.2404e-03, -5.5184e-04,\n",
       "                       1.3034e-03, -1.0825e-03,  2.8848e-04, -6.9726e-04,  6.9275e-04,\n",
       "                       2.6404e-04,  2.3116e-03,  2.1744e-03,  2.0381e-03, -2.1517e-03,\n",
       "                       9.5159e-04, -7.5530e-04, -1.3608e-03,  5.8465e-04,  7.1314e-04,\n",
       "                      -9.3071e-04,  4.7246e-04,  2.8343e-03, -1.9146e-04, -7.3592e-04,\n",
       "                      -1.0438e-03,  2.9188e-03,  7.5097e-04,  8.6870e-04, -2.7054e-03,\n",
       "                      -6.5860e-05, -2.4945e-03,  2.4329e-03,  2.0404e-03,  3.6067e-04,\n",
       "                       1.0442e-03,  1.2999e-03, -1.4762e-03, -6.0371e-05, -3.8432e-04,\n",
       "                       1.8487e-03,  1.8270e-04, -1.8652e-03, -1.8160e-03,  6.6437e-04,\n",
       "                      -5.6282e-04,  2.0484e-03])),\n",
       "             ('transformer.resblocks.3.mlp.c_fc.weight',\n",
       "              tensor([[-0.0029, -0.0201,  0.0074,  ..., -0.0017, -0.0191, -0.0387],\n",
       "                      [ 0.0168, -0.0186, -0.0351,  ..., -0.0518,  0.0473,  0.0177],\n",
       "                      [-0.0536,  0.0384, -0.0092,  ...,  0.0449, -0.0375, -0.0436],\n",
       "                      ...,\n",
       "                      [-0.0122, -0.0258,  0.0102,  ..., -0.0071,  0.0067,  0.0308],\n",
       "                      [ 0.0405,  0.0140,  0.0296,  ...,  0.0139, -0.0145, -0.0472],\n",
       "                      [-0.0043,  0.0148, -0.0282,  ...,  0.0034,  0.0312, -0.0106]])),\n",
       "             ('transformer.resblocks.3.mlp.c_fc.bias',\n",
       "              tensor([-0.0245,  0.0081,  0.0188,  ..., -0.0158, -0.0482,  0.0396])),\n",
       "             ('transformer.resblocks.3.mlp.c_proj.weight',\n",
       "              tensor([[-0.0040,  0.0078,  0.0106,  ..., -0.0152, -0.0005, -0.0208],\n",
       "                      [ 0.0069,  0.0055, -0.0093,  ...,  0.0003, -0.0137, -0.0117],\n",
       "                      [ 0.0061, -0.0025, -0.0059,  ...,  0.0016,  0.0035,  0.0116],\n",
       "                      ...,\n",
       "                      [ 0.0058, -0.0042, -0.0008,  ..., -0.0010, -0.0098, -0.0207],\n",
       "                      [-0.0052,  0.0136,  0.0071,  ...,  0.0049,  0.0078,  0.0006],\n",
       "                      [-0.0057, -0.0127,  0.0025,  ...,  0.0042,  0.0056,  0.0101]])),\n",
       "             ('transformer.resblocks.3.mlp.c_proj.bias',\n",
       "              tensor([-5.8409e-03,  3.8791e-03,  1.7918e-02, -1.9382e-02, -6.0233e-03,\n",
       "                       1.7848e-02, -1.3533e-02, -4.0706e-05,  5.8826e-03,  1.6242e-02,\n",
       "                       2.6704e-04, -1.7231e-02, -1.9241e-02, -8.7804e-03, -1.4382e-02,\n",
       "                       7.9628e-03,  2.6188e-03, -9.4492e-03, -2.6861e-03,  1.7336e-02,\n",
       "                      -1.0771e-03, -1.3445e-02, -7.2540e-03, -1.0637e-02,  3.5719e-03,\n",
       "                       2.0973e-02,  1.5206e-02, -5.3253e-03,  2.1087e-02, -2.2301e-02,\n",
       "                       2.8852e-03,  1.0672e-02,  2.0391e-02, -1.1791e-02, -4.4628e-03,\n",
       "                       2.9414e-03,  1.0190e-02,  2.1073e-02,  1.9499e-02, -1.4440e-03,\n",
       "                      -1.1566e-02, -1.2269e-02, -1.4125e-02,  1.8563e-02,  1.0595e-02,\n",
       "                       2.1040e-02, -9.4561e-03, -5.1367e-03, -1.0913e-02, -1.9641e-02,\n",
       "                      -4.6314e-03,  6.3427e-03,  1.5225e-02, -1.4631e-02, -1.8856e-02,\n",
       "                       1.1665e-02,  2.1885e-02,  1.5867e-02, -3.5648e-03, -4.7866e-03,\n",
       "                      -4.7547e-04,  1.0947e-03, -1.1883e-03, -1.6625e-02, -1.7168e-02,\n",
       "                      -1.8458e-03, -7.5488e-03, -1.4457e-02, -1.6612e-02,  5.2938e-03,\n",
       "                      -1.5321e-03, -4.2207e-03,  1.8070e-02,  1.7735e-02, -2.3199e-02,\n",
       "                       5.2922e-03, -1.7038e-02, -1.6787e-02, -7.0685e-03,  1.8485e-02,\n",
       "                       1.1666e-04, -2.2117e-02,  1.3264e-02, -3.3670e-03, -1.2113e-02,\n",
       "                      -1.5223e-02,  1.7217e-03,  1.6496e-02,  1.8062e-02,  2.6923e-03,\n",
       "                      -3.0115e-03,  1.7005e-02,  9.4466e-04, -1.0927e-02, -8.1845e-04,\n",
       "                      -2.0901e-02, -1.8794e-02, -1.9230e-02, -1.2496e-02,  2.0741e-02,\n",
       "                      -7.7921e-03,  6.0850e-03, -2.0869e-02, -2.2869e-02,  1.9516e-02,\n",
       "                      -1.2480e-02,  1.8440e-02, -9.3801e-03, -1.6639e-02, -2.1401e-02,\n",
       "                       1.9155e-02,  4.1256e-03, -3.8467e-05,  2.1707e-02, -1.8806e-02,\n",
       "                      -1.1920e-02, -8.0906e-03, -3.4068e-03,  1.7143e-02,  1.2732e-02,\n",
       "                      -9.9457e-03, -2.0299e-02, -3.9889e-03, -7.5014e-04,  1.0458e-02,\n",
       "                       1.3778e-03,  1.7049e-02, -9.8862e-03, -4.1749e-03,  9.0777e-03,\n",
       "                       1.6572e-02,  1.7816e-02,  3.0632e-03, -9.7177e-03,  1.9848e-02,\n",
       "                      -1.1933e-02,  5.5496e-04,  1.8768e-02,  1.1540e-02, -7.4095e-03,\n",
       "                       4.2723e-04,  1.6612e-02, -1.7719e-02, -9.7982e-03, -2.3223e-03,\n",
       "                       1.9009e-02,  1.4129e-03, -1.7086e-02, -4.4262e-03, -9.2563e-03,\n",
       "                       1.9929e-02, -1.1113e-04,  1.2885e-02,  1.4541e-02, -4.1334e-03,\n",
       "                       1.2403e-02,  1.3956e-02,  4.0008e-03, -2.1985e-02,  4.0564e-04,\n",
       "                       5.7316e-03, -5.7665e-03,  9.8198e-03, -6.6959e-04, -1.2441e-02,\n",
       "                       2.0795e-02, -1.3711e-02,  2.0339e-02, -9.6687e-03, -1.5100e-03,\n",
       "                      -1.2402e-02,  1.4365e-02,  4.2621e-03,  1.8102e-02, -6.6819e-03,\n",
       "                       4.1195e-03, -3.3865e-03,  1.6948e-02,  8.8938e-03,  1.5476e-02,\n",
       "                       9.5680e-03, -1.3189e-02, -3.3402e-03,  1.3307e-02, -5.7769e-03,\n",
       "                       5.7748e-03,  6.1341e-03,  2.1389e-02,  1.5376e-02,  1.0627e-02,\n",
       "                      -1.6123e-02, -4.2584e-03,  4.8605e-03,  5.8339e-03, -2.0020e-02,\n",
       "                      -1.9905e-02,  1.2909e-02,  6.0877e-03, -9.3208e-03,  7.9707e-03,\n",
       "                      -7.4660e-03, -1.0683e-02,  2.0656e-02,  1.4786e-02,  7.7393e-03,\n",
       "                       2.2091e-02,  3.3865e-03,  1.2695e-02,  8.9610e-03, -2.1932e-02,\n",
       "                       1.3369e-02, -1.1635e-02,  4.6547e-03, -7.0882e-03,  1.2598e-02,\n",
       "                       4.6004e-03, -9.2549e-03,  5.6966e-03,  9.5057e-03, -1.2244e-02,\n",
       "                       8.8674e-03,  1.5985e-02,  2.1815e-02,  7.8997e-03, -1.9818e-02,\n",
       "                       4.8254e-03, -9.5736e-03,  2.1237e-02,  1.6990e-02,  8.6688e-03,\n",
       "                      -1.3262e-02, -4.2839e-04, -1.1357e-02, -1.3570e-02,  1.4499e-02,\n",
       "                      -1.5667e-02, -8.6371e-03, -1.9075e-02,  1.0610e-02,  5.6299e-03,\n",
       "                       1.5768e-02, -7.8117e-03, -8.6240e-03, -3.3488e-03,  4.6609e-03,\n",
       "                      -5.4107e-05, -1.1197e-03,  1.5267e-02,  1.6394e-02, -3.1962e-03,\n",
       "                      -3.5832e-03, -1.4557e-02,  1.8056e-02,  9.0503e-03, -9.2193e-03,\n",
       "                       4.7540e-04,  7.7932e-04,  1.6167e-02, -7.2688e-03,  4.9218e-03,\n",
       "                       1.9990e-02, -2.2309e-03, -1.1245e-02, -1.7299e-04, -1.4347e-02,\n",
       "                      -1.5271e-02,  2.1138e-03,  1.2351e-02,  2.1645e-04,  8.2656e-03,\n",
       "                      -4.3321e-03,  1.5287e-02,  1.5030e-02, -4.4991e-04, -4.7924e-03,\n",
       "                      -9.4809e-03, -4.7777e-03,  2.9117e-03,  1.7179e-02, -4.4496e-03,\n",
       "                       2.1005e-02, -6.0120e-03, -4.8798e-03, -1.2183e-02,  1.2918e-02,\n",
       "                       2.3030e-02, -6.4155e-03, -2.7703e-03, -9.6577e-03, -2.1263e-02,\n",
       "                       2.0255e-02, -3.7575e-03, -1.6812e-03, -1.8652e-02,  2.0889e-02,\n",
       "                       1.5805e-02,  1.7577e-02, -5.0995e-03, -1.9697e-02,  6.3566e-03,\n",
       "                      -2.8048e-03, -4.4728e-03,  7.7371e-03,  5.2879e-04, -1.4063e-02,\n",
       "                       5.2804e-03,  2.1222e-02,  1.2136e-02,  1.5728e-02, -3.7742e-03,\n",
       "                      -2.1144e-02, -1.3548e-02, -1.8529e-02,  1.1347e-02,  1.4645e-02,\n",
       "                       6.5521e-04,  1.4251e-02,  5.4167e-03, -1.8900e-02,  1.7385e-02,\n",
       "                       7.2031e-03, -1.8578e-02, -1.7972e-03,  6.0413e-03, -2.0530e-02,\n",
       "                      -8.3594e-03,  8.2011e-03,  2.1842e-02, -2.0160e-02,  1.6352e-02,\n",
       "                       6.5400e-03, -8.6125e-03, -1.7067e-02, -2.6084e-03,  2.1046e-02,\n",
       "                      -2.0112e-02,  1.1413e-02, -1.2670e-02, -9.4481e-04,  8.5833e-03,\n",
       "                       1.0805e-03,  4.1324e-03,  4.2686e-03, -1.0304e-02, -1.2167e-02,\n",
       "                       1.9717e-02,  6.3432e-03, -1.7240e-02, -1.7574e-02,  9.9338e-03,\n",
       "                      -8.5583e-03,  1.3129e-02, -4.0948e-03, -1.1248e-02, -1.5229e-02,\n",
       "                      -1.8666e-02,  1.4488e-02,  1.8461e-02, -2.3071e-03,  6.5715e-03,\n",
       "                      -1.5402e-02, -3.6749e-03, -6.4246e-03, -1.2099e-02,  1.4376e-03,\n",
       "                      -1.6310e-03, -1.9049e-02,  1.6218e-02, -7.2614e-03,  2.0800e-03,\n",
       "                       1.2371e-02,  1.1689e-02, -8.4292e-03, -1.6004e-02, -4.3028e-03,\n",
       "                       1.6115e-02,  1.3068e-02,  1.2593e-02, -1.4827e-03,  2.2742e-02,\n",
       "                      -1.5195e-02,  2.2040e-02,  1.7178e-02,  1.9612e-02, -1.8362e-02,\n",
       "                       1.3937e-02, -1.0465e-03, -8.4362e-03,  2.1359e-02, -3.6130e-03,\n",
       "                       1.2827e-02, -2.4098e-04, -1.2985e-02,  3.4403e-04, -1.6733e-02,\n",
       "                      -7.9467e-04,  1.5374e-02, -1.9767e-02,  2.1378e-02,  1.4380e-02,\n",
       "                       1.5017e-02,  1.5502e-02,  1.4229e-02,  2.1800e-03, -4.4236e-03,\n",
       "                       2.0604e-02,  2.1985e-02, -1.6457e-02, -5.6937e-03, -1.4203e-02,\n",
       "                       7.8176e-03,  7.3258e-03,  6.5030e-03, -1.6167e-02,  7.9791e-03,\n",
       "                      -1.4603e-02,  9.5296e-03, -5.4324e-03, -1.2740e-02,  1.6540e-02,\n",
       "                      -9.6136e-03,  9.8199e-03,  9.0375e-03, -4.7168e-03,  1.4224e-02,\n",
       "                       1.9338e-02, -1.3866e-03,  1.9867e-02, -1.7772e-02, -3.7069e-03,\n",
       "                       7.1615e-03,  1.3877e-02,  4.0938e-03,  3.6228e-03, -2.2117e-02,\n",
       "                      -3.5575e-03,  1.9530e-02,  4.9798e-03, -1.5519e-02, -1.0984e-02,\n",
       "                      -2.2130e-02,  6.3060e-03,  1.0475e-02,  1.5753e-02, -1.2076e-02,\n",
       "                       1.2497e-02, -2.5638e-03,  1.7128e-02,  4.3722e-04,  2.2321e-02,\n",
       "                       1.5876e-02,  4.7526e-03,  9.6241e-03,  1.2639e-02,  1.5768e-02,\n",
       "                       7.2068e-03, -6.7314e-03,  1.3799e-02,  6.7809e-03,  7.8075e-03,\n",
       "                       1.3050e-02,  1.9341e-02, -3.4571e-04,  3.9084e-03,  3.8042e-03,\n",
       "                       2.1186e-02, -1.6529e-02,  2.1026e-02,  1.7258e-02, -1.1417e-02,\n",
       "                       6.4663e-03,  7.3309e-03, -1.8820e-02,  1.5246e-02, -1.7889e-02,\n",
       "                       9.3579e-04,  3.7743e-03,  1.2566e-02, -1.2411e-02, -2.2130e-02,\n",
       "                       2.0366e-02,  1.2853e-02,  2.1420e-02,  9.4818e-03, -2.2074e-02,\n",
       "                      -9.5855e-03,  2.0504e-02, -6.6835e-03, -7.3368e-03, -1.3472e-02,\n",
       "                      -6.5845e-03, -8.4792e-03,  9.0328e-03, -1.7121e-02, -1.4922e-02,\n",
       "                       6.8720e-03,  1.5402e-02, -6.9500e-03, -7.9388e-03, -5.2289e-03,\n",
       "                       1.0884e-02,  7.5447e-03, -7.7954e-04,  1.4411e-02, -1.3847e-02,\n",
       "                       7.2909e-03, -1.7730e-02,  7.7982e-03,  3.4077e-03,  2.1977e-02,\n",
       "                       4.0875e-03, -2.7331e-03])),\n",
       "             ('transformer.resblocks.3.ln_2.weight',\n",
       "              tensor([0.9985, 1.0006, 0.9968, 1.0017, 1.0012, 1.0000, 1.0016, 1.0014, 1.0040,\n",
       "                      1.0000, 1.0027, 1.0029, 1.0014, 1.0030, 1.0010, 1.0029, 1.0023, 1.0052,\n",
       "                      1.0036, 1.0021, 1.0013, 1.0003, 1.0026, 0.9981, 1.0038, 1.0022, 1.0030,\n",
       "                      0.9989, 0.9993, 1.0000, 1.0001, 1.0023, 1.0005, 0.9995, 1.0036, 1.0009,\n",
       "                      1.0000, 1.0021, 1.0017, 0.9980, 1.0039, 1.0030, 0.9979, 1.0008, 1.0020,\n",
       "                      1.0017, 1.0010, 0.9998, 0.9974, 0.9990, 1.0026, 0.9996, 1.0002, 1.0013,\n",
       "                      1.0027, 1.0020, 1.0016, 1.0029, 1.0021, 1.0024, 1.0008, 1.0026, 1.0010,\n",
       "                      1.0018, 1.0010, 1.0002, 0.9999, 0.9996, 1.0023, 1.0031, 1.0010, 1.0030,\n",
       "                      1.0009, 0.9995, 1.0013, 1.0055, 1.0010, 1.0037, 1.0027, 0.9996, 1.0026,\n",
       "                      1.0019, 1.0018, 1.0001, 1.0024, 1.0022, 1.0020, 1.0024, 1.0014, 1.0031,\n",
       "                      1.0024, 1.0022, 0.9998, 1.0000, 1.0007, 1.0027, 1.0036, 0.9997, 1.0019,\n",
       "                      1.0012, 1.0022, 1.0018, 1.0019, 1.0000, 0.9994, 0.9996, 1.0024, 1.0019,\n",
       "                      1.0006, 1.0020, 1.0041, 1.0006, 1.0012, 0.9982, 1.0029, 1.0024, 1.0015,\n",
       "                      1.0014, 1.0029, 0.9998, 1.0008, 0.9998, 1.0019, 0.9996, 1.0015, 1.0020,\n",
       "                      0.9990, 1.0044, 0.9997, 1.0002, 1.0019, 1.0003, 1.0028, 1.0021, 1.0022,\n",
       "                      1.0028, 1.0011, 1.0006, 0.9979, 1.0004, 1.0023, 1.0015, 1.0007, 1.0028,\n",
       "                      1.0025, 1.0033, 1.0034, 1.0015, 1.0015, 1.0022, 1.0008, 1.0015, 1.0021,\n",
       "                      1.0056, 1.0025, 1.0030, 1.0025, 1.0032, 1.0014, 1.0034, 1.0015, 1.0015,\n",
       "                      1.0004, 1.0040, 0.9991, 1.0022, 1.0029, 1.0013, 1.0012, 0.9992, 1.0040,\n",
       "                      1.0015, 1.0036, 1.0008, 1.0052, 1.0023, 0.9989, 1.0008, 1.0016, 1.0034,\n",
       "                      1.0019, 1.0021, 1.0027, 1.0025, 0.9989, 1.0037, 1.0014, 1.0012, 1.0002,\n",
       "                      0.9968, 1.0040, 1.0025, 1.0011, 0.9977, 1.0028, 1.0022, 1.0037, 0.9990,\n",
       "                      1.0034, 1.0041, 1.0041, 0.9999, 1.0016, 1.0044, 0.9985, 1.0061, 0.9998,\n",
       "                      1.0026, 0.9991, 1.0021, 0.9998, 0.9999, 1.0001, 1.0014, 0.9977, 1.0018,\n",
       "                      1.0000, 1.0031, 1.0000, 1.0005, 1.0015, 1.0036, 1.0021, 1.0010, 1.0013,\n",
       "                      1.0067, 1.0035, 1.0036, 1.0041, 1.0017, 1.0013, 1.0007, 1.0029, 1.0014,\n",
       "                      1.0024, 1.0020, 1.0030, 1.0021, 1.0017, 1.0021, 0.9993, 1.0029, 1.0007,\n",
       "                      0.9998, 1.0015, 1.0042, 1.0031, 1.0013, 1.0029, 1.0006, 1.0007, 1.0024,\n",
       "                      1.0000, 1.0003, 0.9998, 1.0022, 1.0013, 1.0008, 1.0021, 1.0023, 1.0015,\n",
       "                      1.0048, 1.0016, 1.0008, 1.0027, 1.0032, 0.9995, 1.0038, 0.9998, 1.0005,\n",
       "                      1.0037, 1.0022, 1.0028, 1.0012, 0.9992, 1.0010, 1.0002, 1.0017, 1.0004,\n",
       "                      1.0004, 1.0007, 1.0018, 1.0051, 1.0030, 1.0030, 0.9983, 1.0013, 0.9976,\n",
       "                      1.0023, 1.0027, 1.0036, 1.0028, 1.0009, 1.0017, 1.0008, 0.9991, 0.9988,\n",
       "                      1.0038, 1.0019, 1.0020, 1.0028, 0.9992, 1.0037, 1.0015, 1.0007, 1.0028,\n",
       "                      1.0014, 1.0010, 1.0011, 1.0001, 1.0039, 0.9998, 0.9989, 1.0025, 1.0026,\n",
       "                      1.0018, 0.9997, 1.0008, 1.0001, 1.0033, 1.0025, 1.0030, 1.0028, 1.0048,\n",
       "                      1.0051, 1.0014, 0.9975, 1.0032, 1.0023, 1.0011, 1.0017, 0.9997, 1.0017,\n",
       "                      1.0044, 1.0023, 1.0032, 1.0004, 0.9976, 0.9994, 0.9974, 1.0033, 1.0026,\n",
       "                      1.0032, 1.0035, 1.0006, 1.0037, 0.9991, 1.0021, 1.0026, 0.9987, 1.0058,\n",
       "                      1.0001, 1.0025, 1.0017, 1.0024, 0.9992, 1.0007, 1.0031, 0.9987, 1.0024,\n",
       "                      1.0016, 1.0025, 1.0011, 1.0008, 1.0020, 1.0028, 1.0040, 1.0049, 1.0029,\n",
       "                      0.9993, 1.0005, 1.0005, 1.0017, 0.9992, 0.9997, 1.0031, 1.0032, 1.0001,\n",
       "                      0.9996, 1.0042, 1.0052, 1.0006, 1.0034, 1.0031, 1.0015, 0.9978, 1.0019,\n",
       "                      1.0014, 1.0031, 1.0002, 0.9994, 1.0017, 0.9998, 1.0020, 0.9998, 1.0039,\n",
       "                      1.0007, 1.0036, 1.0004, 1.0024, 1.0004, 1.0036, 1.0007, 1.0017, 1.0012,\n",
       "                      1.0028, 1.0025, 1.0034, 1.0034, 1.0027, 1.0003, 1.0018, 1.0019, 1.0022,\n",
       "                      1.0028, 1.0030, 1.0016, 1.0015, 1.0036, 1.0025, 1.0037, 1.0024, 1.0023,\n",
       "                      1.0017, 1.0032, 0.9997, 1.0043, 1.0026, 1.0012, 1.0023, 0.9998, 1.0020,\n",
       "                      1.0010, 1.0012, 1.0021, 1.0043, 1.0030, 1.0021, 1.0029, 1.0015, 0.9996,\n",
       "                      1.0011, 1.0008, 1.0017, 1.0051, 1.0024, 0.9990, 0.9994, 1.0008, 1.0003,\n",
       "                      1.0022, 1.0020, 0.9999, 1.0004, 1.0014, 1.0025, 1.0005, 1.0028, 1.0016,\n",
       "                      1.0006, 1.0011, 1.0009, 1.0003, 1.0016, 1.0039, 1.0017, 1.0019, 0.9987,\n",
       "                      1.0012, 1.0021, 1.0041, 1.0029, 1.0020, 1.0013, 0.9998, 1.0020, 1.0035,\n",
       "                      1.0001, 1.0024, 1.0025, 1.0037, 1.0020, 1.0022, 1.0036, 1.0018, 1.0028,\n",
       "                      1.0004, 1.0017, 1.0020, 1.0027, 1.0017, 0.9998, 1.0016, 1.0027, 1.0010,\n",
       "                      1.0013, 1.0057, 1.0020, 1.0021, 1.0013, 0.9995, 1.0007, 1.0032, 0.9997,\n",
       "                      0.9992, 1.0006, 1.0015, 0.9995, 1.0010, 1.0036, 1.0013, 1.0017])),\n",
       "             ('transformer.resblocks.3.ln_2.bias',\n",
       "              tensor([ 7.0525e-04, -1.0080e-03, -1.1541e-04,  9.8632e-04, -8.4418e-04,\n",
       "                       2.5932e-03,  4.6204e-05, -1.2504e-03, -2.0696e-03, -6.6637e-06,\n",
       "                       4.6071e-04,  1.6400e-03, -2.4061e-03,  7.3408e-04,  1.1761e-03,\n",
       "                       2.9553e-03, -1.0573e-03,  2.5455e-04, -1.2550e-03,  4.4080e-04,\n",
       "                       1.1156e-03,  3.2369e-03, -1.7572e-03, -8.9840e-04, -1.1053e-03,\n",
       "                       8.7524e-04,  1.8608e-03,  4.4265e-05, -1.5523e-03, -3.0897e-03,\n",
       "                       2.4167e-03, -1.1698e-03, -1.5992e-03,  4.0401e-03, -5.2688e-04,\n",
       "                       2.1936e-03, -5.5257e-04,  9.8818e-04, -1.4932e-04, -1.1727e-03,\n",
       "                      -1.6793e-03, -4.0486e-03, -2.5383e-03,  3.1958e-03,  5.8040e-04,\n",
       "                      -2.9995e-03,  1.4835e-03,  4.1808e-04,  1.9329e-03, -2.4676e-03,\n",
       "                      -3.0448e-05,  2.2216e-03, -1.2538e-04, -1.0762e-03, -2.2929e-03,\n",
       "                      -9.9071e-04, -9.7847e-04,  1.4167e-03, -9.4269e-04, -1.3619e-03,\n",
       "                      -1.0026e-04, -8.7794e-04,  1.2830e-03,  2.7048e-03,  1.2464e-03,\n",
       "                      -5.5926e-04,  3.1330e-06,  1.6418e-04, -3.9449e-04,  8.1995e-05,\n",
       "                       4.5408e-04,  1.3482e-03,  2.7104e-03, -4.1295e-04, -3.6592e-04,\n",
       "                      -1.8879e-03,  3.0383e-03,  2.2332e-04,  2.5196e-03,  1.9705e-04,\n",
       "                       3.3726e-04,  4.2562e-04,  1.7332e-03, -7.9571e-04, -1.7246e-03,\n",
       "                      -1.1639e-03, -1.1358e-04, -1.7947e-03,  1.5683e-03,  1.4057e-03,\n",
       "                      -1.3758e-03,  3.4464e-04, -3.2796e-04, -9.9155e-04, -1.3726e-03,\n",
       "                      -1.1227e-03, -4.3490e-03,  1.3230e-03,  1.3892e-03,  3.3990e-04,\n",
       "                       2.3484e-03, -4.1518e-04,  2.4759e-04,  4.4348e-03,  1.1392e-03,\n",
       "                       1.1116e-04,  1.4229e-03,  3.0855e-03,  2.1583e-03, -2.9449e-03,\n",
       "                       1.6063e-03, -1.3810e-03, -2.4750e-03, -6.5625e-04,  3.1825e-04,\n",
       "                       8.3004e-04,  2.3915e-03, -3.5755e-04,  4.2590e-03, -3.0329e-03,\n",
       "                      -9.7484e-04,  4.8311e-04, -7.6144e-04, -3.5786e-04, -1.2319e-03,\n",
       "                       5.2428e-04, -1.3708e-05, -1.2941e-04, -3.7417e-03, -3.3635e-04,\n",
       "                       1.2842e-03,  8.5784e-04,  1.1643e-03,  2.3335e-03, -2.4968e-03,\n",
       "                       1.7850e-03,  1.4181e-03, -2.0127e-04, -5.7117e-04, -2.2644e-03,\n",
       "                       1.0186e-03, -3.8014e-04, -1.5719e-03,  1.2038e-03,  1.1261e-03,\n",
       "                      -5.0198e-04, -1.6160e-03,  9.3859e-04, -1.7272e-03,  2.7996e-03,\n",
       "                       3.1320e-04, -1.9720e-03, -2.3019e-03,  1.2831e-03,  1.1825e-03,\n",
       "                       3.7408e-03, -1.0147e-03,  1.2203e-03,  5.1837e-03, -3.2189e-04,\n",
       "                      -7.3332e-04, -1.1921e-03,  5.1855e-04, -5.8448e-04,  2.3716e-03,\n",
       "                       5.0288e-04, -5.1955e-04, -9.1668e-04,  1.1187e-03,  2.7363e-03,\n",
       "                       2.3246e-03, -2.6166e-03, -2.9054e-04, -7.9319e-04, -1.1660e-03,\n",
       "                      -3.3886e-03,  1.4235e-03,  1.7899e-03,  2.0892e-04, -3.0694e-04,\n",
       "                      -9.2871e-05, -2.2787e-03,  1.2819e-03,  9.2320e-04, -2.2683e-03,\n",
       "                      -4.0170e-05,  5.3582e-05,  7.7139e-05, -3.5484e-04,  5.2989e-03,\n",
       "                       6.3844e-04,  1.3836e-03, -8.0682e-04, -1.4723e-03,  1.6756e-03,\n",
       "                       2.4333e-03,  1.8856e-03, -6.7253e-04,  3.2843e-04,  4.8485e-04,\n",
       "                       4.9786e-04,  1.4691e-03, -2.5092e-03, -6.7086e-04,  1.8036e-03,\n",
       "                       1.9549e-03, -5.2367e-04, -6.2671e-04, -2.7448e-03,  1.4985e-03,\n",
       "                       5.3587e-04, -9.8311e-04,  1.0077e-03,  5.1487e-04,  1.2407e-03,\n",
       "                       6.9351e-04,  3.5713e-04, -1.5763e-03, -1.3304e-03, -6.3982e-04,\n",
       "                      -1.9448e-04,  1.6173e-04,  6.3908e-04, -1.6319e-03, -5.0166e-04,\n",
       "                       8.9092e-04, -2.1586e-03, -9.5202e-05, -1.7039e-03, -1.8413e-03,\n",
       "                       1.9611e-03,  8.5019e-04,  8.2440e-04,  6.4437e-04, -1.0031e-04,\n",
       "                       1.0303e-03, -2.3609e-04,  4.6430e-04,  1.1966e-03,  1.3260e-03,\n",
       "                       3.7952e-03,  5.6845e-04, -6.6301e-04,  3.1701e-03, -5.6359e-04,\n",
       "                      -1.9626e-03,  4.0367e-04, -1.9359e-03, -1.5724e-04, -2.3125e-03,\n",
       "                       9.3039e-04,  8.1273e-04,  1.3213e-03,  1.8451e-03,  1.6286e-03,\n",
       "                      -5.4422e-04, -1.2035e-03,  5.8880e-05,  3.0687e-04,  1.4931e-04,\n",
       "                       1.4582e-03, -5.8100e-05,  1.2549e-03,  1.7984e-03,  6.8165e-04,\n",
       "                       1.4016e-03,  1.4800e-03,  7.6559e-04, -2.4218e-03, -8.5604e-04,\n",
       "                      -2.3288e-03,  8.2200e-04, -4.3826e-04,  1.2490e-03,  1.2797e-03,\n",
       "                       1.1323e-03, -2.9438e-03, -2.1743e-03,  8.0637e-04, -6.5364e-04,\n",
       "                      -4.7579e-04,  1.1417e-03, -8.2485e-04, -6.5200e-04,  3.2630e-04,\n",
       "                      -1.3597e-04,  1.6423e-03, -7.0570e-04,  3.8786e-03,  2.6954e-03,\n",
       "                       7.5291e-04,  4.9423e-04, -1.5876e-03, -1.4201e-03, -2.8533e-03,\n",
       "                      -8.4906e-04,  5.8540e-04, -1.0386e-04, -3.0205e-04, -5.3505e-06,\n",
       "                      -9.6618e-04, -1.2307e-04,  1.4493e-03,  2.4579e-03, -6.4822e-04,\n",
       "                      -1.4247e-04, -9.2518e-04,  1.7299e-03, -1.8297e-03,  9.3454e-04,\n",
       "                      -7.6952e-05, -7.7843e-04, -7.7669e-04,  5.0798e-04,  3.8001e-04,\n",
       "                      -1.9347e-03, -1.4060e-04,  1.1558e-03,  7.8181e-04,  1.0834e-03,\n",
       "                       1.6092e-03, -1.4279e-03,  5.2805e-04, -7.4512e-04, -9.3622e-04,\n",
       "                      -9.5262e-04,  1.0144e-03,  1.2509e-03,  1.7358e-03, -4.3775e-03,\n",
       "                      -3.3861e-03, -1.5937e-03,  1.8479e-03,  1.8326e-03,  2.7920e-03,\n",
       "                       1.2567e-03, -6.7769e-04,  1.1531e-03,  2.5096e-03,  2.3194e-03,\n",
       "                      -3.0320e-04,  5.6412e-04,  3.7122e-04, -8.9617e-04, -6.4568e-04,\n",
       "                       3.1424e-03,  1.6724e-03, -1.7363e-03, -2.1291e-03,  1.6665e-03,\n",
       "                      -6.3102e-04,  1.3857e-03, -1.8079e-03, -1.2942e-03,  6.8173e-04,\n",
       "                      -4.4354e-03,  8.4687e-04, -5.5934e-04,  2.6238e-03,  6.5150e-06,\n",
       "                       9.0495e-04, -1.3522e-03,  6.6624e-04,  2.8273e-03, -2.1609e-03,\n",
       "                       2.0450e-03, -1.6219e-04, -5.6482e-05,  1.8004e-04,  1.2604e-03,\n",
       "                      -1.2699e-03,  4.3629e-04, -1.7586e-03, -5.8706e-04,  5.9260e-04,\n",
       "                       4.4866e-03, -4.9300e-04,  1.6396e-03, -1.1944e-04, -1.1865e-03,\n",
       "                       6.7767e-04, -1.9480e-03, -6.9381e-04,  1.9810e-03, -2.1550e-03,\n",
       "                      -1.2176e-03, -2.8014e-04,  2.8765e-03, -2.1567e-03,  7.9658e-04,\n",
       "                       1.2436e-04,  4.3909e-04, -1.0960e-04,  2.0798e-03, -1.6639e-03,\n",
       "                      -1.7165e-03, -1.7913e-03, -1.4124e-04, -1.5513e-03, -1.1971e-05,\n",
       "                      -1.8078e-03,  9.8482e-04,  4.8929e-04,  3.6623e-04, -7.1866e-04,\n",
       "                      -4.4376e-04, -9.6299e-06,  8.8934e-04, -5.0454e-04, -5.1960e-04,\n",
       "                       4.7536e-04,  2.5970e-03, -1.2312e-03, -1.5680e-03, -4.0195e-04,\n",
       "                      -5.3123e-04, -2.4619e-03, -1.2870e-03,  1.2545e-03,  2.6398e-03,\n",
       "                      -1.4334e-03, -3.1230e-05,  2.5067e-03,  1.0541e-03, -4.8688e-04,\n",
       "                       5.9309e-04,  1.3299e-03,  2.6641e-03, -1.0093e-03,  7.8157e-04,\n",
       "                      -1.6016e-03, -7.3387e-04, -1.5766e-03,  9.1472e-04,  1.7305e-03,\n",
       "                      -1.3138e-03, -9.5588e-04,  4.1251e-03, -1.6452e-04,  2.6640e-04,\n",
       "                      -1.2746e-03, -1.6412e-03,  5.4027e-04,  1.3143e-03, -1.3241e-03,\n",
       "                      -5.0485e-04,  3.5349e-03,  1.9661e-03,  1.7487e-03, -1.7396e-03,\n",
       "                       5.3648e-04, -1.6078e-03,  8.5314e-04,  1.4915e-03,  2.4734e-04,\n",
       "                       2.0026e-03,  1.1642e-03, -2.5665e-03, -3.4679e-03,  5.9986e-04,\n",
       "                      -1.0356e-03, -4.9422e-04, -2.7800e-03, -2.1390e-03,  1.2765e-03,\n",
       "                      -2.0047e-03, -4.7550e-03, -3.9517e-03,  7.3443e-04, -1.0245e-03,\n",
       "                      -8.1882e-04, -3.4139e-04,  2.1744e-03,  3.4185e-04, -5.6270e-04,\n",
       "                       3.3053e-03, -2.6026e-03, -1.9418e-03,  6.0738e-04, -2.6428e-03,\n",
       "                       3.5495e-04, -1.6947e-03,  1.4718e-03, -9.7265e-04,  1.6731e-04,\n",
       "                       2.8896e-03,  3.6771e-05, -2.1400e-03, -1.9045e-03, -1.5388e-03,\n",
       "                      -8.7546e-04, -8.4942e-04, -1.4032e-03,  3.1346e-03, -1.6123e-04,\n",
       "                       1.0896e-03,  6.2741e-04, -1.8359e-03,  1.2386e-03,  1.9437e-03,\n",
       "                       6.2709e-04, -1.5770e-04,  1.0291e-03, -2.1568e-03, -5.8530e-04,\n",
       "                      -1.7397e-03,  2.8070e-03,  5.1234e-04,  2.1194e-03,  1.9292e-03,\n",
       "                       2.1491e-03,  2.5555e-03])),\n",
       "             ('transformer.resblocks.4.attn.in_proj_weight',\n",
       "              tensor([[-2.9977e-05,  9.0779e-02,  2.0533e-02,  ..., -6.6499e-02,\n",
       "                       -1.4057e-02, -5.0733e-02],\n",
       "                      [ 6.5693e-02,  1.3732e-02,  1.0102e-02,  ...,  6.4059e-02,\n",
       "                       -1.2841e-02,  6.6509e-02],\n",
       "                      [-8.3357e-02, -5.1397e-02, -3.6674e-03,  ...,  4.3974e-02,\n",
       "                       -2.0159e-02, -4.2068e-02],\n",
       "                      ...,\n",
       "                      [-1.6104e-02, -1.8409e-02,  4.2974e-02,  ...,  3.0739e-03,\n",
       "                       -5.4371e-02, -1.9126e-02],\n",
       "                      [-9.4987e-02, -1.5964e-02, -4.3564e-03,  ..., -1.2993e-03,\n",
       "                        1.7492e-02, -7.7231e-02],\n",
       "                      [ 1.5379e-02, -7.9369e-02, -6.1884e-02,  ..., -5.2390e-02,\n",
       "                       -1.0102e-01,  4.8598e-02]])),\n",
       "             ('transformer.resblocks.4.attn.in_proj_bias',\n",
       "              tensor([ 0.0005,  0.0044, -0.0019,  ..., -0.0012, -0.0009,  0.0011])),\n",
       "             ('transformer.resblocks.4.attn.out_proj.weight',\n",
       "              tensor([[ 6.8837e-03,  7.1463e-03,  5.9215e-03,  ..., -1.0785e-02,\n",
       "                       -1.1825e-02, -4.3921e-03],\n",
       "                      [ 5.5591e-03,  1.0299e-02,  7.0278e-03,  ...,  1.6966e-02,\n",
       "                        3.4595e-03, -1.0380e-02],\n",
       "                      [-2.2847e-03, -3.1872e-03, -8.4193e-03,  ...,  4.8170e-03,\n",
       "                       -5.6731e-05, -5.5826e-03],\n",
       "                      ...,\n",
       "                      [ 2.7595e-03,  2.1953e-02, -4.1520e-03,  ...,  9.8359e-03,\n",
       "                        2.3345e-03, -5.3715e-04],\n",
       "                      [ 1.2648e-03, -5.3049e-03,  2.1420e-03,  ..., -1.4171e-02,\n",
       "                       -5.0970e-04, -5.5682e-03],\n",
       "                      [ 1.5010e-02,  1.2983e-02,  1.7471e-02,  ..., -1.0500e-03,\n",
       "                        6.8602e-04,  1.6628e-02]])),\n",
       "             ('transformer.resblocks.4.attn.out_proj.bias',\n",
       "              tensor([ 1.0701e-04, -1.4125e-03, -6.0178e-04, -4.1122e-04,  3.7985e-05,\n",
       "                      -1.1110e-03,  8.1144e-04,  1.8608e-03, -1.0009e-03,  4.7030e-04,\n",
       "                      -1.8490e-03, -3.4507e-04, -4.8290e-04, -5.3953e-04, -1.4953e-03,\n",
       "                       5.8117e-04, -6.0534e-04,  6.3953e-04,  6.6298e-05, -1.0151e-03,\n",
       "                      -5.3026e-04, -7.7708e-04,  1.6273e-04, -1.0218e-03, -1.0352e-03,\n",
       "                      -4.3104e-05, -5.6953e-04, -8.0118e-04, -7.6660e-04, -5.9475e-04,\n",
       "                       2.1743e-04,  6.5686e-04,  3.7513e-04, -2.2833e-03, -3.8660e-04,\n",
       "                       1.3470e-03, -1.8474e-05, -3.3186e-04, -4.3260e-04, -1.5652e-03,\n",
       "                       9.5142e-04,  1.2551e-03,  1.5657e-03,  1.4330e-04, -1.4279e-03,\n",
       "                       7.2975e-04, -1.1984e-03, -1.2413e-04, -2.9560e-04,  9.2757e-04,\n",
       "                      -3.4314e-04, -1.2835e-03, -7.1318e-05,  2.7566e-04,  1.6939e-03,\n",
       "                       7.4321e-04, -3.6005e-04, -1.2264e-03,  2.4130e-04, -7.1545e-04,\n",
       "                      -8.4239e-04,  3.6269e-04, -3.8638e-05,  4.3317e-04, -1.4206e-03,\n",
       "                      -6.3904e-04,  1.4028e-03,  9.7105e-04,  6.4532e-04,  1.2155e-03,\n",
       "                      -1.0937e-03,  2.9588e-04, -2.9245e-04, -6.8377e-04, -6.7291e-04,\n",
       "                       1.4650e-03, -1.5478e-03, -6.9594e-04, -1.4376e-03,  1.3641e-03,\n",
       "                      -2.6384e-04, -4.6395e-04,  7.4352e-05,  1.2742e-03,  1.0599e-03,\n",
       "                      -5.3038e-04, -7.4895e-04,  3.4249e-04, -8.3838e-04, -9.0396e-04,\n",
       "                       3.9288e-04,  2.3467e-05,  3.6978e-04,  2.0310e-04, -1.5027e-03,\n",
       "                      -1.5034e-03,  9.1725e-04, -7.2161e-04, -3.0770e-04,  3.4146e-04,\n",
       "                       3.4687e-04, -1.0827e-03,  3.2893e-04, -6.0999e-04, -4.6403e-04,\n",
       "                      -8.9069e-04, -1.1501e-03,  1.8961e-04,  9.9525e-04,  1.2464e-04,\n",
       "                       1.2947e-03,  9.8721e-04,  1.3642e-03,  8.3541e-04, -6.7469e-04,\n",
       "                      -1.6954e-03,  1.1075e-03,  6.6005e-04, -9.8002e-04, -9.1491e-04,\n",
       "                       3.0115e-04, -2.9888e-04,  1.0928e-03, -7.1634e-04, -1.1327e-04,\n",
       "                       3.5564e-04, -9.0694e-04, -1.0264e-03, -1.0630e-04, -9.0509e-05,\n",
       "                       3.1803e-04,  1.0184e-03, -5.8198e-04, -1.4324e-03, -7.7054e-04,\n",
       "                       3.7145e-04, -1.7299e-04,  3.8363e-04, -4.4430e-04,  6.4239e-04,\n",
       "                      -8.9105e-05,  1.5195e-03,  9.3479e-04,  1.6942e-03,  3.6633e-04,\n",
       "                       8.5314e-04, -2.1279e-04, -1.1701e-04,  1.2975e-03,  3.7744e-04,\n",
       "                       1.4615e-03,  2.4738e-03,  2.4512e-03,  5.1166e-04, -1.7935e-04,\n",
       "                       1.2459e-04,  8.8614e-04, -8.1821e-04, -1.5944e-04,  1.1707e-04,\n",
       "                      -5.7445e-04, -2.8283e-04,  9.7671e-04,  1.8011e-04, -1.0316e-03,\n",
       "                       3.7412e-04,  1.1260e-03, -4.1771e-04,  5.5235e-05, -6.4569e-04,\n",
       "                       1.7528e-04,  2.1489e-04, -4.9988e-04, -1.3754e-04,  1.7018e-03,\n",
       "                       2.4914e-04, -8.3500e-04,  3.1355e-04,  5.4053e-04, -1.3121e-04,\n",
       "                      -1.9012e-03, -2.9046e-04,  4.5861e-04, -1.8125e-03,  2.5732e-03,\n",
       "                      -2.4651e-04,  9.4212e-04, -3.1066e-05,  7.1588e-04, -2.4422e-03,\n",
       "                       4.9219e-04,  5.8476e-04,  1.0852e-04,  1.8307e-03, -3.1157e-04,\n",
       "                      -5.9013e-04,  1.2991e-03,  1.6765e-03,  1.2609e-03,  2.0623e-03,\n",
       "                      -2.8558e-04, -1.3755e-03,  1.1442e-03,  2.5031e-03,  2.7520e-04,\n",
       "                      -3.9048e-04, -8.4643e-04, -2.3969e-04,  8.9636e-04,  3.4556e-04,\n",
       "                       4.9573e-04,  3.0262e-04, -8.1957e-04,  8.2145e-04, -1.8370e-03,\n",
       "                      -5.8695e-04,  2.0307e-04, -1.5220e-03, -7.4053e-04,  1.2559e-04,\n",
       "                      -8.4829e-04, -3.8886e-04,  6.6536e-04,  6.2357e-04,  6.5968e-04,\n",
       "                       2.3933e-04, -1.4737e-04, -7.2008e-04,  1.1976e-03,  1.2134e-03,\n",
       "                      -8.9313e-04,  7.6214e-04,  7.4967e-04, -5.7017e-05,  1.0260e-03,\n",
       "                       2.0679e-04,  1.9043e-03, -1.6586e-03,  1.4093e-05,  6.9502e-05,\n",
       "                      -3.4508e-04,  2.6038e-04,  1.1158e-03,  6.0891e-04, -1.1053e-03,\n",
       "                      -6.2534e-06,  6.8595e-04, -3.4166e-04, -6.4796e-04, -1.0835e-03,\n",
       "                       6.3204e-04, -1.0910e-03, -3.9547e-05, -2.5664e-04, -1.3161e-03,\n",
       "                       4.9012e-04,  5.4888e-04, -3.4844e-04, -7.7800e-04, -1.2931e-03,\n",
       "                       9.3830e-04, -6.0804e-04,  1.2003e-03, -1.3325e-03, -7.5898e-04,\n",
       "                       1.5683e-03,  4.5581e-04,  6.0514e-04, -1.3223e-04, -9.5583e-04,\n",
       "                       1.0503e-03, -5.9797e-04, -1.0422e-03, -5.9236e-05,  7.7909e-05,\n",
       "                      -1.1057e-03,  1.7302e-03, -7.8286e-05,  2.2204e-03, -6.8496e-04,\n",
       "                      -3.4709e-04,  2.1739e-03, -2.3103e-04, -9.8244e-04, -1.6738e-03,\n",
       "                       1.3106e-03,  7.3115e-04,  9.2536e-04,  8.6948e-04, -7.8405e-04,\n",
       "                       1.8621e-04,  4.1791e-04, -1.3595e-03,  1.5951e-03, -6.7007e-05,\n",
       "                       1.8047e-03, -6.9384e-04,  7.1180e-04, -1.0028e-03, -1.7915e-03,\n",
       "                      -1.3605e-03,  1.0733e-03,  6.1034e-04, -1.8844e-03,  1.6856e-04,\n",
       "                      -1.0064e-03, -1.0026e-04,  5.5007e-04, -1.5130e-03, -1.8145e-03,\n",
       "                       3.7696e-04,  1.7236e-03, -7.8965e-04, -1.4897e-03, -1.7525e-04,\n",
       "                       1.7113e-03,  1.7536e-03,  8.2100e-04,  2.1555e-04,  8.2075e-04,\n",
       "                       1.5046e-03,  5.5333e-05, -7.5568e-04,  5.6040e-04, -5.8821e-04,\n",
       "                       6.6879e-04,  1.3207e-03,  2.4066e-04, -1.7405e-03, -1.7049e-03,\n",
       "                       1.4008e-04,  1.6504e-03,  4.9109e-04,  3.9070e-04,  1.9642e-04,\n",
       "                      -1.8040e-04, -6.5225e-04, -2.7623e-04, -8.8013e-06,  1.1426e-03,\n",
       "                      -1.5825e-03,  1.2946e-03,  1.0419e-03, -2.9336e-04, -1.8035e-03,\n",
       "                      -2.3512e-04, -1.4635e-03,  8.5209e-04, -7.5036e-05,  1.1072e-03,\n",
       "                      -1.3028e-04, -2.7114e-03, -9.3493e-04, -4.3212e-04, -5.0287e-04,\n",
       "                       3.1841e-04,  2.9169e-03,  1.5966e-03,  9.0227e-04, -1.5018e-03,\n",
       "                       1.8029e-04, -2.6649e-04,  1.0183e-03,  1.7345e-03, -6.7304e-05,\n",
       "                       1.1091e-03,  9.5785e-04, -1.5629e-03,  1.7279e-03,  4.2978e-04,\n",
       "                      -1.9577e-04,  5.3422e-04,  1.1794e-04, -6.0950e-04,  6.8378e-05,\n",
       "                       1.9415e-03, -1.4599e-03,  7.4045e-04, -2.8292e-04,  3.2430e-04,\n",
       "                       5.9249e-04,  7.9143e-04, -1.1558e-03, -5.5710e-04, -3.7360e-04,\n",
       "                      -3.8222e-04,  7.7641e-04, -2.5964e-05,  1.3482e-03,  6.3476e-04,\n",
       "                      -4.5741e-04,  2.2213e-04,  1.8357e-04,  1.3541e-03,  1.5526e-03,\n",
       "                       1.1864e-03,  2.1776e-04,  6.3512e-04,  1.6157e-04,  1.3279e-03,\n",
       "                      -3.5990e-05, -1.0617e-03, -1.6650e-03,  5.4507e-04, -5.0478e-04,\n",
       "                       4.2737e-04,  4.0553e-04, -3.3695e-04,  1.2967e-03,  7.3535e-04,\n",
       "                      -7.3789e-04, -1.4518e-03,  1.3569e-04,  1.2256e-03,  3.1746e-05,\n",
       "                      -1.3513e-03,  1.2968e-03,  6.9805e-04,  1.0264e-03,  8.2835e-04,\n",
       "                      -1.0936e-03, -7.6976e-05, -2.7072e-04, -7.8551e-04, -1.1133e-03,\n",
       "                      -5.5599e-04, -1.4949e-03, -5.3496e-04,  1.0272e-03, -2.9023e-04,\n",
       "                      -1.1913e-03,  1.5372e-03,  1.0182e-03, -1.3887e-03, -1.4237e-03,\n",
       "                      -2.0293e-04,  1.5332e-04, -1.7627e-03,  1.1534e-03, -7.6026e-06,\n",
       "                      -5.5274e-04,  4.9941e-04,  7.4484e-06, -6.7139e-04, -1.3053e-03,\n",
       "                      -6.8040e-04, -1.4089e-03,  8.4157e-04,  4.2063e-04,  1.2869e-03,\n",
       "                      -2.4687e-05,  3.3124e-05, -1.6400e-03,  1.8237e-03,  1.0680e-03,\n",
       "                      -6.2285e-04,  7.6649e-05,  1.5015e-03,  1.5117e-03, -6.5988e-04,\n",
       "                       2.7560e-04, -9.9407e-04,  1.2956e-03,  1.8070e-03,  4.2741e-04,\n",
       "                       4.1715e-05,  2.2991e-04,  7.4719e-04,  5.8048e-05, -1.3835e-03,\n",
       "                      -7.4816e-04,  4.6841e-06, -9.4058e-04,  8.0342e-05, -5.1723e-04,\n",
       "                      -9.1443e-05, -1.7171e-03, -7.0354e-05,  5.0293e-05, -5.7355e-05,\n",
       "                      -1.0845e-03, -8.9608e-04,  5.5330e-04,  3.3742e-04, -5.8636e-04,\n",
       "                      -1.6502e-06, -1.5699e-03, -1.3348e-03,  1.7918e-03,  4.8490e-04,\n",
       "                       6.3214e-04, -4.4346e-04,  1.1869e-04, -1.6104e-03, -4.1017e-04,\n",
       "                      -4.9083e-04,  1.1880e-03, -2.0930e-04, -1.1670e-03, -2.4671e-03,\n",
       "                      -9.2137e-04, -4.3471e-04, -3.0803e-04,  1.3472e-03, -9.0419e-04,\n",
       "                       1.0015e-03, -4.1529e-04, -6.5192e-04,  5.1219e-04, -1.9191e-05,\n",
       "                      -9.1677e-04, -1.6852e-04])),\n",
       "             ('transformer.resblocks.4.ln_1.weight',\n",
       "              tensor([0.9969, 0.9998, 0.9996, 0.9993, 1.0011, 1.0007, 0.9980, 1.0001, 0.9991,\n",
       "                      0.9998, 0.9992, 0.9997, 1.0031, 1.0012, 1.0006, 0.9963, 0.9990, 0.9996,\n",
       "                      1.0019, 0.9998, 0.9999, 1.0001, 1.0009, 0.9992, 0.9992, 0.9995, 0.9984,\n",
       "                      0.9973, 1.0025, 1.0007, 1.0017, 0.9983, 0.9997, 0.9992, 0.9987, 0.9964,\n",
       "                      0.9990, 0.9989, 0.9990, 0.9991, 1.0028, 1.0009, 0.9980, 0.9984, 0.9957,\n",
       "                      0.9989, 1.0022, 0.9994, 1.0005, 1.0004, 0.9993, 0.9989, 0.9983, 0.9993,\n",
       "                      0.9995, 1.0000, 0.9990, 0.9986, 0.9990, 0.9994, 0.9979, 0.9994, 0.9986,\n",
       "                      1.0012, 1.0009, 0.9978, 0.9982, 1.0012, 0.9987, 0.9994, 0.9991, 0.9979,\n",
       "                      0.9999, 0.9984, 1.0029, 1.0006, 1.0008, 0.9976, 0.9995, 1.0006, 0.9991,\n",
       "                      1.0016, 0.9996, 1.0000, 0.9962, 1.0005, 0.9995, 1.0010, 0.9980, 0.9995,\n",
       "                      0.9999, 0.9978, 1.0000, 0.9985, 0.9993, 0.9982, 0.9998, 0.9994, 0.9988,\n",
       "                      1.0012, 1.0022, 1.0011, 1.0014, 1.0018, 0.9984, 0.9991, 0.9973, 0.9991,\n",
       "                      0.9976, 1.0005, 0.9986, 0.9974, 1.0006, 1.0010, 1.0009, 0.9977, 1.0002,\n",
       "                      1.0009, 1.0011, 0.9990, 0.9994, 1.0010, 0.9982, 0.9959, 0.9970, 0.9990,\n",
       "                      0.9989, 0.9987, 1.0004, 1.0007, 1.0005, 0.9996, 1.0010, 1.0000, 0.9990,\n",
       "                      0.9987, 0.9991, 0.9993, 0.9991, 0.9988, 1.0008, 0.9952, 0.9977, 1.0018,\n",
       "                      1.0013, 0.9984, 0.9994, 1.0000, 0.9980, 0.9992, 0.9972, 0.9983, 0.9993,\n",
       "                      0.9989, 0.9984, 1.0007, 1.0009, 0.9999, 0.9974, 0.9986, 0.9983, 0.9997,\n",
       "                      1.0009, 1.0005, 0.9983, 1.0015, 0.9969, 0.9989, 1.0009, 0.9983, 0.9986,\n",
       "                      1.0021, 1.0009, 0.9971, 1.0011, 1.0006, 1.0002, 1.0011, 1.0001, 0.9999,\n",
       "                      1.0007, 1.0024, 0.9999, 0.9976, 1.0002, 0.9985, 0.9982, 0.9998, 1.0019,\n",
       "                      1.0000, 1.0018, 0.9977, 0.9992, 1.0001, 0.9994, 0.9995, 1.0004, 0.9968,\n",
       "                      0.9983, 0.9969, 0.9988, 0.9996, 1.0006, 1.0021, 0.9990, 1.0005, 1.0040,\n",
       "                      1.0016, 1.0034, 1.0014, 1.0047, 1.0012, 1.0020, 0.9983, 0.9979, 0.9994,\n",
       "                      1.0021, 1.0004, 0.9978, 0.9995, 0.9948, 0.9995, 1.0006, 1.0006, 0.9980,\n",
       "                      1.0018, 0.9994, 0.9991, 1.0016, 0.9991, 0.9960, 0.9969, 0.9991, 1.0002,\n",
       "                      0.9999, 0.9985, 0.9978, 1.0020, 0.9992, 0.9972, 0.9997, 1.0004, 1.0019,\n",
       "                      0.9992, 1.0002, 1.0006, 1.0018, 0.9980, 0.9981, 1.0007, 1.0009, 0.9972,\n",
       "                      0.9981, 0.9985, 0.9995, 1.0000, 0.9987, 0.9988, 0.9998, 0.9980, 0.9995,\n",
       "                      0.9987, 0.9991, 1.0035, 1.0000, 0.9994, 1.0004, 1.0007, 0.9978, 1.0005,\n",
       "                      1.0003, 0.9985, 1.0003, 0.9979, 0.9991, 0.9985, 1.0004, 1.0017, 0.9979,\n",
       "                      1.0001, 0.9986, 0.9994, 0.9979, 1.0042, 0.9982, 1.0013, 0.9986, 1.0018,\n",
       "                      0.9975, 0.9990, 0.9978, 1.0005, 1.0001, 0.9992, 1.0021, 1.0023, 0.9975,\n",
       "                      0.9995, 0.9988, 1.0000, 0.9996, 0.9995, 1.0014, 0.9977, 1.0018, 1.0005,\n",
       "                      0.9993, 0.9997, 0.9967, 0.9979, 1.0000, 0.9991, 1.0030, 1.0048, 0.9992,\n",
       "                      1.0001, 0.9990, 1.0012, 1.0001, 1.0034, 1.0004, 1.0002, 1.0003, 0.9981,\n",
       "                      1.0015, 0.9995, 0.9992, 0.9988, 1.0007, 1.0004, 1.0006, 0.9986, 1.0000,\n",
       "                      0.9985, 0.9975, 1.0019, 0.9983, 0.9965, 0.9957, 1.0009, 0.9987, 0.9994,\n",
       "                      0.9973, 1.0010, 0.9977, 0.9999, 0.9998, 0.9982, 0.9991, 1.0007, 1.0009,\n",
       "                      1.0002, 0.9983, 0.9971, 0.9988, 0.9995, 0.9995, 0.9968, 0.9996, 0.9990,\n",
       "                      0.9991, 0.9990, 0.9965, 0.9985, 1.0023, 0.9987, 0.9981, 1.0019, 1.0013,\n",
       "                      0.9964, 0.9984, 0.9982, 1.0006, 1.0014, 1.0004, 1.0006, 1.0001, 0.9985,\n",
       "                      0.9983, 0.9979, 0.9977, 1.0020, 0.9977, 0.9995, 0.9977, 0.9994, 0.9987,\n",
       "                      0.9997, 1.0013, 0.9991, 0.9979, 0.9996, 0.9983, 1.0019, 0.9970, 0.9987,\n",
       "                      0.9973, 1.0020, 1.0007, 0.9965, 0.9992, 0.9961, 0.9985, 0.9995, 0.9990,\n",
       "                      0.9985, 1.0005, 0.9994, 1.0001, 1.0004, 1.0018, 1.0023, 1.0003, 0.9988,\n",
       "                      1.0035, 0.9994, 0.9966, 1.0013, 0.9986, 0.9997, 0.9994, 0.9987, 0.9989,\n",
       "                      0.9982, 0.9959, 1.0015, 0.9977, 1.0010, 0.9980, 0.9953, 0.9972, 0.9977,\n",
       "                      0.9996, 1.0002, 0.9994, 1.0005, 0.9992, 1.0001, 0.9996, 0.9962, 0.9985,\n",
       "                      0.9982, 0.9997, 0.9987, 0.9990, 1.0008, 0.9989, 0.9990, 1.0010, 0.9984,\n",
       "                      0.9972, 0.9971, 0.9992, 0.9997, 0.9981, 1.0012, 1.0012, 0.9974, 0.9953,\n",
       "                      1.0008, 1.0023, 0.9963, 0.9960, 0.9982, 1.0009, 1.0022, 1.0013, 0.9999,\n",
       "                      1.0010, 1.0000, 1.0000, 1.0004, 0.9985, 0.9992, 0.9997, 1.0004, 0.9998,\n",
       "                      1.0012, 0.9997, 1.0000, 1.0038, 0.9999, 0.9988, 0.9975, 0.9982, 0.9987,\n",
       "                      0.9997, 1.0000, 1.0002, 0.9985, 0.9974, 0.9943, 1.0023, 1.0006, 1.0031,\n",
       "                      1.0002, 0.9979, 1.0006, 1.0002, 1.0009, 0.9983, 1.0018, 0.9993, 0.9987,\n",
       "                      0.9978, 0.9993, 0.9990, 1.0027, 1.0016, 0.9998, 1.0001, 0.9963])),\n",
       "             ('transformer.resblocks.4.ln_1.bias',\n",
       "              tensor([ 4.1849e-04,  1.0509e-03, -1.4732e-04,  1.1957e-04,  7.6760e-04,\n",
       "                      -3.3341e-04, -8.5496e-04, -1.5629e-03,  3.7133e-04, -1.2822e-03,\n",
       "                       2.6551e-03,  1.3447e-03, -1.0399e-03,  7.4318e-04, -5.0577e-04,\n",
       "                      -3.5178e-04,  2.4371e-03, -2.0920e-03,  2.2028e-03,  2.4433e-03,\n",
       "                       1.5457e-03, -1.1358e-03,  9.4679e-04,  1.0696e-03, -2.5419e-03,\n",
       "                      -1.7335e-03,  2.1583e-05,  4.9748e-04,  1.0059e-03, -1.2936e-03,\n",
       "                       1.3214e-03,  7.8757e-04,  3.8062e-04,  2.5926e-03, -3.1287e-04,\n",
       "                      -1.9903e-03,  4.5117e-04,  1.9366e-03,  1.8816e-03,  6.1170e-06,\n",
       "                       7.3780e-05,  2.1852e-03, -1.4948e-03, -6.9078e-04,  2.3297e-03,\n",
       "                       1.8041e-03, -5.4216e-04,  1.3346e-03,  4.5043e-04, -8.5813e-05,\n",
       "                      -3.5842e-04,  8.0744e-04, -8.7472e-04, -4.0754e-04, -8.0763e-04,\n",
       "                       1.7486e-03,  8.1464e-04, -7.0614e-04,  9.0701e-04, -5.2821e-04,\n",
       "                       1.3756e-03, -3.1329e-03, -3.1831e-03,  3.5873e-05,  6.1745e-04,\n",
       "                      -3.2886e-03, -2.0882e-03, -2.4227e-03, -2.7260e-03,  5.5955e-04,\n",
       "                       3.7154e-03,  1.4358e-03,  6.0469e-04,  1.9883e-03, -2.7435e-04,\n",
       "                       9.1241e-04,  2.8477e-03,  8.8956e-05, -1.8600e-03, -1.6500e-03,\n",
       "                      -3.1226e-03,  1.9945e-03, -1.6997e-03,  7.6192e-04,  1.1762e-03,\n",
       "                       5.8465e-04,  1.1481e-03,  1.1192e-03,  2.2681e-03, -1.2946e-03,\n",
       "                       8.8673e-04,  1.7150e-03,  1.4883e-04, -1.1398e-03,  5.4878e-03,\n",
       "                       3.6923e-03,  2.6286e-03, -5.3648e-05, -1.8743e-03,  3.8491e-04,\n",
       "                      -1.2171e-03,  8.3581e-04, -2.1576e-04, -2.6385e-03, -6.0328e-04,\n",
       "                      -1.7370e-03,  3.0697e-03,  1.6909e-03,  2.0443e-03, -7.7465e-04,\n",
       "                      -2.8187e-03, -1.3204e-03,  2.4133e-03,  7.0642e-04, -1.0772e-03,\n",
       "                      -1.1934e-03, -3.4674e-03, -1.7463e-03,  2.1479e-03,  2.4390e-03,\n",
       "                       6.7380e-04, -2.8559e-03,  1.2898e-04,  1.1734e-03, -3.4859e-03,\n",
       "                       2.0544e-03,  1.0530e-03, -8.2288e-04,  1.5530e-03, -2.2774e-03,\n",
       "                      -2.3470e-03, -7.2087e-07, -1.7089e-03, -7.9023e-04,  1.9752e-03,\n",
       "                      -1.5312e-04,  3.3027e-03,  4.8974e-04,  3.2047e-05, -1.6260e-03,\n",
       "                      -1.3154e-03, -1.1477e-03,  1.7337e-03,  2.1191e-03,  1.2189e-03,\n",
       "                       1.1661e-03,  1.8615e-03,  3.0115e-03, -9.0482e-05,  6.8998e-04,\n",
       "                       1.2521e-03, -2.1232e-03, -2.3637e-03,  2.2410e-03,  2.2588e-03,\n",
       "                       1.0693e-03,  1.9900e-04,  2.2167e-03, -2.3800e-03,  8.0758e-05,\n",
       "                      -2.0150e-03,  1.8197e-03, -6.4330e-06, -3.9622e-04,  3.8659e-04,\n",
       "                      -6.3609e-05, -3.1763e-03,  1.4943e-03, -2.3172e-04, -2.9062e-03,\n",
       "                      -2.3522e-03, -2.8469e-03,  1.0950e-03, -1.4528e-03, -3.4476e-04,\n",
       "                       3.2363e-03,  7.0278e-05, -2.7954e-04, -1.9240e-03, -3.3448e-04,\n",
       "                       1.3149e-04,  2.6120e-04, -1.9041e-03,  6.0852e-04, -9.9429e-04,\n",
       "                       1.8268e-04, -1.6492e-03, -4.9134e-04, -2.4864e-03,  9.1324e-04,\n",
       "                       2.5441e-04, -1.5387e-03,  2.3500e-03, -3.0935e-03,  3.2126e-03,\n",
       "                       5.7879e-03,  8.0986e-04, -2.5072e-03, -1.5324e-03, -1.5051e-03,\n",
       "                       5.0842e-04, -5.4421e-04, -7.1663e-04,  2.3816e-04, -3.6042e-03,\n",
       "                       2.3628e-03,  1.4981e-03,  8.8549e-04,  8.9252e-04, -1.4333e-03,\n",
       "                      -9.3212e-04,  1.2219e-03,  6.3094e-04,  9.9029e-04, -3.4545e-03,\n",
       "                       1.6282e-03,  1.7618e-03,  1.3484e-03, -6.7706e-04, -1.6863e-03,\n",
       "                       3.6116e-03,  5.8109e-04,  6.5930e-04,  4.2664e-04, -2.6154e-03,\n",
       "                      -1.7953e-03, -3.1158e-04, -1.4602e-04, -1.9735e-03, -1.6418e-03,\n",
       "                      -1.5781e-03,  1.6468e-03, -2.6315e-03, -8.4738e-04, -1.2557e-04,\n",
       "                       1.3933e-03, -1.1702e-03,  6.8713e-04,  6.3104e-04, -8.0031e-05,\n",
       "                      -8.6134e-04,  4.3033e-05, -7.3537e-04, -2.5610e-04,  1.9312e-03,\n",
       "                      -7.7175e-04, -1.1082e-03, -1.3439e-03,  3.7209e-04,  1.1522e-03,\n",
       "                       1.0437e-03,  2.3567e-03,  7.9867e-04, -1.7167e-03, -9.7261e-04,\n",
       "                       1.3528e-03, -2.0154e-03, -1.5545e-03,  9.6530e-04,  5.0143e-04,\n",
       "                      -9.8880e-04,  1.3264e-04, -4.1515e-03, -4.7253e-04,  5.9511e-04,\n",
       "                      -4.1989e-03, -1.7053e-03,  5.1654e-04,  1.7985e-03, -1.3590e-03,\n",
       "                       7.4869e-05,  1.1366e-03,  2.0326e-03,  2.1838e-03,  2.8016e-03,\n",
       "                      -1.8092e-05, -2.5734e-03,  6.4250e-04, -3.4023e-03, -2.8520e-03,\n",
       "                      -1.0782e-03, -1.9039e-04,  3.1225e-04, -9.9231e-04, -1.8892e-04,\n",
       "                      -2.0052e-03, -7.5088e-04,  5.3136e-06, -1.3314e-03, -1.4225e-03,\n",
       "                       8.7766e-04, -2.9934e-03, -2.4672e-03, -1.8834e-04,  6.5098e-04,\n",
       "                      -3.0957e-03,  2.3207e-03, -1.1156e-04,  5.0045e-04,  3.2501e-03,\n",
       "                       2.9561e-03, -2.8238e-03,  1.3772e-03,  5.7995e-04,  6.4291e-04,\n",
       "                      -6.3371e-04,  5.7259e-04, -7.6770e-04,  1.7053e-03,  2.5764e-03,\n",
       "                       2.0296e-03, -1.6200e-03, -1.7055e-03,  7.0267e-04,  6.7013e-04,\n",
       "                      -3.4647e-04, -5.2061e-04,  1.0174e-03,  3.6463e-04, -1.3768e-03,\n",
       "                      -2.3798e-03, -2.7041e-03, -1.2150e-03, -2.4624e-03, -1.9644e-03,\n",
       "                      -8.2414e-04, -2.3854e-03, -3.8093e-05,  9.2514e-04,  1.7768e-03,\n",
       "                       8.3885e-04, -2.8949e-03, -3.9499e-04, -1.8069e-03,  4.8566e-04,\n",
       "                      -8.5843e-04, -9.3588e-04,  3.9050e-04, -7.0268e-04,  9.9952e-05,\n",
       "                       4.1162e-04, -6.9960e-04, -4.5198e-03,  2.7188e-05,  5.7534e-04,\n",
       "                       2.2866e-03,  2.2083e-04, -5.1752e-04,  3.3869e-03, -2.2545e-03,\n",
       "                      -1.4023e-03,  6.5602e-04,  1.0919e-03,  1.3725e-03,  1.7441e-03,\n",
       "                       2.1464e-03, -2.1173e-03, -4.1424e-05, -1.5475e-03, -6.3466e-04,\n",
       "                      -6.6401e-05, -1.0346e-03,  8.4563e-04, -1.0491e-03,  2.0775e-03,\n",
       "                      -1.5716e-04, -1.6706e-03, -1.1294e-04,  1.8149e-03,  1.2543e-03,\n",
       "                       4.6740e-04, -1.0847e-03, -5.7981e-05,  2.9412e-03, -9.3028e-04,\n",
       "                       1.1916e-03,  2.2182e-03,  1.6622e-03,  2.0564e-03,  1.4570e-03,\n",
       "                       9.8767e-04, -1.7323e-03, -2.9296e-03, -7.3355e-04,  4.8907e-04,\n",
       "                       1.3769e-03,  5.9446e-04,  1.6562e-03, -7.9774e-04, -7.8354e-04,\n",
       "                      -1.0896e-03, -2.6517e-03,  2.9490e-04,  1.3145e-05, -3.8839e-03,\n",
       "                       2.1383e-04, -1.5267e-03, -1.0282e-03,  1.5463e-03, -4.0539e-04,\n",
       "                      -2.1387e-04, -3.6015e-04,  1.2729e-03, -1.1076e-03,  8.6566e-04,\n",
       "                       1.5914e-03,  1.5351e-03, -1.0270e-03, -2.5873e-03, -3.5648e-04,\n",
       "                       2.4360e-03,  5.8927e-04,  6.3506e-04, -2.7210e-03,  2.0444e-03,\n",
       "                       6.0329e-04,  1.6314e-03, -4.3436e-04, -8.5923e-04, -3.1784e-03,\n",
       "                      -1.8557e-03, -1.2202e-03, -1.0999e-04,  9.4894e-04,  1.7216e-03,\n",
       "                      -4.6757e-04, -8.1416e-04,  1.7373e-03, -1.0275e-03, -2.6963e-03,\n",
       "                      -3.4588e-04,  3.6756e-03,  1.3312e-03,  2.6060e-03,  1.0873e-03,\n",
       "                      -3.6316e-03,  1.2893e-03, -1.8492e-03,  1.3540e-03, -1.2636e-03,\n",
       "                       3.4736e-04,  1.4198e-03,  1.4115e-03,  3.4938e-03,  8.9338e-04,\n",
       "                      -4.3026e-04, -8.4588e-04, -1.9187e-03, -1.5718e-03, -1.1751e-03,\n",
       "                      -2.5973e-04,  2.1149e-03,  4.1390e-03,  2.7156e-04, -2.5866e-03,\n",
       "                      -6.5614e-04, -3.3063e-04, -1.5647e-03, -1.3460e-03, -1.2627e-03,\n",
       "                      -8.8377e-04,  3.1786e-03,  5.6534e-05, -6.5750e-04, -2.3463e-03,\n",
       "                      -1.0535e-03,  2.0128e-04,  6.0220e-04, -2.0749e-03, -1.9648e-03,\n",
       "                       1.7430e-03, -9.7403e-04,  1.5949e-04, -1.4321e-03, -4.7561e-04,\n",
       "                       9.0337e-05,  2.2209e-03, -1.4816e-03,  3.3545e-04,  5.2927e-04,\n",
       "                      -1.8015e-03,  1.0192e-03,  1.2735e-03,  2.7105e-03, -2.5752e-04,\n",
       "                      -1.4091e-03,  3.0124e-03,  2.6379e-03, -4.4681e-03,  2.8863e-05,\n",
       "                       2.3690e-03,  4.6207e-04,  1.4325e-03,  8.4582e-04, -1.1199e-04,\n",
       "                       1.5826e-03,  1.3403e-03,  1.1966e-03,  7.0007e-04, -1.6249e-04,\n",
       "                       6.1346e-04,  2.0875e-03,  1.0125e-03, -6.2817e-04,  1.8400e-03,\n",
       "                      -1.0221e-03,  1.6247e-06,  6.4815e-04, -4.8043e-03, -1.2310e-03,\n",
       "                       1.1592e-04,  1.2344e-03])),\n",
       "             ('transformer.resblocks.4.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0098, -0.0158, -0.0473,  ...,  0.0316,  0.0359,  0.0312],\n",
       "                      [ 0.0457,  0.0248, -0.0404,  ..., -0.0123,  0.0059,  0.0237],\n",
       "                      [ 0.0136,  0.0594,  0.0232,  ...,  0.0216, -0.0270, -0.0031],\n",
       "                      ...,\n",
       "                      [ 0.0139, -0.0186,  0.0100,  ...,  0.0138, -0.0098, -0.0107],\n",
       "                      [ 0.0410, -0.0325,  0.0260,  ...,  0.0078,  0.0456, -0.0050],\n",
       "                      [ 0.0004, -0.0274, -0.0589,  ...,  0.0116, -0.0147, -0.0339]])),\n",
       "             ('transformer.resblocks.4.mlp.c_fc.bias',\n",
       "              tensor([ 0.0214,  0.0250, -0.0275,  ..., -0.0196, -0.0365,  0.0021])),\n",
       "             ('transformer.resblocks.4.mlp.c_proj.weight',\n",
       "              tensor([[ 1.7848e-02,  1.8178e-03, -3.9683e-03,  ..., -1.1278e-02,\n",
       "                       -1.0144e-02, -1.2181e-02],\n",
       "                      [ 1.3135e-02,  9.0031e-03, -1.4309e-02,  ...,  4.5222e-03,\n",
       "                        1.0200e-02,  3.4806e-04],\n",
       "                      [ 6.8479e-03,  9.9185e-03, -1.4209e-02,  ..., -1.4172e-03,\n",
       "                       -9.2559e-03,  9.7664e-05],\n",
       "                      ...,\n",
       "                      [-8.8473e-03,  8.5056e-03,  6.4733e-03,  ..., -1.0218e-02,\n",
       "                        1.7082e-03, -1.6149e-03],\n",
       "                      [-8.0250e-03, -2.6735e-02, -1.9225e-02,  ...,  1.5969e-03,\n",
       "                       -9.0673e-03, -5.5906e-03],\n",
       "                      [ 1.6868e-02, -6.1775e-03,  1.4351e-03,  ..., -2.2733e-02,\n",
       "                        3.4934e-03, -4.8325e-04]])),\n",
       "             ('transformer.resblocks.4.mlp.c_proj.bias',\n",
       "              tensor([-1.7909e-02,  2.4840e-03, -2.0411e-03,  2.0588e-02,  8.5909e-03,\n",
       "                      -3.7049e-03,  1.4476e-02,  1.1570e-02, -7.3911e-03,  1.7529e-02,\n",
       "                       7.3678e-03, -1.7562e-02,  3.9079e-03,  1.2906e-02,  2.2539e-03,\n",
       "                       1.8867e-02, -8.5530e-03, -8.8994e-03, -1.1533e-02,  5.1083e-03,\n",
       "                       1.9021e-02, -1.0732e-02,  1.9139e-02, -1.5860e-02,  1.6200e-02,\n",
       "                      -3.9300e-03,  1.2956e-02, -2.0637e-02,  1.4934e-03,  1.9675e-02,\n",
       "                      -1.9959e-02, -2.0750e-03, -1.8862e-03, -4.0714e-03, -1.1582e-03,\n",
       "                      -2.9624e-03, -1.5703e-02,  2.1365e-02,  1.0590e-02, -9.7307e-03,\n",
       "                      -7.3795e-03,  1.2613e-02,  1.3637e-02, -1.5487e-02, -9.3243e-03,\n",
       "                       4.8002e-03, -1.6130e-02,  1.5471e-02, -1.8056e-02, -4.4131e-03,\n",
       "                      -8.2554e-03,  3.3696e-03,  1.1178e-02, -9.1563e-03, -1.0578e-02,\n",
       "                      -1.0479e-02,  1.3372e-03,  1.4931e-02, -4.5365e-03, -2.2033e-03,\n",
       "                       4.8243e-03, -1.5251e-02,  3.0196e-03,  2.1775e-02,  1.9607e-02,\n",
       "                      -1.4163e-02, -1.1078e-02,  2.1219e-02, -1.8459e-03, -1.4662e-02,\n",
       "                       6.7663e-03, -1.9159e-03,  1.7092e-02, -1.4342e-02,  6.0850e-03,\n",
       "                      -3.9195e-03, -7.1628e-03,  8.3907e-03, -1.8078e-02,  2.0479e-02,\n",
       "                      -3.4257e-04, -1.4944e-02, -9.5011e-03,  1.0608e-02, -1.1692e-02,\n",
       "                      -8.9496e-03,  8.6009e-03, -4.3031e-03,  2.0980e-02, -2.1383e-02,\n",
       "                      -6.4169e-03, -1.8388e-02,  3.1247e-03, -1.3285e-02,  9.6722e-03,\n",
       "                      -1.6716e-02,  2.0365e-02, -1.5442e-02,  1.5770e-02,  1.2188e-02,\n",
       "                       2.1016e-02,  4.3801e-03,  2.2898e-03, -4.2811e-03, -8.6304e-03,\n",
       "                      -2.3283e-02,  1.1749e-02, -2.0810e-02, -8.7287e-03, -1.7272e-02,\n",
       "                      -5.6503e-03,  1.3952e-02, -7.7234e-03, -1.2124e-02, -1.6900e-02,\n",
       "                      -7.8636e-03, -1.6818e-02, -1.2063e-02,  1.2977e-02, -1.1456e-04,\n",
       "                      -8.6078e-03,  1.2398e-03, -2.1072e-02,  8.4610e-03, -1.7692e-02,\n",
       "                      -2.2651e-03, -2.1919e-02,  1.7262e-03, -2.1429e-02,  1.8056e-02,\n",
       "                      -1.5801e-02,  1.0721e-02,  1.6768e-02, -1.6782e-02,  2.6420e-03,\n",
       "                      -5.2210e-03, -1.5383e-02,  2.0046e-02,  9.5137e-03,  2.0754e-03,\n",
       "                       9.1011e-03,  1.0046e-02,  1.8924e-02,  1.8762e-02,  6.5064e-03,\n",
       "                      -1.3169e-04, -2.1146e-03,  1.1203e-02, -6.2605e-05,  2.0968e-02,\n",
       "                      -2.0255e-02,  5.5300e-03, -7.5995e-03,  1.5940e-02,  6.7842e-03,\n",
       "                      -6.6552e-04,  1.5538e-02, -2.2866e-03,  1.6831e-02, -2.0332e-03,\n",
       "                       2.3675e-03,  2.1217e-02,  1.2584e-02, -2.3780e-03,  1.3933e-02,\n",
       "                      -1.3862e-02,  2.2449e-03, -4.2871e-03, -1.1867e-03, -1.6094e-02,\n",
       "                      -2.3671e-03,  1.1689e-02, -4.3814e-03, -3.9323e-03, -5.5611e-03,\n",
       "                      -5.1477e-03,  1.2622e-02,  1.5625e-02,  1.2050e-02, -1.8203e-02,\n",
       "                      -1.7444e-02, -8.3838e-04,  1.4958e-02,  9.6947e-03,  3.8032e-03,\n",
       "                       1.7126e-02, -1.4713e-02, -2.1340e-02,  1.9803e-04,  9.8708e-03,\n",
       "                      -1.7425e-02, -5.5917e-03, -1.6784e-02,  6.8245e-04,  8.4315e-03,\n",
       "                       4.6044e-03, -1.9213e-03,  9.8178e-03, -8.9074e-03,  8.7406e-03,\n",
       "                       9.9955e-03,  1.8825e-02, -1.9990e-02,  1.5256e-02,  4.5012e-03,\n",
       "                      -6.7368e-03,  8.5042e-03, -1.9857e-02, -1.5724e-03,  1.3032e-02,\n",
       "                       1.2922e-02, -5.7244e-03,  3.4204e-03, -7.8922e-03, -1.9895e-02,\n",
       "                       1.2395e-02, -1.9119e-02,  1.2520e-02, -3.1093e-03,  8.1754e-03,\n",
       "                       7.2273e-03,  1.9418e-02, -6.2420e-03, -2.0700e-02, -1.0495e-02,\n",
       "                      -4.4124e-03, -1.5318e-03,  3.9075e-03, -1.1912e-02, -9.6354e-03,\n",
       "                       2.1355e-03, -3.1552e-03,  5.9906e-03, -2.3140e-04, -3.0216e-03,\n",
       "                      -2.3365e-03,  3.4913e-03, -3.4128e-03, -1.2568e-02,  2.0428e-03,\n",
       "                      -2.2380e-02,  1.5864e-02,  1.5391e-02,  1.6333e-02, -3.6199e-03,\n",
       "                      -1.0770e-02, -8.1670e-03, -2.1623e-02, -6.7519e-03, -1.7227e-02,\n",
       "                      -1.2668e-02, -8.1624e-03, -5.0435e-03, -1.3111e-02, -8.7724e-03,\n",
       "                       1.1100e-02,  2.1642e-02,  1.6833e-02, -1.6995e-02, -1.1710e-02,\n",
       "                      -1.6910e-02, -1.6266e-02, -6.2657e-03,  1.5799e-02, -2.1295e-02,\n",
       "                       1.5787e-02, -5.1488e-03, -1.1019e-03,  1.2995e-02, -1.6715e-02,\n",
       "                       1.3524e-03,  7.1194e-03,  1.1790e-02,  3.3154e-04, -2.0337e-02,\n",
       "                      -1.5884e-02, -1.2360e-02, -2.2144e-02,  5.2928e-03,  1.4690e-02,\n",
       "                       5.4308e-03, -1.9400e-02, -5.1857e-03, -5.4966e-03, -7.5451e-03,\n",
       "                      -1.5080e-02,  1.9087e-02,  2.3313e-02,  1.0950e-02, -5.5712e-03,\n",
       "                       1.3839e-02,  5.4958e-03, -4.3688e-03,  1.4372e-02,  6.3626e-03,\n",
       "                      -4.0752e-03, -4.8537e-03,  2.2866e-02, -1.0204e-02,  1.2485e-02,\n",
       "                      -4.0269e-03, -1.0427e-02,  1.7103e-03, -1.5547e-02,  1.5095e-03,\n",
       "                      -1.9355e-02,  7.9806e-03, -8.4107e-03,  1.7059e-02,  1.9804e-02,\n",
       "                      -1.2499e-02,  1.1544e-02, -1.9366e-02,  5.4062e-03,  9.1896e-03,\n",
       "                       4.0501e-03,  1.0166e-02,  1.9359e-02,  1.4074e-02, -1.9084e-02,\n",
       "                       1.1732e-03, -4.4383e-03, -2.3205e-03,  2.1365e-02, -2.0588e-02,\n",
       "                       1.0184e-03,  3.0336e-03,  2.1238e-02, -1.4383e-02,  1.7428e-02,\n",
       "                      -1.7405e-02,  1.6283e-02,  7.4546e-03,  6.2035e-04, -1.7655e-02,\n",
       "                      -4.2239e-03,  4.8836e-03,  9.5376e-03, -6.5338e-03, -3.2162e-03,\n",
       "                       1.6573e-02, -8.5808e-03,  1.5157e-02,  2.0617e-02, -1.7760e-02,\n",
       "                       2.9624e-03, -1.6229e-02, -1.7644e-02, -4.8620e-03, -1.5405e-02,\n",
       "                      -8.3136e-04, -1.1488e-02, -6.9235e-03, -2.0421e-02,  1.9149e-02,\n",
       "                      -1.2461e-02,  1.2861e-02, -5.5166e-03, -1.5748e-02, -9.2394e-03,\n",
       "                       1.7216e-02,  2.0652e-02,  8.7181e-03,  1.8410e-02,  1.4903e-02,\n",
       "                      -6.0552e-03, -1.9558e-02,  1.0531e-02, -1.0511e-02, -1.5236e-02,\n",
       "                      -1.8059e-02,  6.8614e-03, -1.8788e-02,  8.7747e-03, -2.2315e-02,\n",
       "                       2.2389e-02,  1.2387e-02,  1.4982e-02,  2.0678e-02,  4.6773e-03,\n",
       "                       1.8279e-02, -9.6889e-03, -1.4728e-02,  9.2289e-04,  8.3116e-03,\n",
       "                       1.2314e-02, -9.2880e-03, -2.1755e-02, -1.8588e-04, -7.9500e-04,\n",
       "                       1.9261e-02, -1.5577e-02,  3.5806e-03, -1.0081e-02,  2.4030e-02,\n",
       "                      -1.1296e-02,  1.1992e-02,  2.1238e-02, -2.0989e-02, -1.6407e-02,\n",
       "                       1.7248e-02, -1.2007e-02, -1.6665e-02, -1.9818e-02,  7.1233e-03,\n",
       "                      -5.7725e-03, -1.6730e-02,  9.9224e-03, -1.6966e-02,  1.4748e-02,\n",
       "                      -2.0838e-02, -1.4922e-02, -2.1185e-02,  9.4171e-03, -6.3514e-03,\n",
       "                       1.0314e-02, -9.1079e-03,  1.3172e-02, -1.9430e-02,  1.2753e-02,\n",
       "                       1.9439e-02,  5.5291e-03, -1.4059e-02, -4.8144e-03, -2.2140e-02,\n",
       "                      -1.6990e-02, -3.0454e-04,  1.6201e-02,  5.4114e-03, -1.7567e-02,\n",
       "                       1.8614e-02,  5.6380e-03,  2.1686e-03, -1.4463e-02,  6.8768e-03,\n",
       "                       1.5598e-02,  1.0158e-02, -2.0198e-02, -5.9783e-04, -8.4075e-04,\n",
       "                      -3.7572e-03, -1.9773e-04, -1.5582e-02, -1.9307e-02,  7.4101e-03,\n",
       "                      -5.8894e-03,  1.8758e-03, -1.6984e-02, -1.7395e-02,  8.5604e-03,\n",
       "                       2.0785e-02, -1.1940e-03, -5.0576e-03,  1.8386e-02,  4.4971e-03,\n",
       "                       1.9678e-02, -1.2155e-02,  3.5040e-04, -1.3874e-03,  1.6774e-02,\n",
       "                       2.2330e-03,  6.0075e-03, -8.9683e-03, -2.3986e-03, -4.0814e-03,\n",
       "                       1.2219e-02, -7.0167e-03, -1.2937e-02, -7.2016e-03, -7.8731e-05,\n",
       "                      -8.8205e-03,  1.8901e-02, -1.1294e-03,  7.5605e-03, -8.8429e-04,\n",
       "                       1.1263e-02,  1.3741e-02, -6.9180e-03, -1.0386e-04, -6.1742e-03,\n",
       "                       1.1350e-02, -4.4559e-03, -7.6649e-04, -1.6159e-03,  3.1629e-03,\n",
       "                      -1.1662e-02, -2.2772e-04,  1.1777e-02, -1.1954e-02,  1.2468e-02,\n",
       "                      -1.4164e-02,  6.5356e-03,  1.7415e-02, -1.9133e-02, -4.1817e-03,\n",
       "                       5.9168e-03, -5.8977e-03,  2.3112e-03, -1.6699e-02, -8.5986e-03,\n",
       "                      -1.3469e-02, -1.6627e-02, -1.0367e-06,  8.1138e-03, -1.6088e-02,\n",
       "                      -3.0085e-03, -2.0884e-02,  1.9377e-02, -1.6835e-03,  1.7692e-02,\n",
       "                      -2.0013e-02,  7.7919e-03])),\n",
       "             ('transformer.resblocks.4.ln_2.weight',\n",
       "              tensor([0.9985, 1.0007, 1.0007, 1.0023, 1.0003, 1.0016, 1.0015, 1.0020, 1.0008,\n",
       "                      1.0014, 1.0020, 0.9992, 0.9992, 1.0037, 0.9982, 1.0023, 1.0038, 1.0068,\n",
       "                      1.0013, 1.0016, 1.0046, 1.0034, 1.0003, 1.0011, 1.0013, 1.0035, 1.0006,\n",
       "                      1.0002, 0.9989, 1.0009, 1.0016, 1.0030, 1.0004, 1.0027, 1.0008, 1.0042,\n",
       "                      0.9993, 1.0025, 1.0007, 1.0040, 1.0018, 0.9996, 1.0028, 0.9991, 1.0012,\n",
       "                      1.0007, 1.0021, 1.0039, 0.9993, 1.0010, 1.0012, 1.0002, 1.0022, 1.0002,\n",
       "                      1.0037, 1.0028, 1.0015, 1.0018, 1.0010, 1.0025, 1.0014, 1.0015, 0.9999,\n",
       "                      1.0019, 1.0028, 1.0008, 0.9992, 1.0035, 1.0015, 1.0037, 1.0022, 1.0005,\n",
       "                      1.0006, 1.0004, 1.0024, 1.0039, 1.0020, 1.0018, 1.0007, 1.0013, 1.0001,\n",
       "                      1.0066, 1.0027, 0.9974, 1.0026, 1.0015, 1.0029, 1.0000, 1.0020, 1.0024,\n",
       "                      1.0034, 1.0014, 1.0022, 1.0028, 1.0011, 1.0003, 0.9984, 1.0016, 1.0032,\n",
       "                      1.0025, 1.0025, 1.0003, 0.9963, 0.9996, 1.0018, 1.0014, 1.0029, 1.0024,\n",
       "                      0.9997, 1.0026, 1.0029, 0.9970, 1.0003, 1.0013, 1.0042, 1.0000, 1.0006,\n",
       "                      1.0011, 1.0024, 1.0032, 0.9997, 1.0012, 1.0010, 1.0035, 1.0031, 1.0022,\n",
       "                      1.0022, 1.0023, 1.0057, 1.0027, 1.0037, 1.0016, 1.0006, 1.0045, 1.0029,\n",
       "                      1.0001, 1.0002, 1.0004, 1.0034, 1.0022, 1.0008, 1.0013, 1.0035, 1.0014,\n",
       "                      0.9981, 1.0024, 1.0031, 1.0012, 1.0019, 1.0012, 0.9989, 1.0021, 1.0000,\n",
       "                      1.0029, 1.0043, 1.0025, 0.9978, 1.0021, 1.0007, 1.0032, 1.0022, 0.9988,\n",
       "                      0.9997, 1.0056, 0.9992, 1.0014, 1.0012, 1.0015, 1.0019, 1.0023, 1.0031,\n",
       "                      1.0005, 1.0041, 1.0023, 1.0017, 1.0006, 1.0010, 1.0012, 1.0018, 0.9993,\n",
       "                      1.0044, 1.0028, 1.0013, 0.9994, 0.9999, 1.0007, 1.0024, 1.0019, 1.0005,\n",
       "                      1.0049, 1.0059, 1.0012, 1.0032, 1.0023, 1.0024, 1.0011, 1.0027, 1.0020,\n",
       "                      1.0020, 1.0007, 1.0029, 1.0004, 0.9998, 1.0011, 1.0016, 1.0024, 1.0023,\n",
       "                      1.0016, 1.0033, 1.0036, 0.9980, 1.0012, 1.0019, 1.0014, 1.0013, 1.0033,\n",
       "                      1.0014, 1.0031, 1.0021, 1.0008, 0.9992, 1.0034, 1.0027, 1.0018, 0.9995,\n",
       "                      1.0037, 1.0023, 1.0006, 1.0005, 1.0051, 1.0027, 1.0016, 1.0007, 1.0016,\n",
       "                      1.0015, 0.9982, 1.0001, 1.0031, 1.0030, 1.0014, 1.0027, 1.0007, 1.0022,\n",
       "                      0.9997, 1.0032, 1.0052, 1.0041, 1.0015, 1.0016, 0.9996, 1.0013, 1.0003,\n",
       "                      0.9998, 1.0026, 1.0022, 0.9990, 1.0021, 0.9980, 1.0030, 1.0001, 1.0062,\n",
       "                      1.0021, 0.9999, 1.0023, 1.0001, 1.0038, 0.9994, 1.0005, 1.0014, 1.0019,\n",
       "                      1.0031, 1.0006, 0.9992, 1.0022, 0.9988, 0.9991, 1.0014, 1.0027, 0.9999,\n",
       "                      1.0019, 1.0042, 1.0017, 0.9990, 1.0049, 1.0026, 1.0024, 1.0029, 1.0017,\n",
       "                      1.0062, 1.0008, 1.0038, 0.9997, 1.0035, 0.9996, 1.0003, 1.0012, 1.0012,\n",
       "                      1.0034, 1.0020, 1.0029, 0.9987, 1.0033, 1.0046, 1.0000, 1.0021, 1.0029,\n",
       "                      0.9997, 1.0039, 1.0017, 1.0012, 0.9991, 1.0017, 1.0023, 1.0048, 1.0027,\n",
       "                      1.0019, 1.0002, 1.0042, 1.0059, 0.9994, 0.9992, 1.0022, 1.0029, 1.0023,\n",
       "                      1.0011, 1.0022, 0.9988, 1.0029, 1.0015, 1.0019, 1.0019, 1.0017, 1.0036,\n",
       "                      1.0025, 1.0002, 1.0019, 1.0022, 0.9993, 1.0042, 1.0017, 0.9991, 1.0001,\n",
       "                      1.0001, 1.0035, 1.0025, 1.0024, 1.0001, 1.0005, 1.0001, 1.0014, 0.9975,\n",
       "                      1.0021, 1.0031, 1.0042, 1.0025, 0.9988, 1.0030, 0.9988, 1.0007, 1.0003,\n",
       "                      1.0042, 1.0003, 1.0029, 1.0000, 1.0015, 1.0009, 1.0041, 1.0029, 1.0024,\n",
       "                      1.0035, 1.0017, 1.0000, 1.0036, 1.0012, 1.0022, 1.0039, 1.0025, 0.9991,\n",
       "                      1.0029, 1.0025, 1.0038, 1.0000, 1.0030, 1.0002, 1.0014, 1.0016, 1.0014,\n",
       "                      1.0012, 1.0032, 0.9985, 1.0050, 1.0019, 1.0037, 1.0012, 1.0009, 1.0004,\n",
       "                      1.0006, 1.0004, 1.0003, 1.0014, 0.9991, 1.0019, 1.0010, 1.0014, 1.0003,\n",
       "                      1.0021, 1.0014, 1.0002, 1.0028, 0.9996, 1.0026, 1.0022, 1.0055, 1.0027,\n",
       "                      0.9995, 1.0004, 0.9985, 1.0030, 1.0008, 1.0014, 1.0008, 0.9988, 0.9976,\n",
       "                      1.0005, 1.0035, 1.0037, 1.0038, 1.0014, 1.0003, 1.0023, 1.0014, 1.0024,\n",
       "                      1.0029, 1.0026, 1.0014, 1.0020, 1.0010, 1.0010, 1.0019, 1.0044, 1.0025,\n",
       "                      1.0031, 1.0010, 1.0006, 1.0024, 1.0027, 0.9985, 1.0015, 1.0009, 1.0023,\n",
       "                      1.0026, 0.9997, 1.0022, 0.9971, 1.0011, 1.0031, 1.0032, 0.9998, 1.0036,\n",
       "                      0.9986, 1.0018, 1.0002, 1.0049, 1.0028, 1.0018, 1.0021, 1.0022, 0.9995,\n",
       "                      1.0039, 1.0010, 1.0009, 1.0022, 1.0018, 1.0035, 1.0011, 1.0024, 1.0038,\n",
       "                      1.0025, 0.9987, 1.0017, 1.0021, 1.0028, 1.0021, 1.0018, 0.9982, 1.0006,\n",
       "                      0.9983, 1.0022, 1.0028, 1.0015, 1.0005, 1.0023, 1.0000, 1.0032, 1.0032,\n",
       "                      1.0025, 0.9991, 1.0013, 0.9998, 1.0011, 1.0016, 1.0021, 1.0012, 1.0022,\n",
       "                      0.9976, 1.0012, 1.0014, 1.0001, 0.9999, 1.0033, 0.9977, 1.0014])),\n",
       "             ('transformer.resblocks.4.ln_2.bias',\n",
       "              tensor([-2.1088e-03,  5.1827e-04, -1.8954e-03, -1.2989e-03,  8.7875e-04,\n",
       "                       2.1135e-03,  2.4986e-03, -2.2701e-03,  8.5565e-04, -1.0638e-03,\n",
       "                      -8.3637e-04,  5.1872e-03,  2.2578e-03,  9.6840e-04,  7.9140e-03,\n",
       "                      -2.7123e-04, -2.5433e-03, -2.5430e-03, -2.7849e-03, -1.6321e-03,\n",
       "                       1.0599e-03, -4.9188e-04, -1.3598e-03,  1.5115e-04, -8.0332e-04,\n",
       "                       2.3903e-04,  2.1017e-03,  6.2974e-04, -2.9173e-03,  2.8236e-03,\n",
       "                      -2.5455e-03,  1.2261e-04, -1.7107e-03, -1.4538e-03,  8.2482e-04,\n",
       "                       2.0037e-03,  3.5553e-04,  1.3883e-03, -8.5187e-04, -7.5818e-04,\n",
       "                      -8.7849e-04,  2.2282e-03, -1.6716e-03, -1.9746e-03,  2.0472e-03,\n",
       "                      -4.8989e-04, -1.7514e-03, -1.9362e-03,  2.0366e-03,  1.2387e-03,\n",
       "                       1.5505e-03, -3.0553e-03,  1.3551e-03, -2.0304e-03, -6.4996e-04,\n",
       "                      -8.7840e-04, -1.2885e-03, -2.8886e-03,  1.2770e-03, -3.0091e-03,\n",
       "                       3.0140e-03,  8.0800e-04, -4.4728e-04, -1.3946e-03, -2.7351e-03,\n",
       "                      -2.3380e-03,  1.0766e-03,  3.6543e-04,  3.2641e-03,  4.5218e-04,\n",
       "                      -2.1410e-04,  1.1246e-03,  2.5895e-03, -1.2001e-03,  3.3010e-04,\n",
       "                       8.3150e-04, -1.7842e-03, -7.3127e-04,  1.2781e-03, -3.8038e-05,\n",
       "                      -3.3709e-03,  2.8044e-03,  2.0046e-03, -1.8109e-03, -2.4205e-03,\n",
       "                      -4.9079e-04, -6.3869e-04, -4.7296e-04, -2.9496e-03, -4.7613e-04,\n",
       "                      -4.1795e-04,  1.9861e-03, -1.4015e-03,  2.1416e-03, -6.7258e-04,\n",
       "                       9.4509e-04,  9.3764e-04, -5.7610e-04, -5.4958e-04,  1.6547e-03,\n",
       "                       1.0727e-03,  4.7197e-04, -2.0625e-03,  9.3439e-04,  2.8795e-03,\n",
       "                       7.1990e-04,  7.8253e-04,  1.8702e-04,  3.4639e-03, -6.7790e-04,\n",
       "                      -1.2995e-03, -3.5152e-04,  3.5830e-04, -3.8730e-03,  1.9227e-03,\n",
       "                       1.6902e-03, -2.6782e-03, -2.9078e-03,  1.5801e-03, -1.1982e-03,\n",
       "                       1.3989e-03,  9.8971e-04,  2.0395e-03, -7.8901e-04,  3.9985e-04,\n",
       "                       8.8097e-04, -3.2174e-04,  4.8522e-04,  1.8501e-03,  2.6628e-04,\n",
       "                       4.4464e-04, -1.0066e-03, -1.1742e-03, -2.0848e-03,  1.5449e-03,\n",
       "                       2.6463e-04,  1.0503e-03, -7.7245e-04, -2.2413e-03,  2.1588e-03,\n",
       "                      -2.3192e-04, -3.2811e-03,  3.2279e-03, -1.3912e-04,  1.6573e-03,\n",
       "                      -1.3037e-03,  1.8386e-03, -1.6521e-04, -2.2423e-03, -3.0243e-03,\n",
       "                       9.6323e-04, -1.2760e-03,  4.5957e-04, -3.5818e-03,  5.0797e-04,\n",
       "                      -2.1363e-03, -7.5253e-04, -1.9384e-04, -1.0939e-03, -1.3526e-04,\n",
       "                       3.9305e-03, -8.4958e-04, -2.4578e-03,  2.2382e-03, -2.1390e-03,\n",
       "                      -2.3063e-03,  4.5510e-04, -1.9188e-03, -7.6466e-05,  6.2097e-05,\n",
       "                      -1.1829e-03, -5.9502e-04,  9.4719e-05, -2.6720e-03, -1.7073e-04,\n",
       "                      -1.0680e-03,  1.2574e-03,  9.1701e-04,  1.1067e-03,  1.3669e-03,\n",
       "                      -1.2438e-03, -2.1197e-03,  2.2677e-03, -1.0684e-03, -2.1273e-03,\n",
       "                      -2.0746e-04,  3.5032e-04, -3.3955e-04,  3.9713e-04, -1.7434e-03,\n",
       "                      -8.9186e-04, -2.8095e-04, -7.1345e-04, -1.9496e-03,  8.9987e-04,\n",
       "                      -8.2929e-04,  1.3869e-03,  1.4913e-03, -2.9854e-03, -1.8827e-03,\n",
       "                      -4.2007e-04,  1.0111e-03, -2.9458e-04, -1.9856e-04, -4.9218e-04,\n",
       "                      -2.0833e-03, -6.3693e-04, -2.2221e-05,  1.5139e-03,  8.5988e-05,\n",
       "                      -1.4685e-03,  1.7787e-04,  2.0616e-04, -4.3660e-04, -2.0543e-03,\n",
       "                       9.4790e-04, -2.7430e-03, -1.8840e-03,  3.9888e-04,  1.8763e-03,\n",
       "                       4.9218e-04,  6.9247e-04, -2.5977e-03, -1.1935e-03, -1.2762e-03,\n",
       "                      -2.3299e-04, -1.7351e-04,  1.2514e-03, -8.8074e-04,  1.3658e-04,\n",
       "                      -4.7461e-04, -1.9638e-03, -1.8793e-04,  1.5157e-03, -1.5710e-03,\n",
       "                       1.7671e-03, -3.2469e-03,  2.2188e-03, -1.1386e-03, -1.7832e-03,\n",
       "                       2.1852e-03, -2.8328e-03, -4.0903e-05, -2.6735e-03,  2.1676e-04,\n",
       "                      -3.3501e-04,  1.4523e-03, -1.9514e-03, -2.3426e-03, -2.0651e-03,\n",
       "                       3.2624e-04,  2.6000e-03, -1.2695e-03,  2.0810e-03, -1.2666e-03,\n",
       "                       2.7101e-03,  2.0086e-03, -1.6433e-04, -1.5487e-03,  1.7573e-03,\n",
       "                       1.7607e-03, -4.6813e-03,  3.8892e-03, -1.6995e-03,  4.6997e-03,\n",
       "                      -1.5374e-04, -3.4739e-04, -1.8181e-03, -2.1511e-03, -3.1901e-03,\n",
       "                       1.8731e-03,  7.7908e-04,  7.4910e-04, -4.1517e-04,  5.9257e-04,\n",
       "                      -5.9586e-04, -1.4925e-03,  1.1484e-03, -4.2100e-04, -2.5055e-03,\n",
       "                      -3.0203e-03,  5.0110e-05,  8.2434e-04, -5.6738e-04, -7.9046e-05,\n",
       "                       1.5093e-03,  9.3218e-04,  1.7733e-03,  1.3518e-03, -3.9232e-03,\n",
       "                       1.5402e-03, -3.7190e-03, -1.6753e-03,  6.4656e-05, -2.9425e-03,\n",
       "                       2.3962e-03,  3.5885e-03, -6.5142e-04,  3.3376e-04, -4.8302e-04,\n",
       "                       3.2426e-03,  2.0071e-03, -2.1806e-03,  2.6661e-03,  2.9287e-03,\n",
       "                       3.0910e-03, -9.0298e-05,  2.1611e-03, -2.6124e-03,  8.7867e-04,\n",
       "                      -2.3682e-03, -2.9448e-04, -1.8533e-03, -1.2977e-03, -4.2867e-03,\n",
       "                       9.1487e-04, -2.8141e-04,  7.3042e-04, -2.3196e-03,  2.7931e-04,\n",
       "                       1.2142e-03,  1.4802e-03, -2.1717e-03,  8.0736e-04,  1.0384e-03,\n",
       "                       1.0393e-03, -2.8262e-03,  1.6610e-03,  8.6371e-04,  1.6485e-03,\n",
       "                       1.0074e-03,  2.2756e-04,  2.4006e-05,  4.2303e-03, -9.1750e-04,\n",
       "                      -1.1800e-03, -2.5561e-04, -1.9546e-03,  1.1788e-04, -1.9242e-03,\n",
       "                       5.2425e-04,  7.0829e-05,  3.8065e-04,  8.4156e-04, -1.4611e-04,\n",
       "                      -1.1116e-03, -9.0162e-05,  2.1068e-03,  3.4277e-04,  3.2250e-04,\n",
       "                       3.0418e-03, -6.2832e-04,  1.8962e-04, -1.0053e-03, -9.2209e-04,\n",
       "                       1.7212e-03,  2.3032e-03, -2.8877e-03,  1.6617e-03,  9.3663e-04,\n",
       "                      -1.1336e-03, -1.0458e-03, -1.5417e-03, -2.5013e-03, -4.0129e-03,\n",
       "                      -2.3153e-03,  4.4455e-03,  3.5322e-03, -6.9285e-04,  1.5920e-03,\n",
       "                       2.4185e-03, -3.8840e-03, -2.5044e-03, -9.2316e-04,  2.3093e-03,\n",
       "                      -5.9129e-05, -3.7003e-03, -2.1981e-03,  3.1403e-03,  5.5915e-03,\n",
       "                       1.6021e-03, -2.3513e-04,  1.7396e-03, -3.8845e-05,  1.1832e-03,\n",
       "                       2.3112e-03, -3.4715e-03, -1.1596e-04,  6.2497e-04, -1.6268e-04,\n",
       "                       1.2914e-03, -6.3224e-04,  3.0945e-03,  1.4075e-03, -2.0288e-03,\n",
       "                       3.8964e-04,  1.6901e-03, -2.1932e-03,  8.6370e-04,  2.1946e-03,\n",
       "                       1.3130e-03, -1.9549e-04, -1.0557e-03,  3.2578e-03, -3.6526e-03,\n",
       "                      -2.2915e-03, -1.8573e-03, -2.9129e-03,  1.3664e-03, -2.0430e-03,\n",
       "                      -1.7965e-03,  2.3540e-03, -1.4358e-03, -1.2618e-03, -1.7517e-04,\n",
       "                       2.3229e-05, -5.5001e-04, -7.4347e-04,  3.7008e-03,  3.0971e-05,\n",
       "                      -4.8517e-04, -3.2253e-03, -3.4674e-03, -2.0021e-04,  1.8940e-03,\n",
       "                      -2.0344e-03, -6.6340e-04,  1.3047e-03, -3.4404e-03, -1.6246e-03,\n",
       "                       1.3067e-03,  2.1887e-03,  3.1229e-04, -1.3011e-03,  1.4274e-03,\n",
       "                      -1.5872e-03, -2.6413e-04,  1.7307e-03, -7.9879e-04, -2.4891e-04,\n",
       "                      -8.2877e-04,  1.7539e-03, -9.6112e-05,  1.2027e-03,  2.3305e-03,\n",
       "                       3.2075e-03, -1.2695e-04,  1.4387e-03,  2.6088e-03,  1.8193e-03,\n",
       "                      -6.8730e-06, -1.4576e-03, -1.0897e-03, -2.5944e-03,  1.6546e-03,\n",
       "                       4.4230e-04, -9.1329e-04, -1.4173e-03, -8.8838e-05, -1.0435e-03,\n",
       "                       9.1580e-04, -1.1012e-03,  2.4917e-03, -2.3412e-03, -1.8668e-03,\n",
       "                       3.0792e-03, -8.3794e-04, -2.2140e-03, -9.1496e-04,  2.4296e-04,\n",
       "                       9.4497e-04,  1.9585e-04,  2.2019e-03,  1.9935e-03,  1.0126e-03,\n",
       "                      -1.1694e-03,  1.0340e-04,  1.7243e-03,  6.7992e-04,  2.2612e-03,\n",
       "                       1.8998e-03, -1.0094e-03,  1.7034e-03,  2.3678e-03,  3.2144e-03,\n",
       "                      -1.3459e-03, -1.0936e-03,  7.4782e-04,  2.1632e-03, -5.5496e-05,\n",
       "                       8.7180e-04, -2.6625e-04, -2.3372e-05, -1.2671e-03, -1.9109e-04,\n",
       "                      -1.8764e-03, -2.4892e-03, -2.5613e-03, -3.3258e-03, -2.2604e-03,\n",
       "                      -5.0678e-04, -1.4384e-03,  1.1977e-03,  2.0950e-04,  3.9847e-03,\n",
       "                       1.9780e-03, -7.2917e-04, -9.1127e-04, -2.7850e-03,  1.4063e-03,\n",
       "                       2.0117e-03, -5.5650e-04])),\n",
       "             ('transformer.resblocks.5.attn.in_proj_weight',\n",
       "              tensor([[-0.0261, -0.0406,  0.0477,  ..., -0.0594,  0.0750, -0.0129],\n",
       "                      [-0.0490,  0.0029, -0.0467,  ...,  0.0518,  0.0082,  0.0391],\n",
       "                      [-0.0878, -0.0116,  0.0859,  ..., -0.0594, -0.0120, -0.0313],\n",
       "                      ...,\n",
       "                      [ 0.0500,  0.0117,  0.0238,  ..., -0.0217,  0.0155, -0.0081],\n",
       "                      [-0.0107,  0.0081,  0.0114,  ..., -0.0249,  0.0657,  0.0080],\n",
       "                      [-0.0026,  0.0033,  0.0186,  ..., -0.0055, -0.0728, -0.0795]])),\n",
       "             ('transformer.resblocks.5.attn.in_proj_bias',\n",
       "              tensor([ 0.0047, -0.0007,  0.0021,  ..., -0.0015,  0.0018,  0.0047])),\n",
       "             ('transformer.resblocks.5.attn.out_proj.weight',\n",
       "              tensor([[-0.0048, -0.0083,  0.0123,  ..., -0.0025, -0.0113, -0.0048],\n",
       "                      [ 0.0036,  0.0153, -0.0032,  ...,  0.0077,  0.0074,  0.0025],\n",
       "                      [ 0.0038, -0.0023, -0.0144,  ...,  0.0041,  0.0142, -0.0133],\n",
       "                      ...,\n",
       "                      [ 0.0002,  0.0050,  0.0059,  ..., -0.0105, -0.0093, -0.0117],\n",
       "                      [-0.0106, -0.0004,  0.0153,  ...,  0.0111,  0.0113,  0.0011],\n",
       "                      [ 0.0102,  0.0101,  0.0071,  ..., -0.0087, -0.0061,  0.0079]])),\n",
       "             ('transformer.resblocks.5.attn.out_proj.bias',\n",
       "              tensor([ 2.4651e-04, -2.3750e-03, -5.6595e-04, -3.1697e-04, -5.3946e-04,\n",
       "                      -1.8096e-03,  4.0529e-04,  2.5911e-03, -2.0566e-03,  7.9013e-04,\n",
       "                      -3.1772e-03, -1.5002e-03, -9.5942e-04, -9.8812e-04, -2.8524e-03,\n",
       "                       9.0309e-04, -2.4133e-04,  1.2364e-03,  3.8457e-04, -2.0816e-04,\n",
       "                      -8.2343e-04, -5.9293e-04,  4.3962e-04, -1.4312e-03, -1.0292e-03,\n",
       "                       2.5007e-04, -1.1599e-03, -1.0204e-03, -4.7630e-04, -8.4493e-04,\n",
       "                       8.9666e-04,  2.8398e-04,  1.6901e-03, -2.1723e-03, -3.8391e-04,\n",
       "                       8.7182e-04,  3.0931e-04,  2.3319e-04, -2.3188e-04, -1.6780e-03,\n",
       "                       1.1268e-03,  1.3317e-03,  1.7519e-03,  5.9086e-04, -1.7445e-03,\n",
       "                       1.1746e-03, -1.3770e-03, -3.7659e-04, -9.9192e-04,  1.4303e-03,\n",
       "                      -3.9001e-04, -1.3223e-03, -8.1430e-04,  1.2428e-03,  2.6928e-03,\n",
       "                       6.1791e-04, -3.4893e-04, -9.3965e-04,  4.3712e-04, -5.9840e-04,\n",
       "                      -1.5983e-03,  2.0665e-04, -2.9484e-04,  5.2301e-04, -3.6183e-04,\n",
       "                      -2.8944e-04,  7.4178e-04,  1.6628e-03, -1.4775e-04,  1.1770e-03,\n",
       "                      -6.9389e-04,  1.7061e-04, -1.3068e-03, -9.0906e-04, -7.5223e-04,\n",
       "                       1.6424e-03, -2.0745e-03, -8.1652e-04, -1.6874e-03,  1.8055e-03,\n",
       "                       5.1341e-04, -1.5432e-03, -3.7307e-04,  1.9306e-03,  1.7199e-03,\n",
       "                      -7.6875e-05, -1.2027e-03,  6.1943e-04,  3.4578e-04, -1.1300e-03,\n",
       "                       7.0467e-05, -8.7743e-04,  7.4229e-04, -6.6882e-04, -1.8819e-03,\n",
       "                      -2.3405e-03,  6.2954e-04, -9.7345e-04, -7.2062e-04,  3.7674e-04,\n",
       "                      -1.6602e-05, -1.2380e-03,  3.6161e-04, -1.2744e-03, -1.3781e-03,\n",
       "                      -1.3296e-03, -1.7799e-03, -1.9234e-04,  7.1272e-04, -4.0261e-06,\n",
       "                       1.8176e-03,  7.5297e-04,  1.0864e-03,  1.7538e-03, -1.0110e-03,\n",
       "                      -2.6629e-03,  1.3793e-03,  1.3355e-03, -1.4327e-03, -1.2510e-03,\n",
       "                       7.2530e-04, -5.2262e-04,  1.0301e-03, -9.0193e-04, -5.6368e-04,\n",
       "                       3.8877e-04, -1.1761e-03, -1.9807e-03, -6.4689e-04,  6.2611e-04,\n",
       "                       6.4663e-04,  1.7245e-03,  4.2464e-04, -1.3477e-03, -6.5665e-04,\n",
       "                       5.4853e-05, -6.4736e-04,  1.1954e-03, -5.3474e-04,  8.5093e-04,\n",
       "                       2.0021e-04,  1.7871e-03,  6.6146e-04,  2.6644e-03,  1.5571e-04,\n",
       "                       1.2781e-03, -1.0166e-03,  1.4038e-04,  1.3553e-03,  2.1822e-04,\n",
       "                       1.3223e-03,  3.2826e-03,  3.3183e-03,  8.9792e-04,  1.3039e-04,\n",
       "                       8.6365e-04,  1.2918e-03, -6.7295e-05,  2.1041e-04, -4.7352e-04,\n",
       "                      -1.3837e-03, -3.7449e-04,  1.8123e-03, -6.3464e-05, -1.3676e-03,\n",
       "                       7.5011e-04,  1.2794e-03, -6.9025e-04,  1.7024e-04, -1.3375e-03,\n",
       "                       2.6954e-04,  1.0639e-04, -9.3571e-04,  5.5649e-04,  1.9333e-03,\n",
       "                       6.0563e-04, -1.3607e-03,  3.5695e-04,  5.1089e-04, -8.6592e-04,\n",
       "                      -1.7493e-03,  5.5781e-04,  8.8041e-05, -2.0245e-03,  3.7855e-03,\n",
       "                      -9.6210e-05,  1.0473e-03, -1.2196e-04,  9.7499e-04, -2.0703e-03,\n",
       "                       6.3148e-04,  8.0724e-04,  1.0864e-04,  2.4925e-03, -4.3071e-04,\n",
       "                      -3.7173e-04,  1.4170e-03,  1.4335e-03,  2.2784e-03,  2.8395e-03,\n",
       "                       2.1914e-04, -1.8709e-03,  1.2232e-03,  2.6019e-03,  3.2063e-05,\n",
       "                      -1.8706e-04, -8.9179e-04, -1.6814e-04,  8.1102e-04, -2.1620e-05,\n",
       "                       1.6168e-03,  1.7812e-04, -5.2722e-04,  1.6215e-03, -1.4151e-03,\n",
       "                      -6.3882e-04,  3.9313e-04, -1.8054e-03, -9.7490e-04,  6.9305e-05,\n",
       "                      -1.2773e-03, -5.7119e-04,  1.3029e-03,  5.8674e-04,  1.2678e-03,\n",
       "                       2.6280e-04, -1.0303e-04, -8.2360e-04,  8.4827e-04,  1.2751e-03,\n",
       "                      -1.0737e-03,  1.1674e-03,  9.0670e-04, -6.3208e-04,  1.2390e-03,\n",
       "                      -3.1535e-04,  2.1728e-03, -2.3470e-03,  1.3397e-04,  8.1596e-04,\n",
       "                      -1.1525e-03,  1.6678e-03,  1.1636e-03,  1.5389e-03, -1.5125e-03,\n",
       "                      -1.7473e-04,  2.6575e-04, -1.8458e-04,  1.3876e-05, -1.1471e-03,\n",
       "                       6.8375e-04, -1.6715e-03,  1.1859e-04, -5.6556e-04, -1.0623e-03,\n",
       "                      -8.4278e-04, -2.1532e-04, -6.9865e-04, -4.1084e-04, -2.7469e-03,\n",
       "                       5.5550e-04,  4.2218e-04,  1.2083e-04, -1.6871e-03, -1.3745e-03,\n",
       "                       2.2848e-03,  1.1376e-03,  7.4042e-04,  5.9895e-04, -1.1722e-03,\n",
       "                       7.0944e-04, -5.0399e-04, -1.0909e-03,  2.4486e-04,  5.2412e-04,\n",
       "                      -1.0836e-03,  2.6408e-03, -7.2116e-04,  2.6400e-03, -5.8979e-05,\n",
       "                       5.7883e-04,  2.3839e-03, -5.7722e-04, -1.1334e-03, -1.9856e-03,\n",
       "                       1.2438e-03,  3.3371e-04,  6.5197e-04,  1.0563e-03,  8.0094e-05,\n",
       "                      -2.4664e-04,  1.2443e-03, -8.5997e-04,  2.8914e-03,  3.5185e-04,\n",
       "                       1.2943e-03, -1.0555e-03,  8.1741e-04, -1.0089e-03, -1.3800e-03,\n",
       "                      -1.9559e-03,  6.0413e-04,  1.2915e-03, -2.5264e-03, -5.2050e-04,\n",
       "                      -1.5875e-03,  1.2201e-04, -2.9421e-04, -1.1615e-03, -2.0298e-03,\n",
       "                       6.7110e-04,  1.7155e-03, -7.5403e-04, -2.0997e-03,  9.4139e-04,\n",
       "                       1.5002e-03,  2.2004e-03,  9.4524e-04,  7.5117e-04,  5.5432e-04,\n",
       "                       1.4066e-03, -3.6637e-04, -8.4413e-04,  1.0061e-03, -1.7700e-03,\n",
       "                       5.2916e-04,  2.1432e-03,  2.6542e-04, -2.0079e-03, -2.4511e-03,\n",
       "                      -2.4895e-04,  1.9058e-03,  6.7480e-04, -4.3627e-04,  2.0936e-04,\n",
       "                       1.5874e-04, -7.1614e-04,  3.0722e-04,  3.7926e-04,  1.6402e-03,\n",
       "                      -2.3767e-03,  1.9838e-03,  9.5849e-04, -6.3784e-04, -1.6513e-03,\n",
       "                      -2.9168e-04, -1.6092e-03,  5.0454e-04, -1.0016e-04,  1.1059e-03,\n",
       "                      -9.6288e-04, -2.7723e-03, -1.1225e-03, -6.1160e-04,  1.2553e-05,\n",
       "                       1.8852e-05,  1.9386e-03,  2.8104e-03,  4.3820e-04, -2.1538e-03,\n",
       "                       5.5348e-04, -4.3036e-05,  1.7450e-03,  2.1917e-03,  7.5802e-04,\n",
       "                       2.1399e-03, -1.7301e-04, -3.0129e-03,  1.9685e-03,  1.8417e-04,\n",
       "                      -9.0916e-04,  1.2800e-03,  9.7013e-04, -8.9258e-04, -2.6781e-04,\n",
       "                       1.8849e-03, -8.7174e-04,  1.0099e-03, -1.3076e-03, -1.4136e-03,\n",
       "                       9.2604e-04,  9.8995e-04, -1.9136e-03, -1.0068e-03, -1.2434e-03,\n",
       "                      -7.4420e-04,  1.6711e-03, -1.8896e-04,  1.9978e-03,  1.0280e-03,\n",
       "                      -5.5758e-04,  3.2687e-04, -8.1493e-04,  1.0564e-03,  2.5707e-03,\n",
       "                       1.3770e-03, -3.1227e-04,  1.7365e-03, -1.4470e-04,  4.3056e-04,\n",
       "                      -6.5382e-04, -1.0170e-03, -1.4909e-03, -3.6675e-04,  6.6724e-06,\n",
       "                       1.3480e-03,  9.4683e-04,  9.7055e-04,  1.1786e-03,  1.5207e-03,\n",
       "                       9.2897e-06, -1.9248e-03,  2.0043e-04,  1.8537e-03,  1.6011e-04,\n",
       "                      -1.3712e-03,  1.2919e-03,  1.3901e-03,  2.1510e-04,  1.5648e-03,\n",
       "                      -9.4865e-04,  2.0148e-04,  5.0877e-04, -7.3688e-04, -1.4563e-03,\n",
       "                      -1.1389e-04, -1.1253e-03, -6.0859e-04,  2.0010e-03, -1.5391e-04,\n",
       "                      -1.6675e-03,  6.2360e-04,  8.1935e-04, -8.1064e-04, -2.4830e-03,\n",
       "                       2.0546e-04,  8.7943e-04, -2.2289e-03,  1.5005e-03, -4.8104e-04,\n",
       "                      -6.8149e-04,  7.3658e-05,  1.4483e-04, -5.4926e-04, -1.6325e-03,\n",
       "                      -2.0638e-03, -8.9028e-04,  5.2887e-04, -1.9482e-04,  7.9934e-04,\n",
       "                       1.0198e-04,  8.7143e-04, -1.3476e-03,  2.8699e-03,  1.1753e-03,\n",
       "                      -1.1059e-03,  1.2352e-03,  2.8318e-03,  1.6883e-03,  1.1725e-04,\n",
       "                       6.0854e-04, -1.0516e-03,  1.0083e-03,  2.3357e-03,  1.0400e-03,\n",
       "                       1.0910e-04,  4.3492e-04,  1.3312e-03,  3.4502e-04, -2.2847e-03,\n",
       "                      -1.2615e-03,  7.2298e-05, -2.2157e-03, -6.4723e-04, -1.0571e-03,\n",
       "                      -4.3726e-05, -2.0390e-03, -9.1978e-04, -5.8687e-04, -6.1852e-04,\n",
       "                      -1.5884e-03, -1.0945e-03,  5.1015e-04,  4.5515e-04, -1.3140e-03,\n",
       "                       6.1119e-05, -1.3072e-03, -1.1973e-03,  1.7632e-03,  7.8301e-04,\n",
       "                       6.0709e-04, -1.7508e-04,  2.4433e-04, -1.8912e-03, -9.4491e-04,\n",
       "                       2.1554e-04,  1.6215e-03,  3.1428e-04, -3.7835e-04, -2.5270e-03,\n",
       "                      -5.1109e-04, -1.3628e-05, -6.8120e-04,  1.2651e-03, -1.6933e-03,\n",
       "                       5.7672e-04, -1.1035e-03, -9.9962e-04,  1.4817e-03, -3.1600e-04,\n",
       "                      -1.4488e-03,  4.3532e-06])),\n",
       "             ('transformer.resblocks.5.ln_1.weight',\n",
       "              tensor([0.9992, 0.9967, 1.0000, 0.9999, 1.0015, 0.9970, 1.0013, 0.9978, 0.9992,\n",
       "                      1.0000, 0.9981, 1.0033, 0.9990, 1.0005, 0.9980, 1.0002, 0.9980, 0.9955,\n",
       "                      1.0005, 1.0014, 0.9976, 0.9985, 0.9988, 0.9998, 1.0008, 1.0007, 0.9954,\n",
       "                      0.9975, 1.0023, 0.9998, 0.9970, 1.0004, 0.9977, 0.9994, 0.9969, 0.9988,\n",
       "                      1.0002, 1.0007, 1.0034, 1.0008, 1.0020, 1.0021, 1.0009, 0.9990, 0.9996,\n",
       "                      1.0000, 0.9980, 0.9963, 0.9996, 0.9975, 1.0018, 0.9989, 0.9976, 1.0012,\n",
       "                      0.9999, 1.0000, 0.9993, 1.0019, 0.9993, 0.9996, 0.9978, 0.9955, 1.0006,\n",
       "                      0.9967, 1.0028, 0.9998, 1.0016, 1.0026, 0.9989, 1.0034, 1.0020, 1.0009,\n",
       "                      0.9971, 1.0000, 0.9984, 0.9988, 0.9969, 0.9994, 1.0020, 0.9990, 0.9988,\n",
       "                      1.0033, 0.9975, 1.0011, 1.0008, 1.0005, 1.0026, 0.9990, 0.9977, 0.9996,\n",
       "                      0.9984, 0.9973, 0.9974, 1.0004, 1.0023, 0.9969, 0.9987, 1.0016, 0.9986,\n",
       "                      1.0007, 1.0004, 0.9994, 1.0017, 1.0024, 0.9992, 0.9984, 0.9999, 0.9995,\n",
       "                      0.9975, 1.0008, 1.0004, 0.9976, 1.0013, 0.9960, 0.9982, 0.9990, 1.0006,\n",
       "                      1.0013, 1.0047, 0.9983, 0.9974, 0.9994, 0.9998, 1.0007, 0.9981, 0.9999,\n",
       "                      1.0002, 1.0010, 0.9983, 1.0003, 1.0004, 1.0008, 1.0008, 1.0016, 1.0024,\n",
       "                      1.0027, 0.9999, 0.9982, 1.0007, 0.9977, 0.9994, 1.0010, 1.0029, 0.9990,\n",
       "                      0.9981, 1.0024, 0.9993, 1.0004, 0.9967, 0.9997, 0.9975, 0.9983, 0.9987,\n",
       "                      1.0005, 1.0007, 1.0023, 1.0003, 1.0009, 0.9977, 1.0014, 1.0008, 0.9985,\n",
       "                      0.9976, 0.9999, 0.9981, 1.0018, 1.0021, 0.9983, 0.9980, 1.0003, 0.9965,\n",
       "                      0.9976, 0.9994, 0.9995, 0.9996, 0.9986, 1.0028, 0.9978, 1.0014, 0.9986,\n",
       "                      1.0025, 0.9972, 1.0027, 0.9983, 0.9966, 0.9974, 0.9992, 0.9998, 1.0024,\n",
       "                      0.9997, 0.9985, 1.0025, 0.9992, 0.9984, 0.9996, 0.9978, 0.9973, 0.9988,\n",
       "                      0.9978, 0.9987, 1.0001, 1.0022, 1.0028, 0.9974, 0.9989, 0.9984, 1.0018,\n",
       "                      1.0039, 0.9980, 0.9980, 0.9976, 1.0011, 1.0023, 0.9982, 1.0006, 0.9996,\n",
       "                      0.9972, 0.9973, 0.9976, 0.9972, 1.0012, 0.9996, 0.9971, 0.9995, 0.9990,\n",
       "                      1.0049, 0.9990, 1.0001, 1.0008, 0.9989, 0.9983, 0.9995, 0.9991, 1.0006,\n",
       "                      1.0013, 1.0029, 0.9986, 0.9998, 1.0030, 1.0002, 1.0008, 0.9973, 0.9961,\n",
       "                      1.0004, 0.9988, 1.0012, 1.0023, 0.9990, 0.9982, 0.9982, 0.9988, 0.9992,\n",
       "                      1.0025, 1.0013, 1.0002, 0.9982, 0.9991, 0.9986, 0.9994, 1.0007, 0.9981,\n",
       "                      1.0026, 0.9995, 1.0027, 0.9992, 0.9970, 0.9999, 0.9999, 0.9960, 1.0013,\n",
       "                      0.9984, 0.9979, 1.0000, 0.9977, 0.9970, 0.9999, 0.9983, 0.9974, 1.0007,\n",
       "                      0.9999, 0.9967, 0.9991, 0.9966, 1.0005, 0.9970, 1.0007, 1.0007, 1.0004,\n",
       "                      1.0013, 0.9955, 0.9997, 1.0010, 0.9997, 0.9962, 1.0017, 1.0007, 0.9989,\n",
       "                      1.0006, 1.0005, 1.0020, 0.9992, 0.9981, 1.0014, 1.0004, 0.9988, 1.0015,\n",
       "                      0.9996, 0.9968, 0.9968, 0.9975, 0.9979, 1.0002, 1.0010, 1.0000, 0.9988,\n",
       "                      0.9985, 0.9985, 0.9989, 1.0009, 0.9968, 0.9998, 0.9976, 0.9965, 0.9985,\n",
       "                      1.0005, 0.9985, 1.0023, 1.0051, 0.9994, 0.9990, 0.9993, 0.9975, 1.0005,\n",
       "                      1.0008, 0.9996, 0.9994, 1.0020, 0.9990, 0.9984, 0.9975, 1.0009, 1.0010,\n",
       "                      0.9978, 0.9968, 1.0023, 1.0002, 0.9987, 0.9989, 0.9998, 1.0016, 1.0016,\n",
       "                      0.9976, 1.0002, 0.9970, 0.9986, 0.9996, 1.0010, 0.9989, 1.0010, 0.9995,\n",
       "                      1.0026, 0.9989, 0.9982, 0.9973, 0.9984, 1.0008, 0.9996, 0.9989, 0.9956,\n",
       "                      0.9972, 0.9982, 1.0005, 0.9969, 0.9978, 0.9995, 1.0017, 1.0001, 1.0003,\n",
       "                      0.9979, 0.9992, 0.9961, 1.0017, 0.9992, 1.0001, 0.9980, 0.9994, 0.9976,\n",
       "                      1.0008, 0.9969, 0.9986, 1.0003, 1.0002, 1.0010, 0.9968, 0.9971, 0.9977,\n",
       "                      1.0009, 0.9989, 0.9988, 1.0013, 1.0029, 0.9968, 1.0009, 0.9982, 1.0003,\n",
       "                      1.0012, 0.9987, 0.9963, 1.0013, 0.9991, 0.9979, 1.0007, 1.0019, 1.0009,\n",
       "                      1.0006, 0.9994, 0.9988, 0.9944, 1.0003, 0.9990, 0.9983, 1.0013, 0.9974,\n",
       "                      0.9994, 0.9996, 0.9958, 0.9991, 1.0028, 1.0023, 1.0001, 1.0012, 0.9972,\n",
       "                      1.0027, 0.9981, 1.0012, 0.9965, 0.9989, 0.9991, 0.9994, 1.0007, 1.0000,\n",
       "                      1.0006, 1.0011, 0.9983, 1.0018, 0.9984, 0.9987, 0.9967, 0.9994, 0.9990,\n",
       "                      1.0009, 0.9996, 0.9998, 1.0018, 0.9973, 0.9972, 0.9994, 0.9960, 0.9998,\n",
       "                      0.9976, 0.9980, 0.9968, 1.0027, 1.0014, 0.9983, 0.9974, 0.9993, 0.9976,\n",
       "                      1.0019, 0.9972, 0.9969, 1.0000, 0.9996, 0.9990, 0.9980, 0.9970, 1.0004,\n",
       "                      0.9984, 1.0031, 1.0017, 0.9995, 1.0035, 0.9985, 1.0006, 0.9979, 0.9996,\n",
       "                      0.9970, 0.9996, 0.9995, 0.9976, 0.9961, 0.9986, 0.9978, 0.9978, 1.0004,\n",
       "                      1.0018, 1.0016, 0.9980, 0.9998, 1.0013, 1.0008, 0.9983, 1.0012, 1.0028,\n",
       "                      1.0022, 0.9984, 0.9985, 1.0010, 0.9984, 1.0046, 0.9994, 0.9980])),\n",
       "             ('transformer.resblocks.5.ln_1.bias',\n",
       "              tensor([-7.4996e-04,  1.7314e-03,  1.7885e-03, -2.8553e-04,  2.4047e-03,\n",
       "                      -2.3522e-04,  2.0961e-03,  6.7383e-04,  3.8108e-05,  1.0449e-03,\n",
       "                       3.0284e-03, -1.1532e-03, -1.4138e-03,  2.1201e-03,  3.8107e-04,\n",
       "                      -2.3671e-03, -7.8928e-04, -1.9340e-03,  2.9564e-04,  2.7611e-04,\n",
       "                       1.8795e-03,  3.2937e-04,  1.0796e-03,  1.0800e-03,  1.0556e-03,\n",
       "                       2.4023e-04,  1.2916e-03,  2.2174e-03,  7.5555e-04, -1.0555e-03,\n",
       "                      -1.3453e-03,  3.8066e-03, -1.6335e-03, -4.0885e-04, -1.5286e-03,\n",
       "                       2.7670e-03, -2.5141e-03, -4.7331e-03, -1.1538e-03, -3.4697e-03,\n",
       "                       2.4305e-04, -5.4737e-04,  1.6712e-03,  4.3807e-04,  5.2914e-04,\n",
       "                      -1.6545e-03,  2.3256e-03,  5.0881e-03,  1.7956e-03, -1.4192e-03,\n",
       "                      -1.9066e-03,  5.0051e-04,  6.0221e-04, -1.4223e-03, -2.8022e-03,\n",
       "                       1.1614e-03,  2.1212e-03, -7.2069e-04, -9.9218e-04, -1.6469e-03,\n",
       "                      -3.8471e-04, -2.8915e-04,  1.9594e-03,  5.1341e-04, -8.5428e-04,\n",
       "                      -7.0082e-04,  1.6002e-03, -1.0368e-03,  2.1414e-04,  2.5417e-05,\n",
       "                      -5.0661e-04, -4.6287e-04, -1.7813e-04,  1.3706e-03, -2.8944e-03,\n",
       "                      -1.0611e-03, -2.9608e-04,  9.0814e-04, -1.0501e-03, -3.3861e-04,\n",
       "                      -5.5392e-04,  3.4821e-03,  4.7365e-04, -1.8809e-03,  3.7219e-04,\n",
       "                      -9.2463e-04,  1.2888e-03, -1.8102e-03, -2.7659e-03,  7.5606e-04,\n",
       "                       1.4811e-03,  2.3439e-03,  9.6875e-06,  3.6125e-03,  2.2068e-04,\n",
       "                       4.2275e-03,  1.2306e-03,  4.8257e-04,  2.6877e-03,  1.3747e-04,\n",
       "                       4.7021e-04,  8.6336e-04,  2.8222e-04,  1.2787e-03,  1.1211e-03,\n",
       "                      -1.9670e-04,  1.5191e-03,  2.9114e-03, -1.2101e-03, -2.1541e-04,\n",
       "                      -6.3281e-04,  2.6304e-03,  1.2236e-03, -1.2879e-03, -1.4184e-03,\n",
       "                       3.7953e-03,  5.0479e-04,  2.5668e-03,  4.8150e-04,  1.9305e-03,\n",
       "                      -3.6165e-03,  1.2008e-03, -2.1114e-03,  4.1727e-04, -9.8285e-04,\n",
       "                       1.3568e-03,  2.9762e-03,  1.4411e-03, -3.3852e-04, -2.0923e-03,\n",
       "                      -1.1407e-03, -7.0109e-04, -2.7953e-03,  1.8804e-03, -4.1725e-03,\n",
       "                       2.8689e-03,  1.7555e-03, -1.4764e-03,  8.7752e-04, -5.3935e-04,\n",
       "                      -2.1638e-03,  4.1951e-05, -6.2177e-04, -2.1070e-03, -1.8386e-03,\n",
       "                      -1.8089e-03,  3.3458e-03, -6.8489e-04,  4.0434e-03,  4.6076e-03,\n",
       "                       2.6196e-03, -1.9363e-04, -2.1746e-04,  2.1214e-03,  1.7474e-05,\n",
       "                      -9.3870e-04,  6.8819e-04, -3.1626e-03, -9.5045e-04,  1.9806e-03,\n",
       "                       3.9780e-04,  1.8105e-03,  4.6615e-05, -2.3674e-03,  2.3101e-03,\n",
       "                      -4.6395e-04,  5.6089e-04,  2.0444e-03,  1.3868e-03, -1.0938e-04,\n",
       "                      -2.1917e-03, -1.4762e-03,  1.4925e-03, -1.9697e-03,  8.8289e-04,\n",
       "                      -1.0674e-04, -8.5005e-04, -2.9103e-03, -2.6854e-03,  5.3662e-04,\n",
       "                      -3.7228e-03, -2.2007e-04,  8.1948e-04, -1.1127e-03,  2.5739e-04,\n",
       "                      -3.5823e-04, -8.3768e-04,  6.4477e-04, -7.8584e-04, -1.4137e-03,\n",
       "                       2.7013e-04, -8.8131e-04, -2.4212e-03,  1.6496e-04,  1.7730e-03,\n",
       "                      -1.6493e-03, -2.9197e-04,  5.6104e-04, -3.6875e-04,  1.0348e-03,\n",
       "                      -1.7620e-03, -1.0796e-03,  3.1995e-04,  1.0008e-03, -5.9892e-04,\n",
       "                      -1.1309e-03,  5.2170e-04, -2.0909e-03, -1.1342e-03,  1.1140e-03,\n",
       "                      -1.7626e-03,  8.6023e-04, -4.1007e-04, -2.7914e-03, -5.7390e-04,\n",
       "                      -6.5247e-04,  2.1511e-03,  2.1871e-03, -5.7655e-04, -2.1496e-03,\n",
       "                       9.8537e-04, -3.4386e-04,  1.0605e-03,  1.5454e-04, -1.1227e-03,\n",
       "                      -1.8732e-03,  1.1586e-03, -1.7041e-03,  1.0901e-03,  1.0314e-03,\n",
       "                       2.0983e-03,  3.0946e-04, -1.6624e-03,  2.3821e-03,  2.2314e-03,\n",
       "                       9.6095e-04,  2.2671e-03,  1.1266e-03, -2.6345e-03, -2.1044e-03,\n",
       "                       1.6542e-03, -2.1809e-03,  7.1706e-04, -1.8146e-03, -1.2018e-03,\n",
       "                       2.2774e-03,  3.2035e-04,  5.0942e-04, -1.8847e-03, -7.8383e-04,\n",
       "                      -1.1233e-04, -5.8340e-05,  4.6077e-04, -6.8750e-05, -9.6327e-04,\n",
       "                       2.4035e-03,  3.2336e-04,  1.1975e-03, -1.3576e-03,  6.0902e-04,\n",
       "                      -8.3157e-04, -6.6531e-04,  1.3972e-03,  2.2950e-03, -1.6776e-03,\n",
       "                      -1.2260e-03, -3.5797e-03, -1.5388e-05, -7.5375e-04,  2.9953e-03,\n",
       "                       2.6404e-03,  7.1562e-04, -1.8023e-04, -7.4289e-04, -1.5396e-03,\n",
       "                       1.7347e-03, -1.9905e-03,  4.8463e-03, -1.2171e-03, -1.2064e-03,\n",
       "                      -2.2076e-03,  9.4852e-04, -8.3927e-06, -3.0333e-03,  1.4372e-03,\n",
       "                       1.2297e-03,  1.5434e-03,  7.8152e-04, -1.0717e-03, -1.7045e-04,\n",
       "                       1.6448e-03,  4.7451e-04, -8.0063e-04, -2.0844e-03,  1.7383e-04,\n",
       "                      -9.2402e-04, -1.1344e-03, -2.3531e-05, -7.5991e-04, -2.3915e-03,\n",
       "                       2.5212e-04,  2.7103e-04, -9.6547e-04, -3.0342e-03, -3.9626e-04,\n",
       "                      -2.7010e-03, -1.7943e-03,  2.4736e-03, -2.5345e-04,  7.5647e-04,\n",
       "                       2.2499e-03, -9.4067e-06, -1.6288e-03,  1.0694e-03, -2.0464e-03,\n",
       "                      -1.5915e-03, -1.9719e-04, -1.9226e-03, -8.8333e-04,  8.0225e-04,\n",
       "                      -3.1661e-04, -1.6601e-03,  4.3098e-04, -3.5952e-03,  2.6522e-03,\n",
       "                       4.4286e-04, -8.3733e-04, -8.4094e-04, -1.3219e-03, -1.3502e-03,\n",
       "                       1.4243e-03, -1.9234e-03, -1.8125e-03,  4.1477e-04,  2.0095e-03,\n",
       "                      -1.5721e-03, -1.7953e-03, -4.6096e-04, -7.1178e-04,  6.2932e-05,\n",
       "                       6.9038e-04, -4.2445e-04, -9.5111e-04,  7.2314e-04, -1.3120e-03,\n",
       "                       2.5488e-03,  5.3639e-04, -1.2855e-03,  1.2635e-04, -1.4782e-03,\n",
       "                      -1.3140e-03, -1.0205e-03, -9.8290e-04, -1.2114e-04, -1.9110e-03,\n",
       "                       9.8590e-04,  1.0289e-03, -3.9495e-04, -2.0980e-03,  3.6720e-03,\n",
       "                      -8.6788e-04, -2.3849e-03, -7.3241e-04,  1.8703e-03, -5.6212e-04,\n",
       "                       1.0544e-04,  1.6224e-03,  1.7215e-03, -1.2952e-04,  2.5462e-04,\n",
       "                       9.6360e-05, -2.9675e-04, -1.2887e-03,  5.7862e-03, -1.1459e-03,\n",
       "                      -5.9369e-04,  7.6580e-04,  8.9095e-04,  2.6459e-03,  3.7534e-03,\n",
       "                      -1.5066e-03, -1.0979e-03, -6.6517e-04,  1.0593e-03,  1.7786e-03,\n",
       "                      -1.4689e-03, -1.3189e-03,  1.1463e-03, -1.9162e-03,  3.1887e-04,\n",
       "                      -2.8760e-03, -1.0306e-03,  1.0793e-03,  5.6343e-04, -2.6618e-03,\n",
       "                      -1.7350e-03, -1.2871e-03, -2.7211e-03, -2.5373e-03, -6.2192e-05,\n",
       "                       7.1735e-04, -8.8985e-04, -2.5972e-03,  9.9242e-04,  4.8208e-04,\n",
       "                      -1.1002e-03, -1.6900e-03, -8.8933e-04, -1.5453e-03, -2.0839e-03,\n",
       "                      -1.5131e-03, -2.3665e-03, -1.4082e-03, -1.0966e-03, -9.4966e-04,\n",
       "                      -3.2464e-03,  5.5196e-04, -1.7452e-03, -2.9273e-04, -3.5852e-03,\n",
       "                       1.0252e-04,  2.4863e-03,  7.8785e-04, -5.0491e-04, -6.7504e-04,\n",
       "                      -6.5868e-04, -4.0840e-03,  2.5301e-04,  1.5360e-03, -6.2759e-04,\n",
       "                      -1.5111e-03,  2.3682e-03,  3.7259e-03, -2.5149e-03,  2.8002e-03,\n",
       "                      -2.7049e-05, -2.8518e-03, -1.4769e-03,  1.2823e-03,  3.6810e-04,\n",
       "                      -9.2108e-04,  5.3845e-04, -1.6424e-03, -1.6198e-03, -7.9428e-04,\n",
       "                       1.6216e-03, -3.4185e-03,  1.2946e-03, -2.5752e-04, -6.7780e-04,\n",
       "                      -1.9305e-04, -1.5266e-03, -2.4757e-03,  1.7096e-03, -1.1203e-03,\n",
       "                      -5.5650e-04, -1.6490e-03, -3.9308e-03, -1.5007e-03, -2.2808e-03,\n",
       "                      -1.9720e-03,  7.0036e-04, -5.5674e-04,  2.7335e-04, -1.2525e-03,\n",
       "                      -7.2644e-05, -4.4264e-04, -1.2921e-03,  3.4462e-04,  5.5111e-04,\n",
       "                       5.9861e-04, -9.4597e-04,  3.1783e-03, -1.4757e-04,  2.1335e-03,\n",
       "                      -1.2762e-03,  4.1311e-04,  1.9641e-03,  1.4431e-03, -2.8552e-04,\n",
       "                      -6.8858e-04, -1.2946e-03, -1.5547e-03, -1.1784e-03,  6.1284e-04,\n",
       "                       4.9074e-04, -3.2329e-04,  2.9229e-04,  1.3394e-03, -2.5065e-03,\n",
       "                      -3.5768e-04,  2.7922e-03,  1.0880e-04,  7.3546e-04,  2.4607e-03,\n",
       "                      -5.5678e-04,  2.3405e-03,  7.1886e-04, -1.3619e-03, -2.1329e-03,\n",
       "                      -1.5621e-03,  1.2278e-03, -4.8667e-04,  2.9155e-03, -7.8002e-04,\n",
       "                      -1.1657e-03,  3.9511e-03,  3.8763e-03, -1.4627e-03,  9.1289e-04,\n",
       "                      -2.7637e-04,  1.3010e-03])),\n",
       "             ('transformer.resblocks.5.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0336,  0.0161,  0.0345,  ...,  0.0252,  0.0392,  0.0549],\n",
       "                      [ 0.0070,  0.0008, -0.0700,  ..., -0.0138,  0.0379,  0.0089],\n",
       "                      [ 0.0812,  0.0042, -0.0431,  ...,  0.0022, -0.0076, -0.0410],\n",
       "                      ...,\n",
       "                      [-0.0270,  0.0256,  0.0233,  ..., -0.0248,  0.0042, -0.0119],\n",
       "                      [ 0.0151,  0.0544, -0.0220,  ...,  0.0146, -0.0113,  0.0724],\n",
       "                      [ 0.0412, -0.0234, -0.0588,  ...,  0.0412, -0.0055,  0.0221]])),\n",
       "             ('transformer.resblocks.5.mlp.c_fc.bias',\n",
       "              tensor([ 4.0883e-02, -9.1978e-05,  2.5642e-02,  ...,  1.4060e-02,\n",
       "                       6.9389e-03, -1.3042e-02])),\n",
       "             ('transformer.resblocks.5.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0194, -0.0029,  0.0048,  ..., -0.0087, -0.0044,  0.0078],\n",
       "                      [-0.0129,  0.0094, -0.0171,  ...,  0.0082,  0.0005, -0.0093],\n",
       "                      [ 0.0047, -0.0021, -0.0014,  ...,  0.0114,  0.0169,  0.0147],\n",
       "                      ...,\n",
       "                      [ 0.0172,  0.0096, -0.0260,  ..., -0.0019,  0.0114,  0.0114],\n",
       "                      [ 0.0049, -0.0044, -0.0003,  ..., -0.0051, -0.0020, -0.0068],\n",
       "                      [ 0.0016, -0.0034, -0.0149,  ...,  0.0093, -0.0028,  0.0049]])),\n",
       "             ('transformer.resblocks.5.mlp.c_proj.bias',\n",
       "              tensor([ 3.8090e-03, -4.1596e-03, -2.1126e-03, -1.9086e-02,  1.4347e-02,\n",
       "                       1.8959e-02,  1.9611e-02, -1.0601e-03,  1.8811e-02, -6.1465e-03,\n",
       "                      -1.9813e-02, -9.1634e-03, -2.1539e-02,  1.6845e-02, -1.7739e-02,\n",
       "                      -1.0974e-03,  6.8480e-03,  4.2489e-04,  1.4333e-02, -2.2813e-03,\n",
       "                      -1.0348e-02, -2.1638e-02, -3.6405e-03, -1.2041e-02,  6.0415e-03,\n",
       "                       1.5879e-03,  1.2608e-02, -1.1951e-02, -7.8406e-03, -1.6992e-02,\n",
       "                       1.9214e-02, -2.0333e-02, -5.6055e-03, -4.4738e-03, -1.1284e-03,\n",
       "                      -1.4783e-02,  1.1312e-02, -1.2825e-03,  1.8161e-02,  1.3580e-02,\n",
       "                      -1.4278e-02,  1.2920e-02, -5.5141e-03, -1.0879e-02,  5.2243e-04,\n",
       "                      -6.9158e-03,  1.8913e-02,  5.6466e-03,  1.4029e-02, -4.2405e-03,\n",
       "                       7.1747e-03, -7.0743e-03,  5.0692e-03,  1.8230e-02, -1.5899e-02,\n",
       "                      -1.1411e-02, -1.6672e-03, -1.3367e-02,  2.1590e-02,  1.8930e-02,\n",
       "                      -1.4586e-02,  2.0584e-02,  4.4462e-03,  1.1363e-02, -1.5325e-02,\n",
       "                      -1.2942e-02, -6.7491e-03,  1.0581e-02, -1.9033e-03,  3.0806e-03,\n",
       "                      -1.9375e-02,  2.1014e-02, -5.7601e-03, -1.3609e-03, -1.3348e-02,\n",
       "                       9.1070e-03,  8.2516e-03, -1.7258e-02, -2.8320e-04,  1.4252e-02,\n",
       "                      -7.9824e-03, -2.2561e-02,  5.7997e-03,  2.4409e-03, -6.1711e-03,\n",
       "                       1.9373e-02,  9.1259e-03,  2.0207e-02,  1.8310e-02, -1.8033e-02,\n",
       "                      -1.3589e-02, -1.9370e-02,  8.4455e-03,  1.9145e-02, -1.8218e-02,\n",
       "                       6.4616e-03,  8.3324e-04, -5.4322e-05,  9.0735e-03, -1.2850e-03,\n",
       "                      -1.5253e-02,  9.9492e-03,  8.7724e-03, -1.1035e-05, -1.1699e-02,\n",
       "                      -6.0629e-03, -1.6242e-02, -2.1311e-02,  6.7254e-03, -1.9538e-02,\n",
       "                      -1.4780e-02, -1.7555e-02,  1.8692e-02,  1.3827e-02,  9.3521e-04,\n",
       "                       7.4023e-03,  8.8523e-03,  1.4557e-02,  1.8890e-02, -5.4012e-03,\n",
       "                      -1.3738e-02, -1.3236e-02, -1.2766e-02, -2.7142e-03, -1.0606e-02,\n",
       "                      -1.1979e-03,  2.0043e-02,  1.9527e-02,  1.0299e-02,  1.4480e-02,\n",
       "                       1.7364e-02,  3.0036e-03,  3.7225e-03, -2.3799e-02, -1.6144e-02,\n",
       "                      -2.1028e-02,  3.8878e-03,  6.4287e-03, -1.4469e-02, -1.8779e-02,\n",
       "                       1.8176e-02,  6.4430e-03, -6.7397e-03,  1.8341e-02, -1.8699e-02,\n",
       "                      -1.9779e-02,  1.1175e-02, -7.4339e-03,  3.5250e-03, -1.0117e-02,\n",
       "                      -1.5436e-02, -1.7274e-02, -7.1374e-03,  2.3563e-03, -3.9995e-03,\n",
       "                       1.0279e-02, -1.3981e-03,  2.0949e-02, -2.0001e-02,  1.7554e-02,\n",
       "                       3.5580e-03, -3.5980e-03, -5.8791e-04, -1.2671e-02,  1.4552e-02,\n",
       "                      -1.2418e-02,  1.0073e-02,  6.7185e-03, -8.2379e-03, -7.9689e-03,\n",
       "                       1.9410e-02, -1.0295e-02, -2.4536e-03, -2.0021e-02, -2.6817e-03,\n",
       "                       2.1496e-03, -4.6988e-05, -1.8607e-02,  1.9773e-02,  1.2189e-02,\n",
       "                       1.7689e-03,  7.6384e-03, -2.2975e-03,  7.0018e-03,  7.5158e-03,\n",
       "                      -1.1665e-02,  2.2654e-02, -1.5865e-02, -5.2565e-03, -7.3047e-03,\n",
       "                       1.0380e-02, -9.8965e-03,  2.5838e-03, -1.1678e-02, -2.0953e-03,\n",
       "                      -1.5992e-02,  2.8877e-03, -5.3310e-03,  2.1546e-02, -7.9872e-03,\n",
       "                      -2.1761e-02,  1.0032e-02,  1.5858e-02,  1.4627e-02,  1.8201e-02,\n",
       "                      -9.8026e-03,  8.7961e-03, -7.3303e-03,  1.1017e-02, -5.1714e-03,\n",
       "                       2.2429e-02, -2.2956e-03,  7.7240e-03, -1.1715e-02, -1.6188e-02,\n",
       "                       2.0449e-02, -1.3874e-02, -5.2474e-03,  1.1036e-02, -1.7534e-02,\n",
       "                      -1.2994e-02,  8.4376e-05,  1.9454e-03, -1.1780e-02, -4.1247e-03,\n",
       "                       2.0510e-02,  1.7848e-02,  2.0902e-02, -1.7749e-02, -1.5446e-02,\n",
       "                       1.1925e-02,  1.5438e-02,  1.5633e-02, -1.0935e-02,  6.3756e-03,\n",
       "                      -1.6675e-02, -6.7894e-03, -8.6980e-03,  1.2976e-02, -1.6028e-02,\n",
       "                       2.0244e-03, -1.8727e-02,  9.3745e-03, -1.9121e-02,  7.0188e-03,\n",
       "                      -1.7654e-02, -9.7794e-04, -1.3307e-02,  1.0886e-02, -2.0101e-02,\n",
       "                       4.4043e-03, -4.9631e-04, -5.9189e-03,  1.3854e-02,  1.3318e-02,\n",
       "                      -2.2312e-02, -8.9422e-03, -1.5272e-02,  2.9871e-03,  1.0729e-02,\n",
       "                       1.7433e-02,  4.6101e-03,  3.8148e-04, -8.9168e-04, -9.6080e-04,\n",
       "                       9.1356e-03,  1.4179e-02, -2.1095e-02, -3.7239e-03, -8.7451e-03,\n",
       "                      -8.9452e-03, -8.0809e-03, -2.0322e-02,  7.4671e-03,  1.6023e-02,\n",
       "                       1.9407e-02, -8.8658e-04,  6.0438e-03, -1.6740e-02,  1.2788e-02,\n",
       "                      -5.4035e-03, -1.5755e-03, -1.5483e-02,  6.0260e-03, -1.9293e-02,\n",
       "                      -4.6397e-03, -1.2202e-02,  1.5525e-02,  1.1013e-02, -1.6415e-02,\n",
       "                       1.9954e-02,  1.1500e-02, -2.1562e-02, -1.5366e-02,  8.7364e-04,\n",
       "                      -1.6324e-02, -1.1521e-03,  6.8791e-03, -1.3494e-02,  3.4048e-03,\n",
       "                      -1.2053e-02,  1.4370e-02, -1.0340e-02, -1.4651e-02, -1.7750e-02,\n",
       "                       1.3460e-02, -5.0230e-04,  5.5665e-03,  1.0493e-02,  1.1956e-02,\n",
       "                       2.2573e-02, -7.8582e-04,  3.7918e-03, -1.1742e-02, -8.5383e-03,\n",
       "                       2.2725e-02, -9.3342e-03,  1.0213e-02, -1.2831e-02, -1.9531e-02,\n",
       "                      -1.2853e-02, -5.1911e-03, -1.3342e-02,  8.2190e-03,  1.1353e-02,\n",
       "                      -1.1442e-02,  8.2604e-04,  2.8016e-03, -2.3358e-02, -1.4147e-02,\n",
       "                       1.3303e-02,  2.2017e-02,  2.1469e-02, -5.3185e-03,  2.2081e-02,\n",
       "                       1.9036e-02, -8.7346e-03,  9.6920e-03, -1.8282e-02,  1.8689e-03,\n",
       "                      -4.5616e-04,  6.0398e-03, -4.9255e-03, -1.4578e-02, -8.0688e-03,\n",
       "                       1.2054e-02,  7.2638e-03,  8.6440e-03,  2.0915e-02, -1.7219e-03,\n",
       "                      -1.4431e-02, -1.2651e-02, -1.6610e-02, -1.9500e-03, -4.1261e-03,\n",
       "                       1.4390e-02, -1.2450e-02,  1.4674e-02, -2.0732e-02, -3.4284e-03,\n",
       "                      -1.6823e-02,  1.5775e-02,  4.5943e-03,  4.8888e-03, -1.5155e-02,\n",
       "                       2.1379e-02, -4.8557e-03,  1.0173e-02, -6.2110e-03, -1.9876e-02,\n",
       "                       5.5664e-03,  1.5518e-02,  3.3092e-03, -2.1361e-04, -1.5704e-02,\n",
       "                       2.6927e-03,  4.6150e-03,  8.9225e-03,  1.0427e-02,  8.1215e-03,\n",
       "                       1.9410e-02, -1.0127e-02, -2.3668e-02, -1.1372e-02,  1.7025e-03,\n",
       "                      -1.7538e-02,  3.7005e-03, -1.7435e-02,  1.4258e-02,  7.8761e-03,\n",
       "                       8.7090e-03,  2.0649e-02,  6.7657e-03,  4.2213e-03, -1.5721e-02,\n",
       "                      -1.0276e-02, -6.8664e-03, -3.8594e-03,  2.9068e-03,  1.7547e-02,\n",
       "                      -1.5841e-03,  6.5688e-03,  1.9734e-02,  1.2215e-02,  1.4766e-02,\n",
       "                       1.9389e-02,  8.8871e-03,  1.8344e-02, -1.4390e-02,  1.0544e-02,\n",
       "                      -1.4916e-02,  6.4034e-03,  2.3711e-03,  8.0650e-03, -1.1318e-02,\n",
       "                      -4.0346e-04,  1.4524e-03,  9.4952e-03,  2.0760e-02,  1.9974e-02,\n",
       "                      -2.1388e-03,  2.0576e-02, -1.0907e-02, -1.1123e-02, -1.6530e-02,\n",
       "                      -1.7327e-02, -7.6474e-03,  1.0771e-02,  6.8253e-03, -1.4292e-02,\n",
       "                       1.6140e-02, -6.1912e-03, -3.1153e-03, -2.0943e-02, -2.4618e-02,\n",
       "                       1.2208e-02,  2.0005e-02, -1.3029e-02,  1.1567e-02, -1.2359e-02,\n",
       "                      -3.3070e-03, -1.7491e-02, -4.3642e-03,  6.1965e-03,  1.1719e-02,\n",
       "                      -2.0396e-02, -2.2779e-02,  2.5208e-03, -2.2619e-03,  1.6329e-02,\n",
       "                      -1.3380e-02,  2.1292e-02, -8.8586e-03,  1.5592e-03, -1.2660e-02,\n",
       "                      -9.7686e-03, -4.3734e-03,  1.9922e-02, -1.2932e-02, -5.2034e-03,\n",
       "                      -1.2238e-02,  9.3815e-03,  9.8133e-04,  1.4320e-02,  4.3137e-03,\n",
       "                      -1.9694e-02, -1.3650e-02,  5.2425e-03, -5.6277e-03, -2.1921e-02,\n",
       "                       1.3699e-02,  9.2494e-03, -2.5063e-02, -2.7118e-03, -1.3567e-02,\n",
       "                       1.7669e-02,  1.8575e-02,  9.2918e-03, -1.2977e-02, -1.4528e-02,\n",
       "                      -9.8587e-03, -9.6838e-03, -9.8678e-03,  3.8500e-03,  9.9164e-03,\n",
       "                       8.6798e-04, -5.5980e-03, -1.0264e-02,  1.1863e-02, -8.0126e-03,\n",
       "                      -2.7185e-03,  1.2965e-02, -1.3274e-03, -1.5832e-02, -2.0323e-02,\n",
       "                      -1.9357e-02,  1.3547e-02,  7.9921e-03, -1.5348e-02, -1.7474e-02,\n",
       "                      -1.6059e-02,  4.5195e-03,  6.4089e-03, -1.4105e-02, -1.0152e-02,\n",
       "                      -5.0543e-03, -3.2164e-03, -1.8223e-03,  1.1142e-02, -8.4963e-03,\n",
       "                       1.1451e-02,  4.3079e-03])),\n",
       "             ('transformer.resblocks.5.ln_2.weight',\n",
       "              tensor([1.0020, 1.0010, 1.0021, 1.0020, 1.0027, 0.9974, 1.0051, 1.0006, 0.9996,\n",
       "                      1.0023, 1.0030, 1.0033, 0.9999, 1.0028, 0.9991, 1.0010, 1.0018, 1.0026,\n",
       "                      1.0029, 0.9999, 1.0037, 1.0012, 1.0009, 1.0007, 0.9999, 1.0006, 1.0033,\n",
       "                      1.0023, 1.0032, 1.0005, 1.0029, 1.0032, 1.0010, 1.0001, 1.0042, 1.0039,\n",
       "                      1.0001, 1.0027, 0.9993, 1.0021, 1.0003, 1.0012, 1.0041, 1.0020, 1.0035,\n",
       "                      1.0037, 1.0020, 1.0006, 1.0028, 1.0017, 1.0046, 1.0044, 1.0001, 0.9995,\n",
       "                      1.0040, 1.0022, 1.0050, 1.0007, 0.9989, 1.0035, 1.0023, 0.9996, 0.9993,\n",
       "                      1.0024, 1.0048, 1.0003, 0.9989, 1.0017, 1.0020, 1.0014, 1.0030, 0.9980,\n",
       "                      1.0018, 1.0002, 1.0040, 1.0018, 1.0026, 1.0011, 1.0020, 1.0007, 1.0011,\n",
       "                      1.0005, 1.0005, 0.9980, 1.0020, 1.0020, 1.0012, 1.0022, 1.0013, 1.0029,\n",
       "                      1.0033, 1.0023, 1.0003, 1.0002, 1.0011, 0.9976, 1.0017, 1.0018, 1.0016,\n",
       "                      1.0013, 1.0027, 1.0020, 1.0016, 1.0025, 1.0031, 0.9979, 1.0013, 1.0022,\n",
       "                      1.0033, 1.0016, 1.0020, 1.0007, 0.9981, 0.9980, 1.0002, 0.9987, 1.0019,\n",
       "                      1.0023, 1.0016, 1.0000, 1.0032, 1.0025, 1.0004, 1.0014, 1.0031, 1.0013,\n",
       "                      1.0011, 1.0023, 1.0031, 1.0040, 1.0032, 0.9999, 1.0017, 1.0021, 1.0011,\n",
       "                      1.0006, 1.0025, 1.0023, 1.0033, 1.0005, 1.0046, 1.0016, 1.0046, 1.0023,\n",
       "                      0.9993, 1.0004, 1.0015, 1.0004, 1.0023, 1.0012, 1.0005, 0.9989, 0.9975,\n",
       "                      1.0022, 0.9996, 0.9999, 0.9995, 1.0005, 0.9988, 1.0008, 1.0007, 1.0021,\n",
       "                      0.9997, 1.0007, 1.0023, 0.9996, 1.0053, 1.0004, 0.9993, 0.9988, 1.0010,\n",
       "                      1.0020, 1.0063, 1.0039, 0.9982, 0.9995, 1.0003, 1.0006, 1.0034, 1.0045,\n",
       "                      1.0035, 1.0029, 1.0015, 1.0018, 0.9986, 1.0010, 1.0019, 1.0013, 1.0026,\n",
       "                      1.0019, 1.0016, 1.0032, 1.0019, 1.0029, 1.0007, 1.0021, 0.9990, 0.9964,\n",
       "                      1.0035, 1.0021, 1.0015, 1.0045, 1.0006, 1.0070, 1.0007, 1.0017, 1.0010,\n",
       "                      1.0055, 1.0037, 1.0022, 1.0046, 1.0005, 1.0006, 1.0002, 0.9983, 1.0026,\n",
       "                      1.0031, 1.0012, 0.9998, 1.0020, 1.0025, 1.0012, 1.0009, 1.0011, 1.0014,\n",
       "                      1.0031, 0.9995, 1.0020, 1.0031, 0.9982, 1.0008, 1.0034, 1.0032, 1.0034,\n",
       "                      1.0023, 1.0005, 1.0012, 1.0005, 1.0023, 1.0009, 1.0024, 1.0026, 1.0022,\n",
       "                      1.0011, 1.0012, 1.0064, 1.0023, 1.0018, 1.0000, 1.0050, 1.0053, 1.0021,\n",
       "                      0.9990, 1.0025, 0.9979, 1.0002, 1.0004, 0.9991, 1.0051, 1.0007, 1.0036,\n",
       "                      1.0017, 1.0035, 1.0007, 1.0005, 1.0011, 1.0014, 1.0070, 1.0028, 0.9986,\n",
       "                      1.0039, 1.0003, 1.0025, 1.0015, 1.0030, 0.9994, 1.0026, 1.0034, 1.0035,\n",
       "                      0.9995, 1.0009, 1.0008, 1.0021, 1.0012, 1.0055, 1.0014, 1.0047, 1.0009,\n",
       "                      1.0024, 1.0004, 1.0030, 0.9998, 1.0041, 0.9992, 1.0028, 1.0011, 1.0008,\n",
       "                      1.0020, 1.0009, 1.0010, 1.0017, 1.0033, 1.0029, 1.0001, 1.0013, 1.0023,\n",
       "                      1.0014, 1.0021, 0.9998, 0.9980, 1.0016, 1.0033, 1.0008, 1.0016, 1.0007,\n",
       "                      1.0017, 1.0022, 1.0007, 1.0047, 0.9985, 0.9983, 1.0044, 1.0027, 1.0018,\n",
       "                      1.0055, 0.9990, 1.0023, 1.0036, 1.0022, 1.0030, 1.0018, 1.0064, 1.0009,\n",
       "                      1.0027, 0.9999, 1.0037, 1.0048, 1.0015, 1.0026, 1.0004, 1.0004, 1.0003,\n",
       "                      1.0015, 1.0009, 1.0034, 1.0024, 0.9987, 0.9998, 1.0014, 1.0016, 1.0017,\n",
       "                      1.0019, 1.0023, 1.0028, 1.0023, 1.0042, 1.0015, 1.0033, 1.0020, 1.0007,\n",
       "                      0.9996, 1.0000, 1.0018, 1.0004, 1.0032, 1.0010, 1.0026, 1.0039, 1.0021,\n",
       "                      1.0032, 1.0010, 1.0027, 0.9997, 1.0031, 1.0029, 1.0031, 0.9997, 1.0014,\n",
       "                      1.0038, 1.0020, 1.0032, 1.0023, 1.0001, 1.0035, 1.0010, 0.9990, 1.0015,\n",
       "                      1.0043, 1.0011, 0.9981, 1.0022, 1.0053, 0.9998, 1.0022, 1.0024, 1.0003,\n",
       "                      0.9987, 1.0020, 0.9997, 1.0018, 1.0023, 1.0026, 1.0025, 1.0023, 1.0025,\n",
       "                      1.0040, 0.9994, 1.0017, 1.0011, 1.0033, 0.9993, 1.0069, 1.0064, 1.0030,\n",
       "                      0.9993, 0.9989, 1.0012, 1.0045, 1.0024, 1.0031, 1.0044, 1.0027, 1.0010,\n",
       "                      1.0008, 1.0032, 1.0020, 1.0037, 1.0015, 1.0038, 1.0006, 1.0025, 1.0026,\n",
       "                      1.0045, 1.0015, 1.0019, 1.0042, 1.0016, 1.0025, 0.9981, 1.0029, 1.0023,\n",
       "                      0.9996, 1.0035, 0.9996, 1.0054, 1.0034, 1.0007, 0.9992, 1.0036, 1.0005,\n",
       "                      1.0008, 1.0039, 1.0050, 0.9997, 1.0015, 1.0008, 1.0040, 1.0001, 1.0000,\n",
       "                      1.0008, 1.0034, 1.0009, 1.0021, 1.0024, 1.0029, 1.0010, 0.9977, 1.0009,\n",
       "                      1.0023, 1.0044, 0.9991, 1.0032, 1.0004, 1.0012, 0.9987, 1.0046, 1.0027,\n",
       "                      1.0022, 1.0017, 0.9981, 1.0006, 1.0053, 1.0015, 1.0021, 1.0036, 1.0004,\n",
       "                      1.0016, 1.0013, 1.0015, 1.0038, 1.0023, 1.0009, 1.0026, 1.0032, 1.0032,\n",
       "                      0.9996, 1.0016, 1.0001, 0.9981, 1.0030, 1.0029, 1.0031, 1.0015, 1.0025,\n",
       "                      0.9980, 1.0033, 1.0034, 1.0005, 1.0019, 1.0047, 1.0031, 0.9987])),\n",
       "             ('transformer.resblocks.5.ln_2.bias',\n",
       "              tensor([-9.0884e-04, -1.9027e-04, -2.7100e-03,  4.9528e-04, -6.0181e-05,\n",
       "                       3.1716e-03,  1.7020e-03, -1.5907e-05,  4.1872e-03, -4.2924e-03,\n",
       "                       2.9429e-03,  1.4913e-03,  3.6586e-04,  2.3062e-03,  2.4874e-03,\n",
       "                      -1.5238e-03, -5.3704e-04, -2.4679e-03,  1.9239e-03,  2.9048e-03,\n",
       "                       2.0402e-03, -9.1698e-04, -2.1338e-03,  1.4147e-03, -1.1278e-03,\n",
       "                       3.2269e-03,  2.5108e-04,  1.8703e-03,  2.6752e-04,  2.5941e-03,\n",
       "                       1.4391e-04, -1.2329e-04, -2.9890e-03,  8.7915e-04,  1.2008e-03,\n",
       "                      -3.3620e-03, -4.4466e-03, -3.7024e-03,  1.1545e-03,  1.2697e-03,\n",
       "                      -2.2816e-03,  1.3734e-03,  1.9631e-03,  1.6375e-03, -7.4098e-04,\n",
       "                      -5.0369e-04,  4.3523e-03, -2.1332e-03,  9.8970e-05, -2.9724e-04,\n",
       "                       3.6881e-03, -1.6156e-03,  4.3931e-04, -3.6534e-03,  3.9748e-03,\n",
       "                      -9.3974e-04,  1.1177e-03,  8.0541e-05, -1.2078e-03, -4.8920e-04,\n",
       "                      -1.6754e-03,  1.0295e-03, -9.8796e-04,  2.6496e-03,  1.1107e-03,\n",
       "                       6.1025e-04,  3.5343e-04, -1.5881e-03,  3.8690e-04,  2.9496e-04,\n",
       "                      -2.3871e-03,  6.3959e-04,  3.9535e-04, -1.1929e-04, -3.3075e-04,\n",
       "                      -2.1746e-03, -3.5349e-03,  2.8929e-03, -1.3589e-03,  1.5311e-03,\n",
       "                      -3.0872e-03, -2.7129e-05, -5.8717e-04, -2.6766e-03,  1.3788e-03,\n",
       "                      -3.4388e-03,  2.0013e-04, -6.7213e-04, -1.7264e-03, -9.4141e-04,\n",
       "                      -1.9832e-03, -1.6087e-03, -2.4151e-03,  3.2521e-03,  8.6852e-04,\n",
       "                       1.7642e-03,  1.0211e-03, -3.3793e-04,  6.2881e-03,  1.9690e-03,\n",
       "                      -2.3905e-03,  9.9892e-04, -1.7138e-03, -2.1245e-03,  4.1309e-04,\n",
       "                       3.1094e-03, -1.0427e-03,  2.3466e-03,  9.9025e-04,  5.8880e-04,\n",
       "                      -8.8320e-04, -2.9557e-03, -1.6627e-03,  1.5545e-03, -3.7993e-04,\n",
       "                       4.3170e-03, -1.3997e-03,  4.4055e-04,  3.7634e-03, -1.5426e-03,\n",
       "                       1.3002e-04,  1.4409e-03, -3.4840e-03, -6.9016e-04,  3.4099e-03,\n",
       "                      -7.6833e-04, -9.6206e-05, -2.0434e-03, -7.8534e-04,  8.7624e-05,\n",
       "                       4.2476e-03, -1.0876e-03,  3.0614e-03,  2.8146e-03, -1.5099e-03,\n",
       "                       1.8782e-03,  3.7443e-03,  2.5087e-04,  5.3347e-04,  9.5293e-04,\n",
       "                       9.5607e-04, -2.7803e-03, -2.8016e-03,  2.1329e-03, -3.3470e-03,\n",
       "                       3.9157e-04,  3.3357e-03, -2.4398e-04, -7.2925e-05, -1.3951e-03,\n",
       "                      -4.0274e-04, -2.0745e-03, -4.5059e-03, -4.1616e-03,  9.1876e-04,\n",
       "                       4.2466e-03, -1.2654e-03,  3.6637e-03, -1.9929e-03, -2.9446e-04,\n",
       "                       2.6652e-03,  6.7160e-04, -3.5338e-03, -1.5324e-03,  2.7106e-04,\n",
       "                      -2.9312e-03, -1.9063e-03,  5.7745e-04,  1.8981e-03,  4.2065e-04,\n",
       "                      -2.8906e-03,  1.6657e-03,  1.0129e-03, -3.2997e-03,  6.6278e-04,\n",
       "                       4.3292e-04, -9.4953e-05, -1.3218e-03, -5.8418e-04, -1.9722e-03,\n",
       "                       1.0902e-04, -6.7627e-04, -2.1197e-03, -6.3976e-04, -4.5179e-03,\n",
       "                       1.0518e-03, -1.1268e-03,  2.5376e-04,  1.2477e-03,  3.4065e-04,\n",
       "                      -4.1449e-04, -1.6217e-03,  4.9629e-04,  2.3493e-03,  1.7945e-03,\n",
       "                       4.6344e-03, -1.7849e-03, -3.3456e-03,  7.0369e-04,  6.7255e-04,\n",
       "                       1.0460e-03, -3.1407e-03, -9.4938e-04,  2.5072e-03,  8.3210e-04,\n",
       "                      -1.5457e-03,  1.1778e-03, -3.5521e-04, -9.6580e-04, -1.3338e-03,\n",
       "                      -2.2342e-03,  8.3124e-04,  3.6752e-04, -9.6484e-04,  2.1562e-03,\n",
       "                       3.2101e-03, -1.3516e-04,  7.4689e-04, -4.6812e-05, -2.7305e-03,\n",
       "                      -4.0250e-03, -3.3398e-03, -1.4590e-03, -1.0953e-03, -1.8552e-03,\n",
       "                      -2.9155e-03,  1.5401e-03,  1.5783e-03,  1.5342e-03,  1.7024e-04,\n",
       "                       2.0293e-03,  2.9739e-03,  2.4061e-04, -6.0409e-04,  1.4966e-03,\n",
       "                       8.4825e-04,  7.1610e-04,  4.5624e-03, -3.1818e-03, -4.6241e-04,\n",
       "                       1.4192e-03, -1.1548e-03, -2.8208e-03,  5.9435e-04,  7.3246e-04,\n",
       "                       1.9392e-04, -7.8024e-04, -2.4202e-03,  3.5516e-03, -1.5948e-03,\n",
       "                      -1.4112e-03, -8.7754e-04, -1.5242e-03,  3.8183e-03, -1.3750e-03,\n",
       "                       1.4232e-03, -1.1282e-03,  4.5063e-03, -2.2797e-03,  6.6488e-04,\n",
       "                      -4.6561e-03,  2.1516e-03, -2.5067e-03,  4.2069e-03,  8.1673e-04,\n",
       "                       5.4139e-04,  9.2375e-04,  6.7484e-04,  1.4864e-03,  2.1844e-04,\n",
       "                       3.4254e-04,  2.0529e-03,  9.8918e-04, -1.0825e-03, -2.3911e-03,\n",
       "                       5.8073e-04,  3.1863e-04, -7.6349e-04,  3.8705e-03,  4.3094e-03,\n",
       "                       4.2025e-04, -2.9869e-03,  1.5503e-03, -1.1076e-04,  7.1724e-04,\n",
       "                      -1.5756e-03,  8.3998e-04, -1.1542e-03,  3.7681e-05, -7.5506e-04,\n",
       "                      -2.2595e-04, -1.8354e-04,  1.2994e-03, -2.6004e-03, -1.0272e-03,\n",
       "                       9.2352e-04,  3.3713e-03, -2.1151e-03,  3.8917e-04, -7.0793e-04,\n",
       "                      -5.8810e-04, -1.1626e-03, -6.4970e-04,  2.0296e-03, -1.9565e-04,\n",
       "                       1.8171e-03,  1.0049e-03, -2.2931e-03,  6.2347e-04,  6.6053e-04,\n",
       "                      -1.2754e-03, -4.4548e-04, -5.5478e-04, -2.2941e-03,  2.0199e-03,\n",
       "                       1.6408e-03, -3.5582e-03, -2.5796e-04,  1.4972e-05, -2.3827e-03,\n",
       "                      -1.5964e-03,  1.4962e-03, -4.2195e-03,  1.5883e-03,  2.1722e-03,\n",
       "                      -1.2420e-03, -2.1150e-03, -2.3708e-03,  2.4108e-04, -1.6319e-03,\n",
       "                       5.5591e-04, -1.3273e-03, -2.2680e-03,  2.1610e-03, -3.6489e-03,\n",
       "                      -2.6363e-04, -1.9647e-03, -1.3957e-03, -2.5083e-03, -1.6854e-03,\n",
       "                       4.2435e-04, -2.6498e-03,  1.0452e-03,  1.7236e-03,  1.7440e-03,\n",
       "                      -1.5026e-03, -2.7112e-03,  2.0204e-03,  1.1071e-03, -3.0428e-03,\n",
       "                       1.2605e-03,  2.0015e-04,  5.5835e-04, -7.8461e-04,  1.6795e-03,\n",
       "                      -1.0148e-03,  5.8090e-04,  8.4262e-04, -7.6969e-04,  2.3378e-04,\n",
       "                       6.7356e-04,  2.1927e-03, -1.6904e-03, -2.3378e-03,  4.0100e-03,\n",
       "                      -2.2614e-03, -6.1921e-04, -6.2155e-04,  4.7289e-03,  3.2574e-03,\n",
       "                      -2.0299e-03,  7.0954e-04,  1.9032e-03, -2.5570e-03, -1.5261e-03,\n",
       "                      -2.7100e-03,  1.4848e-03, -4.4515e-03,  1.5916e-03,  6.5220e-04,\n",
       "                      -1.8287e-03, -2.1001e-04, -3.6695e-04, -1.8408e-03, -1.8816e-03,\n",
       "                       2.2016e-03,  2.8681e-03,  1.9079e-03, -9.0320e-04, -2.9604e-03,\n",
       "                       8.0524e-04,  5.9613e-04,  2.0625e-04,  2.8540e-03,  6.5363e-04,\n",
       "                      -3.5402e-03,  4.1626e-04, -4.9558e-04, -2.8930e-03, -5.3819e-04,\n",
       "                      -2.1905e-03, -7.4310e-04, -2.5181e-03, -4.0639e-04,  5.3912e-04,\n",
       "                      -8.1275e-04, -1.0332e-03,  2.4601e-04,  2.0110e-03,  3.8152e-03,\n",
       "                      -1.5560e-03, -1.3350e-03, -1.7724e-03, -4.5620e-04, -3.9848e-03,\n",
       "                       3.6227e-03, -9.1968e-04,  1.1157e-03,  1.8140e-03, -1.9566e-03,\n",
       "                       1.4356e-03,  1.9705e-04, -2.3533e-03,  3.7163e-03,  1.7969e-05,\n",
       "                      -1.7960e-03, -1.9754e-03,  1.4388e-04, -4.5255e-04, -1.1713e-03,\n",
       "                       7.9793e-04,  2.2082e-04,  5.8940e-04,  1.2261e-03,  6.6576e-04,\n",
       "                       1.8095e-03, -3.4615e-03,  1.8652e-03, -4.8815e-03, -9.9917e-04,\n",
       "                       1.0988e-03, -1.0448e-03, -2.3791e-04, -1.3281e-03, -1.9832e-03,\n",
       "                       1.4040e-03,  1.7462e-03,  2.1176e-03,  8.5506e-04, -3.5226e-03,\n",
       "                      -3.3427e-03, -8.6475e-04,  1.9757e-03, -1.0109e-03, -1.3913e-03,\n",
       "                       5.4112e-04, -7.1495e-04, -1.1568e-03, -4.0678e-03, -1.5151e-03,\n",
       "                       4.8029e-04,  7.9286e-04, -5.1158e-04, -1.6498e-03,  1.6033e-03,\n",
       "                       3.3382e-04,  2.5187e-03,  1.1503e-04,  1.2931e-03, -1.7986e-04,\n",
       "                       1.3324e-03,  5.6666e-04,  2.6387e-03,  2.5293e-03,  2.8141e-03,\n",
       "                      -1.7295e-03,  1.8542e-03,  2.3362e-03,  1.1692e-03,  1.4312e-03,\n",
       "                       1.5117e-03,  1.1446e-03, -1.7873e-03, -5.8891e-04,  1.0929e-03,\n",
       "                      -1.6191e-03,  1.9628e-05,  2.4966e-03, -1.0720e-03,  2.5349e-03,\n",
       "                      -1.1955e-03, -1.7606e-03, -4.3826e-03,  1.2377e-03, -2.9239e-03,\n",
       "                      -3.0837e-04, -2.8555e-05,  3.0504e-03, -2.7659e-03,  7.0058e-04,\n",
       "                       1.5659e-03,  2.6227e-03,  1.2355e-03, -9.8912e-04,  1.0599e-03,\n",
       "                      -2.8789e-03, -2.4480e-03, -5.0647e-04,  9.7929e-04,  1.8917e-03,\n",
       "                       3.4700e-04,  7.7309e-04])),\n",
       "             ('transformer.resblocks.6.attn.in_proj_weight',\n",
       "              tensor([[ 0.0069,  0.0295,  0.0017,  ..., -0.0727,  0.0752,  0.0243],\n",
       "                      [-0.0311,  0.0139, -0.0090,  ..., -0.0145, -0.0353,  0.0622],\n",
       "                      [ 0.0327,  0.0372, -0.0078,  ...,  0.0353,  0.0139, -0.0309],\n",
       "                      ...,\n",
       "                      [ 0.0275,  0.0271,  0.0441,  ...,  0.0102, -0.0150,  0.0169],\n",
       "                      [ 0.0393,  0.0030, -0.0182,  ..., -0.0280, -0.0134, -0.0418],\n",
       "                      [ 0.1527, -0.0326, -0.0288,  ...,  0.0535,  0.0385,  0.0437]])),\n",
       "             ('transformer.resblocks.6.attn.in_proj_bias',\n",
       "              tensor([ 0.0017, -0.0013, -0.0066,  ...,  0.0006, -0.0005,  0.0007])),\n",
       "             ('transformer.resblocks.6.attn.out_proj.weight',\n",
       "              tensor([[-0.0008, -0.0019, -0.0103,  ..., -0.0074,  0.0177,  0.0072],\n",
       "                      [ 0.0011, -0.0010,  0.0165,  ..., -0.0006,  0.0080, -0.0036],\n",
       "                      [ 0.0178, -0.0186, -0.0036,  ..., -0.0069, -0.0081,  0.0042],\n",
       "                      ...,\n",
       "                      [ 0.0040,  0.0218,  0.0023,  ...,  0.0142, -0.0033,  0.0095],\n",
       "                      [ 0.0266,  0.0024,  0.0179,  ..., -0.0021,  0.0150,  0.0174],\n",
       "                      [-0.0017, -0.0129,  0.0019,  ..., -0.0035,  0.0020, -0.0023]])),\n",
       "             ('transformer.resblocks.6.attn.out_proj.bias',\n",
       "              tensor([ 2.0508e-04, -3.0794e-03, -4.6834e-04, -8.4849e-04, -6.3076e-04,\n",
       "                      -3.0238e-03,  4.3836e-04,  2.4507e-03, -3.0958e-03,  1.2710e-03,\n",
       "                      -3.6333e-03, -2.0705e-03, -9.1563e-04, -1.3542e-03, -3.9915e-03,\n",
       "                       1.5207e-03, -1.0283e-04,  1.6157e-03, -3.3184e-04, -7.3750e-04,\n",
       "                      -1.4907e-03, -3.5607e-04,  1.1018e-03, -2.0806e-03, -1.3600e-03,\n",
       "                      -4.0190e-04, -1.2666e-03, -1.1133e-03, -1.8364e-04, -1.4102e-03,\n",
       "                       1.1142e-03,  8.3249e-04,  2.2077e-03, -2.7928e-03, -6.2596e-04,\n",
       "                       9.9763e-04,  8.7195e-04,  1.1257e-03, -3.4146e-04, -2.5388e-03,\n",
       "                       1.2509e-03,  1.7470e-03,  1.5162e-03,  5.1715e-04, -2.2342e-03,\n",
       "                       1.4096e-03, -2.5287e-03,  4.1299e-04, -6.3473e-04,  1.1911e-03,\n",
       "                      -8.3568e-04, -1.0546e-03, -1.5558e-03,  2.4967e-03,  2.3592e-03,\n",
       "                       8.8475e-04, -5.7517e-04, -1.0017e-03,  6.8013e-04, -7.1211e-04,\n",
       "                      -1.5269e-03, -9.9965e-05,  3.3933e-04,  4.8626e-05, -7.6201e-04,\n",
       "                      -9.2229e-04,  9.6043e-04,  1.8072e-03, -4.1179e-04,  1.6724e-03,\n",
       "                      -2.5165e-04,  2.9559e-04, -1.8275e-03, -1.0816e-03, -6.5803e-04,\n",
       "                       1.8124e-03, -1.8451e-03, -1.7052e-03, -1.5881e-03,  1.5096e-03,\n",
       "                       1.2480e-03, -1.7860e-03,  6.6785e-05,  1.9653e-03,  1.5371e-03,\n",
       "                      -3.6293e-05, -1.1971e-03,  1.1399e-03,  4.1835e-04, -9.1725e-04,\n",
       "                       4.3804e-04, -8.5000e-04,  1.4722e-03, -1.5494e-03, -1.9148e-03,\n",
       "                      -3.3000e-03,  1.0014e-03, -6.9906e-04, -1.7789e-03,  2.4449e-04,\n",
       "                       4.0783e-04, -1.6287e-03,  5.2006e-04, -1.2134e-03, -1.2882e-03,\n",
       "                      -2.1080e-03, -2.4605e-03, -3.9552e-04,  9.4411e-04,  3.3956e-05,\n",
       "                       2.3454e-03,  9.8902e-04,  1.6146e-03,  1.1261e-03, -7.9547e-04,\n",
       "                      -3.3803e-03,  1.5337e-03,  1.6727e-03, -2.2044e-03, -6.2191e-04,\n",
       "                       6.9240e-04, -8.4413e-04,  1.4755e-03, -9.5377e-04, -1.3331e-03,\n",
       "                       7.6600e-04, -1.5233e-03, -1.6945e-03, -1.3685e-03,  4.7884e-04,\n",
       "                      -9.7400e-05,  2.2476e-03, -7.0158e-04, -2.0688e-03, -7.8060e-04,\n",
       "                      -4.3569e-04, -1.2377e-03,  2.0474e-03, -6.1916e-04,  1.0016e-03,\n",
       "                       4.1921e-04,  2.3206e-03,  1.0956e-03,  2.7685e-03,  5.3420e-04,\n",
       "                       1.7941e-03, -1.6830e-03,  3.6992e-07,  1.1057e-03,  5.1384e-04,\n",
       "                       1.5358e-03,  4.2724e-03,  4.2490e-03,  1.6352e-03,  3.4686e-04,\n",
       "                       1.0181e-04,  1.5139e-03, -7.8131e-04,  7.0144e-04, -6.6710e-04,\n",
       "                      -1.8439e-03, -8.6632e-04,  2.6699e-03,  2.0709e-04, -1.1344e-03,\n",
       "                       9.6107e-04,  2.2891e-03, -8.2469e-04, -6.1785e-04, -1.5253e-03,\n",
       "                       1.2377e-03, -2.8491e-04, -1.0748e-03,  1.1440e-03,  2.3468e-03,\n",
       "                       1.2827e-03, -1.9217e-03,  5.0294e-04,  1.0987e-03, -4.2075e-04,\n",
       "                      -2.4817e-03,  4.9626e-04,  3.1243e-04, -2.0981e-03,  4.1469e-03,\n",
       "                       3.4515e-04,  1.2494e-03, -4.5234e-05,  1.1143e-03, -2.4325e-03,\n",
       "                       3.2208e-04,  1.2092e-03, -2.1297e-04,  1.6143e-03, -5.9992e-04,\n",
       "                      -1.2697e-03,  1.8111e-03,  2.1776e-03,  2.7587e-03,  2.8248e-03,\n",
       "                      -3.6326e-05, -1.3663e-03,  1.2702e-03,  2.1475e-03,  2.5445e-04,\n",
       "                       3.1575e-04, -1.3504e-03, -2.8388e-04,  1.5863e-03,  3.8455e-04,\n",
       "                       1.8569e-03, -9.3392e-06, -2.6104e-04,  1.3834e-03, -2.3648e-03,\n",
       "                      -1.7941e-03,  4.4984e-04, -2.2360e-03, -1.1732e-03,  6.1190e-04,\n",
       "                      -7.2400e-04, -1.9572e-06,  1.3713e-03,  6.0965e-04,  1.2420e-03,\n",
       "                       1.2708e-03, -3.7852e-04, -1.2063e-03,  8.9643e-04,  1.5076e-03,\n",
       "                      -1.6426e-03,  1.0595e-03,  8.2437e-04, -3.9876e-04,  1.1461e-03,\n",
       "                      -3.7732e-04,  2.2989e-03, -2.6702e-03,  1.1521e-03,  9.1037e-04,\n",
       "                      -1.9614e-03,  1.8235e-03,  2.1833e-03,  1.5195e-03, -2.1088e-03,\n",
       "                      -3.0986e-04,  5.4452e-04,  3.5343e-04,  8.2279e-05, -1.1295e-03,\n",
       "                       1.0843e-03, -1.7352e-03,  1.4773e-04, -1.1150e-03, -7.5941e-04,\n",
       "                      -9.8894e-04,  3.5783e-04, -1.5040e-03, -1.3590e-04, -3.1770e-03,\n",
       "                       1.1950e-03, -2.6722e-04,  8.0561e-04, -2.5052e-03, -1.1035e-03,\n",
       "                       2.8507e-03,  8.8636e-04,  9.1249e-04, -2.6594e-04, -1.4645e-03,\n",
       "                       8.8857e-04, -7.4439e-04, -1.5956e-03,  1.0256e-03,  1.3916e-03,\n",
       "                      -1.2673e-03,  2.7674e-03, -7.7240e-04,  2.5192e-03, -8.4098e-04,\n",
       "                      -2.5344e-04,  2.5616e-03, -1.4316e-03, -1.6435e-03, -2.1624e-03,\n",
       "                       1.4756e-03,  2.9123e-04,  1.2768e-03,  1.5978e-03,  1.9917e-04,\n",
       "                      -2.0121e-04,  1.7745e-03, -1.3549e-03,  3.6457e-03,  8.9301e-04,\n",
       "                       1.0694e-03, -2.0754e-03,  1.2116e-03, -6.2055e-04, -1.1973e-03,\n",
       "                      -2.1928e-03,  1.1208e-03,  1.2803e-03, -3.1791e-03, -1.2576e-04,\n",
       "                      -2.0912e-03,  2.7416e-04,  3.7214e-04, -1.8860e-03, -2.2156e-03,\n",
       "                       1.2259e-03,  1.7145e-03, -8.4629e-04, -1.8426e-03,  7.2493e-04,\n",
       "                       1.7442e-03,  2.6773e-03,  1.0202e-03,  6.4215e-04,  7.0856e-04,\n",
       "                       2.1979e-03, -6.7861e-04, -4.6834e-04,  6.3390e-04, -2.5541e-03,\n",
       "                       1.1176e-03,  2.3606e-03,  8.7618e-04, -2.6880e-03, -2.1616e-03,\n",
       "                      -2.2808e-04,  2.0325e-03,  7.0560e-04, -6.3096e-04,  8.2355e-04,\n",
       "                      -2.9774e-04, -5.1276e-04,  7.9383e-04,  7.4674e-04,  2.3244e-03,\n",
       "                      -2.6050e-03,  2.4449e-03,  6.2950e-04, -7.8576e-04, -2.5035e-03,\n",
       "                       5.9613e-06, -1.9561e-03, -8.1080e-05, -3.9712e-04,  1.2839e-03,\n",
       "                      -1.5041e-03, -3.0656e-03, -1.3597e-03, -9.8619e-04, -5.4261e-04,\n",
       "                       4.5023e-04,  2.4324e-03,  2.2158e-03,  7.0942e-04, -2.1619e-03,\n",
       "                       5.6739e-04, -7.0340e-04,  2.2978e-03,  2.7275e-03,  1.2526e-04,\n",
       "                       2.9509e-03, -3.5754e-04, -3.2237e-03,  1.6751e-03, -1.1765e-03,\n",
       "                      -7.7585e-04,  1.3595e-03,  8.7937e-04, -4.8603e-04,  1.9753e-04,\n",
       "                       2.3211e-03, -6.0359e-04,  1.6521e-03, -1.5586e-03, -1.8082e-03,\n",
       "                       1.4035e-03,  1.2947e-03, -1.7242e-03, -2.5348e-04, -1.5229e-03,\n",
       "                      -7.1510e-04,  1.6274e-03, -3.0615e-04,  2.5902e-03,  1.6434e-03,\n",
       "                      -7.2422e-04,  4.1567e-04, -7.9479e-04,  4.6990e-04,  3.0072e-03,\n",
       "                       1.9428e-03, -5.7190e-04,  1.8320e-03,  2.3552e-04,  3.6175e-04,\n",
       "                      -4.9311e-04, -1.3195e-03, -1.1063e-03, -5.0903e-04, -5.2086e-04,\n",
       "                       1.4099e-03,  9.2701e-04,  1.3788e-03,  1.1992e-03,  1.1857e-03,\n",
       "                      -3.0324e-05, -2.1611e-03,  5.7260e-04,  2.4895e-03,  9.5739e-04,\n",
       "                      -2.1018e-03,  1.6947e-03,  1.3340e-03, -9.1813e-05,  2.4051e-03,\n",
       "                      -1.2018e-03,  6.9215e-04,  1.1251e-03, -1.6917e-03, -1.6620e-03,\n",
       "                       8.8575e-05, -7.9218e-04, -5.5959e-04,  1.6613e-03,  6.7280e-05,\n",
       "                      -1.7697e-03,  5.0618e-04,  1.2825e-03, -1.4082e-03, -2.8925e-03,\n",
       "                      -1.0693e-03,  1.6568e-03, -2.4279e-03,  2.4788e-03, -6.4841e-04,\n",
       "                      -1.1618e-03,  3.9957e-04,  3.1935e-05, -1.7488e-04, -1.6788e-03,\n",
       "                      -2.6126e-03, -8.7234e-04,  1.1066e-04, -2.9970e-04,  8.9691e-04,\n",
       "                       4.1254e-04,  9.3746e-04, -1.2993e-03,  3.4044e-03,  1.3471e-03,\n",
       "                      -1.6571e-03,  1.3493e-03,  2.9637e-03,  2.1467e-03,  4.7166e-04,\n",
       "                       1.0036e-03, -1.4513e-03,  1.3870e-03,  2.6421e-03,  1.1203e-03,\n",
       "                      -8.0526e-05, -3.5573e-05,  1.4064e-03,  2.7455e-04, -2.5209e-03,\n",
       "                      -1.8062e-03,  4.0525e-04, -2.7672e-03, -1.0573e-03, -1.8248e-03,\n",
       "                       1.7071e-04, -2.6198e-03, -1.0166e-03, -9.2369e-04, -1.1520e-03,\n",
       "                      -1.9335e-03, -1.5882e-03,  9.9207e-04,  4.9213e-04, -1.2790e-03,\n",
       "                       3.7445e-04, -1.0675e-03, -1.7800e-03,  2.1808e-03,  7.3595e-04,\n",
       "                       1.0796e-03, -9.7810e-05,  1.5770e-03, -2.3834e-03, -5.8275e-04,\n",
       "                       2.0998e-04,  1.7639e-03, -9.9857e-05,  2.2913e-04, -3.2932e-03,\n",
       "                      -7.4216e-04,  1.3488e-04, -8.1569e-04,  1.7931e-03, -2.5495e-03,\n",
       "                       1.4924e-03, -7.5945e-04, -1.1888e-03,  1.6798e-03, -5.1629e-04,\n",
       "                      -1.7270e-03, -2.3394e-04])),\n",
       "             ('transformer.resblocks.6.ln_1.weight',\n",
       "              tensor([0.9992, 1.0012, 0.9993, 0.9995, 1.0012, 1.0018, 0.9959, 1.0005, 0.9985,\n",
       "                      1.0020, 0.9988, 0.9999, 0.9984, 1.0036, 0.9970, 1.0017, 0.9993, 1.0049,\n",
       "                      1.0018, 0.9973, 1.0013, 0.9992, 0.9996, 0.9973, 0.9994, 1.0006, 1.0007,\n",
       "                      0.9991, 1.0029, 0.9997, 1.0015, 1.0016, 1.0002, 0.9996, 1.0012, 1.0003,\n",
       "                      0.9977, 0.9991, 0.9964, 1.0006, 1.0001, 0.9991, 0.9969, 0.9958, 0.9979,\n",
       "                      0.9992, 0.9993, 0.9996, 1.0014, 0.9970, 1.0024, 1.0025, 0.9988, 0.9981,\n",
       "                      0.9986, 1.0019, 1.0002, 1.0013, 0.9983, 0.9954, 0.9959, 0.9988, 0.9979,\n",
       "                      0.9971, 1.0022, 1.0028, 0.9986, 0.9981, 1.0016, 0.9989, 1.0025, 1.0026,\n",
       "                      0.9989, 0.9973, 0.9997, 0.9995, 0.9983, 1.0000, 0.9974, 1.0004, 0.9964,\n",
       "                      0.9969, 1.0024, 1.0032, 0.9997, 0.9993, 0.9993, 0.9997, 0.9970, 1.0014,\n",
       "                      0.9999, 0.9997, 1.0009, 1.0017, 0.9966, 0.9970, 1.0019, 1.0031, 0.9995,\n",
       "                      1.0021, 1.0020, 1.0015, 1.0020, 0.9986, 1.0010, 0.9984, 0.9964, 1.0014,\n",
       "                      0.9987, 0.9977, 0.9971, 1.0040, 0.9972, 1.0010, 0.9983, 0.9989, 1.0006,\n",
       "                      1.0011, 1.0045, 1.0001, 0.9991, 0.9996, 0.9976, 0.9973, 0.9988, 0.9991,\n",
       "                      0.9983, 1.0008, 0.9971, 1.0018, 1.0023, 0.9997, 0.9985, 0.9983, 0.9999,\n",
       "                      1.0001, 0.9965, 1.0013, 1.0019, 0.9986, 1.0024, 1.0004, 1.0029, 1.0005,\n",
       "                      0.9988, 1.0000, 0.9981, 0.9998, 1.0014, 0.9986, 0.9997, 0.9999, 0.9985,\n",
       "                      1.0012, 1.0009, 1.0006, 0.9998, 0.9997, 0.9974, 1.0008, 1.0001, 0.9989,\n",
       "                      1.0013, 1.0011, 1.0031, 1.0002, 1.0026, 0.9996, 0.9991, 0.9990, 0.9997,\n",
       "                      0.9986, 1.0013, 0.9974, 0.9992, 1.0027, 1.0033, 0.9979, 0.9987, 0.9988,\n",
       "                      1.0011, 1.0011, 0.9999, 1.0010, 1.0013, 1.0025, 0.9971, 0.9978, 1.0010,\n",
       "                      1.0009, 0.9976, 0.9997, 1.0012, 1.0009, 0.9992, 0.9983, 0.9994, 0.9984,\n",
       "                      0.9983, 0.9994, 1.0001, 0.9995, 0.9991, 1.0017, 1.0042, 1.0025, 1.0010,\n",
       "                      0.9986, 1.0011, 1.0003, 0.9997, 1.0012, 1.0006, 1.0000, 0.9990, 0.9976,\n",
       "                      0.9979, 0.9979, 0.9999, 1.0001, 0.9979, 0.9990, 1.0009, 0.9985, 0.9993,\n",
       "                      1.0044, 1.0008, 0.9982, 1.0007, 0.9962, 0.9996, 0.9983, 0.9958, 1.0016,\n",
       "                      0.9985, 0.9976, 1.0003, 0.9989, 0.9995, 1.0005, 0.9977, 0.9985, 0.9973,\n",
       "                      0.9988, 1.0023, 1.0004, 1.0034, 0.9952, 1.0009, 0.9995, 0.9997, 1.0005,\n",
       "                      0.9971, 1.0009, 0.9987, 1.0008, 1.0012, 1.0004, 1.0032, 1.0008, 1.0028,\n",
       "                      1.0019, 1.0039, 1.0027, 0.9987, 0.9991, 0.9995, 0.9976, 0.9986, 0.9981,\n",
       "                      0.9974, 0.9990, 1.0021, 0.9996, 0.9973, 1.0014, 1.0001, 1.0035, 0.9991,\n",
       "                      0.9990, 1.0013, 1.0020, 0.9984, 0.9988, 0.9997, 0.9996, 1.0032, 0.9973,\n",
       "                      1.0004, 0.9996, 0.9977, 0.9998, 0.9969, 0.9967, 1.0014, 1.0001, 0.9967,\n",
       "                      0.9984, 0.9966, 0.9985, 0.9992, 0.9978, 1.0014, 1.0006, 0.9997, 1.0032,\n",
       "                      0.9987, 0.9967, 0.9982, 0.9993, 0.9998, 1.0023, 1.0002, 1.0009, 0.9994,\n",
       "                      0.9991, 0.9988, 1.0015, 1.0027, 1.0036, 0.9966, 1.0011, 0.9996, 1.0000,\n",
       "                      1.0058, 1.0047, 0.9998, 0.9962, 0.9999, 0.9979, 0.9993, 0.9976, 0.9994,\n",
       "                      1.0037, 1.0015, 1.0007, 0.9996, 0.9978, 1.0012, 0.9975, 1.0009, 1.0019,\n",
       "                      0.9977, 1.0027, 1.0007, 0.9993, 1.0007, 0.9984, 0.9988, 1.0042, 1.0019,\n",
       "                      1.0012, 0.9995, 1.0006, 1.0016, 0.9995, 0.9972, 1.0015, 1.0027, 1.0009,\n",
       "                      0.9974, 0.9994, 0.9991, 0.9995, 1.0005, 0.9964, 1.0009, 1.0001, 0.9995,\n",
       "                      1.0000, 0.9987, 0.9980, 0.9994, 0.9997, 1.0015, 0.9982, 1.0012, 0.9983,\n",
       "                      0.9993, 0.9986, 0.9970, 1.0016, 1.0003, 1.0020, 0.9991, 0.9969, 0.9971,\n",
       "                      0.9968, 0.9974, 0.9984, 1.0008, 1.0029, 1.0021, 0.9997, 0.9971, 1.0002,\n",
       "                      1.0005, 0.9992, 1.0000, 1.0029, 0.9990, 0.9964, 0.9981, 0.9963, 0.9994,\n",
       "                      0.9999, 1.0005, 0.9976, 0.9963, 1.0012, 0.9998, 0.9962, 0.9981, 1.0003,\n",
       "                      0.9992, 0.9981, 1.0005, 0.9963, 0.9982, 0.9958, 1.0017, 0.9991, 0.9989,\n",
       "                      1.0010, 0.9968, 0.9988, 1.0013, 1.0008, 0.9996, 0.9971, 1.0009, 0.9966,\n",
       "                      0.9981, 1.0010, 0.9972, 0.9998, 0.9992, 0.9987, 0.9989, 1.0031, 0.9983,\n",
       "                      1.0003, 1.0018, 0.9982, 0.9975, 0.9993, 0.9951, 0.9992, 0.9993, 0.9986,\n",
       "                      0.9992, 1.0027, 1.0020, 0.9996, 0.9987, 0.9994, 1.0015, 0.9986, 1.0002,\n",
       "                      1.0011, 0.9999, 0.9996, 0.9974, 0.9998, 0.9982, 0.9988, 1.0017, 1.0002,\n",
       "                      1.0025, 0.9994, 0.9988, 0.9989, 1.0016, 0.9985, 1.0016, 1.0023, 1.0009,\n",
       "                      1.0002, 0.9985, 1.0023, 1.0004, 0.9962, 1.0000, 1.0031, 1.0004, 1.0001,\n",
       "                      1.0010, 1.0008, 0.9966, 0.9993, 1.0008, 0.9973, 0.9986, 1.0014, 1.0002,\n",
       "                      1.0002, 1.0002, 0.9993, 1.0009, 1.0004, 1.0009, 1.0026, 1.0004, 0.9979,\n",
       "                      0.9980, 0.9984, 0.9996, 0.9973, 0.9991, 1.0025, 0.9989, 0.9993])),\n",
       "             ('transformer.resblocks.6.ln_1.bias',\n",
       "              tensor([-9.1834e-04,  2.9750e-03,  7.5157e-04, -5.8513e-04, -8.9851e-04,\n",
       "                       1.1630e-03, -1.7378e-03,  2.2301e-03, -1.1212e-03,  9.6058e-04,\n",
       "                       1.4485e-03,  8.0267e-04, -1.0186e-03,  1.3167e-03,  1.9644e-04,\n",
       "                      -3.3268e-03,  3.6903e-04,  3.4783e-04,  1.6615e-03, -2.6836e-04,\n",
       "                       5.9366e-04,  1.6438e-03, -8.0952e-04,  2.3626e-03, -9.6004e-04,\n",
       "                       5.2220e-04, -2.0389e-03, -1.8103e-03, -9.7100e-04, -1.7852e-03,\n",
       "                      -8.1757e-04, -1.1387e-03,  1.9164e-03, -1.0926e-03, -1.4295e-04,\n",
       "                       4.3175e-03,  1.7911e-03,  1.3034e-03,  1.9381e-04, -5.0299e-04,\n",
       "                       2.7228e-03, -2.3466e-03, -1.1493e-03, -1.5817e-03,  3.8706e-03,\n",
       "                      -4.9253e-05,  1.1624e-03, -3.0318e-03, -5.8766e-04,  2.1963e-03,\n",
       "                      -2.3077e-03,  1.4175e-03,  1.8021e-03, -2.4631e-03, -7.3215e-04,\n",
       "                      -1.6463e-04,  7.6541e-04,  9.6274e-05,  1.2794e-03, -5.8386e-04,\n",
       "                      -8.2250e-05,  1.1036e-04, -5.6415e-04,  1.1933e-03,  8.7529e-04,\n",
       "                      -2.7142e-04,  9.9583e-05,  2.1742e-03,  1.2846e-03,  3.4213e-04,\n",
       "                      -4.5647e-04, -1.7739e-03,  1.7540e-04,  2.6178e-03, -9.8319e-04,\n",
       "                       2.1681e-03,  7.1944e-04,  2.2296e-03,  8.1193e-04,  2.4812e-03,\n",
       "                      -7.9215e-04,  3.0419e-03, -1.0793e-03,  2.1487e-03, -2.1629e-03,\n",
       "                       4.1834e-03, -1.2028e-03, -1.1217e-03,  3.2335e-03, -1.3973e-03,\n",
       "                       2.0665e-04,  1.2011e-03, -1.0126e-03,  1.5740e-03, -1.1502e-03,\n",
       "                       2.8461e-03, -1.1698e-03,  2.5014e-03, -7.2987e-05, -7.5490e-04,\n",
       "                      -5.2962e-04, -1.1621e-04, -2.2594e-04,  8.7174e-04, -1.0317e-03,\n",
       "                      -1.9902e-03,  3.3209e-03, -8.3813e-04, -2.2413e-04, -1.9922e-03,\n",
       "                      -3.3721e-03,  1.6272e-03, -1.2010e-05,  4.3876e-03, -1.9587e-03,\n",
       "                      -2.7444e-04,  1.9258e-03, -7.5815e-04,  1.4553e-03, -2.2876e-03,\n",
       "                       7.9966e-04, -1.3714e-04,  1.2717e-03, -4.7197e-04, -2.2787e-03,\n",
       "                       7.7849e-04,  1.5804e-03, -3.2508e-03,  3.4062e-03,  7.7736e-04,\n",
       "                      -1.6868e-05,  8.7500e-04,  2.0207e-03,  1.3817e-03,  1.4407e-03,\n",
       "                       1.0420e-03,  3.1084e-03, -1.0659e-03,  4.9591e-04, -9.6019e-04,\n",
       "                      -2.3024e-03, -3.3995e-03, -2.4620e-03, -6.8370e-04, -4.9444e-04,\n",
       "                      -7.2462e-04,  2.8953e-03,  2.4663e-03,  1.6465e-03,  1.0911e-03,\n",
       "                       1.0694e-03, -2.1900e-03, -2.2987e-04,  2.7596e-03, -1.5126e-03,\n",
       "                      -3.6557e-04,  1.6544e-03, -1.0693e-03, -1.3527e-03, -1.2126e-04,\n",
       "                      -2.2284e-03,  2.7267e-03, -7.4686e-04,  3.9206e-04, -3.1315e-03,\n",
       "                       5.3144e-04, -2.1839e-03,  7.4830e-06, -1.2349e-03, -1.2243e-03,\n",
       "                      -1.8501e-03,  1.9500e-04,  1.1981e-03, -4.0692e-03, -3.0834e-03,\n",
       "                      -3.8958e-03, -7.7885e-04,  1.2732e-03,  1.1359e-03, -4.8756e-04,\n",
       "                       2.8045e-03,  1.8208e-03,  2.7398e-03, -3.3105e-03,  2.7054e-03,\n",
       "                      -1.0353e-03,  1.2412e-03, -6.7453e-04, -3.4024e-03,  1.4341e-03,\n",
       "                      -6.1690e-05, -1.5011e-03,  8.3335e-04,  2.5208e-04, -1.0607e-03,\n",
       "                      -3.4106e-04,  6.9778e-04, -1.8089e-03, -3.3059e-03,  4.2472e-03,\n",
       "                       2.2764e-03, -1.3526e-03,  4.5047e-04, -2.2468e-04, -2.8492e-03,\n",
       "                       5.3791e-04, -2.9178e-04, -1.5694e-03, -1.8273e-03, -8.6027e-04,\n",
       "                      -5.0111e-04, -4.7532e-04, -8.8595e-04,  7.2974e-04,  9.4662e-04,\n",
       "                       2.2956e-03, -4.5828e-04, -2.0770e-03, -1.9897e-04,  3.2993e-04,\n",
       "                       1.3930e-03,  7.4331e-04,  3.3206e-03, -1.8020e-05,  7.7539e-04,\n",
       "                      -5.0405e-03, -4.0603e-04,  1.2214e-04, -1.2886e-03, -5.7746e-04,\n",
       "                       2.5227e-03,  3.8107e-04, -2.4737e-03, -6.0943e-04,  8.5666e-04,\n",
       "                      -2.7515e-05,  1.4337e-04, -1.2183e-03, -5.4309e-03, -2.7828e-04,\n",
       "                       9.7056e-04, -2.3041e-04, -1.7919e-03, -7.8228e-05,  1.0037e-03,\n",
       "                       5.8033e-04, -7.4463e-04, -2.2101e-03, -3.7571e-03, -2.2629e-03,\n",
       "                       1.3240e-03,  5.5391e-04,  1.8043e-03, -1.3657e-03, -1.4780e-03,\n",
       "                      -4.6202e-04, -1.9047e-03, -1.3646e-03, -1.4138e-03, -1.4058e-03,\n",
       "                       1.5434e-03, -1.1090e-04, -1.6529e-03,  9.6807e-04, -2.7960e-03,\n",
       "                       1.3497e-03, -4.6570e-04, -2.9576e-03,  2.1871e-03,  4.9132e-04,\n",
       "                       1.7476e-03, -7.9021e-04,  1.0892e-03, -4.7591e-03,  1.4681e-05,\n",
       "                       1.1338e-03, -8.4852e-04,  9.4428e-04, -5.6075e-04,  1.7979e-03,\n",
       "                       3.2163e-03,  2.6538e-03,  1.3144e-03, -1.7641e-03,  5.9799e-04,\n",
       "                      -4.6201e-04,  3.5505e-04, -4.3731e-03,  1.0988e-04, -2.8879e-04,\n",
       "                      -2.4762e-04,  4.8618e-04,  2.1291e-04,  1.0227e-03, -1.5248e-03,\n",
       "                       6.8373e-04,  3.2433e-03, -8.5302e-04, -4.2111e-03,  1.0401e-03,\n",
       "                       3.1636e-03,  1.4638e-03,  1.5723e-03,  3.2270e-04, -2.5349e-03,\n",
       "                      -1.0664e-04, -1.1492e-03,  6.0636e-04, -4.8275e-04, -1.3950e-03,\n",
       "                      -1.3911e-03,  4.7657e-04, -1.7877e-03, -1.1988e-03,  1.5518e-03,\n",
       "                      -9.3161e-04,  2.2834e-03, -1.2601e-04,  1.5020e-03,  1.7111e-03,\n",
       "                      -4.1301e-03,  5.0017e-04, -2.1663e-04, -4.9960e-04,  1.9492e-03,\n",
       "                      -4.9325e-04, -2.3065e-04, -5.6615e-05,  6.1820e-04, -2.0207e-04,\n",
       "                       1.6936e-03,  1.5032e-03,  1.7376e-04, -9.1807e-04,  4.2002e-04,\n",
       "                       4.0430e-04, -8.6955e-04,  1.6194e-03,  1.3036e-03, -1.2895e-03,\n",
       "                      -6.8566e-04,  1.0964e-03, -2.3626e-04,  1.4712e-04,  1.9336e-03,\n",
       "                       1.0939e-03,  2.5943e-03,  1.7047e-03,  1.6686e-03,  1.3320e-03,\n",
       "                      -3.5208e-04, -9.6333e-04,  4.0470e-04,  1.5572e-03,  2.8481e-03,\n",
       "                      -2.0192e-03,  6.4797e-04,  1.4698e-03, -1.8916e-03, -9.4559e-04,\n",
       "                      -1.6384e-03,  2.6670e-04, -3.2159e-04, -1.2036e-03,  1.5781e-03,\n",
       "                      -2.2429e-03,  3.7984e-04, -7.7406e-04, -1.8010e-03,  1.1837e-03,\n",
       "                       2.4921e-03, -5.7146e-04, -8.6878e-04, -2.1876e-04, -4.9498e-04,\n",
       "                       8.4881e-04, -1.1648e-03,  6.9044e-04, -4.3662e-04, -7.9484e-04,\n",
       "                       4.8442e-05, -1.2562e-03, -1.4452e-03, -3.2599e-03,  1.5282e-03,\n",
       "                      -7.2558e-04, -1.7098e-04, -1.0620e-05, -3.3725e-03, -1.9722e-03,\n",
       "                      -2.0756e-03, -2.7747e-03, -3.8991e-04,  2.8841e-04, -9.2613e-04,\n",
       "                      -1.0688e-04, -5.6416e-04, -1.9157e-04, -2.9359e-04,  1.2520e-03,\n",
       "                       3.9567e-04,  1.9538e-03, -5.4543e-04,  1.8926e-05,  7.3982e-04,\n",
       "                       9.5673e-04,  2.8351e-03, -1.0565e-03, -2.4171e-03, -8.7047e-05,\n",
       "                       2.7323e-03,  2.2701e-03, -2.0427e-04, -3.0421e-03, -1.6957e-03,\n",
       "                      -8.1214e-04,  1.2104e-03,  2.4319e-04,  2.0937e-03, -4.6292e-03,\n",
       "                       1.6336e-03, -2.0347e-03, -2.2728e-05,  2.1289e-03, -1.3057e-03,\n",
       "                       6.2925e-04, -1.3537e-03,  3.2763e-04,  4.6691e-03, -3.2698e-03,\n",
       "                       2.0916e-03,  1.6588e-04, -2.3994e-03, -5.4629e-04,  1.1889e-03,\n",
       "                       2.4402e-03, -1.9144e-03, -1.3669e-03,  1.7769e-03,  6.5185e-04,\n",
       "                       4.6812e-05, -1.1519e-04,  1.0839e-03, -1.5442e-03,  6.5436e-04,\n",
       "                       1.0085e-03, -2.8998e-03, -1.1022e-03,  5.6742e-06,  3.0075e-03,\n",
       "                      -7.0158e-04,  2.1150e-03, -2.0793e-03,  1.5596e-03, -1.8785e-04,\n",
       "                       3.9686e-04, -1.0454e-03,  1.0243e-03,  8.4500e-04,  1.8264e-04,\n",
       "                      -3.2820e-03,  6.6741e-04, -5.9016e-04,  3.3162e-04, -1.6949e-03,\n",
       "                       3.3331e-03,  5.0710e-04,  8.2652e-04, -1.6355e-03,  2.0257e-03,\n",
       "                       4.1406e-03, -1.8941e-03, -2.8552e-03,  8.8385e-04,  1.0682e-03,\n",
       "                       1.9991e-03, -1.0803e-03, -2.3341e-03,  4.0353e-04,  4.3801e-04,\n",
       "                      -1.1740e-03, -1.8485e-03, -1.5142e-04,  2.7664e-03, -1.5403e-03,\n",
       "                       4.1112e-04, -2.8094e-03,  1.0767e-03,  9.4863e-04, -2.9718e-03,\n",
       "                      -5.8336e-05,  1.3849e-03, -1.3674e-03,  3.7635e-04,  9.1004e-04,\n",
       "                       2.6050e-04,  1.7198e-03, -1.2656e-04, -1.6692e-03,  4.9713e-04,\n",
       "                       1.9798e-03, -3.1656e-03, -2.3291e-03, -1.1046e-03,  6.8452e-04,\n",
       "                      -6.8983e-04,  6.2997e-04,  5.8856e-04, -8.0944e-04,  4.1309e-04,\n",
       "                       1.2512e-03,  5.5337e-04])),\n",
       "             ('transformer.resblocks.6.mlp.c_fc.weight',\n",
       "              tensor([[-0.0209,  0.0428, -0.0362,  ..., -0.0637, -0.0086,  0.0682],\n",
       "                      [-0.0444, -0.0392, -0.0574,  ...,  0.0253, -0.0283, -0.0445],\n",
       "                      [ 0.0343, -0.0215,  0.0183,  ...,  0.0526, -0.0312, -0.0555],\n",
       "                      ...,\n",
       "                      [-0.0200, -0.0149,  0.0097,  ..., -0.0445,  0.0187, -0.0312],\n",
       "                      [-0.0227, -0.0143, -0.0202,  ...,  0.0028,  0.0286, -0.0689],\n",
       "                      [ 0.0333, -0.0238,  0.0928,  ...,  0.0198,  0.0633,  0.0048]])),\n",
       "             ('transformer.resblocks.6.mlp.c_fc.bias',\n",
       "              tensor([-0.0180,  0.0323, -0.0442,  ..., -0.0332,  0.0227,  0.0351])),\n",
       "             ('transformer.resblocks.6.mlp.c_proj.weight',\n",
       "              tensor([[-1.2150e-02,  3.8463e-03,  1.0110e-02,  ...,  2.4118e-03,\n",
       "                       -4.4035e-03,  9.2073e-05],\n",
       "                      [-8.7751e-04,  2.8965e-03,  1.2525e-02,  ...,  1.1389e-02,\n",
       "                       -4.9393e-03,  4.5326e-03],\n",
       "                      [ 2.7733e-03, -1.1926e-02,  4.1723e-03,  ..., -5.6820e-05,\n",
       "                        1.2400e-03, -9.7740e-03],\n",
       "                      ...,\n",
       "                      [ 1.5711e-02, -1.6587e-03,  1.2353e-02,  ...,  6.6583e-03,\n",
       "                       -6.8475e-03,  1.4588e-03],\n",
       "                      [ 6.4034e-03,  1.1464e-02, -4.2806e-03,  ..., -1.2713e-02,\n",
       "                       -1.5040e-02, -3.3174e-03],\n",
       "                      [ 6.6056e-03,  1.0737e-02,  4.8021e-03,  ..., -3.2035e-03,\n",
       "                       -1.2946e-02,  1.0876e-02]])),\n",
       "             ('transformer.resblocks.6.mlp.c_proj.bias',\n",
       "              tensor([ 2.1000e-02,  5.3720e-03,  8.4958e-03, -7.6604e-03, -1.7028e-02,\n",
       "                       3.0675e-03,  2.8685e-03, -4.9645e-03,  5.4799e-04,  5.7984e-03,\n",
       "                       4.2161e-03,  4.7624e-03,  1.8357e-02,  2.0797e-02,  6.7659e-03,\n",
       "                       4.0498e-04,  2.3292e-03,  9.5782e-03,  1.4918e-02, -1.3740e-03,\n",
       "                       5.3634e-03,  1.3829e-02, -2.7449e-03,  4.4380e-03,  6.7872e-03,\n",
       "                       7.9193e-03,  1.1273e-02,  2.7730e-03,  9.3195e-03,  2.7952e-03,\n",
       "                       1.9787e-02, -9.0506e-03, -1.2837e-02,  1.8257e-02,  2.0736e-02,\n",
       "                       2.2524e-02, -7.6897e-03,  1.7767e-02, -2.1931e-03,  2.8246e-03,\n",
       "                      -1.4306e-02, -5.1496e-03, -1.7610e-02, -2.0456e-03, -2.3767e-02,\n",
       "                      -5.9120e-03, -1.6063e-02,  4.7442e-03, -1.0011e-02, -1.0657e-02,\n",
       "                      -2.2068e-02,  1.6672e-02, -2.0558e-02,  2.0107e-02, -5.0413e-03,\n",
       "                      -1.9999e-02, -2.0535e-02,  2.2187e-03, -2.1493e-02, -1.3197e-02,\n",
       "                       2.9981e-03,  3.5250e-03, -1.9243e-02,  1.5122e-02,  8.6325e-03,\n",
       "                      -2.1484e-02,  2.0103e-02, -1.0536e-02, -1.8955e-02, -7.8324e-03,\n",
       "                       8.6724e-03,  5.4602e-03, -8.0769e-03, -6.4056e-03, -1.3578e-02,\n",
       "                       1.8647e-02, -1.7778e-02, -2.1706e-02, -1.8595e-02,  1.5447e-02,\n",
       "                       3.0526e-03, -1.5086e-02,  1.6206e-02,  3.5476e-03,  5.8623e-03,\n",
       "                       3.1561e-03,  1.8628e-02, -1.5097e-02, -1.4631e-02,  2.0416e-03,\n",
       "                      -4.9609e-03, -1.8614e-02,  9.3293e-03, -2.3125e-02,  1.7517e-02,\n",
       "                       2.7580e-03, -7.3760e-03, -6.2085e-03,  1.4772e-02, -9.0074e-03,\n",
       "                       9.1441e-03,  1.8564e-02, -1.2143e-02,  6.3608e-03,  3.5774e-03,\n",
       "                       1.4910e-02,  1.3514e-02,  1.5976e-02, -1.8968e-02,  1.2858e-02,\n",
       "                       4.2495e-03,  7.2548e-05, -2.8279e-03, -9.7841e-03,  1.2629e-03,\n",
       "                      -1.7093e-03,  6.1518e-03, -1.4706e-03, -8.5233e-03,  1.0594e-02,\n",
       "                      -2.0546e-02, -2.8787e-03, -9.0987e-04,  6.7448e-03,  1.5459e-02,\n",
       "                       8.3889e-03,  6.7042e-03, -1.2194e-02, -6.8665e-03, -9.9250e-03,\n",
       "                       1.5356e-02,  1.8615e-02, -7.9848e-03, -6.2040e-03,  1.2737e-02,\n",
       "                       7.7180e-03,  6.4849e-03, -1.9514e-02,  5.4245e-03,  2.0981e-02,\n",
       "                       3.3549e-03, -7.7429e-03, -6.6785e-03,  1.5662e-02,  1.3366e-02,\n",
       "                       2.1914e-02,  8.3002e-03, -1.7622e-02, -1.9491e-02,  5.1493e-04,\n",
       "                       1.5276e-02,  1.8783e-02,  1.2378e-02,  6.1712e-03, -1.0908e-02,\n",
       "                      -2.2328e-03, -3.9371e-03,  8.5986e-03, -2.0610e-02,  2.0663e-02,\n",
       "                      -7.4536e-03, -1.5612e-02,  7.0011e-03,  1.3268e-02,  1.7211e-02,\n",
       "                      -1.6708e-02, -1.3069e-02, -4.2138e-03,  1.6488e-02,  1.2200e-02,\n",
       "                      -1.1909e-03, -1.2509e-02, -1.0791e-02,  2.2439e-02, -1.6507e-02,\n",
       "                       2.2712e-02,  1.7073e-02, -1.8764e-02,  1.8493e-02,  6.3574e-03,\n",
       "                      -1.0541e-02,  7.9035e-03,  9.1070e-03,  1.3473e-02,  2.5125e-02,\n",
       "                      -1.6904e-02, -1.0065e-03,  1.6453e-02,  1.7018e-03, -1.4583e-02,\n",
       "                      -1.1832e-02,  6.0530e-03,  6.3917e-03,  1.3574e-02, -1.9310e-02,\n",
       "                      -7.3347e-03,  1.7001e-02,  2.2821e-02, -9.6911e-04,  2.2404e-02,\n",
       "                      -1.8478e-02,  1.0412e-02,  1.1093e-02, -2.2344e-03, -1.1121e-05,\n",
       "                       1.0210e-02, -1.4439e-02,  1.2825e-02, -9.3694e-03, -2.0669e-02,\n",
       "                       1.3731e-02, -4.5955e-03,  9.4696e-03, -1.3567e-02, -1.9262e-02,\n",
       "                      -1.2429e-02, -1.3275e-02, -1.3317e-03,  1.9615e-02, -1.5147e-02,\n",
       "                      -1.4167e-02,  9.9753e-03,  1.5754e-02, -1.9860e-02, -1.2246e-02,\n",
       "                       1.1620e-02,  9.8453e-03, -2.1339e-02, -2.8654e-04,  2.2522e-02,\n",
       "                      -1.6098e-02, -3.3525e-03,  1.5002e-02,  1.7516e-02,  1.3474e-02,\n",
       "                       1.2901e-02,  1.3709e-02, -1.3445e-03,  8.2213e-03,  1.4341e-02,\n",
       "                      -1.3435e-03, -1.2745e-03, -6.9170e-03,  2.1884e-02,  2.6808e-03,\n",
       "                      -1.3456e-02, -4.0318e-03,  1.9599e-03,  2.0231e-02,  2.0900e-02,\n",
       "                      -1.6798e-02,  1.9430e-02,  1.7895e-02,  1.0312e-02, -4.3690e-03,\n",
       "                       9.1486e-03,  7.5038e-03,  9.8644e-03,  1.9761e-02, -1.6411e-02,\n",
       "                       9.8824e-03, -1.0446e-02,  1.6041e-02, -1.9918e-02, -1.7185e-02,\n",
       "                       3.7878e-03,  2.0915e-02,  1.6967e-02,  6.0616e-03,  6.6424e-03,\n",
       "                      -5.6451e-03, -1.4942e-02,  9.8808e-04, -1.5767e-02, -9.2987e-03,\n",
       "                       1.3929e-02, -1.4707e-02,  1.3497e-02,  7.7256e-03,  4.9445e-03,\n",
       "                      -1.9534e-02, -1.0494e-02,  1.8523e-02,  1.1586e-02,  1.7546e-02,\n",
       "                      -6.7108e-03, -3.4798e-03, -7.9624e-03,  1.3111e-03, -1.6302e-03,\n",
       "                       1.8402e-02, -1.9793e-02,  1.6032e-02, -1.7008e-02,  6.4996e-03,\n",
       "                      -1.2728e-03, -6.5543e-03,  2.1030e-03, -1.9264e-02, -1.0045e-02,\n",
       "                      -7.9453e-03, -6.4090e-03, -5.8321e-03, -6.9987e-03, -2.1238e-02,\n",
       "                      -9.6199e-03, -1.8835e-03,  1.5784e-02,  1.3597e-02,  6.9875e-03,\n",
       "                       9.9547e-03,  2.3419e-02,  5.2241e-03,  7.9242e-03,  2.9796e-03,\n",
       "                      -2.6139e-03, -7.9200e-03,  1.8842e-02, -1.7043e-02,  1.4872e-02,\n",
       "                      -3.0974e-03,  1.7570e-02,  1.5187e-02, -1.5011e-02, -1.1897e-02,\n",
       "                      -5.2594e-03,  1.8056e-02, -1.8046e-03,  1.3807e-02,  2.0256e-03,\n",
       "                       1.5667e-02, -1.6443e-03,  4.6019e-04, -1.8690e-03, -9.9383e-03,\n",
       "                      -6.7331e-03, -9.2208e-03, -2.1011e-02,  5.1374e-03,  6.2569e-04,\n",
       "                      -1.8119e-02,  1.9689e-02, -7.0133e-03, -2.2009e-03, -6.1029e-04,\n",
       "                       1.9842e-02, -1.7509e-02, -3.1327e-03, -8.2407e-04, -3.1380e-03,\n",
       "                      -5.5466e-03, -1.0346e-02,  6.0444e-04,  1.8419e-02,  2.0867e-02,\n",
       "                      -4.6549e-03,  4.1401e-03,  2.3049e-02,  1.2078e-02,  1.4979e-02,\n",
       "                      -9.7566e-03, -1.3549e-02,  3.2893e-03,  1.9039e-02,  1.1516e-02,\n",
       "                       9.9839e-04,  4.5046e-03,  5.0443e-03,  1.7930e-02,  1.2763e-02,\n",
       "                      -5.2344e-03, -8.6712e-03,  1.0762e-02, -5.4484e-03,  1.7626e-02,\n",
       "                      -4.9081e-03,  1.5467e-02,  1.9977e-02,  1.3592e-02, -7.6322e-03,\n",
       "                      -4.9984e-04, -1.3835e-02, -1.9084e-04, -1.8490e-02, -2.2876e-02,\n",
       "                      -6.2792e-03,  2.1549e-02,  1.9102e-02,  2.9892e-03,  5.7421e-04,\n",
       "                      -1.9832e-02,  5.5475e-03, -1.0578e-02,  9.1698e-03,  7.1075e-03,\n",
       "                      -8.4840e-03, -1.2635e-02, -4.8388e-03, -6.6500e-03,  1.2046e-02,\n",
       "                       2.0066e-02, -1.6787e-02, -1.8837e-03,  1.3347e-02, -1.5424e-02,\n",
       "                      -1.9212e-02, -1.3309e-02, -1.9439e-02,  1.0749e-02,  7.8048e-03,\n",
       "                       7.9492e-03, -1.1156e-02, -7.3189e-03,  3.7825e-04, -1.4241e-02,\n",
       "                      -5.7460e-03, -1.8977e-02, -1.2281e-02,  1.4806e-02, -1.8565e-02,\n",
       "                      -1.0817e-04,  1.8202e-02,  1.0237e-02, -1.6914e-02, -9.1061e-03,\n",
       "                       1.6153e-02,  4.5330e-03, -1.9146e-02,  1.9782e-03,  1.6547e-02,\n",
       "                      -1.2339e-03,  2.2404e-02, -4.0294e-03, -7.8429e-03,  4.6733e-03,\n",
       "                       1.5322e-02, -1.2707e-03,  7.4841e-04,  1.2173e-03, -1.3571e-02,\n",
       "                      -1.6684e-03,  1.3912e-02,  1.7704e-02, -1.4535e-02,  1.5463e-02,\n",
       "                       4.6260e-03, -1.2654e-02,  6.1942e-03,  2.2164e-02,  2.0125e-02,\n",
       "                      -4.4063e-03, -1.8801e-02,  1.7249e-02,  2.3292e-02,  1.8991e-02,\n",
       "                       2.2216e-03,  6.3398e-03,  8.0816e-03,  1.3227e-03,  8.1049e-03,\n",
       "                      -3.2768e-03,  1.9291e-02,  1.2137e-02, -9.8602e-03,  2.2856e-02,\n",
       "                      -7.6263e-03, -1.9462e-02, -6.1907e-03,  1.6915e-02, -2.1915e-02,\n",
       "                      -1.0217e-02, -1.9128e-02, -1.3925e-02, -1.7647e-02, -9.0676e-03,\n",
       "                      -6.8943e-03,  1.2946e-02, -2.0036e-02,  1.4598e-03,  1.9919e-02,\n",
       "                       4.4314e-03,  1.4285e-02, -6.6416e-04,  1.1391e-02,  1.3711e-02,\n",
       "                       4.6837e-03,  1.2315e-02, -1.1289e-03,  2.3167e-02, -6.1943e-03,\n",
       "                       5.9997e-03, -3.3956e-04,  1.1636e-02,  5.6993e-03, -2.2041e-02,\n",
       "                      -8.0908e-03, -3.9949e-03,  1.9827e-03, -2.1122e-02,  1.7425e-03,\n",
       "                       1.5285e-03,  6.7130e-03,  1.7415e-04,  3.9827e-03, -6.5802e-03,\n",
       "                      -1.8083e-02, -2.8957e-03,  5.5248e-03, -9.8323e-03,  1.1170e-02,\n",
       "                       9.9087e-03,  8.2170e-03])),\n",
       "             ('transformer.resblocks.6.ln_2.weight',\n",
       "              tensor([1.0023, 0.9978, 0.9982, 1.0019, 1.0001, 1.0005, 1.0010, 1.0011, 0.9998,\n",
       "                      1.0010, 1.0003, 1.0009, 0.9981, 1.0031, 0.9956, 1.0011, 1.0028, 1.0037,\n",
       "                      1.0022, 1.0008, 1.0030, 1.0009, 1.0042, 1.0005, 0.9998, 1.0060, 1.0008,\n",
       "                      1.0008, 0.9993, 1.0019, 1.0057, 1.0032, 0.9990, 1.0012, 1.0031, 0.9985,\n",
       "                      1.0023, 1.0023, 1.0004, 0.9995, 1.0017, 1.0013, 0.9999, 1.0027, 0.9980,\n",
       "                      0.9993, 1.0019, 1.0030, 1.0005, 1.0033, 1.0018, 1.0046, 1.0015, 1.0004,\n",
       "                      1.0030, 1.0011, 1.0019, 1.0040, 0.9998, 1.0022, 0.9978, 0.9992, 0.9998,\n",
       "                      1.0030, 1.0012, 1.0008, 1.0012, 1.0021, 1.0002, 1.0038, 1.0016, 0.9999,\n",
       "                      1.0012, 1.0048, 0.9990, 1.0029, 0.9928, 1.0011, 1.0061, 0.9984, 0.9968,\n",
       "                      1.0023, 1.0036, 0.9988, 1.0001, 1.0037, 1.0034, 1.0018, 1.0024, 1.0004,\n",
       "                      1.0038, 1.0045, 0.9982, 1.0011, 1.0042, 0.9966, 0.9993, 0.9979, 1.0029,\n",
       "                      0.9997, 1.0041, 1.0021, 1.0027, 0.9998, 1.0043, 1.0016, 1.0011, 1.0014,\n",
       "                      1.0016, 1.0004, 0.9991, 1.0028, 0.9985, 0.9978, 1.0029, 0.9992, 1.0026,\n",
       "                      1.0022, 0.9997, 1.0006, 0.9999, 1.0028, 0.9989, 1.0037, 1.0023, 1.0002,\n",
       "                      1.0033, 0.9996, 1.0024, 0.9997, 1.0029, 0.9979, 1.0043, 0.9996, 0.9999,\n",
       "                      1.0054, 1.0028, 1.0007, 1.0036, 0.9988, 0.9996, 1.0035, 1.0032, 0.9966,\n",
       "                      1.0002, 1.0027, 1.0009, 1.0033, 1.0016, 1.0010, 0.9992, 0.9992, 0.9972,\n",
       "                      1.0029, 1.0031, 1.0013, 0.9984, 1.0015, 1.0028, 1.0072, 1.0020, 0.9980,\n",
       "                      0.9987, 1.0037, 1.0010, 0.9991, 1.0011, 0.9981, 1.0032, 1.0000, 1.0012,\n",
       "                      1.0023, 1.0059, 1.0025, 1.0017, 1.0016, 1.0000, 0.9998, 1.0024, 1.0036,\n",
       "                      0.9991, 1.0002, 1.0010, 0.9978, 1.0002, 1.0028, 1.0040, 1.0009, 1.0028,\n",
       "                      1.0004, 1.0000, 1.0030, 1.0013, 1.0009, 1.0015, 1.0046, 1.0057, 1.0011,\n",
       "                      1.0003, 1.0006, 1.0041, 1.0039, 1.0007, 1.0020, 0.9994, 1.0016, 1.0046,\n",
       "                      1.0067, 1.0047, 1.0031, 1.0041, 1.0028, 1.0003, 1.0023, 1.0026, 1.0025,\n",
       "                      0.9985, 0.9993, 1.0035, 1.0008, 1.0011, 1.0012, 1.0015, 1.0052, 1.0016,\n",
       "                      1.0030, 1.0009, 0.9995, 1.0005, 0.9992, 1.0006, 1.0022, 1.0003, 1.0013,\n",
       "                      1.0025, 1.0001, 0.9985, 0.9998, 0.9994, 1.0014, 1.0028, 1.0043, 1.0005,\n",
       "                      1.0035, 1.0004, 1.0015, 1.0024, 1.0023, 1.0011, 1.0031, 1.0020, 1.0018,\n",
       "                      0.9982, 1.0000, 0.9996, 1.0022, 1.0020, 0.9957, 0.9989, 0.9987, 1.0021,\n",
       "                      1.0022, 1.0037, 0.9977, 1.0019, 1.0009, 1.0002, 1.0027, 0.9982, 1.0015,\n",
       "                      0.9994, 0.9993, 1.0003, 1.0011, 0.9997, 0.9980, 0.9995, 1.0044, 0.9965,\n",
       "                      0.9991, 1.0000, 0.9993, 1.0020, 1.0035, 1.0017, 1.0005, 0.9998, 1.0021,\n",
       "                      1.0030, 1.0023, 1.0005, 1.0034, 1.0024, 0.9999, 1.0053, 1.0017, 1.0006,\n",
       "                      1.0008, 1.0018, 1.0010, 0.9978, 1.0018, 1.0020, 0.9984, 1.0020, 1.0010,\n",
       "                      1.0043, 1.0025, 0.9992, 0.9979, 1.0021, 1.0049, 1.0029, 1.0024, 1.0005,\n",
       "                      1.0009, 0.9994, 0.9970, 1.0036, 1.0026, 1.0023, 1.0029, 1.0006, 1.0036,\n",
       "                      1.0035, 0.9998, 1.0023, 1.0028, 1.0010, 1.0035, 0.9999, 1.0020, 1.0037,\n",
       "                      1.0047, 0.9991, 1.0016, 1.0015, 1.0021, 1.0030, 1.0040, 0.9989, 1.0038,\n",
       "                      1.0016, 1.0033, 1.0025, 1.0050, 1.0009, 1.0002, 0.9985, 1.0027, 1.0020,\n",
       "                      0.9987, 1.0021, 1.0046, 1.0011, 1.0044, 1.0002, 1.0022, 1.0004, 1.0000,\n",
       "                      1.0043, 1.0034, 1.0002, 0.9988, 1.0017, 0.9986, 1.0052, 1.0017, 1.0028,\n",
       "                      0.9997, 1.0034, 0.9999, 0.9992, 0.9992, 1.0021, 1.0010, 1.0002, 1.0032,\n",
       "                      0.9992, 1.0015, 1.0017, 0.9992, 1.0016, 0.9968, 1.0030, 1.0025, 0.9979,\n",
       "                      0.9998, 1.0000, 1.0012, 1.0022, 1.0026, 1.0011, 0.9997, 1.0019, 1.0004,\n",
       "                      1.0019, 0.9986, 1.0034, 1.0018, 1.0047, 0.9999, 1.0031, 1.0038, 1.0018,\n",
       "                      1.0005, 1.0015, 0.9995, 0.9987, 1.0025, 1.0022, 1.0005, 1.0036, 1.0018,\n",
       "                      1.0039, 1.0005, 1.0027, 1.0018, 1.0003, 0.9987, 1.0014, 1.0019, 1.0025,\n",
       "                      1.0010, 1.0006, 1.0020, 1.0010, 1.0020, 1.0004, 1.0018, 1.0008, 1.0006,\n",
       "                      0.9997, 1.0050, 1.0017, 1.0038, 0.9997, 1.0013, 1.0020, 1.0021, 1.0023,\n",
       "                      1.0003, 1.0018, 1.0031, 1.0037, 0.9993, 1.0001, 1.0019, 1.0003, 1.0023,\n",
       "                      1.0022, 1.0019, 1.0025, 0.9966, 1.0023, 1.0008, 1.0009, 1.0033, 0.9989,\n",
       "                      0.9973, 1.0023, 1.0009, 1.0010, 0.9997, 1.0032, 1.0005, 1.0022, 1.0016,\n",
       "                      1.0040, 0.9993, 1.0024, 1.0026, 0.9992, 1.0012, 1.0018, 1.0007, 1.0001,\n",
       "                      1.0011, 0.9998, 0.9986, 1.0018, 1.0038, 1.0043, 1.0037, 1.0005, 1.0005,\n",
       "                      1.0003, 1.0016, 1.0021, 1.0012, 0.9987, 1.0026, 1.0013, 1.0028, 1.0043,\n",
       "                      1.0010, 1.0007, 1.0018, 1.0018, 0.9998, 1.0026, 1.0020, 1.0004, 1.0026,\n",
       "                      1.0000, 1.0001, 0.9981, 0.9987, 1.0019, 1.0041, 0.9997, 1.0015])),\n",
       "             ('transformer.resblocks.6.ln_2.bias',\n",
       "              tensor([ 1.3904e-03,  5.0823e-03, -7.5259e-04, -5.0005e-04,  7.9263e-04,\n",
       "                       7.1926e-04,  4.1060e-05,  7.2485e-04, -6.8250e-04, -1.8095e-03,\n",
       "                       2.6283e-03,  2.7163e-04, -1.2882e-03, -1.0774e-03,  5.0673e-03,\n",
       "                      -2.0086e-03, -2.7258e-04, -2.0370e-03, -1.5661e-03, -1.1314e-03,\n",
       "                      -3.1780e-03,  1.5042e-03,  1.3795e-03,  6.6221e-04,  9.4652e-04,\n",
       "                       2.2777e-03,  3.4187e-04,  6.8277e-04, -1.0296e-03,  7.6257e-04,\n",
       "                       3.2657e-03,  2.1522e-03, -1.4613e-03,  1.0049e-03,  8.0189e-04,\n",
       "                      -2.6602e-03, -1.9458e-04,  2.0922e-03, -3.9050e-03, -2.1574e-03,\n",
       "                      -4.2460e-03, -8.5562e-04, -1.9989e-03, -3.2052e-03,  5.1260e-03,\n",
       "                      -1.6086e-03,  3.9294e-03,  1.4618e-03,  1.7777e-03,  2.8305e-03,\n",
       "                       1.5886e-03, -1.8835e-03, -5.8716e-04,  6.6717e-04,  1.8245e-04,\n",
       "                      -2.6453e-03,  1.5306e-03, -3.1146e-03,  1.8969e-03, -3.3724e-04,\n",
       "                       1.3051e-03, -1.1926e-04, -5.1736e-04, -2.5362e-03, -2.0775e-03,\n",
       "                      -1.7084e-03, -1.8651e-03, -3.3501e-04, -5.4460e-04,  7.6243e-04,\n",
       "                       2.5684e-03,  7.6752e-04, -8.5973e-04, -3.5081e-03,  3.3404e-03,\n",
       "                      -2.3982e-03,  3.9279e-03,  3.0111e-04,  1.1058e-03, -1.2555e-03,\n",
       "                      -1.4649e-03, -6.4062e-04,  7.0016e-04, -2.2877e-03, -2.2621e-03,\n",
       "                      -9.6563e-04, -1.7807e-03, -1.8312e-03,  5.1713e-05,  4.3143e-03,\n",
       "                       7.8990e-04, -2.4246e-05,  4.8844e-04,  7.9899e-04, -7.4040e-04,\n",
       "                       3.1956e-03,  3.0307e-03,  2.4763e-03,  8.8931e-04, -2.7346e-03,\n",
       "                      -1.6739e-03, -1.5973e-03,  1.2027e-03, -3.8521e-04, -1.5159e-03,\n",
       "                       3.1525e-04, -4.7294e-05,  4.1643e-03,  7.4542e-04,  5.8934e-04,\n",
       "                      -5.8517e-03, -6.3811e-04, -1.7782e-03, -2.1970e-03,  2.2326e-03,\n",
       "                       2.6429e-03, -1.0546e-03,  3.6197e-04,  8.6329e-04,  7.8590e-04,\n",
       "                      -8.8247e-04,  2.8805e-03, -2.2718e-03, -4.1804e-03,  1.1139e-03,\n",
       "                      -3.0741e-05,  7.1542e-04, -1.4478e-03, -1.3500e-03,  2.3401e-04,\n",
       "                      -2.6888e-03, -4.7863e-03, -1.0174e-03,  3.5101e-03,  1.6133e-03,\n",
       "                       1.5411e-03,  3.0949e-03,  1.7343e-03, -2.2790e-03, -2.8349e-03,\n",
       "                      -1.1687e-03, -1.7025e-03, -7.8457e-04, -3.2401e-03, -3.4952e-03,\n",
       "                      -3.5640e-04,  2.4118e-03, -2.4386e-03, -1.8838e-04,  1.3612e-03,\n",
       "                       9.6691e-04, -1.7687e-03, -4.8867e-03,  4.5187e-04,  1.1265e-03,\n",
       "                       2.6118e-04,  2.9557e-03,  1.4093e-04, -8.1737e-04, -2.7522e-03,\n",
       "                       1.6993e-04,  1.0081e-03, -1.7027e-03,  8.1129e-04,  1.4617e-03,\n",
       "                      -1.6249e-03,  1.6795e-03, -1.2846e-04,  2.3157e-04,  1.7479e-03,\n",
       "                      -1.9305e-03, -2.0815e-03,  1.0624e-03, -1.9132e-04,  9.9153e-04,\n",
       "                       9.9969e-04, -2.6996e-03, -1.1801e-03,  4.8870e-05,  2.5348e-03,\n",
       "                      -2.6206e-04,  1.9571e-03, -2.7182e-03,  4.9548e-03, -3.0856e-03,\n",
       "                       1.5565e-03,  2.5465e-03,  6.6554e-05,  1.0095e-03,  1.6737e-03,\n",
       "                      -4.3657e-03,  1.7456e-03,  1.9461e-04, -2.8629e-03,  6.0186e-04,\n",
       "                      -3.4011e-03,  1.6255e-03, -7.1203e-04,  4.2104e-04,  2.8145e-03,\n",
       "                       2.2965e-03, -2.7832e-03, -4.7263e-03, -2.8006e-03, -6.6933e-04,\n",
       "                      -4.8703e-04, -2.1431e-03, -2.7462e-03,  1.2277e-03,  2.1289e-03,\n",
       "                      -2.8753e-03, -1.1334e-03,  2.3425e-03, -8.7678e-04, -1.2948e-03,\n",
       "                       9.5300e-04,  1.3609e-03, -1.9170e-03, -2.5804e-03, -4.8763e-04,\n",
       "                      -4.5866e-04,  9.3683e-04, -1.5070e-03, -5.8321e-03,  8.6387e-05,\n",
       "                      -2.7469e-03,  1.9146e-03,  3.8408e-03,  8.7894e-04,  1.0119e-03,\n",
       "                      -1.2254e-03, -6.3048e-04, -1.8320e-04,  8.1679e-05, -5.6389e-04,\n",
       "                      -9.0615e-04, -2.1941e-03,  5.1960e-03, -2.4234e-03, -1.4415e-03,\n",
       "                      -2.3196e-03, -1.8255e-03, -7.1343e-04,  2.3368e-03,  2.7019e-03,\n",
       "                      -9.1866e-04,  4.3087e-04, -1.3967e-03,  2.0878e-03, -3.6390e-03,\n",
       "                      -3.0550e-04,  7.5205e-04,  1.4393e-03,  9.2830e-04,  1.4815e-03,\n",
       "                       2.0550e-03,  3.9690e-03,  4.4960e-03,  4.0001e-04,  1.1700e-03,\n",
       "                      -3.8085e-04, -2.8420e-03, -9.2026e-04,  2.3198e-03,  1.3538e-03,\n",
       "                       1.4501e-03, -1.3469e-03, -6.6710e-04, -2.2549e-03, -1.1704e-04,\n",
       "                      -5.5874e-04,  1.5417e-03,  2.0567e-03,  3.2329e-03,  6.2895e-04,\n",
       "                       3.1603e-03, -1.7205e-03, -1.7276e-03, -2.3642e-03,  2.5539e-03,\n",
       "                       7.4958e-04, -4.2557e-03,  5.3110e-03,  7.2040e-04,  3.1035e-03,\n",
       "                       1.5516e-03, -4.1690e-04, -3.3834e-03,  1.2420e-03,  8.0496e-04,\n",
       "                      -8.0698e-04, -1.0035e-03,  1.9720e-03, -4.4758e-04, -3.2711e-04,\n",
       "                      -1.0683e-03,  1.6402e-03,  8.2796e-04, -2.6970e-03, -7.7879e-04,\n",
       "                       3.7030e-03,  4.0541e-03,  1.1126e-03,  1.6286e-03, -2.2881e-03,\n",
       "                       4.0407e-03, -2.2325e-03, -1.1650e-04,  5.3199e-04,  8.1135e-05,\n",
       "                      -3.7270e-03, -1.4447e-04, -2.5050e-03, -4.0004e-04, -3.4620e-03,\n",
       "                       1.4309e-03, -1.7439e-03, -2.4036e-03, -1.8149e-03, -2.1475e-03,\n",
       "                      -2.5604e-03, -6.6086e-04,  1.9343e-03, -7.4263e-04, -1.8017e-03,\n",
       "                       3.4093e-03, -2.4872e-03,  1.3220e-03,  2.1555e-03, -1.8847e-05,\n",
       "                      -2.9882e-04, -1.6643e-03,  1.0677e-03,  2.0056e-03, -2.7275e-03,\n",
       "                      -1.2414e-03,  2.2193e-04,  2.5420e-03, -8.1313e-04, -1.8710e-03,\n",
       "                       2.6872e-03, -2.9613e-03, -5.0735e-04, -1.4141e-03,  7.9352e-04,\n",
       "                       1.3003e-03, -4.9452e-03, -6.6394e-04,  8.5415e-04, -2.7084e-03,\n",
       "                       1.6489e-03,  4.7015e-03,  2.1571e-03, -3.7719e-03, -1.3598e-03,\n",
       "                      -1.6929e-05, -1.0146e-03,  1.3685e-03, -5.2356e-05,  1.6898e-03,\n",
       "                      -1.8948e-03,  1.4025e-03, -1.7991e-03, -3.4595e-03,  4.6639e-05,\n",
       "                      -1.4090e-03,  1.0018e-03, -1.8062e-04,  1.8802e-03, -3.1626e-03,\n",
       "                       1.0529e-04,  8.2418e-04, -8.8503e-04,  2.0983e-03,  1.5845e-03,\n",
       "                      -2.0665e-03, -1.8480e-03,  9.3430e-04, -4.0268e-05,  2.2229e-03,\n",
       "                      -2.2267e-03, -2.5660e-03,  1.5885e-03,  7.9323e-04,  2.0648e-03,\n",
       "                      -1.4089e-03, -4.8236e-03,  7.4344e-05, -2.0544e-03, -1.5495e-03,\n",
       "                      -3.0614e-03,  3.4569e-03, -3.8566e-03, -1.8954e-03, -1.1088e-03,\n",
       "                      -1.3962e-03, -3.1746e-04, -2.0500e-03, -1.9432e-03, -1.9303e-03,\n",
       "                       4.0640e-04, -6.0053e-04,  3.4556e-04,  1.0715e-04,  2.7547e-05,\n",
       "                      -1.4284e-03, -2.4241e-03, -3.8013e-05, -3.4589e-03, -2.8891e-03,\n",
       "                      -1.2209e-03, -1.1950e-03, -4.9170e-05, -2.5566e-03, -1.7921e-03,\n",
       "                       1.5004e-03,  5.6539e-04, -1.4772e-04,  2.6730e-03, -5.2319e-03,\n",
       "                      -8.4782e-04, -9.2961e-04, -2.0248e-03,  4.5233e-05,  2.0280e-03,\n",
       "                       2.2191e-04,  4.2110e-04, -1.7546e-03,  2.2626e-03,  1.2292e-03,\n",
       "                       8.4429e-04, -2.2649e-03,  9.8558e-04, -2.0974e-03, -7.7735e-05,\n",
       "                       7.8511e-04, -3.0234e-04,  1.5605e-03,  4.2168e-03,  2.2630e-03,\n",
       "                      -1.2846e-03,  7.1816e-05, -9.3514e-04,  6.4853e-04,  1.0094e-03,\n",
       "                       3.8533e-03,  1.8874e-03, -2.2118e-03, -2.3349e-03, -2.8671e-03,\n",
       "                       9.3927e-04, -1.7419e-03,  1.7933e-03, -4.1248e-03,  2.5427e-03,\n",
       "                       2.0523e-04,  1.8814e-04,  2.7167e-03, -1.4241e-03,  4.9761e-04,\n",
       "                       8.5467e-04, -2.7764e-03,  2.3800e-03, -3.4184e-03,  3.5569e-03,\n",
       "                      -2.1925e-03, -2.9657e-03, -5.0151e-04,  3.6005e-03, -6.8474e-04,\n",
       "                       2.2570e-03, -2.6953e-03,  5.2678e-04, -2.7693e-03, -6.6789e-04,\n",
       "                      -3.9034e-03,  1.3335e-03,  3.5865e-03, -1.7888e-03,  1.9858e-03,\n",
       "                       8.6701e-04, -2.1198e-03,  2.4936e-03, -2.4504e-03, -2.3439e-04,\n",
       "                      -9.5308e-04, -2.1308e-04,  5.7731e-04, -3.2196e-03, -4.6873e-03,\n",
       "                      -3.3877e-03, -1.2798e-03,  1.4184e-03, -5.8959e-04,  3.2670e-03,\n",
       "                       2.7266e-03,  2.5965e-03,  4.9435e-04,  1.5431e-03, -4.0392e-04,\n",
       "                       3.6614e-03, -1.8892e-03, -6.0349e-04, -3.2721e-03,  2.4122e-03,\n",
       "                       1.2197e-03,  3.5084e-03,  4.9866e-04,  2.6954e-03,  2.0435e-03,\n",
       "                       8.3571e-04,  1.2982e-03])),\n",
       "             ('transformer.resblocks.7.attn.in_proj_weight',\n",
       "              tensor([[ 0.0399, -0.0095, -0.0381,  ..., -0.0305, -0.0418,  0.0512],\n",
       "                      [ 0.0595, -0.0159, -0.0369,  ..., -0.0737, -0.0150,  0.0113],\n",
       "                      [ 0.0318,  0.1095,  0.0365,  ..., -0.0309, -0.0069, -0.0498],\n",
       "                      ...,\n",
       "                      [ 0.0227,  0.0378, -0.0664,  ..., -0.0497, -0.1504,  0.0043],\n",
       "                      [-0.0284,  0.0157, -0.0182,  ..., -0.0339,  0.0279, -0.0425],\n",
       "                      [ 0.0023, -0.0629, -0.0264,  ...,  0.0096, -0.0782, -0.0662]])),\n",
       "             ('transformer.resblocks.7.attn.in_proj_bias',\n",
       "              tensor([-0.0032,  0.0016,  0.0024,  ...,  0.0019, -0.0028,  0.0005])),\n",
       "             ('transformer.resblocks.7.attn.out_proj.weight',\n",
       "              tensor([[ 1.6820e-02, -2.2952e-03,  2.6935e-05,  ..., -5.1608e-03,\n",
       "                       -7.5535e-03, -6.8509e-03],\n",
       "                      [-1.0837e-03,  1.1355e-02,  8.6768e-03,  ..., -4.8113e-03,\n",
       "                       -7.5551e-03,  1.4494e-03],\n",
       "                      [-9.6055e-04,  1.1259e-02, -1.5487e-02,  ..., -5.1615e-03,\n",
       "                        9.0393e-03,  1.7616e-04],\n",
       "                      ...,\n",
       "                      [-1.2212e-02,  3.2373e-03, -2.0335e-04,  ...,  1.0513e-04,\n",
       "                       -2.6122e-03, -6.0605e-03],\n",
       "                      [ 3.0833e-03,  6.4879e-03,  8.5312e-03,  ...,  1.0216e-02,\n",
       "                       -2.6032e-04,  1.2561e-02],\n",
       "                      [-1.0128e-03, -1.0627e-02,  5.3552e-03,  ...,  2.3091e-03,\n",
       "                       -5.0702e-03,  2.9256e-03]])),\n",
       "             ('transformer.resblocks.7.attn.out_proj.bias',\n",
       "              tensor([-3.9819e-04, -3.8318e-03, -5.5199e-04, -5.1916e-04, -8.9043e-04,\n",
       "                      -2.8451e-03,  5.7345e-04,  2.2992e-03, -3.3761e-03,  1.6109e-03,\n",
       "                      -4.2975e-03, -2.3779e-03, -6.7658e-04, -1.0397e-03, -4.9534e-03,\n",
       "                       1.8425e-03,  4.6207e-05,  1.7820e-03, -3.5684e-06, -7.5321e-04,\n",
       "                      -5.7099e-04,  1.5105e-05,  9.2331e-04, -2.0060e-03, -1.6483e-03,\n",
       "                      -8.4738e-04, -1.2815e-03, -1.0161e-03, -4.5511e-04, -1.7005e-03,\n",
       "                       8.2019e-04,  4.3463e-04,  2.7603e-03, -3.2844e-03, -5.6122e-04,\n",
       "                       1.1818e-03,  9.4543e-04,  7.0376e-04,  1.2054e-04, -2.7155e-03,\n",
       "                       1.5587e-03,  2.0498e-03,  1.8776e-03,  1.2528e-03, -3.0615e-03,\n",
       "                       1.5858e-03, -3.1626e-03,  6.0260e-04, -9.2631e-04,  1.1137e-03,\n",
       "                      -1.1751e-03, -6.2387e-04, -1.5733e-03,  2.3609e-03,  2.4212e-03,\n",
       "                       9.7373e-04, -7.0809e-04, -5.9722e-04,  1.9515e-04, -8.3702e-04,\n",
       "                      -1.4440e-03, -4.0560e-05,  3.4080e-04,  4.1183e-04, -2.5280e-04,\n",
       "                      -6.0550e-04,  1.1616e-03,  1.7030e-03, -5.8386e-04,  1.8107e-03,\n",
       "                      -9.6982e-04, -1.7030e-05, -1.9326e-03, -3.6258e-04, -9.5265e-04,\n",
       "                       1.9339e-03, -2.3758e-03, -1.8579e-03, -1.9023e-03,  2.2640e-03,\n",
       "                       1.4414e-03, -1.7165e-03,  3.1715e-04,  2.1610e-03,  1.6938e-03,\n",
       "                       2.2309e-04, -8.5346e-04,  1.0906e-03,  7.1493e-04, -1.6498e-03,\n",
       "                       1.2321e-04, -1.1210e-03,  1.4704e-03, -1.6965e-03, -1.9810e-03,\n",
       "                      -4.0899e-03,  2.8190e-04, -1.2063e-03, -1.8592e-03,  9.6540e-04,\n",
       "                       7.3564e-04, -1.4508e-03,  3.9133e-04, -9.1503e-04, -7.5057e-04,\n",
       "                      -2.2539e-03, -2.8740e-03, -1.0049e-03,  9.6771e-04, -2.2415e-04,\n",
       "                       3.1384e-03,  9.6722e-04,  2.0070e-03,  1.4284e-03, -1.1860e-03,\n",
       "                      -3.8340e-03,  1.4940e-03,  1.5651e-03, -2.5084e-03, -9.6590e-04,\n",
       "                       6.1975e-04, -1.0847e-03,  1.7948e-03, -6.3962e-04, -1.3794e-03,\n",
       "                       7.2389e-04, -1.5151e-03, -1.6658e-03, -1.5444e-03,  4.5474e-05,\n",
       "                      -1.0951e-04,  3.1220e-03, -2.8401e-04, -1.9812e-03, -9.4048e-04,\n",
       "                      -7.1998e-04, -1.3800e-03,  1.8774e-03, -7.0258e-04,  1.2773e-03,\n",
       "                       4.0150e-04,  2.2237e-03,  1.1531e-03,  3.4931e-03,  1.1365e-03,\n",
       "                       1.8875e-03, -2.3339e-03,  3.5777e-04,  1.2206e-03,  6.1132e-05,\n",
       "                       1.6864e-03,  4.4399e-03,  5.3925e-03,  1.6582e-03, -1.4435e-04,\n",
       "                       9.1961e-05,  9.8262e-04, -1.0037e-03,  8.3742e-04, -6.0580e-06,\n",
       "                      -1.7504e-03, -9.5973e-04,  2.9595e-03, -2.2912e-04, -1.4550e-03,\n",
       "                       1.1688e-03,  2.7027e-03, -9.4053e-04, -4.2323e-04, -2.1023e-03,\n",
       "                       1.4968e-03, -2.1895e-04, -1.5843e-03,  9.5818e-04,  2.1351e-03,\n",
       "                       1.2732e-03, -1.4651e-03,  3.9424e-04,  9.9279e-04, -8.1609e-04,\n",
       "                      -2.6441e-03,  1.5293e-04,  9.2124e-04, -2.7788e-03,  4.4486e-03,\n",
       "                       7.4901e-05,  4.3115e-04, -4.2311e-04,  1.0481e-03, -2.4980e-03,\n",
       "                       6.4601e-04,  8.6797e-04, -5.1933e-04,  1.7219e-03, -7.9563e-04,\n",
       "                      -8.5007e-04,  1.6733e-03,  2.5283e-03,  2.7971e-03,  2.7748e-03,\n",
       "                      -2.2035e-04, -1.2351e-03,  1.6866e-03,  2.1878e-03,  2.7039e-04,\n",
       "                       3.5577e-04, -1.1506e-03,  9.1984e-06,  1.2173e-03,  2.3236e-04,\n",
       "                       2.1244e-03,  5.9183e-04, -1.2591e-06,  1.5358e-03, -2.0936e-03,\n",
       "                      -2.2168e-03,  2.8967e-04, -2.0494e-03, -9.0640e-04,  8.4875e-04,\n",
       "                      -7.7876e-04, -4.4094e-04,  1.6488e-03,  1.9308e-03,  1.1284e-03,\n",
       "                       1.2325e-03, -4.7150e-04, -2.0355e-03,  8.1498e-04,  1.6363e-03,\n",
       "                      -1.6216e-03,  9.8537e-04,  6.8681e-04, -4.2502e-04,  1.4579e-03,\n",
       "                      -2.9698e-04,  2.5660e-03, -3.2636e-03,  1.4054e-03,  9.9965e-04,\n",
       "                      -1.7047e-03,  2.0401e-03,  2.5054e-03,  1.5453e-03, -2.8016e-03,\n",
       "                      -2.5289e-04,  5.2652e-04,  7.3915e-04, -2.3934e-04, -1.0128e-03,\n",
       "                       9.0722e-04, -1.9112e-03,  2.1342e-04, -1.0700e-03, -1.2778e-03,\n",
       "                      -1.1558e-03, -3.8683e-04, -2.3505e-03, -1.5388e-04, -3.3465e-03,\n",
       "                       1.2400e-03,  4.9914e-04,  1.0328e-03, -3.2660e-03, -1.6743e-03,\n",
       "                       2.6033e-03,  1.0251e-03,  1.2021e-03, -3.6692e-04, -1.6747e-03,\n",
       "                       9.1371e-04, -9.8202e-04, -1.8466e-03,  5.9961e-04,  1.1966e-03,\n",
       "                      -1.1664e-03,  3.4559e-03, -7.0705e-04,  3.0867e-03, -1.3123e-03,\n",
       "                      -7.6927e-04,  3.2925e-03, -1.9908e-03, -1.5780e-03, -2.7072e-03,\n",
       "                       1.1331e-03,  3.7777e-04,  1.6036e-03,  1.8088e-03,  2.0839e-04,\n",
       "                      -1.7419e-04,  1.8160e-03, -1.6388e-03,  4.0295e-03,  6.7245e-04,\n",
       "                       1.3077e-03, -2.0684e-03,  8.7800e-04, -7.7816e-04, -1.1655e-03,\n",
       "                      -2.5635e-03,  8.7471e-04,  1.0818e-03, -3.9118e-03, -1.0144e-04,\n",
       "                      -2.5156e-03,  3.2294e-04,  1.7695e-04, -2.2824e-03, -2.0654e-03,\n",
       "                       1.5667e-03,  2.0853e-03, -7.3808e-04, -1.9341e-03,  1.3149e-03,\n",
       "                       1.9237e-03,  3.0376e-03,  1.2772e-03,  9.9954e-04,  1.1519e-03,\n",
       "                       2.4655e-03, -7.2872e-04, -1.1007e-03,  6.0965e-04, -2.1124e-03,\n",
       "                       6.8951e-04,  2.7744e-03,  6.2780e-04, -3.2484e-03, -1.9927e-03,\n",
       "                      -1.5086e-04,  1.7041e-03,  5.4118e-04, -8.5018e-04,  1.7690e-03,\n",
       "                      -2.7168e-04, -3.0955e-04,  4.5838e-04,  9.4048e-04,  2.6482e-03,\n",
       "                      -3.1672e-03,  2.7901e-03,  7.2997e-04, -5.5460e-04, -2.8317e-03,\n",
       "                      -8.2582e-07, -1.9463e-03,  1.0740e-05, -3.8530e-04,  1.4939e-03,\n",
       "                      -1.6528e-03, -3.7955e-03, -1.8936e-03, -7.7472e-05, -2.3100e-04,\n",
       "                       6.0307e-04,  2.9044e-03,  2.4809e-03,  9.7174e-04, -2.7053e-03,\n",
       "                       9.4228e-04, -8.2222e-04,  2.6517e-03,  3.2654e-03,  3.0861e-05,\n",
       "                       2.9234e-03, -1.8538e-04, -3.6861e-03,  1.5229e-03, -8.4946e-04,\n",
       "                      -9.4569e-04,  1.2976e-03,  7.7250e-04, -7.5794e-04,  1.0679e-04,\n",
       "                       3.0663e-03, -6.9127e-04,  1.9967e-03, -1.5817e-03, -1.9216e-03,\n",
       "                       2.0838e-03,  1.9878e-03, -1.7762e-03, -5.2882e-04, -1.8313e-03,\n",
       "                      -6.4027e-04,  2.1556e-03, -7.2608e-04,  3.1515e-03,  1.8905e-03,\n",
       "                      -3.5197e-04,  8.1531e-05, -2.2421e-04,  1.1349e-03,  3.0664e-03,\n",
       "                       2.3214e-03, -6.4349e-04,  2.0866e-03,  3.7302e-04,  3.8211e-05,\n",
       "                      -6.1455e-04, -1.7479e-03, -7.2157e-04, -5.4445e-04, -6.0545e-04,\n",
       "                       1.6667e-03,  1.3100e-03,  1.2162e-03,  1.6294e-03,  1.5556e-03,\n",
       "                      -4.0787e-04, -2.5535e-03,  7.6322e-04,  2.6362e-03,  9.0509e-04,\n",
       "                      -2.1964e-03,  1.8571e-03,  1.1546e-03, -4.1321e-04,  2.8543e-03,\n",
       "                      -1.1439e-03,  7.0412e-04,  1.2531e-03, -1.5021e-03, -1.8647e-03,\n",
       "                       2.3995e-04, -5.9447e-04, -2.7655e-04,  1.2147e-03,  4.4832e-05,\n",
       "                      -1.8243e-03,  8.8506e-04,  1.4165e-03, -8.4224e-04, -2.8522e-03,\n",
       "                      -1.3459e-03,  1.5478e-03, -2.6712e-03,  1.8666e-03, -1.0377e-03,\n",
       "                      -1.1297e-03,  7.1985e-04, -1.6033e-04, -4.1030e-04, -1.8821e-03,\n",
       "                      -2.9617e-03, -6.1972e-04,  9.4953e-04,  4.6664e-04,  1.2211e-03,\n",
       "                       4.5845e-04,  8.2189e-04, -1.2936e-03,  4.2385e-03,  8.1185e-04,\n",
       "                      -2.0558e-03,  7.4325e-04,  2.3943e-03,  2.2098e-03,  5.4448e-04,\n",
       "                       6.7412e-04, -1.0702e-03,  1.1143e-03,  3.2297e-03,  4.8587e-04,\n",
       "                       6.0252e-04,  3.4018e-05,  1.4084e-03, -8.6771e-05, -2.2822e-03,\n",
       "                      -2.0741e-03,  1.3800e-03, -3.0630e-03, -5.5828e-04, -1.7992e-03,\n",
       "                       9.1543e-04, -2.8833e-03, -1.3933e-03, -8.4650e-04, -1.7561e-03,\n",
       "                      -2.3696e-03, -1.0547e-03,  7.1874e-04,  5.0801e-04, -1.0807e-03,\n",
       "                       8.7500e-04, -1.0923e-03, -1.8054e-03,  2.5055e-03,  1.1778e-03,\n",
       "                       1.1925e-03, -5.1578e-04,  1.2046e-03, -2.7356e-03, -1.2583e-03,\n",
       "                      -5.7667e-05,  1.2917e-03, -1.2719e-04, -7.2294e-05, -3.2951e-03,\n",
       "                      -1.1442e-03,  6.3539e-04, -9.5798e-04,  2.4920e-03, -2.9164e-03,\n",
       "                       1.2391e-03, -1.4543e-03, -1.1180e-03,  1.0185e-03, -5.9698e-04,\n",
       "                      -1.6739e-03, -4.1489e-04])),\n",
       "             ('transformer.resblocks.7.ln_1.weight',\n",
       "              tensor([0.9989, 0.9999, 0.9982, 1.0014, 1.0001, 1.0008, 0.9975, 0.9992, 0.9971,\n",
       "                      0.9987, 0.9977, 0.9998, 0.9989, 0.9998, 0.9992, 0.9985, 0.9979, 1.0009,\n",
       "                      1.0001, 0.9975, 1.0000, 0.9994, 1.0004, 0.9995, 1.0003, 0.9992, 0.9970,\n",
       "                      1.0001, 0.9998, 0.9975, 0.9987, 0.9990, 0.9998, 0.9983, 0.9992, 1.0015,\n",
       "                      0.9982, 1.0039, 0.9980, 0.9995, 0.9973, 0.9972, 1.0000, 0.9991, 0.9997,\n",
       "                      0.9982, 0.9942, 0.9978, 0.9972, 0.9990, 0.9989, 0.9988, 1.0002, 0.9992,\n",
       "                      1.0010, 0.9984, 0.9990, 1.0020, 1.0003, 0.9983, 1.0010, 0.9972, 1.0012,\n",
       "                      0.9994, 1.0012, 0.9999, 0.9999, 0.9985, 0.9989, 0.9986, 0.9970, 0.9998,\n",
       "                      1.0003, 0.9989, 1.0022, 0.9999, 0.9992, 1.0027, 0.9990, 0.9996, 0.9986,\n",
       "                      1.0026, 1.0009, 1.0002, 1.0013, 0.9991, 0.9996, 1.0023, 1.0003, 1.0014,\n",
       "                      1.0001, 0.9986, 0.9985, 0.9991, 1.0000, 0.9978, 1.0018, 0.9962, 1.0013,\n",
       "                      1.0014, 0.9992, 0.9996, 0.9998, 1.0006, 0.9990, 1.0006, 0.9966, 0.9979,\n",
       "                      1.0010, 1.0027, 0.9967, 1.0032, 0.9987, 1.0004, 0.9999, 0.9975, 1.0008,\n",
       "                      1.0007, 0.9970, 0.9980, 0.9993, 1.0011, 1.0002, 0.9981, 0.9984, 0.9997,\n",
       "                      0.9986, 0.9992, 1.0007, 0.9985, 1.0000, 0.9984, 1.0019, 1.0009, 1.0000,\n",
       "                      0.9987, 0.9987, 1.0014, 0.9988, 0.9959, 1.0033, 1.0007, 0.9997, 0.9992,\n",
       "                      1.0017, 0.9982, 0.9972, 1.0021, 1.0004, 1.0006, 1.0010, 0.9993, 0.9971,\n",
       "                      1.0016, 0.9997, 1.0006, 1.0050, 1.0005, 0.9984, 0.9998, 1.0032, 1.0001,\n",
       "                      0.9979, 0.9990, 1.0022, 0.9997, 1.0001, 0.9990, 0.9962, 1.0021, 0.9990,\n",
       "                      0.9993, 1.0004, 0.9980, 0.9987, 1.0027, 0.9986, 1.0007, 0.9965, 0.9990,\n",
       "                      1.0005, 0.9990, 0.9997, 0.9989, 0.9999, 0.9987, 0.9987, 1.0015, 0.9991,\n",
       "                      0.9988, 1.0022, 0.9993, 1.0001, 0.9983, 0.9988, 1.0015, 1.0012, 0.9977,\n",
       "                      0.9983, 1.0006, 0.9963, 0.9989, 0.9995, 0.9987, 1.0016, 0.9984, 1.0018,\n",
       "                      0.9989, 1.0015, 1.0023, 1.0011, 1.0023, 1.0022, 0.9965, 0.9992, 0.9979,\n",
       "                      1.0002, 1.0005, 1.0000, 0.9964, 1.0002, 1.0038, 1.0009, 0.9994, 0.9998,\n",
       "                      1.0013, 0.9982, 1.0005, 0.9990, 0.9981, 0.9985, 1.0018, 0.9987, 0.9996,\n",
       "                      0.9981, 0.9998, 1.0022, 0.9994, 1.0050, 0.9997, 1.0025, 0.9995, 0.9975,\n",
       "                      0.9977, 0.9982, 1.0032, 0.9966, 0.9998, 0.9994, 1.0035, 1.0003, 0.9999,\n",
       "                      1.0001, 1.0022, 1.0021, 1.0011, 0.9993, 0.9965, 0.9999, 0.9990, 1.0018,\n",
       "                      1.0003, 1.0006, 0.9998, 0.9988, 1.0008, 0.9992, 1.0002, 0.9986, 0.9992,\n",
       "                      0.9987, 1.0015, 0.9988, 1.0004, 0.9973, 0.9997, 0.9991, 1.0025, 1.0017,\n",
       "                      1.0024, 0.9978, 0.9997, 0.9989, 1.0034, 0.9994, 1.0025, 1.0004, 0.9993,\n",
       "                      0.9972, 0.9990, 0.9987, 1.0010, 1.0003, 0.9977, 0.9997, 1.0000, 1.0000,\n",
       "                      0.9984, 0.9987, 0.9962, 0.9986, 0.9990, 1.0021, 0.9997, 0.9989, 1.0017,\n",
       "                      0.9968, 1.0033, 0.9986, 1.0003, 0.9997, 0.9976, 1.0013, 1.0007, 1.0005,\n",
       "                      0.9990, 0.9979, 1.0005, 0.9986, 1.0003, 1.0008, 0.9953, 0.9981, 0.9967,\n",
       "                      0.9994, 0.9997, 0.9960, 1.0022, 0.9973, 0.9996, 1.0014, 1.0015, 1.0004,\n",
       "                      0.9989, 0.9980, 1.0017, 0.9985, 0.9980, 1.0007, 0.9981, 0.9977, 1.0001,\n",
       "                      0.9993, 0.9993, 0.9997, 0.9963, 1.0026, 0.9982, 0.9995, 0.9987, 1.0018,\n",
       "                      0.9981, 0.9989, 1.0010, 0.9998, 0.9993, 1.0002, 0.9988, 1.0049, 0.9969,\n",
       "                      1.0007, 1.0006, 0.9996, 0.9976, 1.0006, 1.0027, 1.0017, 1.0033, 1.0022,\n",
       "                      0.9999, 0.9968, 0.9980, 0.9979, 0.9976, 0.9972, 1.0021, 0.9996, 1.0012,\n",
       "                      0.9984, 1.0003, 1.0012, 1.0015, 1.0036, 0.9985, 0.9983, 1.0004, 1.0022,\n",
       "                      1.0007, 0.9982, 1.0013, 0.9992, 1.0013, 1.0001, 1.0022, 0.9975, 0.9984,\n",
       "                      1.0003, 1.0011, 1.0003, 0.9989, 0.9985, 0.9998, 0.9996, 0.9952, 1.0029,\n",
       "                      0.9988, 0.9979, 1.0043, 0.9994, 0.9993, 1.0015, 0.9997, 0.9965, 1.0024,\n",
       "                      1.0008, 1.0001, 0.9991, 1.0016, 1.0000, 0.9997, 1.0020, 1.0022, 1.0013,\n",
       "                      1.0009, 0.9986, 1.0003, 0.9985, 1.0019, 1.0026, 1.0007, 0.9996, 0.9980,\n",
       "                      1.0019, 0.9983, 1.0002, 0.9975, 1.0010, 0.9979, 1.0023, 0.9971, 0.9938,\n",
       "                      0.9961, 1.0008, 0.9981, 1.0013, 1.0013, 1.0018, 1.0023, 0.9957, 1.0005,\n",
       "                      1.0012, 1.0016, 1.0009, 0.9975, 0.9969, 1.0016, 1.0028, 0.9981, 0.9986,\n",
       "                      1.0012, 0.9997, 0.9991, 0.9974, 1.0031, 1.0017, 0.9980, 0.9972, 1.0001,\n",
       "                      1.0017, 0.9990, 1.0003, 1.0004, 0.9993, 0.9993, 0.9978, 0.9969, 0.9989,\n",
       "                      0.9994, 0.9995, 1.0021, 1.0003, 1.0019, 1.0012, 1.0007, 1.0015, 1.0024,\n",
       "                      1.0005, 1.0004, 0.9996, 0.9997, 0.9988, 0.9981, 0.9992, 0.9978, 0.9987,\n",
       "                      0.9989, 1.0008, 1.0001, 0.9984, 1.0022, 1.0009, 0.9993, 0.9988, 1.0007,\n",
       "                      0.9991, 0.9973, 0.9995, 1.0012, 0.9994, 1.0020, 0.9990, 0.9976])),\n",
       "             ('transformer.resblocks.7.ln_1.bias',\n",
       "              tensor([ 2.0320e-03, -1.7922e-03,  4.0021e-04, -2.0042e-03,  2.8473e-03,\n",
       "                      -1.5266e-03, -1.2667e-03, -6.5435e-04,  6.4542e-04, -4.6413e-04,\n",
       "                       5.6355e-04, -3.4063e-05, -1.6121e-03, -1.7892e-03, -3.9349e-04,\n",
       "                      -2.3024e-04, -1.7017e-03, -2.1490e-03, -1.2485e-03,  1.2416e-03,\n",
       "                      -2.9901e-03, -2.1983e-03, -8.4973e-04, -1.7382e-03, -2.6510e-04,\n",
       "                       9.4000e-04,  4.8750e-04, -4.3096e-03,  1.6892e-03,  1.0433e-03,\n",
       "                       5.2640e-04,  3.5532e-03, -1.3929e-04,  1.6433e-03, -2.1684e-03,\n",
       "                       1.9582e-03,  3.7676e-04,  1.5083e-03,  5.9787e-04,  6.2456e-04,\n",
       "                       1.8305e-03,  1.1128e-04,  1.1021e-03, -2.6475e-03,  2.0259e-03,\n",
       "                       7.1229e-04,  1.9458e-03, -1.1063e-03,  2.4447e-03, -1.3976e-03,\n",
       "                       5.1040e-05, -2.3352e-03,  5.3075e-06,  1.2503e-03,  1.9241e-03,\n",
       "                       2.9853e-03,  5.3544e-04, -9.2063e-04,  5.1144e-04, -5.6661e-04,\n",
       "                      -2.7631e-03, -7.6363e-04,  1.0835e-03,  3.6606e-04, -2.1602e-03,\n",
       "                      -2.6601e-03,  1.6795e-03, -1.1023e-04,  1.7052e-03,  2.5237e-03,\n",
       "                       2.0618e-03,  2.5362e-03, -1.2015e-03, -1.6272e-03, -2.5954e-03,\n",
       "                       2.8150e-03, -2.4185e-03,  7.9843e-04, -2.6343e-04, -1.8355e-03,\n",
       "                       4.7094e-04,  1.6388e-04, -1.4286e-03,  1.5441e-03,  2.2073e-03,\n",
       "                      -5.2407e-04, -9.7781e-04,  4.5601e-03, -2.2501e-03, -5.5466e-04,\n",
       "                       1.6430e-03,  3.0208e-03, -4.9106e-04,  1.5724e-03,  1.3942e-03,\n",
       "                       8.1938e-04,  4.6947e-03,  2.6876e-03,  6.8105e-04, -1.0879e-03,\n",
       "                      -7.0655e-04, -2.1079e-04, -1.0355e-03, -7.6838e-04, -1.8002e-03,\n",
       "                      -1.7104e-03,  2.2747e-03,  2.2033e-03, -6.6864e-04,  9.8742e-04,\n",
       "                      -1.4470e-03,  1.5210e-03, -4.0560e-04,  2.0596e-03,  4.3675e-04,\n",
       "                       1.1788e-03,  3.6689e-04,  1.7976e-03,  1.4027e-03,  5.8972e-04,\n",
       "                       1.9000e-03,  6.6979e-04,  7.9909e-05,  2.2331e-03, -3.1835e-03,\n",
       "                       2.7989e-03, -1.9552e-03,  2.2917e-04,  2.9229e-04,  3.2141e-03,\n",
       "                       2.8323e-03,  2.9402e-04, -1.7882e-03, -2.4310e-03, -1.7016e-03,\n",
       "                      -1.5108e-03,  5.8372e-04,  4.6397e-04,  1.4464e-03,  4.9794e-04,\n",
       "                       1.1553e-03, -1.5104e-03,  5.1018e-04, -1.8409e-04, -1.9495e-03,\n",
       "                      -1.3573e-03,  1.6488e-03,  4.6605e-04, -4.1101e-04,  2.3599e-03,\n",
       "                      -1.1394e-03, -2.2092e-05, -1.9459e-03,  3.1797e-03,  2.0282e-03,\n",
       "                       1.4837e-04,  1.6124e-03,  1.1574e-03,  3.6396e-05, -1.8564e-03,\n",
       "                      -3.6733e-03, -2.5840e-03,  8.9227e-05,  8.8079e-04,  2.5561e-04,\n",
       "                       1.9507e-03,  5.0797e-04,  9.6775e-04, -9.5387e-04,  2.8386e-03,\n",
       "                       1.5103e-04,  5.5282e-04,  2.6098e-03,  1.4934e-04, -1.3070e-03,\n",
       "                       3.2473e-04,  5.2426e-06,  1.6936e-03,  1.1577e-03,  2.1500e-03,\n",
       "                      -1.5998e-03,  1.3378e-03,  1.5056e-03, -2.5531e-04, -1.9010e-03,\n",
       "                       1.0827e-03,  1.7663e-03,  3.1631e-03, -8.4689e-04, -2.4377e-03,\n",
       "                      -7.1471e-04, -1.1574e-03,  1.7584e-03,  1.0794e-03,  4.7235e-04,\n",
       "                       6.7629e-04, -1.3178e-03, -5.7702e-04, -1.3085e-03, -1.5577e-03,\n",
       "                       3.4497e-04,  1.2083e-04,  3.0203e-03,  2.8706e-03, -2.4722e-03,\n",
       "                      -7.8357e-04, -6.8072e-04, -1.9080e-03,  1.7813e-03, -3.9390e-04,\n",
       "                       7.1894e-04,  7.9952e-05, -4.7079e-03, -1.1275e-03,  3.7464e-03,\n",
       "                       6.0170e-04, -1.5791e-03, -1.4001e-03,  5.1835e-04, -3.3510e-03,\n",
       "                       1.8822e-03, -2.8967e-04,  1.8266e-03, -3.0752e-03, -1.0339e-05,\n",
       "                       2.3111e-03, -5.6727e-04,  1.6198e-03,  1.2058e-04, -1.4460e-04,\n",
       "                      -1.6730e-03,  1.8777e-03,  1.3640e-03,  1.1074e-03, -1.8435e-03,\n",
       "                       2.5143e-04,  1.9609e-03, -6.6717e-04,  1.5774e-03,  1.0556e-03,\n",
       "                      -2.2808e-03, -5.6494e-04, -1.1652e-03, -1.4393e-03,  1.9837e-03,\n",
       "                       9.5355e-04, -1.5826e-03, -9.5056e-04,  1.4270e-03, -2.3088e-04,\n",
       "                       1.4733e-03, -9.7105e-04, -1.7898e-03, -1.4089e-03, -2.4562e-03,\n",
       "                      -4.0551e-04,  2.2200e-03,  2.3506e-03, -3.2267e-03, -5.2424e-04,\n",
       "                       7.4626e-04, -2.8496e-03,  4.8047e-04,  1.5930e-03,  3.2471e-03,\n",
       "                       1.5144e-03, -1.0332e-03, -1.6424e-03,  1.6735e-03,  2.9354e-03,\n",
       "                       3.0607e-03,  1.1950e-03,  3.1713e-05,  5.4518e-04,  2.1375e-03,\n",
       "                      -2.3552e-03, -8.2433e-04,  4.6477e-04,  5.6305e-06,  2.7753e-03,\n",
       "                       2.2296e-03,  1.9213e-03, -2.2394e-03, -2.9987e-03,  3.9398e-04,\n",
       "                       1.4690e-03, -3.7478e-04, -3.4393e-04, -5.7264e-04, -1.5722e-03,\n",
       "                       9.5841e-04,  1.0103e-03, -1.6263e-03, -4.3260e-05,  2.3264e-03,\n",
       "                       4.3947e-05, -1.7336e-03,  7.8844e-04,  2.4524e-03,  1.5455e-03,\n",
       "                      -8.9441e-04, -8.9381e-04,  1.7294e-03,  1.2197e-03,  2.7186e-03,\n",
       "                      -1.7272e-03,  6.7713e-04, -1.7904e-04, -2.9344e-04, -1.9920e-03,\n",
       "                       1.5480e-04, -3.6945e-03,  2.5907e-04, -5.8250e-04,  3.1941e-04,\n",
       "                       3.4557e-04, -1.0992e-03,  2.3617e-03,  9.1598e-04, -9.6741e-04,\n",
       "                       1.4661e-03, -5.0238e-04,  2.3644e-03,  9.3185e-05, -2.1807e-03,\n",
       "                       1.6821e-03, -6.8206e-04, -1.2269e-03,  5.9496e-04, -7.3095e-04,\n",
       "                       2.2570e-04,  2.4384e-03, -9.9975e-04, -7.3818e-04, -2.2465e-03,\n",
       "                      -1.8719e-03, -1.8095e-03, -1.3883e-03, -1.7207e-03, -2.2811e-03,\n",
       "                       2.0585e-03,  3.0854e-04, -1.7601e-03, -1.8994e-03,  3.2010e-04,\n",
       "                      -1.0445e-06,  2.3583e-03, -3.3365e-04, -9.1056e-04, -1.6722e-04,\n",
       "                      -2.2818e-03, -8.0573e-04, -7.6916e-04, -7.6875e-04, -6.9327e-04,\n",
       "                      -7.1521e-04, -1.9801e-03, -7.0758e-04, -2.0839e-03,  3.1293e-03,\n",
       "                       8.4652e-04, -2.0688e-03, -1.3035e-03, -1.5473e-04,  1.3895e-03,\n",
       "                       3.4974e-03, -3.2994e-03,  3.2226e-04,  2.0864e-03, -4.2771e-04,\n",
       "                       2.0081e-03, -1.1319e-03,  8.3233e-04,  9.9283e-04, -6.8646e-04,\n",
       "                      -2.4112e-04,  5.0449e-03, -1.7066e-03, -6.8627e-04,  2.1760e-04,\n",
       "                      -4.3038e-04, -2.1232e-03, -3.6925e-03,  1.6677e-05,  1.8170e-04,\n",
       "                       1.5815e-03,  2.1651e-03,  4.0138e-03, -1.8345e-03,  6.4254e-04,\n",
       "                      -3.5347e-03, -2.1362e-03,  2.1708e-03, -1.1329e-03,  5.0949e-04,\n",
       "                       1.1133e-03, -3.1674e-04, -2.6845e-03, -7.4443e-05,  3.6961e-03,\n",
       "                      -4.2566e-04,  3.1854e-03, -2.7999e-03, -1.8135e-04, -1.6857e-05,\n",
       "                      -1.9475e-03, -3.0743e-04,  2.7884e-03,  7.4274e-04,  3.8419e-04,\n",
       "                       3.4648e-03,  2.2152e-03, -3.4181e-03, -7.5771e-04,  2.6286e-03,\n",
       "                      -1.1446e-03, -2.6241e-03, -3.4562e-04, -4.0939e-04,  2.2500e-03,\n",
       "                       2.2681e-04,  2.0187e-04,  6.4323e-04, -3.1811e-03, -5.1512e-04,\n",
       "                      -3.4299e-03, -1.8116e-03,  7.0650e-04,  1.1243e-03, -2.2963e-03,\n",
       "                      -2.0712e-03,  1.0474e-03,  1.7499e-03, -1.8651e-03, -7.1609e-05,\n",
       "                      -4.4313e-04,  8.7182e-04, -2.9375e-04,  7.1642e-04, -4.0331e-04,\n",
       "                      -2.4263e-04, -1.5559e-03,  1.3847e-03,  1.5110e-03, -1.1859e-04,\n",
       "                      -9.3843e-04, -4.4505e-03, -2.9920e-03, -4.0895e-03,  7.7613e-04,\n",
       "                      -2.1970e-03,  2.4108e-03, -4.6736e-03, -1.4984e-03,  2.0776e-03,\n",
       "                       1.8840e-03,  3.1184e-03,  9.2320e-04,  6.9927e-04, -1.4350e-03,\n",
       "                      -4.9161e-04,  3.0556e-04, -1.2497e-03,  2.0497e-04,  3.0525e-03,\n",
       "                      -2.7538e-04,  2.2674e-03,  7.0017e-04,  1.1541e-03, -1.1829e-03,\n",
       "                      -6.3075e-04, -4.4999e-03, -4.4160e-04, -1.0133e-03, -7.5997e-04,\n",
       "                      -1.4456e-03, -2.2265e-03, -1.2515e-04,  3.8643e-03,  3.0225e-03,\n",
       "                       1.7108e-03, -3.1991e-03, -6.9382e-05,  3.3899e-03, -1.8038e-03,\n",
       "                      -1.6369e-03, -2.3636e-03, -7.2418e-04,  1.0089e-03,  1.8823e-03,\n",
       "                       4.6688e-03,  3.9390e-03, -6.2385e-04, -1.8827e-04,  1.3847e-03,\n",
       "                      -9.5296e-04,  1.8876e-03,  5.0723e-04, -3.0868e-04, -1.1132e-03,\n",
       "                       1.5531e-04, -2.6300e-03, -7.8754e-04, -1.0612e-03,  7.9299e-05,\n",
       "                      -1.3194e-03,  8.7837e-04, -1.9139e-03,  1.2831e-03, -1.3014e-04,\n",
       "                      -1.4470e-03,  2.6933e-03])),\n",
       "             ('transformer.resblocks.7.mlp.c_fc.weight',\n",
       "              tensor([[-3.4201e-03, -5.4096e-02,  3.3410e-02,  ..., -1.0417e-02,\n",
       "                        3.4700e-02,  5.2310e-02],\n",
       "                      [-7.8602e-02, -4.2287e-02,  7.8941e-02,  ...,  2.8237e-02,\n",
       "                       -3.9403e-02, -1.0712e-05],\n",
       "                      [-3.2273e-02,  1.8318e-02,  3.9952e-02,  ...,  4.8266e-02,\n",
       "                        1.0541e-03,  3.3863e-02],\n",
       "                      ...,\n",
       "                      [ 4.3499e-02,  1.5058e-02,  6.3088e-02,  ..., -3.5174e-03,\n",
       "                       -1.0595e-02,  4.1267e-02],\n",
       "                      [-1.0215e-02,  6.6352e-03,  4.6744e-02,  ...,  5.1681e-02,\n",
       "                        1.0934e-03,  2.5526e-02],\n",
       "                      [ 1.1569e-02, -6.4613e-02, -1.8136e-02,  ...,  8.4668e-03,\n",
       "                        1.4296e-02,  1.3836e-02]])),\n",
       "             ('transformer.resblocks.7.mlp.c_fc.bias',\n",
       "              tensor([-0.0103,  0.0292, -0.0300,  ...,  0.0288,  0.0279,  0.0334])),\n",
       "             ('transformer.resblocks.7.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0138,  0.0103, -0.0002,  ...,  0.0065,  0.0033,  0.0119],\n",
       "                      [-0.0010,  0.0044, -0.0035,  ..., -0.0047, -0.0030, -0.0065],\n",
       "                      [ 0.0042, -0.0024,  0.0130,  ...,  0.0015, -0.0018, -0.0065],\n",
       "                      ...,\n",
       "                      [-0.0183,  0.0206,  0.0109,  ..., -0.0073, -0.0006, -0.0237],\n",
       "                      [-0.0031,  0.0137, -0.0038,  ...,  0.0125, -0.0141, -0.0101],\n",
       "                      [-0.0114,  0.0067,  0.0065,  ...,  0.0077, -0.0027, -0.0156]])),\n",
       "             ('transformer.resblocks.7.mlp.c_proj.bias',\n",
       "              tensor([ 7.7501e-03,  1.1521e-02,  4.4149e-03, -9.0982e-03, -8.7880e-03,\n",
       "                       1.1039e-03,  5.9313e-04, -8.4914e-03, -1.8308e-02,  2.1268e-02,\n",
       "                       3.8509e-03,  4.5741e-03,  7.3159e-03, -2.8994e-03, -2.1733e-02,\n",
       "                       2.2327e-02,  7.1907e-03,  6.6275e-03, -1.1074e-02,  1.7205e-02,\n",
       "                      -4.3663e-03, -1.4299e-02, -2.0827e-02, -1.7923e-02, -7.7432e-03,\n",
       "                      -1.3869e-02, -2.4158e-03, -8.2610e-03,  5.8217e-03,  7.1557e-03,\n",
       "                      -9.7962e-03,  4.2627e-03, -1.3086e-03, -2.2180e-02, -6.0731e-03,\n",
       "                       2.3310e-02, -3.9683e-03,  9.9008e-03, -1.3973e-02, -2.1857e-03,\n",
       "                       1.7490e-03, -1.5042e-03, -6.6040e-03, -2.0174e-03, -1.9926e-02,\n",
       "                      -4.5032e-03, -2.0696e-02, -1.1382e-04,  8.3889e-03, -6.8034e-03,\n",
       "                       6.6005e-03, -4.5885e-03, -1.0428e-02,  2.2848e-02, -1.7033e-02,\n",
       "                      -1.5698e-02,  8.9810e-03, -1.8110e-02, -9.7176e-04, -1.7857e-02,\n",
       "                       1.9521e-02,  1.5516e-02,  1.6791e-02, -1.1015e-02,  5.5459e-03,\n",
       "                      -1.8475e-02,  1.7124e-02,  1.0496e-02,  1.9909e-02,  4.4970e-03,\n",
       "                      -2.1162e-02, -2.0320e-02,  1.4425e-02, -1.4811e-02,  1.4827e-02,\n",
       "                      -1.3113e-02,  1.6913e-02, -1.7170e-02,  1.5700e-02,  2.0784e-02,\n",
       "                      -5.1159e-03, -1.0723e-02,  1.4881e-02,  1.5214e-02,  1.8050e-02,\n",
       "                       1.7932e-02,  1.9036e-02,  1.0550e-02,  4.8186e-03,  6.1304e-03,\n",
       "                       1.3763e-02, -6.9763e-03, -1.4023e-02,  1.9316e-02, -1.1575e-02,\n",
       "                       1.3268e-02, -1.2515e-02,  1.3522e-03,  6.9298e-03,  1.8155e-02,\n",
       "                       1.3659e-04, -1.7673e-02,  1.6747e-02,  8.1338e-03, -2.1262e-02,\n",
       "                       1.1697e-03,  6.2510e-03,  1.0447e-02,  5.2362e-03,  5.7210e-03,\n",
       "                      -1.2718e-03, -1.1315e-03,  9.7783e-03,  1.1301e-02,  1.9058e-02,\n",
       "                       8.2059e-03,  1.9673e-02,  1.6166e-02, -5.9793e-04, -7.7103e-03,\n",
       "                      -1.7375e-02,  8.0357e-03, -9.6775e-03,  7.6598e-04,  8.9598e-03,\n",
       "                      -3.0041e-03, -2.2593e-03,  1.7749e-02, -2.2413e-03, -2.2047e-02,\n",
       "                      -1.9656e-02,  1.0989e-02,  1.2407e-02, -1.6272e-02, -1.0330e-02,\n",
       "                      -5.5695e-03,  1.4602e-02,  2.9028e-03, -2.2635e-02, -3.3909e-03,\n",
       "                       1.5721e-02,  2.3120e-02, -1.2798e-03,  2.2009e-02, -9.3337e-03,\n",
       "                       1.9146e-03,  4.4618e-03,  2.2540e-02,  1.6400e-02, -7.1988e-03,\n",
       "                       1.7423e-02,  1.3282e-02,  2.1873e-02,  1.5743e-02, -1.1909e-02,\n",
       "                      -1.9593e-02,  1.9891e-02,  1.9419e-02,  1.5865e-02,  1.3180e-02,\n",
       "                       1.6509e-02,  6.7229e-03, -1.0194e-02,  1.5659e-02, -1.9705e-02,\n",
       "                       1.1314e-02,  1.7812e-02, -2.2459e-02,  7.5248e-03,  9.1068e-03,\n",
       "                       1.4766e-02,  3.1895e-03, -5.8879e-03,  8.8066e-03, -2.7366e-03,\n",
       "                      -4.6823e-03,  1.7506e-02, -1.8959e-02,  1.2891e-02,  1.5334e-02,\n",
       "                       1.6621e-03,  4.7301e-03, -5.7934e-03, -1.8350e-02,  4.9664e-03,\n",
       "                       4.4580e-04, -2.0371e-02, -5.5473e-03, -1.7022e-02, -8.6679e-03,\n",
       "                      -1.0861e-02,  8.3696e-03, -3.9068e-03,  1.1415e-02,  5.4639e-03,\n",
       "                       3.7666e-03,  2.0058e-03,  6.5812e-04,  2.0339e-02,  2.2863e-02,\n",
       "                       1.3557e-02, -2.1282e-02, -1.6661e-02,  2.8513e-03, -8.6451e-03,\n",
       "                      -1.5118e-02, -8.9492e-03, -6.3042e-03, -1.8264e-02, -2.1557e-03,\n",
       "                       1.1821e-02, -1.1662e-02, -1.6129e-02,  2.1142e-02, -1.3449e-02,\n",
       "                      -2.2120e-03,  1.6930e-02,  3.6893e-03,  3.7194e-05, -5.5222e-03,\n",
       "                      -1.5688e-03, -9.3621e-03, -5.1144e-03,  1.6690e-02,  1.6354e-02,\n",
       "                      -1.3592e-02,  4.9145e-03, -5.5035e-03, -4.5252e-03, -4.3751e-03,\n",
       "                      -1.8311e-02, -1.6729e-02,  6.0467e-04,  1.5372e-02,  1.1984e-02,\n",
       "                      -5.5605e-04,  1.9998e-02, -2.4433e-02,  1.2512e-02, -1.2873e-02,\n",
       "                      -1.3957e-02, -8.7496e-03, -1.6302e-02,  8.1679e-03,  1.3372e-02,\n",
       "                      -5.0703e-03,  1.3492e-02,  9.1404e-03,  1.1271e-02, -1.5602e-02,\n",
       "                      -3.3182e-03, -5.3828e-03, -3.8672e-03,  2.0195e-02, -1.9042e-02,\n",
       "                      -1.3063e-02,  1.6067e-02, -6.9351e-03,  1.7557e-02, -9.8479e-03,\n",
       "                      -8.0518e-04,  1.1461e-02, -2.9958e-03, -1.9208e-02,  1.5356e-02,\n",
       "                      -5.5010e-03,  1.7357e-02, -1.1552e-02, -2.2147e-02, -2.3466e-02,\n",
       "                      -6.9565e-03,  1.5793e-02,  1.5664e-02,  1.6774e-02,  2.0971e-02,\n",
       "                      -4.2417e-03, -1.7514e-02,  1.8050e-02, -1.3049e-02,  1.1857e-02,\n",
       "                      -4.6650e-03, -1.3092e-03, -2.1130e-02, -1.7289e-02, -1.9797e-02,\n",
       "                       1.8274e-02,  2.0327e-02, -1.9819e-02,  1.8895e-02, -1.3756e-03,\n",
       "                       9.5653e-03,  5.7632e-03,  5.1888e-04,  1.4526e-02, -1.5052e-02,\n",
       "                       2.3370e-02,  1.1136e-02,  1.6466e-02,  1.2197e-02, -1.4254e-02,\n",
       "                       1.6626e-02,  1.0272e-02,  1.5809e-02,  1.4578e-02,  1.3714e-02,\n",
       "                       5.9540e-03, -7.7550e-04,  2.0382e-02,  9.2402e-04, -6.9004e-03,\n",
       "                       3.1039e-04, -1.2745e-02, -2.0325e-02, -1.8291e-03, -1.1005e-02,\n",
       "                       3.5538e-03,  3.0309e-03, -1.4370e-02,  1.8563e-02,  2.0403e-03,\n",
       "                       1.0716e-02, -5.5673e-03, -8.4243e-03, -7.8701e-03, -1.3654e-02,\n",
       "                      -8.6774e-03, -1.5651e-03, -1.0711e-02, -1.0042e-02, -2.5391e-03,\n",
       "                       1.6916e-02, -2.0346e-03, -2.1085e-02, -2.1297e-02,  8.1231e-03,\n",
       "                      -1.9422e-02,  1.1655e-02, -4.1244e-03, -1.6554e-02,  9.8880e-03,\n",
       "                      -1.1424e-02,  4.1030e-05,  1.0042e-02,  2.0256e-02, -3.6828e-03,\n",
       "                      -1.6749e-02, -4.3920e-03,  1.1353e-02,  1.8793e-02,  1.1024e-02,\n",
       "                       8.8371e-03,  1.1338e-03,  1.2558e-02,  9.5333e-03,  5.7636e-03,\n",
       "                       1.6392e-02,  6.0834e-03,  2.1668e-02, -9.4733e-03, -5.0338e-03,\n",
       "                      -1.8313e-03, -2.1716e-02,  2.3168e-02,  4.4032e-04,  1.6471e-02,\n",
       "                      -1.5926e-02,  1.2098e-03,  2.7813e-03,  1.6144e-02,  1.2215e-02,\n",
       "                      -2.3641e-03, -1.7897e-02, -1.2949e-02, -6.5923e-03,  1.7077e-02,\n",
       "                      -5.6415e-06, -6.9492e-03,  5.5361e-03,  6.1453e-03, -9.2794e-03,\n",
       "                      -1.3834e-02,  1.4012e-02,  4.6694e-03,  1.8115e-02, -2.1806e-02,\n",
       "                       1.0293e-02,  2.2468e-02, -9.7537e-03, -1.6200e-02, -1.1292e-02,\n",
       "                      -5.5068e-03,  1.1505e-02, -1.6263e-03, -8.8299e-04,  1.0110e-02,\n",
       "                      -1.6524e-03, -1.7737e-02, -1.3312e-02, -1.5228e-02,  1.3871e-02,\n",
       "                       3.8842e-03, -8.8679e-03,  1.7775e-02,  1.7968e-02,  1.0917e-02,\n",
       "                       1.6111e-02,  3.6764e-03, -1.5971e-02, -1.6030e-02, -2.6048e-03,\n",
       "                      -8.5848e-03,  1.0080e-02, -1.8167e-03,  1.0220e-02,  6.4939e-03,\n",
       "                      -1.9162e-02,  2.0504e-02,  6.2166e-03, -8.9227e-03, -1.2971e-03,\n",
       "                      -2.2492e-02, -7.8262e-03, -1.7859e-02, -1.1599e-02, -1.3822e-02,\n",
       "                       1.8604e-02,  1.1188e-02,  7.2233e-03, -7.3859e-03,  1.6045e-02,\n",
       "                       2.2091e-03, -7.5225e-03,  4.9921e-04, -2.2155e-02, -1.7904e-02,\n",
       "                       1.8160e-02, -1.1758e-02, -2.2378e-02,  1.9612e-02, -6.5366e-03,\n",
       "                      -1.6161e-02,  5.5155e-03,  1.7024e-02,  7.1478e-03, -7.9722e-03,\n",
       "                       2.0189e-03, -5.4247e-03,  1.3208e-02,  1.5061e-02,  1.4764e-02,\n",
       "                       1.4497e-02, -7.1008e-03,  1.9598e-02,  2.5882e-02,  8.3715e-03,\n",
       "                       1.5412e-02, -8.2332e-03,  2.2013e-02,  2.2643e-02,  1.5311e-03,\n",
       "                       8.2362e-03,  8.1234e-03,  8.1466e-03,  1.3338e-02,  1.4934e-02,\n",
       "                       6.4060e-03,  1.3413e-02, -8.2868e-03, -1.7086e-02,  2.5803e-03,\n",
       "                      -2.0614e-03, -1.4570e-02,  3.2954e-03,  4.8009e-03, -1.0354e-02,\n",
       "                      -2.0329e-03, -8.4972e-03,  1.7939e-02,  9.5257e-03,  6.2259e-03,\n",
       "                      -7.3757e-03, -8.6740e-03, -1.1874e-02, -1.7982e-02, -2.1480e-02,\n",
       "                       6.6930e-03, -6.0872e-03,  1.6493e-02,  2.1069e-02,  1.5428e-02,\n",
       "                       2.0733e-02, -5.5872e-03,  2.1745e-03, -1.6835e-02,  1.4912e-02,\n",
       "                       9.0465e-03,  7.3972e-04, -1.4034e-03,  1.5362e-02, -2.3037e-02,\n",
       "                      -9.8951e-03, -2.1249e-02, -1.6253e-02,  5.0940e-03, -6.9756e-03,\n",
       "                      -9.4507e-03, -1.0201e-03, -1.9022e-02,  1.8077e-02,  9.9246e-03,\n",
       "                      -1.6853e-02, -2.0082e-02])),\n",
       "             ('transformer.resblocks.7.ln_2.weight',\n",
       "              tensor([1.0057, 0.9982, 0.9996, 1.0025, 1.0004, 0.9998, 1.0026, 1.0013, 0.9995,\n",
       "                      1.0026, 0.9975, 1.0010, 1.0004, 1.0015, 0.9959, 1.0023, 1.0017, 1.0030,\n",
       "                      1.0036, 1.0028, 1.0044, 1.0007, 1.0037, 1.0035, 1.0037, 1.0039, 1.0045,\n",
       "                      0.9981, 1.0058, 1.0000, 1.0024, 1.0004, 0.9980, 0.9998, 1.0020, 0.9999,\n",
       "                      1.0006, 1.0053, 1.0004, 1.0017, 1.0035, 1.0047, 1.0038, 1.0022, 0.9966,\n",
       "                      1.0014, 0.9969, 1.0038, 1.0020, 1.0011, 0.9996, 1.0027, 1.0029, 1.0013,\n",
       "                      0.9980, 1.0030, 0.9997, 1.0031, 0.9989, 1.0003, 1.0007, 0.9980, 1.0006,\n",
       "                      1.0026, 1.0025, 1.0024, 1.0013, 1.0023, 1.0020, 1.0031, 1.0015, 1.0003,\n",
       "                      1.0004, 1.0022, 1.0033, 1.0045, 0.9985, 1.0036, 1.0009, 1.0016, 1.0020,\n",
       "                      0.9990, 1.0024, 0.9998, 1.0015, 1.0020, 1.0020, 1.0029, 0.9995, 1.0008,\n",
       "                      1.0009, 1.0042, 1.0003, 1.0040, 1.0003, 0.9973, 0.9983, 0.9983, 1.0017,\n",
       "                      1.0022, 1.0044, 1.0007, 1.0033, 1.0000, 1.0072, 0.9978, 0.9990, 1.0011,\n",
       "                      1.0001, 1.0035, 0.9998, 1.0021, 1.0006, 0.9990, 1.0033, 0.9977, 0.9995,\n",
       "                      1.0009, 1.0030, 0.9998, 1.0030, 1.0020, 1.0024, 1.0041, 1.0031, 1.0011,\n",
       "                      1.0007, 0.9990, 1.0007, 1.0049, 1.0037, 0.9992, 0.9996, 0.9982, 1.0007,\n",
       "                      1.0032, 0.9993, 0.9991, 1.0038, 0.9997, 1.0003, 1.0010, 1.0020, 1.0000,\n",
       "                      1.0038, 0.9984, 0.9993, 0.9984, 1.0041, 1.0020, 1.0004, 1.0007, 0.9964,\n",
       "                      1.0006, 1.0002, 0.9993, 0.9980, 1.0015, 1.0019, 1.0053, 1.0008, 1.0007,\n",
       "                      1.0001, 1.0023, 1.0033, 1.0012, 1.0029, 0.9985, 1.0037, 1.0026, 1.0025,\n",
       "                      1.0015, 1.0012, 1.0054, 1.0006, 1.0005, 0.9999, 1.0035, 1.0036, 0.9995,\n",
       "                      1.0038, 1.0028, 1.0035, 0.9978, 0.9973, 0.9988, 1.0012, 1.0009, 1.0004,\n",
       "                      0.9992, 1.0008, 0.9978, 1.0023, 1.0035, 0.9993, 1.0024, 1.0020, 0.9995,\n",
       "                      0.9992, 0.9991, 1.0021, 1.0008, 1.0032, 1.0045, 0.9989, 0.9996, 1.0017,\n",
       "                      1.0062, 0.9982, 0.9978, 1.0018, 1.0026, 1.0014, 1.0012, 1.0007, 1.0031,\n",
       "                      1.0022, 1.0021, 1.0003, 0.9991, 1.0038, 1.0015, 1.0038, 1.0055, 1.0064,\n",
       "                      1.0030, 0.9990, 1.0027, 1.0026, 0.9985, 1.0014, 1.0000, 1.0039, 1.0030,\n",
       "                      1.0006, 0.9985, 0.9991, 0.9976, 1.0017, 1.0005, 1.0028, 1.0002, 1.0025,\n",
       "                      1.0016, 0.9997, 1.0028, 1.0022, 0.9994, 0.9966, 1.0030, 1.0022, 1.0004,\n",
       "                      0.9983, 0.9967, 1.0016, 1.0024, 1.0009, 1.0004, 1.0011, 0.9984, 1.0032,\n",
       "                      0.9998, 1.0024, 1.0010, 1.0010, 1.0012, 1.0012, 1.0045, 1.0003, 1.0029,\n",
       "                      1.0011, 1.0032, 1.0030, 1.0018, 1.0008, 0.9996, 1.0016, 1.0015, 1.0016,\n",
       "                      1.0011, 1.0011, 0.9991, 1.0006, 1.0035, 1.0028, 0.9990, 1.0019, 1.0002,\n",
       "                      1.0018, 0.9996, 1.0020, 1.0004, 1.0018, 0.9992, 1.0026, 1.0058, 0.9988,\n",
       "                      1.0022, 1.0004, 1.0018, 1.0022, 1.0053, 1.0032, 0.9962, 0.9997, 1.0010,\n",
       "                      1.0010, 1.0047, 0.9986, 0.9990, 1.0017, 0.9969, 1.0048, 1.0059, 1.0029,\n",
       "                      0.9982, 1.0025, 1.0011, 1.0031, 1.0024, 1.0033, 1.0028, 1.0006, 1.0023,\n",
       "                      1.0016, 1.0019, 1.0009, 1.0003, 0.9964, 0.9977, 1.0028, 1.0019, 1.0005,\n",
       "                      1.0042, 1.0008, 1.0049, 1.0065, 0.9987, 1.0030, 1.0018, 0.9992, 0.9996,\n",
       "                      0.9997, 1.0049, 0.9991, 1.0054, 0.9997, 1.0015, 1.0024, 1.0017, 1.0016,\n",
       "                      0.9972, 1.0002, 0.9995, 1.0039, 1.0039, 1.0015, 0.9996, 1.0006, 1.0026,\n",
       "                      1.0047, 1.0018, 0.9991, 0.9998, 1.0028, 0.9972, 1.0045, 1.0002, 1.0003,\n",
       "                      1.0005, 0.9979, 1.0005, 1.0055, 0.9974, 1.0035, 0.9975, 1.0003, 1.0002,\n",
       "                      1.0016, 1.0040, 0.9964, 0.9983, 1.0016, 1.0038, 1.0000, 1.0005, 0.9975,\n",
       "                      1.0016, 1.0023, 1.0018, 1.0016, 1.0036, 1.0027, 1.0022, 0.9992, 1.0027,\n",
       "                      1.0016, 1.0009, 1.0018, 1.0027, 1.0010, 1.0010, 1.0022, 1.0026, 1.0014,\n",
       "                      1.0018, 1.0034, 1.0022, 0.9998, 1.0005, 1.0040, 0.9972, 1.0026, 1.0013,\n",
       "                      1.0018, 0.9993, 1.0033, 1.0020, 1.0001, 0.9995, 1.0006, 1.0000, 1.0043,\n",
       "                      1.0011, 0.9989, 1.0042, 1.0051, 1.0073, 1.0006, 1.0005, 0.9981, 1.0025,\n",
       "                      1.0026, 1.0015, 0.9981, 1.0004, 0.9977, 0.9999, 1.0007, 1.0008, 1.0011,\n",
       "                      1.0015, 1.0043, 1.0038, 1.0018, 0.9990, 1.0003, 1.0015, 1.0021, 1.0027,\n",
       "                      1.0004, 0.9988, 1.0027, 0.9972, 1.0001, 1.0042, 1.0038, 1.0009, 0.9998,\n",
       "                      1.0006, 1.0029, 1.0010, 1.0024, 1.0022, 1.0012, 1.0030, 1.0039, 1.0000,\n",
       "                      1.0053, 1.0013, 0.9991, 1.0009, 0.9984, 1.0057, 1.0007, 1.0033, 1.0033,\n",
       "                      0.9989, 0.9979, 1.0038, 1.0012, 1.0037, 0.9987, 1.0009, 1.0018, 1.0025,\n",
       "                      1.0015, 0.9992, 1.0031, 0.9997, 0.9984, 0.9987, 1.0048, 1.0064, 1.0035,\n",
       "                      1.0017, 0.9985, 1.0032, 1.0004, 0.9999, 1.0004, 1.0025, 0.9959, 0.9980,\n",
       "                      0.9999, 1.0022, 1.0014, 0.9992, 1.0006, 1.0022, 1.0011, 1.0006])),\n",
       "             ('transformer.resblocks.7.ln_2.bias',\n",
       "              tensor([ 1.9120e-03,  1.2492e-03, -5.8806e-04,  8.4214e-04, -6.3248e-04,\n",
       "                       8.7263e-04,  5.6638e-04, -1.6202e-03,  4.6000e-04, -1.5942e-03,\n",
       "                       5.1197e-03, -1.7961e-03,  1.4005e-03,  1.8323e-03,  3.5560e-03,\n",
       "                       1.9761e-04,  9.4254e-04, -3.5774e-04,  1.6101e-03,  2.4259e-04,\n",
       "                      -2.3225e-03, -2.2924e-03, -7.7881e-04, -1.8179e-03, -1.8751e-03,\n",
       "                       1.9847e-03,  2.4645e-04,  8.9932e-04, -1.8609e-03, -1.2666e-03,\n",
       "                      -2.7455e-03, -5.7041e-04, -2.5682e-03,  1.7253e-03, -1.2598e-03,\n",
       "                      -1.2743e-03, -5.7353e-04,  3.3213e-03, -3.9758e-04,  8.5673e-04,\n",
       "                       1.3007e-04,  1.0920e-03, -1.4911e-03, -1.7997e-03,  6.4929e-03,\n",
       "                      -5.2686e-04,  3.0211e-03,  2.0728e-03,  1.9070e-03, -3.8237e-03,\n",
       "                      -1.3407e-04, -1.2122e-03, -1.1095e-04, -4.5896e-05, -4.4074e-03,\n",
       "                      -3.7924e-03,  5.1420e-04,  1.4520e-03,  1.9568e-03,  9.1357e-04,\n",
       "                       3.6880e-03,  1.8749e-03,  5.3206e-04, -5.6108e-04,  1.2993e-03,\n",
       "                       3.0152e-03, -1.3803e-03,  2.9151e-03, -5.1207e-05,  7.1753e-04,\n",
       "                      -1.7015e-03,  2.0345e-04, -7.3411e-04,  1.2014e-03,  1.0551e-03,\n",
       "                      -2.8677e-03,  1.8198e-03, -7.5679e-04,  2.7250e-03,  1.5404e-03,\n",
       "                      -2.4561e-03,  7.5497e-04, -1.4155e-03,  1.5603e-05, -4.9766e-04,\n",
       "                      -1.5393e-03,  2.5770e-03,  3.9162e-04,  1.2604e-04,  6.5901e-04,\n",
       "                      -1.8593e-03, -3.3863e-04,  1.1151e-03,  2.2445e-03, -6.9571e-04,\n",
       "                       1.7082e-03, -4.3673e-04, -2.9651e-04, -8.0461e-04,  5.6500e-04,\n",
       "                      -1.3454e-03,  2.4969e-03,  1.0694e-03, -4.3777e-04, -7.3547e-04,\n",
       "                       1.5813e-03,  2.9946e-03,  3.5659e-04, -5.9267e-04, -3.1411e-05,\n",
       "                      -2.8763e-03, -1.5256e-03, -3.6360e-03, -7.4161e-04, -3.1873e-04,\n",
       "                       4.0024e-03, -9.1847e-04,  1.2296e-03,  1.5498e-03,  1.7173e-03,\n",
       "                       9.1786e-04,  3.5519e-03, -1.1775e-03,  4.0397e-04, -9.0017e-04,\n",
       "                       1.4531e-03,  3.2710e-04,  1.2749e-04, -1.9136e-04,  3.0239e-03,\n",
       "                      -3.7914e-04, -2.5354e-03,  7.2393e-04,  2.2390e-03,  2.7021e-03,\n",
       "                       1.4682e-03,  6.0539e-04, -4.0369e-03, -1.5326e-03, -6.7095e-04,\n",
       "                      -8.7197e-04, -2.3181e-03,  4.6580e-04, -1.0428e-03, -1.8766e-03,\n",
       "                      -3.5411e-03,  1.3414e-03, -1.6377e-03,  2.8507e-03, -2.8273e-04,\n",
       "                      -1.8367e-03, -7.9291e-04, -5.2094e-03, -2.1215e-03,  2.7875e-03,\n",
       "                       2.0541e-03, -1.3425e-04,  1.1698e-03,  3.0444e-04, -2.5967e-03,\n",
       "                       6.1871e-04, -2.4468e-04, -1.2429e-03, -2.2461e-03, -2.0541e-03,\n",
       "                      -1.5016e-03, -1.7050e-04, -2.6099e-04, -3.6723e-04, -5.5588e-04,\n",
       "                      -8.3147e-04, -4.2610e-04, -1.0464e-03,  1.8815e-04, -1.2253e-03,\n",
       "                       1.6486e-03,  1.6133e-03, -1.7371e-03,  8.3811e-04,  2.2423e-03,\n",
       "                       2.8407e-05, -4.6400e-04, -1.4318e-03,  2.9455e-03, -3.7566e-03,\n",
       "                      -1.9562e-04,  2.2581e-03, -2.2640e-03,  1.0601e-03,  1.8086e-03,\n",
       "                      -5.4166e-04,  8.8206e-05, -2.1155e-04, -1.3451e-03,  2.3655e-03,\n",
       "                       5.6030e-04, -8.2784e-04, -2.1690e-03, -2.9484e-03, -2.5504e-03,\n",
       "                       8.0360e-04,  1.2431e-03,  1.9707e-03,  5.4794e-05,  1.7710e-03,\n",
       "                      -2.9335e-03,  5.0606e-04, -3.1616e-03,  5.4617e-04,  2.0311e-03,\n",
       "                      -1.5369e-03,  9.3986e-04, -9.7640e-04, -8.1794e-04,  1.3674e-03,\n",
       "                       2.4387e-03,  1.9163e-03,  1.3560e-03, -1.4945e-04, -1.9651e-04,\n",
       "                       1.0527e-03, -1.4578e-03,  7.3006e-04, -4.4798e-04, -6.7746e-04,\n",
       "                      -4.4904e-04,  4.5711e-03,  1.3912e-03, -1.8513e-03, -3.0024e-03,\n",
       "                       6.6640e-04, -1.5043e-03,  5.0375e-04,  4.4867e-03,  2.3527e-03,\n",
       "                      -1.3393e-03, -3.6365e-03,  2.0285e-03, -3.3149e-03, -2.1530e-03,\n",
       "                       2.7684e-03, -1.9653e-03,  4.6456e-04,  8.9723e-04, -9.2491e-04,\n",
       "                       9.6960e-04, -9.4532e-04, -4.4090e-04,  2.9413e-03, -2.3455e-04,\n",
       "                      -2.9623e-03,  1.2482e-03,  1.7868e-03,  4.8039e-04, -2.3751e-03,\n",
       "                      -7.9053e-05, -9.7774e-04,  7.9209e-04, -2.4311e-03,  3.4949e-03,\n",
       "                      -1.9601e-03, -3.6739e-03, -8.2952e-04, -5.2944e-04,  4.4487e-04,\n",
       "                       3.8432e-04,  1.1598e-03, -6.8343e-05, -8.1613e-04,  3.0847e-03,\n",
       "                      -8.4628e-04,  3.3119e-04,  5.4071e-04, -1.4910e-03,  1.1351e-03,\n",
       "                      -1.1661e-03, -9.5368e-04, -9.0554e-04,  5.3729e-04,  9.2795e-04,\n",
       "                      -9.4316e-04, -2.0839e-03,  1.7247e-03,  9.7889e-04,  2.1270e-03,\n",
       "                      -2.1545e-03, -9.3409e-04,  4.7550e-04, -1.1113e-03,  2.2536e-03,\n",
       "                       1.0413e-04,  9.0252e-05,  2.3343e-03, -2.2110e-03,  2.3065e-04,\n",
       "                      -1.2827e-03,  7.2048e-04,  2.6654e-04, -2.7386e-04,  2.2993e-04,\n",
       "                      -7.8872e-04, -5.0626e-04, -1.2525e-03,  1.8430e-03,  2.2162e-03,\n",
       "                      -1.8582e-03, -7.1528e-04,  1.3029e-03, -2.3908e-03,  2.7339e-03,\n",
       "                      -9.1241e-04, -5.4813e-03,  6.0945e-04, -2.2711e-03, -5.2585e-05,\n",
       "                      -2.6648e-03, -2.2443e-04,  1.0455e-03, -1.0759e-03,  2.3658e-03,\n",
       "                       4.3654e-04,  1.6060e-03,  4.4394e-05, -1.1013e-03,  3.4354e-03,\n",
       "                       1.8467e-03, -3.6484e-03,  2.7788e-03,  5.5702e-03,  1.3736e-03,\n",
       "                      -1.4555e-03,  8.0085e-04, -8.4502e-04,  1.3127e-03,  2.8132e-03,\n",
       "                       1.3430e-03, -2.1127e-03,  1.9602e-05,  2.0588e-03,  1.2084e-05,\n",
       "                       1.3575e-03, -4.1643e-03,  2.8339e-03,  2.8535e-04,  1.3957e-03,\n",
       "                       1.0561e-03,  1.0192e-03, -1.8807e-03,  2.8019e-03,  3.1152e-04,\n",
       "                       2.3694e-03,  2.6905e-03,  3.1683e-03, -5.6225e-04, -1.7797e-03,\n",
       "                      -3.3340e-04,  1.3396e-03, -2.6562e-03, -4.3867e-05, -1.5814e-04,\n",
       "                       3.8643e-04, -2.6260e-04, -2.1966e-03,  8.9152e-04,  7.5778e-04,\n",
       "                      -2.8088e-03,  2.4761e-03,  2.6303e-03, -2.8053e-03,  1.0967e-03,\n",
       "                       2.1238e-03, -3.2309e-04,  7.6329e-04,  8.5271e-04,  1.5480e-03,\n",
       "                      -2.1446e-03, -5.9337e-04, -9.8239e-04,  1.4395e-03, -3.0633e-04,\n",
       "                      -3.5888e-03, -2.1517e-03, -9.4621e-04,  2.2264e-03, -1.0735e-03,\n",
       "                       6.7853e-04, -1.0057e-03, -3.9307e-04, -1.4717e-03, -4.1628e-04,\n",
       "                      -4.2796e-04, -7.2240e-04, -3.5783e-04,  2.1011e-03, -1.7636e-03,\n",
       "                       6.3273e-04,  5.8108e-04, -5.3317e-04, -3.6578e-04,  7.6042e-04,\n",
       "                      -7.3929e-04,  1.5052e-03, -2.2562e-03,  1.9232e-04, -2.7005e-03,\n",
       "                       6.3607e-04,  7.0060e-04,  2.3685e-03, -5.5065e-04, -4.0131e-04,\n",
       "                       4.6369e-04, -4.5697e-04,  4.0042e-05, -1.4553e-03, -6.3581e-04,\n",
       "                       2.3597e-03, -2.2494e-04, -2.3406e-03,  3.5790e-03, -1.6609e-03,\n",
       "                       1.1873e-03, -2.1310e-03, -1.5300e-03,  9.6360e-04,  8.2269e-04,\n",
       "                       5.1708e-05,  8.8518e-04,  2.2833e-03, -2.1042e-03, -1.0700e-03,\n",
       "                       2.8675e-03, -1.3462e-03, -1.0593e-04,  9.5254e-04,  4.5843e-03,\n",
       "                      -3.3589e-03, -3.7919e-03,  2.9196e-03,  5.4165e-04, -2.0252e-03,\n",
       "                       3.7900e-03, -3.3844e-03, -4.9694e-03, -1.4083e-03,  6.0636e-04,\n",
       "                       5.5152e-05,  6.1362e-04, -1.1180e-03,  7.0835e-04, -1.4174e-03,\n",
       "                       1.6602e-03, -1.8444e-03,  1.7963e-03, -2.1529e-03, -1.5856e-03,\n",
       "                       1.5749e-03, -2.4170e-04, -1.2826e-03, -3.7903e-05,  2.8740e-04,\n",
       "                      -1.9094e-03,  3.7335e-04, -1.0764e-03, -2.9125e-03, -2.1148e-03,\n",
       "                       4.2542e-03,  4.7824e-04,  1.7564e-04,  3.9243e-04, -4.5776e-04,\n",
       "                       8.1830e-04,  1.0525e-03,  2.5196e-03,  2.2938e-03,  1.2798e-03,\n",
       "                       1.4187e-03,  1.5529e-05,  2.6297e-03,  3.3451e-03, -6.0974e-04,\n",
       "                       3.0147e-03,  1.8991e-03, -2.9369e-03, -1.1343e-03,  1.1698e-03,\n",
       "                       1.4808e-03,  3.9475e-04,  8.1485e-04,  7.4996e-05, -2.2718e-03,\n",
       "                      -1.1164e-03,  4.9322e-03,  8.3725e-04, -1.9057e-03, -2.8127e-03,\n",
       "                      -2.2218e-03, -1.5169e-03,  1.6441e-03,  2.8409e-04, -6.3511e-04,\n",
       "                       3.4977e-03,  1.2757e-03,  4.4879e-03, -5.7025e-03,  4.0898e-03,\n",
       "                       2.1438e-03,  2.8192e-03, -1.3882e-04, -2.0930e-03,  3.9915e-06,\n",
       "                       1.4398e-03,  1.6673e-04])),\n",
       "             ('transformer.resblocks.8.attn.in_proj_weight',\n",
       "              tensor([[-0.0199, -0.0390, -0.0022,  ..., -0.0306, -0.0280,  0.0313],\n",
       "                      [-0.0130, -0.0180, -0.0315,  ..., -0.0592,  0.0135,  0.0946],\n",
       "                      [-0.0206, -0.0341, -0.0170,  ...,  0.0402, -0.0132,  0.0429],\n",
       "                      ...,\n",
       "                      [ 0.0031,  0.0033,  0.0183,  ..., -0.0062,  0.0098,  0.0114],\n",
       "                      [ 0.0697,  0.0272,  0.0701,  ..., -0.0113, -0.0014, -0.0295],\n",
       "                      [-0.0572, -0.0092,  0.0140,  ...,  0.0052, -0.0153,  0.0539]])),\n",
       "             ('transformer.resblocks.8.attn.in_proj_bias',\n",
       "              tensor([ 0.0043, -0.0022,  0.0048,  ..., -0.0025, -0.0009,  0.0001])),\n",
       "             ('transformer.resblocks.8.attn.out_proj.weight',\n",
       "              tensor([[ 0.0059, -0.0032, -0.0047,  ..., -0.0055,  0.0032,  0.0037],\n",
       "                      [ 0.0069,  0.0075, -0.0084,  ..., -0.0043,  0.0005, -0.0272],\n",
       "                      [ 0.0033,  0.0053,  0.0043,  ..., -0.0025, -0.0042, -0.0060],\n",
       "                      ...,\n",
       "                      [-0.0046, -0.0151, -0.0138,  ..., -0.0019,  0.0065,  0.0005],\n",
       "                      [-0.0104, -0.0096, -0.0082,  ...,  0.0001,  0.0028, -0.0081],\n",
       "                      [-0.0031, -0.0137, -0.0044,  ...,  0.0098,  0.0069, -0.0084]])),\n",
       "             ('transformer.resblocks.8.attn.out_proj.bias',\n",
       "              tensor([-7.5088e-04, -4.1215e-03, -2.7986e-04, -7.5313e-04, -8.7992e-04,\n",
       "                      -3.1680e-03,  4.1795e-04,  2.6085e-03, -3.3220e-03,  1.9130e-03,\n",
       "                      -4.9135e-03, -2.2089e-03, -7.8226e-04, -1.5349e-03, -5.7918e-03,\n",
       "                       2.0038e-03, -2.3169e-04,  1.7661e-03, -1.1575e-04, -6.6859e-04,\n",
       "                      -3.2614e-04,  5.1868e-04,  4.6861e-04, -1.7215e-03, -1.0502e-03,\n",
       "                      -1.0918e-03, -1.1704e-03, -8.6645e-04, -4.4895e-04, -1.6742e-03,\n",
       "                       8.7701e-04,  1.0110e-04,  3.1767e-03, -3.8010e-03, -3.8578e-04,\n",
       "                       1.4436e-03,  1.2265e-03,  4.0943e-04,  4.9981e-04, -2.9316e-03,\n",
       "                       1.6461e-03,  2.1485e-03,  1.8588e-03,  1.3084e-03, -4.1341e-03,\n",
       "                       1.5852e-03, -3.3758e-03,  5.3569e-04, -8.4986e-04,  1.5456e-03,\n",
       "                      -1.3072e-03, -7.2867e-04, -1.4411e-03,  2.7890e-03,  3.0571e-03,\n",
       "                       1.0905e-03, -6.5820e-04, -7.4281e-04, -2.0700e-05, -8.9554e-04,\n",
       "                      -1.7591e-03, -4.2172e-05,  5.1202e-04,  3.5948e-04, -2.3024e-04,\n",
       "                      -7.7941e-04,  1.6460e-03,  1.1297e-03, -8.4512e-04,  2.0354e-03,\n",
       "                      -9.0771e-04, -1.3851e-04, -2.1037e-03, -2.1650e-04, -1.1825e-03,\n",
       "                       2.1632e-03, -2.0603e-03, -1.9447e-03, -2.3338e-03,  2.2761e-03,\n",
       "                       1.4224e-03, -2.2040e-03,  1.0431e-03,  1.9576e-03,  2.1480e-03,\n",
       "                       5.3965e-04, -9.8324e-04,  8.9811e-04,  4.6117e-04, -1.8797e-03,\n",
       "                       6.3118e-04, -8.4910e-04,  1.6309e-03, -1.8920e-03, -2.0496e-03,\n",
       "                      -4.1903e-03,  1.0795e-04, -1.1026e-03, -1.6979e-03,  6.9304e-04,\n",
       "                       8.9130e-04, -1.6642e-03,  2.1926e-04, -7.5152e-04, -9.2006e-04,\n",
       "                      -2.5496e-03, -3.5757e-03, -9.3415e-04,  1.1972e-03, -4.3425e-04,\n",
       "                       3.2998e-03,  1.0092e-03,  2.8292e-03,  1.4283e-03, -1.2632e-03,\n",
       "                      -4.3852e-03,  1.4373e-03,  1.4103e-03, -2.6626e-03, -1.1260e-03,\n",
       "                       5.9041e-04, -1.4949e-03,  1.6817e-03, -5.9769e-04, -1.4060e-03,\n",
       "                       4.9453e-04, -1.6805e-03, -1.6478e-03, -1.6786e-03, -4.8873e-04,\n",
       "                       8.1055e-05,  3.4317e-03, -1.7139e-04, -2.5160e-03, -9.9964e-04,\n",
       "                      -1.2445e-03, -1.5174e-03,  2.3421e-03, -7.2579e-04,  1.0308e-03,\n",
       "                       1.8239e-04,  1.9187e-03,  1.1141e-03,  3.8904e-03,  1.1871e-03,\n",
       "                       2.4316e-03, -2.3469e-03,  7.8041e-04,  8.4203e-04,  4.4440e-04,\n",
       "                       2.0720e-03,  4.6538e-03,  5.9407e-03,  2.0007e-03, -8.6581e-04,\n",
       "                      -3.1707e-04,  1.0019e-03, -1.4777e-03,  3.4953e-04,  4.3186e-04,\n",
       "                      -2.0448e-03, -1.1081e-03,  2.9860e-03,  2.6339e-04, -1.1109e-03,\n",
       "                       1.2473e-03,  2.4881e-03, -6.2657e-04, -3.6149e-04, -1.9706e-03,\n",
       "                       1.3580e-03, -3.5482e-04, -1.8819e-03,  7.5881e-04,  2.0324e-03,\n",
       "                       1.4252e-03, -1.8064e-03,  1.3033e-04,  7.4081e-04, -1.4457e-03,\n",
       "                      -2.5632e-03, -9.1864e-05,  1.1523e-03, -3.1515e-03,  4.4531e-03,\n",
       "                       1.7183e-04,  4.9161e-04, -9.3560e-05,  1.1338e-03, -2.4531e-03,\n",
       "                       5.3039e-04,  8.3936e-04, -5.0972e-04,  1.6954e-03, -1.3612e-03,\n",
       "                      -1.4429e-03,  1.7077e-03,  2.6938e-03,  3.5668e-03,  3.0519e-03,\n",
       "                      -2.7610e-04, -1.3063e-03,  1.6120e-03,  1.9108e-03, -1.0299e-05,\n",
       "                       7.9035e-04, -1.4979e-03,  3.6782e-04,  1.5387e-03,  2.0479e-04,\n",
       "                       2.4463e-03,  4.4679e-04,  2.0351e-04,  1.7215e-03, -2.2351e-03,\n",
       "                      -2.3857e-03,  1.7345e-04, -2.1132e-03, -1.1637e-03,  7.6857e-04,\n",
       "                      -7.4310e-04, -5.7398e-04,  1.7465e-03,  2.0850e-03,  1.0713e-03,\n",
       "                       8.7834e-04, -1.0527e-03, -2.3174e-03,  1.3264e-03,  2.0983e-03,\n",
       "                      -1.7831e-03,  1.2568e-03,  3.8993e-04, -1.0445e-03,  1.3463e-03,\n",
       "                      -2.7488e-05,  2.9013e-03, -3.5317e-03,  1.5285e-03,  1.5055e-03,\n",
       "                      -1.9364e-03,  2.3873e-03,  2.7803e-03,  1.4750e-03, -2.8179e-03,\n",
       "                       2.2766e-06,  5.7687e-04,  6.5108e-04, -5.5292e-04, -1.5659e-03,\n",
       "                       1.1543e-03, -2.0754e-03,  3.1925e-05, -1.3312e-03, -9.0403e-04,\n",
       "                      -1.1197e-03, -3.1332e-04, -2.6012e-03, -1.7880e-04, -3.5430e-03,\n",
       "                       1.1849e-03,  9.7065e-04,  1.2233e-03, -3.2471e-03, -1.7098e-03,\n",
       "                       3.2111e-03,  6.7787e-04,  1.2156e-03, -2.3596e-04, -1.7529e-03,\n",
       "                       7.3365e-04, -1.1942e-03, -1.6633e-03,  4.4752e-04,  1.1460e-03,\n",
       "                      -6.4971e-04,  3.4459e-03, -6.9413e-04,  3.3123e-03, -1.5355e-03,\n",
       "                      -4.0367e-04,  3.8304e-03, -2.2801e-03, -1.7519e-03, -2.5877e-03,\n",
       "                       1.3885e-03,  5.5208e-04,  1.6270e-03,  2.2230e-03,  5.0538e-05,\n",
       "                      -6.1470e-04,  1.5152e-03, -2.2644e-03,  4.2629e-03,  5.3693e-04,\n",
       "                       1.7953e-03, -1.9778e-03,  9.9922e-04, -4.9695e-04, -1.0830e-03,\n",
       "                      -2.7842e-03,  9.2829e-04,  1.4244e-03, -4.1778e-03, -1.5082e-04,\n",
       "                      -2.3097e-03,  5.1455e-04,  9.2164e-05, -1.7145e-03, -2.3479e-03,\n",
       "                       2.0055e-03,  3.0175e-03, -7.8908e-04, -1.8762e-03,  1.0212e-03,\n",
       "                       2.5633e-03,  3.1657e-03,  1.2033e-03,  8.1420e-04,  8.9397e-04,\n",
       "                       2.2561e-03, -6.6971e-04, -1.0839e-03,  3.6879e-04, -2.6843e-03,\n",
       "                       4.4143e-04,  3.2669e-03,  9.7305e-05, -3.5577e-03, -2.0503e-03,\n",
       "                      -4.0241e-04,  1.3312e-03,  5.9867e-04, -9.5821e-04,  1.4606e-03,\n",
       "                      -3.9150e-04, -8.3486e-05,  3.0661e-05,  7.7368e-04,  2.4957e-03,\n",
       "                      -3.1554e-03,  3.1509e-03,  1.5540e-04, -6.2749e-04, -3.3949e-03,\n",
       "                      -2.3094e-05, -2.3799e-03, -2.3954e-05, -6.9617e-04,  1.4589e-03,\n",
       "                      -1.7219e-03, -4.0343e-03, -2.0929e-03, -2.8055e-04, -5.9420e-05,\n",
       "                       6.7868e-04,  3.2262e-03,  2.5852e-03,  9.1279e-04, -2.6120e-03,\n",
       "                       1.3758e-03, -5.9323e-04,  2.6597e-03,  3.1406e-03, -6.9335e-05,\n",
       "                       2.9350e-03, -5.1246e-04, -4.2060e-03,  1.9819e-03, -8.3635e-04,\n",
       "                      -1.4163e-03,  1.3663e-03,  6.7933e-04, -5.5298e-04,  5.8443e-05,\n",
       "                       3.4074e-03, -9.6412e-04,  2.5697e-03, -1.6236e-03, -1.8739e-03,\n",
       "                       2.5361e-03,  2.2433e-03, -1.6707e-03, -9.3940e-04, -1.2647e-03,\n",
       "                      -7.6588e-04,  2.6858e-03, -7.3159e-04,  3.4922e-03,  1.9223e-03,\n",
       "                      -9.7492e-05, -9.6776e-05, -2.0691e-05,  9.2268e-04,  2.9894e-03,\n",
       "                       2.2615e-03, -1.0317e-03,  2.0414e-03,  2.9349e-04,  2.0795e-04,\n",
       "                      -3.5416e-04, -1.5311e-03, -4.4431e-04, -5.4772e-04, -5.4872e-04,\n",
       "                       1.6816e-03,  1.2324e-03,  8.4623e-04,  1.7830e-03,  1.7781e-03,\n",
       "                      -6.0813e-04, -3.0706e-03,  4.5846e-04,  2.9127e-03,  1.0635e-03,\n",
       "                      -2.4559e-03,  1.9201e-03,  1.7880e-03, -4.9191e-04,  2.6120e-03,\n",
       "                      -1.3095e-03,  1.0637e-03,  1.5843e-03, -1.8324e-03, -1.7486e-03,\n",
       "                       1.8175e-04, -5.4948e-04, -4.7101e-04,  1.3786e-03,  4.3408e-04,\n",
       "                      -2.2917e-03,  1.2766e-03,  1.4910e-03, -1.1825e-03, -3.2026e-03,\n",
       "                      -1.1818e-03,  1.7177e-03, -2.6488e-03,  1.7486e-03, -1.1486e-03,\n",
       "                      -1.1773e-03,  1.1026e-03,  6.3230e-04, -1.6026e-04, -2.0645e-03,\n",
       "                      -3.0014e-03, -5.8258e-04,  1.2579e-03, -8.9603e-05,  1.5746e-03,\n",
       "                       3.6639e-04,  9.5191e-04, -1.1475e-03,  4.8608e-03,  1.4041e-03,\n",
       "                      -1.8979e-03,  6.0331e-04,  2.2175e-03,  2.2810e-03,  3.7191e-04,\n",
       "                       8.5956e-04, -8.7847e-04,  1.1989e-03,  3.4148e-03,  5.7100e-04,\n",
       "                       1.8740e-04,  1.7292e-04,  1.0939e-03, -1.8261e-05, -1.9975e-03,\n",
       "                      -2.0012e-03,  1.4140e-03, -3.2583e-03, -1.2976e-03, -1.9569e-03,\n",
       "                       7.9959e-04, -2.8244e-03, -1.7611e-03, -1.5449e-03, -1.2937e-03,\n",
       "                      -2.9809e-03, -1.7163e-03,  1.2332e-03,  6.8275e-04, -1.2549e-03,\n",
       "                       5.7204e-04, -1.1632e-03, -2.0352e-03,  2.4815e-03,  1.5128e-03,\n",
       "                       1.2496e-03, -1.0011e-03,  1.2965e-03, -2.4007e-03, -9.7461e-04,\n",
       "                       5.5440e-04,  1.6628e-03, -2.1075e-04, -8.2909e-05, -3.2202e-03,\n",
       "                      -1.4370e-03,  8.1343e-04, -1.6466e-03,  3.1991e-03, -3.1078e-03,\n",
       "                       7.4268e-04, -1.7794e-03, -1.3222e-03,  1.0766e-03, -4.0325e-04,\n",
       "                      -1.6060e-03, -4.4681e-04])),\n",
       "             ('transformer.resblocks.8.ln_1.weight',\n",
       "              tensor([0.9985, 1.0016, 0.9978, 1.0005, 0.9993, 0.9994, 1.0020, 0.9987, 0.9985,\n",
       "                      0.9967, 0.9996, 1.0017, 1.0007, 0.9980, 0.9976, 1.0001, 1.0007, 1.0040,\n",
       "                      0.9979, 0.9991, 0.9997, 0.9988, 1.0027, 1.0007, 0.9997, 0.9985, 0.9988,\n",
       "                      0.9989, 1.0007, 0.9999, 0.9982, 1.0006, 0.9980, 0.9995, 1.0001, 0.9978,\n",
       "                      0.9980, 0.9990, 0.9964, 1.0027, 1.0009, 0.9967, 0.9995, 0.9996, 0.9998,\n",
       "                      1.0034, 1.0012, 0.9978, 0.9999, 0.9981, 0.9986, 0.9992, 1.0010, 0.9963,\n",
       "                      1.0007, 0.9976, 0.9999, 0.9994, 0.9973, 0.9986, 1.0011, 0.9979, 1.0044,\n",
       "                      0.9972, 0.9997, 0.9973, 0.9966, 0.9980, 0.9999, 1.0000, 0.9990, 1.0048,\n",
       "                      0.9990, 0.9991, 0.9964, 1.0013, 1.0012, 0.9984, 1.0015, 0.9994, 0.9998,\n",
       "                      0.9979, 1.0007, 1.0010, 0.9980, 1.0006, 0.9997, 0.9988, 1.0009, 0.9997,\n",
       "                      1.0014, 0.9982, 1.0019, 0.9997, 0.9992, 0.9986, 0.9990, 1.0023, 1.0013,\n",
       "                      1.0000, 0.9998, 1.0033, 0.9985, 1.0002, 0.9969, 0.9989, 0.9961, 1.0001,\n",
       "                      0.9966, 1.0004, 0.9982, 0.9988, 0.9972, 0.9989, 0.9989, 0.9981, 1.0018,\n",
       "                      0.9992, 0.9994, 1.0015, 0.9983, 1.0012, 0.9969, 0.9983, 0.9995, 0.9994,\n",
       "                      0.9978, 1.0003, 0.9997, 0.9987, 1.0009, 0.9990, 0.9995, 1.0020, 1.0021,\n",
       "                      0.9974, 0.9995, 0.9999, 1.0010, 0.9992, 0.9997, 1.0004, 0.9996, 1.0005,\n",
       "                      1.0016, 0.9983, 0.9988, 0.9974, 0.9970, 0.9996, 0.9970, 0.9969, 1.0006,\n",
       "                      1.0003, 1.0012, 1.0026, 0.9989, 0.9989, 0.9975, 0.9996, 0.9966, 0.9986,\n",
       "                      0.9996, 1.0002, 0.9981, 0.9985, 1.0017, 0.9998, 1.0003, 0.9997, 1.0003,\n",
       "                      0.9983, 1.0007, 0.9967, 1.0001, 0.9997, 0.9997, 1.0019, 0.9982, 1.0002,\n",
       "                      1.0016, 1.0000, 1.0003, 1.0012, 0.9979, 1.0004, 0.9988, 0.9977, 0.9971,\n",
       "                      0.9997, 1.0011, 1.0023, 0.9981, 0.9994, 0.9997, 0.9978, 1.0025, 0.9971,\n",
       "                      0.9960, 1.0005, 0.9975, 0.9997, 0.9983, 1.0036, 0.9994, 0.9978, 1.0005,\n",
       "                      0.9989, 0.9923, 1.0002, 1.0032, 1.0009, 1.0035, 0.9968, 0.9991, 0.9988,\n",
       "                      0.9994, 0.9993, 1.0010, 0.9998, 0.9998, 1.0004, 0.9987, 0.9992, 0.9998,\n",
       "                      1.0013, 0.9971, 0.9961, 1.0023, 0.9987, 0.9963, 1.0008, 0.9987, 0.9983,\n",
       "                      1.0018, 0.9997, 0.9996, 0.9986, 1.0008, 1.0001, 0.9998, 1.0007, 1.0013,\n",
       "                      0.9983, 0.9982, 1.0045, 1.0023, 0.9978, 0.9995, 1.0006, 1.0023, 0.9989,\n",
       "                      1.0011, 0.9979, 1.0004, 0.9988, 1.0002, 0.9982, 1.0006, 0.9969, 0.9996,\n",
       "                      0.9982, 0.9982, 0.9999, 1.0005, 0.9979, 1.0026, 1.0028, 0.9981, 1.0006,\n",
       "                      0.9969, 0.9998, 1.0003, 1.0046, 0.9999, 1.0008, 1.0011, 1.0031, 0.9970,\n",
       "                      1.0047, 0.9965, 0.9956, 0.9990, 1.0000, 1.0023, 0.9997, 0.9987, 0.9979,\n",
       "                      0.9992, 0.9990, 1.0014, 1.0012, 0.9958, 0.9958, 0.9999, 0.9991, 0.9994,\n",
       "                      1.0015, 0.9979, 1.0002, 0.9992, 0.9995, 0.9993, 0.9991, 1.0035, 1.0012,\n",
       "                      1.0001, 0.9984, 1.0014, 0.9991, 0.9992, 1.0008, 0.9996, 1.0009, 1.0016,\n",
       "                      0.9984, 0.9982, 1.0006, 0.9986, 0.9977, 1.0016, 1.0010, 0.9979, 0.9987,\n",
       "                      1.0013, 1.0014, 0.9987, 0.9996, 1.0004, 1.0031, 1.0019, 1.0006, 1.0016,\n",
       "                      0.9988, 0.9982, 0.9973, 1.0009, 0.9986, 0.9977, 0.9977, 0.9995, 1.0008,\n",
       "                      0.9987, 0.9982, 0.9980, 0.9991, 1.0003, 1.0020, 0.9986, 1.0003, 0.9983,\n",
       "                      1.0026, 1.0007, 0.9999, 1.0007, 1.0004, 0.9999, 1.0019, 1.0013, 0.9989,\n",
       "                      1.0006, 0.9995, 1.0008, 0.9992, 0.9964, 1.0019, 0.9993, 1.0026, 0.9968,\n",
       "                      0.9980, 0.9967, 1.0021, 0.9998, 1.0002, 1.0017, 1.0011, 0.9992, 0.9985,\n",
       "                      1.0008, 0.9965, 0.9986, 1.0015, 0.9999, 0.9981, 0.9999, 0.9977, 0.9982,\n",
       "                      0.9978, 0.9978, 1.0002, 1.0005, 1.0000, 1.0025, 1.0007, 0.9985, 0.9993,\n",
       "                      0.9985, 1.0007, 0.9988, 1.0001, 0.9989, 0.9998, 0.9989, 1.0002, 1.0066,\n",
       "                      1.0004, 0.9996, 1.0001, 1.0004, 0.9983, 0.9998, 0.9976, 1.0001, 0.9976,\n",
       "                      0.9997, 0.9975, 0.9972, 1.0035, 1.0026, 0.9997, 0.9996, 0.9981, 1.0009,\n",
       "                      0.9996, 1.0008, 1.0004, 1.0006, 0.9996, 0.9989, 0.9977, 0.9985, 0.9975,\n",
       "                      0.9989, 0.9995, 0.9999, 0.9992, 1.0020, 1.0020, 1.0002, 0.9964, 0.9990,\n",
       "                      0.9991, 0.9983, 0.9977, 1.0000, 0.9981, 0.9965, 1.0024, 1.0003, 0.9992,\n",
       "                      0.9968, 1.0011, 1.0000, 0.9981, 0.9976, 1.0018, 1.0002, 0.9997, 1.0001,\n",
       "                      1.0010, 1.0018, 1.0012, 1.0003, 1.0019, 0.9999, 1.0020, 0.9977, 1.0002,\n",
       "                      1.0014, 1.0033, 0.9998, 0.9999, 0.9986, 1.0011, 1.0013, 1.0025, 1.0001,\n",
       "                      0.9990, 0.9994, 1.0019, 1.0001, 0.9994, 0.9995, 0.9993, 1.0005, 1.0026,\n",
       "                      0.9973, 1.0002, 1.0002, 1.0027, 1.0014, 1.0022, 0.9993, 0.9996, 0.9990,\n",
       "                      0.9997, 0.9997, 1.0016, 0.9989, 0.9991, 0.9996, 0.9977, 1.0004, 0.9974,\n",
       "                      0.9986, 0.9981, 1.0009, 1.0002, 0.9994, 1.0015, 0.9993, 0.9998])),\n",
       "             ('transformer.resblocks.8.ln_1.bias',\n",
       "              tensor([ 1.7554e-03, -3.4255e-04, -1.2469e-03,  5.1212e-04,  1.7684e-03,\n",
       "                       3.7930e-04,  2.4636e-03, -8.8878e-04, -1.7339e-04,  7.1302e-04,\n",
       "                       9.9177e-04, -1.8310e-03,  4.7154e-04,  2.7610e-03,  1.0618e-03,\n",
       "                      -9.1348e-04,  2.8281e-03,  1.9481e-03,  4.1020e-04, -7.8919e-04,\n",
       "                       1.3362e-03, -4.1136e-04,  2.9831e-03, -7.5625e-04, -2.0958e-03,\n",
       "                       1.5424e-03, -1.5263e-04, -1.5481e-03,  1.4259e-03, -4.4752e-04,\n",
       "                       2.5212e-03,  3.5393e-03, -2.0387e-05, -7.8517e-04, -2.5678e-03,\n",
       "                       8.0045e-04, -2.4182e-03,  2.2535e-04, -2.5302e-03, -6.0511e-04,\n",
       "                      -8.6176e-04, -3.4416e-04,  1.5165e-03, -3.8960e-04, -4.0563e-04,\n",
       "                       3.7125e-04, -1.2790e-03,  1.8602e-03, -1.3851e-03,  7.0339e-04,\n",
       "                       1.8849e-04,  6.2753e-04, -7.6842e-04, -2.7325e-03,  2.8809e-04,\n",
       "                       3.3790e-03,  8.5420e-04, -1.0227e-03,  1.1467e-03, -1.3062e-03,\n",
       "                      -8.7341e-04, -1.5504e-03, -1.0022e-03,  1.7087e-03, -9.2086e-04,\n",
       "                      -3.1408e-03, -2.3934e-03,  1.7149e-03,  2.9036e-03, -1.3921e-03,\n",
       "                       4.8849e-04,  2.6091e-03, -1.3422e-03, -1.0214e-04, -5.1097e-04,\n",
       "                      -6.9969e-04, -2.9229e-03,  1.1370e-03,  1.2797e-03, -2.3368e-03,\n",
       "                       2.4474e-03,  3.9452e-03, -2.9240e-03,  3.3970e-03, -3.4015e-03,\n",
       "                      -1.8396e-03,  9.0683e-04,  2.0714e-03,  1.8678e-03,  1.5060e-03,\n",
       "                      -1.9325e-03, -7.4419e-04, -1.9918e-03,  1.5872e-03,  2.6943e-03,\n",
       "                       2.0149e-03,  3.1870e-03,  1.3759e-03, -1.1122e-03,  2.3130e-03,\n",
       "                      -5.6304e-04, -1.9052e-03,  5.3025e-04,  3.8221e-04,  8.2987e-04,\n",
       "                      -1.0333e-03,  2.6341e-03, -4.6176e-04,  6.7071e-04,  2.8713e-03,\n",
       "                      -9.0422e-04,  2.3176e-03, -3.5710e-03,  1.0484e-04, -5.2454e-04,\n",
       "                       1.5137e-03,  2.9843e-03,  5.2676e-04,  1.1721e-04, -4.7385e-04,\n",
       "                      -6.0900e-04,  1.3476e-03,  2.5883e-03, -6.1227e-04, -1.3598e-03,\n",
       "                       2.1363e-03,  6.4574e-04, -6.6287e-04,  1.9602e-03,  5.5521e-04,\n",
       "                      -1.5110e-03,  3.7905e-04, -1.6277e-03,  5.0070e-04, -1.5905e-03,\n",
       "                       1.8757e-03,  1.7117e-03,  6.0522e-04,  6.5585e-04,  3.2475e-03,\n",
       "                       1.9368e-03,  3.0950e-03,  1.1897e-03,  1.7249e-03, -4.4579e-05,\n",
       "                      -2.4648e-03, -6.7771e-04, -2.4356e-03,  5.8244e-04, -1.1809e-03,\n",
       "                      -1.8412e-03, -2.6032e-03,  1.1208e-03,  1.2068e-03,  1.9758e-03,\n",
       "                       5.6659e-04,  2.3635e-03,  2.1455e-03,  4.3871e-03,  1.5136e-03,\n",
       "                       2.4384e-03,  5.0318e-04, -8.5749e-04, -2.2416e-03, -1.2985e-03,\n",
       "                      -1.5042e-03,  2.6065e-03, -2.0367e-03,  1.8541e-04, -8.1939e-04,\n",
       "                       1.1067e-03,  1.7870e-03,  3.7164e-03,  2.8775e-04, -4.9225e-04,\n",
       "                      -1.3708e-03,  1.7894e-03,  2.7499e-03,  1.4065e-03,  2.6756e-03,\n",
       "                      -1.4340e-03,  2.3279e-03,  5.9727e-04, -3.5028e-03, -2.2691e-03,\n",
       "                       6.7652e-04, -2.5652e-03, -1.1015e-03, -1.9202e-03, -1.8654e-03,\n",
       "                       1.7592e-03, -7.6925e-04, -7.9422e-04, -1.6563e-03,  1.9041e-03,\n",
       "                       4.8240e-03,  1.1815e-03, -2.8695e-03, -2.3707e-03,  7.3021e-04,\n",
       "                       7.3322e-04, -8.2270e-04, -3.0806e-03,  2.5779e-03, -1.2678e-03,\n",
       "                      -7.9720e-04,  4.0845e-03, -1.3217e-03, -6.5619e-04, -2.6962e-03,\n",
       "                      -1.6351e-03,  1.4906e-03, -9.1845e-04, -1.6578e-03, -1.3483e-03,\n",
       "                      -1.9218e-03, -1.5391e-03, -7.8243e-04,  3.0657e-04,  4.4955e-04,\n",
       "                      -5.1165e-04,  1.9941e-03,  7.3734e-04, -1.1146e-03,  6.3176e-04,\n",
       "                       1.4303e-03,  7.1342e-04,  1.5370e-03, -2.0783e-03, -5.7946e-04,\n",
       "                       3.0715e-04, -1.5821e-04,  1.6664e-03,  4.5140e-04, -2.0370e-05,\n",
       "                      -8.0454e-04,  1.8656e-03,  1.7253e-03,  1.3343e-03, -1.7120e-03,\n",
       "                      -2.6997e-04, -1.6835e-03, -1.7315e-03, -3.8691e-04,  2.2078e-03,\n",
       "                      -1.7352e-03, -1.2119e-03,  4.3650e-04, -8.2815e-05,  3.0438e-03,\n",
       "                      -9.9277e-04, -3.6549e-04,  2.4048e-04,  8.9315e-04, -2.3770e-03,\n",
       "                      -6.0113e-05,  1.5078e-03,  1.2127e-03,  9.2260e-04, -2.2677e-04,\n",
       "                       2.8800e-03, -1.4244e-03,  7.1246e-04, -1.7678e-04, -2.9943e-03,\n",
       "                      -3.1375e-03,  5.2665e-04,  2.0186e-03, -6.2666e-04, -1.6452e-03,\n",
       "                       5.1436e-03,  5.1936e-04, -1.9691e-03,  3.3277e-03,  1.2102e-03,\n",
       "                      -1.4804e-03,  2.0044e-03, -3.4629e-04, -1.3353e-03,  2.3291e-03,\n",
       "                      -1.8288e-03,  9.5517e-04,  4.2138e-05, -3.1758e-03, -2.1357e-03,\n",
       "                      -8.0902e-04, -9.4339e-04, -3.3955e-04, -3.3575e-04, -1.7946e-03,\n",
       "                       3.5585e-03,  2.0134e-03,  1.2291e-03, -2.9829e-03,  1.7386e-03,\n",
       "                      -2.8437e-03, -1.3730e-03, -9.2133e-04, -1.6294e-03, -8.7390e-04,\n",
       "                       7.3660e-04,  1.4752e-03, -1.9162e-03, -7.0156e-04, -3.7347e-03,\n",
       "                      -2.5022e-03, -2.0762e-04,  4.0883e-04, -3.2200e-03,  1.2694e-03,\n",
       "                      -2.9907e-03, -2.1447e-03, -2.5335e-03, -6.6748e-04,  2.0514e-03,\n",
       "                      -3.4885e-04, -1.4504e-03,  2.2549e-03,  2.3646e-03, -4.9175e-05,\n",
       "                       1.5952e-03, -3.7937e-03, -1.6541e-04,  5.9269e-04,  2.0899e-03,\n",
       "                       1.7578e-03,  5.5904e-04,  2.9900e-04, -2.1545e-03, -2.9481e-03,\n",
       "                       1.6211e-03,  1.7964e-03,  1.1621e-03, -1.0329e-03, -1.1267e-03,\n",
       "                      -1.2000e-03,  4.6054e-04,  2.2313e-03, -4.9175e-04,  4.3168e-04,\n",
       "                      -1.6278e-03,  1.0319e-03,  2.5757e-03,  8.5052e-04,  8.1947e-06,\n",
       "                       1.6906e-03, -4.3064e-04,  9.0541e-04, -2.0326e-04,  5.4492e-04,\n",
       "                      -2.5906e-03, -3.6587e-03, -2.5103e-03,  2.1978e-03,  1.0132e-03,\n",
       "                      -6.5585e-04, -1.1067e-03,  3.3459e-03,  3.0213e-04, -9.0204e-05,\n",
       "                      -2.3661e-03, -3.3638e-03,  3.2007e-03,  1.4123e-03, -4.7776e-04,\n",
       "                       3.0727e-03,  3.6695e-04, -5.9886e-05, -3.7457e-04, -3.5739e-03,\n",
       "                       2.8971e-03,  6.9539e-04,  9.4288e-05, -1.7535e-03,  1.4389e-04,\n",
       "                      -1.4928e-03,  3.7813e-03, -3.7771e-03, -8.1038e-04, -8.0082e-05,\n",
       "                       3.8727e-04,  1.9237e-03, -7.0319e-04,  3.6197e-04, -2.0821e-03,\n",
       "                      -8.1812e-04, -5.2618e-04,  1.6517e-03, -1.3448e-03, -4.5094e-04,\n",
       "                      -3.2686e-03,  1.3874e-03, -5.2511e-04,  9.4665e-04,  3.8608e-04,\n",
       "                      -4.7459e-04,  2.1502e-03,  1.5141e-03, -4.4034e-04, -6.7343e-04,\n",
       "                      -2.7146e-03, -2.3826e-03,  3.1487e-04, -1.1791e-03, -1.3298e-04,\n",
       "                       2.9529e-04,  4.1617e-04,  2.3376e-03, -8.3989e-04, -7.3513e-04,\n",
       "                       6.9623e-04,  1.1328e-03,  1.7982e-03, -2.0694e-03, -5.8281e-04,\n",
       "                      -1.1198e-05,  2.7226e-04, -2.3765e-03, -1.8782e-03,  8.1995e-04,\n",
       "                       2.7207e-04, -1.9458e-03, -2.9372e-04,  2.5892e-03, -2.8758e-03,\n",
       "                      -8.8319e-04, -2.1169e-03,  1.2426e-03,  9.6645e-04, -1.5097e-03,\n",
       "                      -1.2302e-04,  3.6870e-04,  4.9388e-04, -3.9205e-04, -8.2648e-04,\n",
       "                       1.6654e-03,  3.4462e-03, -2.6696e-03,  2.9715e-03,  3.7751e-03,\n",
       "                      -3.3576e-03,  2.7478e-05, -2.8428e-03,  1.8633e-04, -6.6374e-04,\n",
       "                      -1.9411e-03, -2.1111e-03,  1.4428e-03,  3.4771e-03, -2.0295e-03,\n",
       "                      -2.6866e-03,  1.3360e-03, -3.6227e-03, -1.5911e-03, -1.7382e-03,\n",
       "                      -2.3004e-03,  8.6282e-05,  1.5839e-03,  5.0591e-05,  1.5532e-03,\n",
       "                       2.4694e-04, -2.6621e-03,  2.2665e-03,  2.6214e-03,  1.7056e-03,\n",
       "                       9.1693e-04, -1.4841e-03,  2.3331e-03,  8.2665e-04, -1.2086e-03,\n",
       "                      -9.8235e-04, -2.2228e-03, -1.0828e-03,  2.1091e-03,  9.1162e-05,\n",
       "                       1.2987e-05, -9.8188e-04,  1.3175e-03,  2.7379e-03, -1.4231e-03,\n",
       "                       3.4077e-04,  2.2651e-03, -1.1162e-03,  1.5756e-03, -1.6275e-04,\n",
       "                       2.0570e-03,  4.5783e-04, -1.4666e-03, -8.3990e-04, -2.5423e-03,\n",
       "                       1.3955e-03,  9.4057e-04, -1.2196e-03, -1.6521e-03,  8.7766e-04,\n",
       "                      -2.1678e-03, -1.2072e-03, -3.1242e-04, -5.8134e-05, -1.1689e-03,\n",
       "                      -2.5901e-04, -3.8586e-04,  5.8553e-04, -1.0774e-03,  5.4122e-04,\n",
       "                       7.4513e-04,  5.6596e-04,  9.1702e-04,  4.7898e-04, -1.7284e-03,\n",
       "                      -3.1765e-03,  7.9225e-04])),\n",
       "             ('transformer.resblocks.8.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0098, -0.0310,  0.0159,  ...,  0.0134, -0.0613, -0.0115],\n",
       "                      [ 0.0469, -0.0302,  0.0518,  ...,  0.0108, -0.0337, -0.0401],\n",
       "                      [ 0.0179,  0.0088, -0.0161,  ...,  0.0142, -0.0555,  0.0086],\n",
       "                      ...,\n",
       "                      [ 0.0064,  0.0231, -0.0181,  ..., -0.0105,  0.0243, -0.0124],\n",
       "                      [-0.0283,  0.0682, -0.0346,  ...,  0.0309,  0.0514,  0.0605],\n",
       "                      [ 0.0065, -0.0354,  0.0581,  ...,  0.0263,  0.0254,  0.0388]])),\n",
       "             ('transformer.resblocks.8.mlp.c_fc.bias',\n",
       "              tensor([-0.0154,  0.0229, -0.0074,  ...,  0.0208, -0.0067,  0.0270])),\n",
       "             ('transformer.resblocks.8.mlp.c_proj.weight',\n",
       "              tensor([[-0.0018,  0.0131,  0.0023,  ...,  0.0064,  0.0131, -0.0074],\n",
       "                      [ 0.0047,  0.0050, -0.0075,  ...,  0.0130,  0.0154,  0.0028],\n",
       "                      [-0.0061, -0.0248,  0.0005,  ..., -0.0015,  0.0091, -0.0012],\n",
       "                      ...,\n",
       "                      [-0.0052, -0.0063, -0.0098,  ..., -0.0155, -0.0036, -0.0042],\n",
       "                      [-0.0051, -0.0009, -0.0052,  ...,  0.0046, -0.0169, -0.0048],\n",
       "                      [ 0.0157, -0.0076,  0.0040,  ..., -0.0037, -0.0096,  0.0033]])),\n",
       "             ('transformer.resblocks.8.mlp.c_proj.bias',\n",
       "              tensor([-2.5296e-05,  8.6021e-03,  1.3509e-02, -1.9486e-02, -1.5424e-02,\n",
       "                       4.5935e-03, -1.0283e-02,  3.1246e-03,  1.1389e-02, -1.6186e-02,\n",
       "                       1.0969e-02, -1.7684e-02,  1.2311e-02,  1.1781e-02, -2.4166e-03,\n",
       "                       1.4870e-02,  3.5363e-03,  1.1675e-02,  1.7892e-02, -2.2365e-02,\n",
       "                       1.5376e-02, -5.1089e-03,  1.5077e-02, -4.9583e-03, -2.1563e-02,\n",
       "                      -1.7730e-02, -3.8386e-03, -6.2204e-03, -1.2548e-02,  1.3751e-02,\n",
       "                       1.5446e-02,  1.0139e-02,  1.1244e-02, -2.0505e-03, -3.2003e-04,\n",
       "                       1.4739e-02, -1.4690e-02, -1.4687e-02,  1.0855e-02,  2.2336e-03,\n",
       "                      -2.6166e-03,  2.4343e-02,  9.0913e-03,  2.0481e-02,  9.0070e-04,\n",
       "                      -4.5352e-03, -3.8400e-03,  9.9056e-03, -6.0087e-03, -1.5780e-02,\n",
       "                       1.9703e-02, -1.1155e-02, -1.8136e-02, -4.9052e-04, -5.2416e-03,\n",
       "                       1.1952e-02,  9.5873e-03, -1.4051e-02,  1.1995e-02,  3.2768e-03,\n",
       "                       8.7809e-03,  8.4083e-03, -4.4597e-03, -2.2330e-03,  1.3251e-02,\n",
       "                      -9.5220e-03, -7.3724e-03,  1.3579e-02,  4.5898e-03,  2.1847e-02,\n",
       "                      -1.7694e-02,  5.0799e-03, -1.9598e-02,  1.8166e-02,  3.3949e-03,\n",
       "                      -6.8485e-03, -1.5138e-02, -3.0354e-03, -1.6731e-02, -1.1320e-02,\n",
       "                       1.1755e-02, -5.1039e-04,  2.5000e-03, -1.4587e-02, -9.8266e-03,\n",
       "                      -1.8397e-02, -2.1240e-04,  1.6256e-02,  2.5381e-03,  1.0204e-02,\n",
       "                       8.5116e-03, -2.4179e-03,  4.2320e-03,  9.1121e-03, -2.2982e-02,\n",
       "                      -2.6524e-04, -1.1595e-02, -2.0501e-02, -9.9712e-03,  5.5208e-03,\n",
       "                      -1.2575e-02,  1.3868e-03, -9.3514e-03, -1.3693e-02,  6.8373e-03,\n",
       "                      -1.3376e-02, -9.7235e-03, -1.8643e-03, -1.0536e-02,  1.2372e-02,\n",
       "                       2.1946e-02, -5.7998e-03, -6.0372e-04, -7.9270e-03,  2.9536e-03,\n",
       "                       1.1471e-02, -1.9438e-02, -8.2673e-04, -1.7543e-02, -4.7492e-03,\n",
       "                       2.1362e-02, -4.1490e-03, -1.2381e-03,  5.1900e-03,  1.3324e-02,\n",
       "                       1.5838e-02,  1.9331e-02, -1.2606e-02, -1.8106e-02,  1.8511e-02,\n",
       "                      -5.6298e-03,  1.9445e-02, -1.9334e-02,  1.3539e-02,  1.1533e-02,\n",
       "                      -2.2985e-03, -1.1511e-02,  3.8627e-03, -1.5096e-02, -7.0906e-03,\n",
       "                      -4.9443e-03, -6.6826e-03,  3.0095e-03,  7.3400e-03,  4.1833e-03,\n",
       "                      -3.4438e-03,  6.5548e-03, -3.6550e-03, -1.6677e-02,  1.5536e-02,\n",
       "                       1.3185e-02, -4.2559e-04,  5.8391e-03, -1.7615e-02,  1.8660e-02,\n",
       "                      -1.3060e-02,  4.4043e-03,  3.3244e-03,  1.6191e-02,  7.9887e-03,\n",
       "                      -7.9712e-03,  4.6968e-03, -1.4745e-02, -8.1538e-03,  8.4228e-03,\n",
       "                       1.6311e-02,  8.2582e-03,  1.1071e-03,  6.0352e-05,  6.9719e-03,\n",
       "                      -1.5524e-02,  1.4085e-02, -2.3126e-03,  1.6733e-02,  2.0642e-02,\n",
       "                       2.5641e-03, -1.9750e-02,  7.2296e-03,  4.4264e-03,  2.8410e-03,\n",
       "                       1.0550e-02,  1.8747e-02, -1.0921e-03,  1.2847e-02, -1.3882e-02,\n",
       "                      -1.6426e-02,  1.5133e-02, -1.3916e-02, -4.1368e-03,  1.9288e-02,\n",
       "                       1.2809e-02,  2.1561e-02,  7.3134e-03,  1.5963e-02,  1.5582e-02,\n",
       "                       5.6684e-03,  1.7123e-02,  1.7916e-02,  1.5152e-02, -1.8025e-02,\n",
       "                       2.0785e-02, -2.1441e-02,  1.2084e-02,  8.8457e-03, -7.8324e-03,\n",
       "                      -1.0896e-02,  2.5737e-03,  1.3905e-02, -1.6007e-02, -7.1276e-03,\n",
       "                      -2.4402e-03,  1.8548e-02, -1.9991e-02,  6.5569e-03,  6.1018e-03,\n",
       "                      -9.8370e-03, -7.5330e-03, -2.3999e-02,  4.7469e-03,  1.7039e-02,\n",
       "                       1.0566e-02, -1.0323e-02, -7.1299e-03, -4.8594e-03, -1.1746e-02,\n",
       "                      -1.5483e-02, -1.8939e-02, -7.4793e-03, -1.1454e-03, -7.9531e-03,\n",
       "                       7.6518e-03,  1.1496e-03, -4.5839e-03, -2.0057e-02,  1.0912e-02,\n",
       "                      -1.3454e-02, -1.8296e-02, -4.9068e-03,  5.0681e-04,  2.2901e-02,\n",
       "                       4.3768e-03,  2.1500e-02,  2.5201e-03, -1.5483e-02, -1.4534e-02,\n",
       "                       1.8654e-03, -5.1788e-03,  2.1909e-02, -5.9493e-03, -3.1705e-03,\n",
       "                       1.6816e-02,  7.8604e-03,  2.9902e-03,  1.5535e-02, -7.6229e-03,\n",
       "                      -2.0848e-02, -3.9915e-03, -1.4286e-02, -2.1400e-02,  1.6540e-02,\n",
       "                       7.7310e-03,  1.4569e-02,  1.0788e-02, -1.7900e-02,  6.9386e-03,\n",
       "                      -1.5391e-02,  7.2268e-03,  1.5428e-02,  6.3120e-03, -7.2554e-03,\n",
       "                      -5.6801e-03,  9.0941e-03,  1.1391e-03,  9.3069e-03, -2.5844e-03,\n",
       "                      -1.7281e-02,  1.3072e-02, -6.2806e-05,  2.2382e-02, -3.6676e-03,\n",
       "                       2.1498e-02, -8.7163e-03, -8.9692e-03,  9.1741e-03,  1.2994e-02,\n",
       "                       7.9646e-03, -1.6388e-02,  1.5559e-02, -9.4793e-03,  3.0806e-03,\n",
       "                      -2.1487e-02, -3.5285e-03,  1.7738e-02,  2.6495e-02, -1.8327e-02,\n",
       "                       2.0031e-02, -1.3733e-03,  2.9605e-03, -6.1840e-03, -1.0016e-02,\n",
       "                       2.4413e-03, -4.1577e-03,  4.5541e-03, -3.5612e-03, -1.1554e-02,\n",
       "                      -8.0544e-03,  1.1912e-03,  1.0377e-02,  4.9575e-04, -3.9223e-03,\n",
       "                       1.7815e-02,  1.4421e-02, -1.1417e-02, -4.5244e-03, -1.0392e-02,\n",
       "                      -6.3348e-03, -1.6638e-03,  1.6716e-02, -4.5065e-04, -4.5892e-03,\n",
       "                       1.8934e-02, -2.2016e-02, -9.9505e-03, -1.9585e-02,  6.8153e-03,\n",
       "                       4.4764e-03,  1.5217e-02,  2.4721e-03,  7.5749e-03, -1.2341e-02,\n",
       "                       1.0360e-02,  2.1054e-02, -4.3803e-03, -1.7476e-02,  1.0579e-02,\n",
       "                      -6.3715e-03, -1.3974e-03,  4.7588e-03,  6.4768e-03,  1.6025e-02,\n",
       "                       1.4402e-02, -5.3973e-03,  1.3127e-02, -3.9363e-03,  1.4417e-02,\n",
       "                       2.6110e-03,  1.7072e-02,  1.4200e-02, -7.6260e-03,  9.7839e-05,\n",
       "                       1.9094e-02,  1.4153e-02,  9.9108e-03,  1.3226e-02,  8.5815e-03,\n",
       "                       1.8344e-03, -8.7436e-03, -4.6355e-06,  1.0057e-02,  1.4250e-02,\n",
       "                      -1.6502e-02, -2.1466e-02,  1.6449e-02,  1.8114e-02, -9.5615e-03,\n",
       "                       2.0137e-02, -1.3479e-02, -1.7048e-02, -5.8458e-03, -3.2632e-03,\n",
       "                      -4.1597e-03,  7.9217e-03,  1.3028e-02,  1.9513e-02, -2.1358e-02,\n",
       "                      -4.6091e-03, -1.9996e-02,  1.9680e-02, -7.2644e-03,  1.4588e-03,\n",
       "                       2.0885e-03,  2.3671e-02,  1.1612e-02, -4.3854e-03,  1.4317e-02,\n",
       "                       6.3890e-03, -3.8229e-03,  1.1197e-02,  2.0539e-02,  1.0107e-02,\n",
       "                      -5.6684e-03,  1.4989e-02, -1.8309e-02, -3.0256e-03,  7.9633e-03,\n",
       "                      -1.3849e-02, -2.2256e-02, -7.2263e-03,  9.6629e-03,  1.7325e-02,\n",
       "                      -8.0054e-04,  7.3092e-03, -6.7865e-03,  8.1452e-03, -1.6100e-02,\n",
       "                      -8.9073e-03,  2.4580e-03,  2.2387e-02,  2.2089e-04,  2.2251e-02,\n",
       "                      -1.3657e-02,  7.6111e-03,  6.7229e-03,  2.3329e-02, -1.3540e-02,\n",
       "                      -1.4386e-02, -5.8769e-03, -1.8972e-02, -2.1518e-02, -5.2199e-03,\n",
       "                      -7.4214e-04, -1.0193e-03, -4.2887e-03, -8.7210e-04,  6.4497e-03,\n",
       "                      -1.6150e-02,  3.9284e-03, -1.4470e-02, -9.2229e-03, -1.9891e-02,\n",
       "                      -9.5994e-03,  4.4013e-03,  1.2373e-02,  5.6705e-03, -1.5298e-02,\n",
       "                      -9.2325e-03,  4.2320e-04, -8.1421e-03,  7.3816e-03,  7.0503e-04,\n",
       "                       1.8965e-02,  1.4160e-02, -4.0777e-03, -1.7622e-02,  1.5129e-02,\n",
       "                      -2.2517e-02,  1.0590e-02, -1.4044e-02,  1.7436e-02,  1.2645e-02,\n",
       "                      -1.0009e-02,  1.7115e-02, -6.7669e-03, -6.4543e-03, -3.2441e-04,\n",
       "                       5.7290e-03, -1.2059e-02,  1.1901e-02, -1.2084e-02,  2.1627e-02,\n",
       "                       2.1650e-02,  4.5685e-04,  1.6858e-02,  2.0557e-02, -1.4864e-02,\n",
       "                      -1.9009e-02,  1.2304e-02,  5.2896e-03, -2.4220e-04, -8.4042e-03,\n",
       "                       1.1559e-03, -1.8430e-02, -2.4951e-02,  1.7329e-04, -1.7949e-05,\n",
       "                       9.1971e-03, -2.0937e-02,  7.3502e-03, -1.5045e-02,  1.6585e-02,\n",
       "                      -2.0505e-02, -1.2106e-02,  2.2075e-03, -1.1999e-02,  2.0355e-02,\n",
       "                      -1.6940e-02, -1.8720e-02, -1.0461e-02,  2.1524e-02, -1.5426e-03,\n",
       "                      -9.0384e-03, -9.4214e-03, -1.5787e-02, -1.5093e-02, -2.2314e-02,\n",
       "                       2.0403e-02,  1.7757e-02, -2.0176e-02, -8.2192e-03, -3.7660e-03,\n",
       "                      -2.0168e-02,  9.3564e-03, -1.1622e-02,  1.7172e-02, -1.3025e-02,\n",
       "                       1.7522e-02,  9.0045e-03, -6.8383e-03,  1.9593e-02, -3.7601e-03,\n",
       "                       1.9526e-02, -3.4076e-03])),\n",
       "             ('transformer.resblocks.8.ln_2.weight',\n",
       "              tensor([0.9979, 0.9971, 0.9995, 1.0005, 1.0029, 1.0006, 0.9999, 1.0007, 0.9984,\n",
       "                      0.9983, 1.0000, 1.0009, 0.9988, 0.9993, 0.9973, 1.0004, 1.0024, 1.0013,\n",
       "                      1.0020, 1.0010, 0.9978, 1.0033, 1.0036, 1.0013, 0.9982, 0.9983, 1.0021,\n",
       "                      0.9980, 1.0026, 1.0019, 1.0028, 1.0011, 1.0020, 1.0018, 0.9997, 1.0008,\n",
       "                      1.0009, 1.0028, 1.0017, 1.0002, 0.9999, 0.9988, 1.0005, 1.0004, 0.9993,\n",
       "                      1.0035, 0.9984, 1.0039, 0.9981, 0.9986, 1.0002, 1.0043, 0.9999, 0.9978,\n",
       "                      0.9999, 1.0031, 1.0039, 1.0004, 0.9995, 1.0013, 0.9994, 0.9986, 0.9984,\n",
       "                      1.0007, 0.9979, 1.0005, 0.9994, 1.0016, 1.0021, 1.0008, 1.0043, 0.9992,\n",
       "                      0.9994, 1.0033, 0.9994, 1.0020, 0.9977, 1.0026, 1.0001, 1.0020, 0.9994,\n",
       "                      1.0046, 1.0003, 0.9963, 0.9977, 0.9998, 1.0029, 1.0054, 1.0030, 1.0017,\n",
       "                      1.0012, 1.0009, 1.0025, 1.0033, 1.0031, 0.9980, 1.0000, 0.9995, 0.9986,\n",
       "                      1.0015, 1.0018, 0.9999, 0.9972, 0.9975, 1.0020, 0.9995, 0.9996, 1.0019,\n",
       "                      1.0018, 0.9998, 0.9992, 1.0031, 0.9989, 1.0015, 1.0036, 0.9955, 1.0006,\n",
       "                      1.0017, 0.9994, 0.9990, 1.0011, 1.0011, 1.0026, 1.0024, 1.0038, 0.9981,\n",
       "                      1.0030, 0.9984, 1.0027, 1.0054, 1.0022, 0.9976, 0.9986, 0.9975, 1.0036,\n",
       "                      1.0033, 0.9993, 1.0020, 1.0014, 1.0022, 1.0012, 0.9975, 0.9990, 0.9981,\n",
       "                      1.0019, 0.9998, 0.9996, 1.0012, 1.0050, 1.0027, 1.0011, 0.9934, 0.9953,\n",
       "                      0.9997, 1.0017, 1.0044, 1.0008, 1.0009, 1.0012, 1.0033, 1.0013, 0.9990,\n",
       "                      1.0003, 1.0016, 0.9988, 0.9960, 1.0032, 0.9973, 1.0010, 1.0009, 0.9992,\n",
       "                      0.9994, 1.0032, 1.0026, 0.9999, 1.0015, 1.0017, 1.0014, 1.0018, 1.0008,\n",
       "                      1.0035, 1.0025, 1.0013, 1.0018, 0.9946, 1.0016, 1.0028, 1.0010, 1.0017,\n",
       "                      0.9982, 1.0024, 0.9973, 1.0037, 0.9967, 1.0005, 1.0001, 1.0016, 0.9976,\n",
       "                      0.9973, 0.9980, 1.0012, 0.9992, 1.0039, 0.9994, 1.0007, 1.0000, 1.0014,\n",
       "                      1.0030, 1.0025, 1.0008, 1.0025, 1.0011, 0.9989, 0.9991, 0.9987, 1.0028,\n",
       "                      0.9984, 1.0021, 1.0008, 1.0022, 1.0001, 1.0001, 1.0024, 1.0033, 0.9968,\n",
       "                      1.0036, 1.0027, 0.9967, 1.0000, 1.0036, 1.0010, 1.0019, 1.0020, 1.0012,\n",
       "                      1.0034, 1.0029, 0.9977, 0.9982, 1.0020, 0.9993, 1.0020, 1.0003, 1.0006,\n",
       "                      0.9996, 1.0018, 1.0045, 1.0026, 0.9990, 1.0021, 1.0020, 1.0063, 1.0037,\n",
       "                      1.0013, 1.0012, 1.0006, 1.0015, 1.0031, 0.9993, 0.9994, 0.9957, 1.0011,\n",
       "                      1.0023, 1.0025, 0.9972, 1.0002, 0.9979, 1.0012, 1.0026, 0.9998, 1.0025,\n",
       "                      0.9997, 0.9997, 0.9989, 1.0004, 1.0013, 1.0004, 0.9975, 1.0040, 0.9977,\n",
       "                      1.0005, 1.0014, 0.9996, 0.9971, 1.0030, 0.9999, 0.9968, 1.0018, 1.0006,\n",
       "                      1.0012, 0.9980, 1.0019, 0.9990, 1.0001, 0.9954, 1.0024, 1.0067, 0.9993,\n",
       "                      1.0015, 1.0031, 0.9993, 0.9969, 0.9996, 1.0002, 1.0005, 1.0025, 0.9999,\n",
       "                      1.0001, 1.0048, 1.0002, 1.0006, 1.0041, 1.0012, 1.0024, 1.0039, 1.0037,\n",
       "                      1.0038, 0.9994, 0.9983, 1.0014, 1.0031, 0.9989, 0.9981, 0.9994, 1.0013,\n",
       "                      1.0005, 1.0010, 0.9998, 1.0016, 0.9957, 1.0029, 1.0004, 1.0022, 1.0001,\n",
       "                      1.0004, 1.0002, 1.0010, 1.0023, 1.0011, 1.0031, 1.0008, 0.9988, 1.0011,\n",
       "                      0.9987, 1.0007, 1.0016, 1.0013, 1.0003, 1.0025, 1.0026, 1.0017, 1.0007,\n",
       "                      0.9975, 1.0000, 1.0009, 1.0049, 1.0016, 1.0001, 0.9992, 0.9996, 0.9970,\n",
       "                      1.0039, 1.0014, 0.9985, 0.9990, 1.0017, 1.0005, 1.0037, 0.9958, 0.9974,\n",
       "                      1.0011, 1.0054, 1.0002, 1.0037, 1.0007, 1.0026, 0.9997, 0.9984, 0.9992,\n",
       "                      1.0015, 1.0018, 0.9990, 1.0012, 0.9990, 0.9982, 1.0015, 0.9993, 0.9965,\n",
       "                      1.0011, 0.9978, 0.9986, 1.0030, 1.0042, 0.9978, 1.0020, 0.9989, 1.0012,\n",
       "                      1.0019, 0.9981, 0.9977, 1.0001, 1.0006, 0.9984, 1.0044, 1.0008, 1.0019,\n",
       "                      1.0010, 1.0003, 1.0007, 0.9992, 1.0022, 1.0031, 1.0001, 1.0036, 0.9995,\n",
       "                      0.9993, 0.9999, 0.9999, 0.9998, 1.0008, 0.9972, 0.9993, 1.0018, 1.0025,\n",
       "                      0.9994, 0.9980, 1.0032, 1.0024, 1.0051, 0.9992, 1.0005, 1.0016, 1.0015,\n",
       "                      0.9998, 1.0005, 1.0000, 0.9994, 1.0011, 1.0030, 1.0006, 1.0003, 0.9984,\n",
       "                      0.9995, 1.0021, 0.9988, 1.0025, 1.0009, 1.0006, 1.0023, 1.0026, 0.9994,\n",
       "                      1.0029, 0.9977, 0.9986, 0.9974, 1.0024, 0.9999, 0.9991, 0.9972, 0.9980,\n",
       "                      1.0008, 1.0051, 0.9981, 1.0034, 0.9990, 0.9992, 1.0008, 1.0034, 0.9982,\n",
       "                      1.0027, 0.9976, 1.0002, 1.0043, 1.0008, 1.0020, 0.9993, 0.9995, 1.0039,\n",
       "                      0.9990, 0.9976, 0.9991, 1.0008, 0.9991, 1.0029, 1.0045, 1.0010, 1.0031,\n",
       "                      1.0017, 1.0010, 0.9997, 1.0026, 1.0008, 0.9987, 1.0019, 0.9986, 1.0042,\n",
       "                      1.0033, 1.0010, 1.0021, 1.0003, 0.9985, 1.0019, 1.0064, 0.9996, 0.9992,\n",
       "                      0.9995, 1.0011, 1.0027, 1.0017, 1.0022, 1.0001, 0.9974, 0.9987])),\n",
       "             ('transformer.resblocks.8.ln_2.bias',\n",
       "              tensor([ 2.3011e-03,  2.5127e-03, -1.7098e-03, -9.5917e-04, -1.2735e-03,\n",
       "                      -8.7483e-05,  1.2196e-03, -1.0831e-03, -9.8001e-04, -7.4484e-04,\n",
       "                       1.1132e-03,  4.1934e-03, -3.4075e-04,  7.0580e-04,  1.5773e-04,\n",
       "                      -1.7331e-03, -2.1581e-03, -2.4673e-03,  1.9193e-03,  1.7352e-03,\n",
       "                      -2.4851e-03, -1.2136e-03,  1.2043e-04, -1.9191e-03,  2.4586e-04,\n",
       "                       1.0937e-03,  1.9631e-04,  2.2133e-03,  1.9286e-03,  1.5427e-03,\n",
       "                      -2.0090e-03, -3.8428e-04,  2.5001e-03, -7.3364e-04,  7.7987e-04,\n",
       "                      -1.2650e-03, -2.7140e-03,  7.3956e-04, -2.3368e-03, -7.5897e-04,\n",
       "                      -1.8110e-05, -1.0108e-03, -1.5585e-03, -2.1222e-03,  1.0353e-03,\n",
       "                      -1.4157e-03,  1.4018e-05, -1.6121e-03,  1.5480e-04,  1.2041e-03,\n",
       "                       1.6011e-03, -2.6031e-04, -2.3672e-04, -3.4007e-03,  4.6770e-04,\n",
       "                      -4.7875e-03,  2.7161e-03,  1.5053e-03, -1.3829e-03,  1.5180e-03,\n",
       "                       3.3027e-03, -2.2938e-03,  1.4283e-04, -5.4528e-04, -2.2669e-03,\n",
       "                       5.9171e-05, -2.5232e-03,  1.9072e-04,  1.5001e-03,  2.0051e-03,\n",
       "                      -2.9396e-04,  9.1459e-04,  1.9216e-03, -1.7287e-03, -4.4068e-04,\n",
       "                      -2.3827e-03,  1.5486e-03,  1.9353e-03,  4.9485e-04,  3.2687e-04,\n",
       "                      -2.4475e-03, -7.5320e-06, -1.6034e-04, -2.3269e-03, -4.7197e-04,\n",
       "                       2.6825e-03, -1.9015e-04,  1.2920e-03, -6.4671e-05,  2.5925e-03,\n",
       "                       6.9577e-04,  9.4135e-04,  9.4233e-05,  2.3024e-05, -2.2326e-03,\n",
       "                       1.3020e-03,  2.8989e-03,  2.4743e-04,  2.0695e-03, -9.6136e-04,\n",
       "                       8.5689e-04,  9.9830e-04, -2.7555e-03,  1.1846e-03, -8.3171e-04,\n",
       "                       1.4157e-03,  1.2360e-03, -3.1704e-04,  7.6072e-04,  1.1961e-03,\n",
       "                      -1.2548e-03,  1.4143e-03, -2.6876e-03,  3.2265e-03,  2.2365e-03,\n",
       "                       3.4006e-03, -1.5755e-03, -6.6756e-04,  1.7010e-03,  2.0767e-04,\n",
       "                       2.0364e-03,  4.1720e-03,  8.1718e-05,  2.3296e-03, -6.1511e-04,\n",
       "                      -4.2302e-04, -2.8007e-04,  2.6034e-03, -5.3874e-03, -7.3165e-04,\n",
       "                       7.9796e-04, -1.2762e-03,  1.8492e-03,  4.1444e-03, -3.8130e-03,\n",
       "                      -3.6220e-03,  1.1032e-03,  1.5070e-03,  1.5484e-03, -6.5102e-04,\n",
       "                      -2.9310e-03, -5.7443e-03, -3.0446e-03, -6.1908e-04, -3.5396e-03,\n",
       "                       1.4684e-04,  3.2196e-03, -4.4634e-04, -6.2507e-04, -1.1959e-03,\n",
       "                       9.4363e-04, -6.0624e-03, -4.8345e-03, -1.0864e-03, -7.6390e-04,\n",
       "                      -7.1857e-04,  1.4274e-03,  1.7342e-03, -8.8981e-04,  5.9688e-04,\n",
       "                      -7.1088e-04, -1.6846e-03, -3.0978e-03, -9.0981e-04,  1.9010e-03,\n",
       "                      -1.0041e-03, -6.4508e-04, -9.5461e-04,  2.8803e-04, -2.7789e-03,\n",
       "                      -2.5258e-03, -1.2042e-03,  2.7835e-03,  6.4306e-04, -6.7749e-04,\n",
       "                       9.5592e-04, -4.4445e-04, -3.0641e-03,  2.9518e-03,  2.5887e-04,\n",
       "                      -8.4060e-04,  1.6207e-03, -2.0011e-03,  9.3856e-04, -4.0013e-03,\n",
       "                       1.9947e-03, -1.1795e-03, -1.1101e-03,  2.0091e-05,  3.2921e-03,\n",
       "                      -5.8335e-04, -1.4088e-03, -2.8567e-03, -3.3054e-03,  9.8485e-04,\n",
       "                       1.2550e-03, -1.1250e-03, -2.9060e-03, -6.3516e-03, -2.8632e-04,\n",
       "                       1.5171e-03,  2.4654e-03, -1.3080e-03, -8.0783e-04, -4.8405e-03,\n",
       "                      -1.4247e-03,  9.6909e-04, -4.3409e-03,  1.0504e-03, -1.5964e-03,\n",
       "                      -2.2171e-03, -3.4153e-04,  1.6238e-03, -6.7550e-05,  4.0060e-04,\n",
       "                      -2.2271e-03,  2.7284e-03, -9.3363e-04,  6.9929e-04,  1.2584e-03,\n",
       "                       3.6967e-03,  5.1197e-04, -9.2422e-04, -3.0239e-03, -2.8477e-04,\n",
       "                      -1.1025e-03, -4.7654e-04,  4.2901e-03,  5.1810e-04,  1.9671e-04,\n",
       "                       7.2710e-04,  5.8677e-03, -1.0180e-03,  2.7254e-03, -3.0994e-04,\n",
       "                      -3.0674e-03, -2.0908e-03,  4.4594e-03, -2.3005e-03, -4.2505e-04,\n",
       "                      -1.0046e-03, -2.0580e-03, -7.9234e-05,  2.2607e-03, -2.9511e-04,\n",
       "                      -1.0309e-03, -9.5994e-04, -1.7391e-03, -1.7058e-03,  1.0450e-03,\n",
       "                      -8.4951e-04,  1.8070e-03, -7.6619e-04,  3.3608e-04,  6.7552e-04,\n",
       "                      -4.9295e-04,  1.0356e-03,  1.1623e-04,  7.3234e-04,  1.8644e-03,\n",
       "                      -4.2196e-03, -2.5269e-03,  3.0237e-03,  2.9964e-03,  1.0808e-03,\n",
       "                       2.7707e-04, -1.8622e-03, -3.4797e-03,  1.7023e-03,  2.0536e-04,\n",
       "                      -1.9898e-03,  2.3049e-03,  1.6806e-03, -6.1741e-04, -2.6792e-04,\n",
       "                       8.3595e-04, -2.7336e-03, -1.6403e-03, -5.5917e-03,  2.0565e-03,\n",
       "                      -3.0893e-03, -8.6169e-04,  2.9296e-03, -7.4359e-04,  3.6472e-03,\n",
       "                      -5.0286e-03,  3.1248e-03, -2.4380e-03, -9.8484e-04,  1.8533e-03,\n",
       "                      -1.3626e-03, -2.5225e-03, -5.0611e-04, -3.0911e-03, -1.2588e-03,\n",
       "                      -1.3413e-03,  8.7885e-04, -1.3439e-03,  4.4958e-05,  8.0927e-04,\n",
       "                       2.5100e-03,  1.1108e-03, -2.1967e-03, -1.1868e-03, -1.5497e-03,\n",
       "                       1.9211e-03, -2.9929e-03,  8.0506e-04,  6.9635e-04,  2.5967e-03,\n",
       "                      -8.3096e-04,  8.6623e-04, -1.8483e-03, -9.6527e-04,  1.2019e-03,\n",
       "                       2.2758e-03, -6.4545e-04, -3.1267e-03, -6.0004e-04, -5.5635e-04,\n",
       "                      -4.7371e-03, -3.0549e-06, -4.2792e-04, -1.1816e-03, -5.8148e-04,\n",
       "                       1.0296e-03, -6.2120e-03, -1.3526e-03,  5.4416e-03, -1.3122e-03,\n",
       "                       6.4585e-04,  1.0699e-03, -6.4224e-04, -5.1055e-04,  7.9158e-04,\n",
       "                      -2.2742e-03, -5.0969e-03, -1.1540e-03, -1.2046e-03, -8.2521e-04,\n",
       "                       2.9673e-03, -2.1139e-03, -6.1241e-05,  3.5101e-04,  1.0398e-03,\n",
       "                       2.5963e-03, -1.7563e-03,  1.0135e-03, -4.9608e-04, -2.3422e-03,\n",
       "                       2.1585e-03,  2.1936e-03,  3.4960e-04, -1.6799e-03,  8.1529e-04,\n",
       "                      -1.1467e-03, -1.0628e-03, -2.0779e-03, -1.0394e-03,  1.5767e-05,\n",
       "                       1.0253e-03, -1.7827e-03, -5.4474e-04, -4.3795e-04, -7.0778e-04,\n",
       "                      -3.1101e-04, -4.5856e-04,  5.2532e-03, -2.7651e-03, -1.0200e-03,\n",
       "                       4.4005e-04, -1.3951e-03, -1.9548e-03,  4.8686e-03,  1.3928e-03,\n",
       "                      -3.6204e-03,  1.0028e-04, -1.5836e-03, -8.5770e-04,  3.9760e-03,\n",
       "                      -2.8459e-03,  8.5329e-04,  2.2546e-03, -1.2419e-03,  2.5461e-03,\n",
       "                       1.2835e-03, -3.9443e-03,  2.3949e-03, -4.2037e-03,  1.6183e-03,\n",
       "                      -2.1869e-03,  1.2254e-03,  2.5127e-03,  1.1541e-03, -3.7045e-03,\n",
       "                       7.5064e-04, -9.5913e-04, -1.5132e-03,  7.5692e-04, -1.0666e-03,\n",
       "                      -2.8910e-03,  4.4359e-03, -2.6082e-04, -2.4501e-04, -2.5003e-03,\n",
       "                      -8.7393e-04, -3.6243e-04, -4.3460e-03, -8.7239e-04, -2.4072e-03,\n",
       "                      -2.5282e-03, -2.2690e-04, -7.7598e-05, -1.6894e-03, -1.3158e-03,\n",
       "                       2.1724e-03, -4.4613e-03, -1.5624e-03, -1.6514e-03, -2.4076e-03,\n",
       "                       1.3766e-03, -2.8732e-03, -2.0709e-03,  3.1379e-03,  1.8511e-03,\n",
       "                      -2.2130e-03, -3.2951e-03,  1.8696e-03,  2.5044e-04, -1.0773e-03,\n",
       "                      -4.0514e-04, -1.4749e-03,  1.5049e-03, -8.6080e-04,  3.4484e-03,\n",
       "                       1.9927e-04, -9.5371e-04,  4.1490e-03, -3.0351e-03,  7.6022e-04,\n",
       "                      -6.5889e-04,  4.8237e-05, -1.3898e-03,  1.3811e-03,  1.7152e-03,\n",
       "                      -1.2794e-03, -6.9724e-04,  2.3479e-03,  2.7309e-03, -2.6751e-03,\n",
       "                      -4.3004e-03, -4.4666e-03,  1.3423e-03, -2.0584e-03,  2.7461e-04,\n",
       "                       2.2630e-03, -2.7613e-03, -2.8348e-03, -2.8607e-03,  3.6467e-03,\n",
       "                      -1.7426e-03,  2.0524e-03,  1.2506e-03, -4.0578e-03, -4.2209e-05,\n",
       "                       7.7902e-04, -1.7668e-03, -2.1940e-03,  1.8394e-04,  3.1900e-03,\n",
       "                       9.7551e-04,  3.1049e-04, -1.4804e-03,  2.5510e-03,  4.3037e-03,\n",
       "                       1.7963e-03, -2.8899e-04, -7.3728e-04,  9.0286e-04, -1.3698e-03,\n",
       "                       3.2556e-03,  5.3199e-03,  7.1726e-04,  1.0728e-03,  6.2453e-04,\n",
       "                       1.0623e-03, -2.7081e-04,  2.6997e-03, -5.4729e-04,  5.0365e-05,\n",
       "                      -1.0921e-03,  3.9381e-04, -2.3578e-04,  1.0873e-03,  3.6798e-03,\n",
       "                       2.7517e-03,  1.2051e-03,  1.7532e-03,  6.4707e-04,  2.5986e-03,\n",
       "                       5.2898e-03,  1.6435e-03,  1.7517e-03, -2.2567e-03, -7.5296e-04,\n",
       "                      -2.7974e-03,  2.2436e-03, -4.8046e-03, -1.4909e-03, -6.2556e-04,\n",
       "                       5.5841e-03,  6.1170e-04])),\n",
       "             ('transformer.resblocks.9.attn.in_proj_weight',\n",
       "              tensor([[ 0.0071,  0.0530, -0.0220,  ..., -0.0666,  0.0309, -0.0225],\n",
       "                      [ 0.0244,  0.0708,  0.0133,  ...,  0.0113, -0.0081,  0.0320],\n",
       "                      [ 0.0296,  0.0087,  0.0470,  ...,  0.0020,  0.0155,  0.0470],\n",
       "                      ...,\n",
       "                      [-0.0092,  0.0050,  0.0205,  ...,  0.0542,  0.0326,  0.0224],\n",
       "                      [-0.0479,  0.0339, -0.0130,  ...,  0.0269,  0.0277, -0.0006],\n",
       "                      [ 0.0115,  0.0367,  0.0759,  ...,  0.0317, -0.0082, -0.0673]])),\n",
       "             ('transformer.resblocks.9.attn.in_proj_bias',\n",
       "              tensor([ 0.0005,  0.0058,  0.0005,  ..., -0.0003,  0.0011, -0.0019])),\n",
       "             ('transformer.resblocks.9.attn.out_proj.weight',\n",
       "              tensor([[-0.0121,  0.0140,  0.0105,  ..., -0.0002, -0.0134,  0.0114],\n",
       "                      [ 0.0042, -0.0101,  0.0046,  ..., -0.0106,  0.0022,  0.0071],\n",
       "                      [-0.0031,  0.0144,  0.0031,  ..., -0.0046, -0.0071, -0.0061],\n",
       "                      ...,\n",
       "                      [ 0.0132, -0.0124,  0.0209,  ..., -0.0105, -0.0043, -0.0039],\n",
       "                      [ 0.0055, -0.0024,  0.0040,  ...,  0.0083,  0.0012,  0.0075],\n",
       "                      [ 0.0054,  0.0123,  0.0020,  ..., -0.0012, -0.0033, -0.0086]])),\n",
       "             ('transformer.resblocks.9.attn.out_proj.bias',\n",
       "              tensor([-7.8611e-04, -4.1480e-03, -3.0011e-04, -6.8723e-04, -8.0057e-04,\n",
       "                      -3.0540e-03,  2.3493e-04,  2.3846e-03, -3.2709e-03,  2.0080e-03,\n",
       "                      -4.8667e-03, -2.5177e-03, -6.7211e-04, -1.7566e-03, -6.0076e-03,\n",
       "                       2.1253e-03, -2.9371e-04,  1.9740e-03, -3.4398e-04, -7.1077e-04,\n",
       "                      -2.4873e-04,  7.7036e-04,  4.3105e-04, -1.2572e-03, -8.1285e-04,\n",
       "                      -1.2987e-03, -1.2425e-03, -1.1216e-03, -8.4757e-04, -1.8711e-03,\n",
       "                       9.0067e-04,  1.9751e-04,  2.7130e-03, -3.5064e-03, -3.6596e-04,\n",
       "                       1.2567e-03,  1.1117e-03, -1.7535e-05,  8.7396e-04, -2.9942e-03,\n",
       "                       1.4582e-03,  2.3016e-03,  1.8208e-03,  1.4081e-03, -4.2146e-03,\n",
       "                       1.6593e-03, -3.4737e-03,  6.7375e-04, -5.2942e-04,  1.5587e-03,\n",
       "                      -1.1432e-03, -7.8037e-04, -1.2426e-03,  2.5533e-03,  3.0394e-03,\n",
       "                       1.2950e-03, -1.1692e-03, -1.0374e-03, -4.3428e-05, -9.2296e-04,\n",
       "                      -2.1219e-03,  2.4965e-04,  5.7147e-04,  5.3037e-04, -2.2471e-04,\n",
       "                      -8.1872e-04,  1.8596e-03,  6.4585e-04, -1.4406e-03,  1.6076e-03,\n",
       "                      -8.6549e-04, -1.8957e-04, -2.3363e-03,  1.8604e-04, -9.2358e-04,\n",
       "                       1.9913e-03, -1.7313e-03, -1.9960e-03, -2.4263e-03,  1.9654e-03,\n",
       "                       1.7256e-03, -2.2199e-03,  8.4835e-04,  1.9309e-03,  2.0012e-03,\n",
       "                       1.7630e-04, -6.3123e-04,  6.0444e-04,  2.5830e-04, -2.2353e-03,\n",
       "                       3.4738e-04, -7.8883e-04,  1.7279e-03, -1.8614e-03, -1.7901e-03,\n",
       "                      -4.4490e-03, -3.3805e-04, -1.3516e-03, -1.8508e-03,  5.2330e-04,\n",
       "                       8.0987e-04, -1.6114e-03,  4.5499e-04, -5.7093e-04, -6.8121e-04,\n",
       "                      -3.0382e-03, -3.6870e-03, -9.7791e-04,  9.9437e-04, -3.7880e-04,\n",
       "                       2.9346e-03,  1.0344e-03,  3.0610e-03,  9.9701e-04, -1.6245e-03,\n",
       "                      -4.8327e-03,  1.4109e-03,  1.8418e-03, -3.0078e-03, -1.0848e-03,\n",
       "                       5.5593e-04, -1.8832e-03,  1.9018e-03, -6.9477e-04, -1.3577e-03,\n",
       "                       5.7366e-04, -1.5297e-03, -2.4021e-03, -1.0777e-03, -6.2343e-05,\n",
       "                      -2.2302e-04,  3.7643e-03, -4.0780e-04, -2.6528e-03, -5.2126e-04,\n",
       "                      -1.0929e-03, -1.3222e-03,  2.1803e-03, -1.1100e-03,  8.4854e-04,\n",
       "                       3.5640e-04,  1.9047e-03,  1.3330e-03,  3.4678e-03,  1.3545e-03,\n",
       "                       2.5211e-03, -2.4516e-03,  8.3894e-04,  6.4599e-04,  8.1361e-04,\n",
       "                       1.9790e-03,  5.0189e-03,  6.4045e-03,  1.8526e-03, -7.4895e-04,\n",
       "                      -2.1268e-04,  1.0986e-03, -1.7473e-03,  3.2026e-04,  3.6565e-04,\n",
       "                      -1.9972e-03, -1.1629e-03,  3.4312e-03,  2.1887e-04, -1.2603e-03,\n",
       "                       1.3928e-03,  2.2412e-03, -2.0530e-04, -3.4147e-04, -1.9398e-03,\n",
       "                       1.4270e-03, -2.9408e-04, -2.0041e-03,  4.3909e-04,  2.1544e-03,\n",
       "                       1.1626e-03, -1.5744e-03,  5.2899e-04,  3.0048e-04, -1.2257e-03,\n",
       "                      -2.3803e-03, -5.5006e-04,  1.3050e-03, -3.1782e-03,  4.7754e-03,\n",
       "                      -1.6954e-04,  6.2265e-04, -2.6615e-04,  1.2474e-03, -2.6062e-03,\n",
       "                       6.5303e-04,  6.6679e-04, -3.5746e-04,  1.8069e-03, -1.3851e-03,\n",
       "                      -1.5126e-03,  1.7162e-03,  3.0673e-03,  4.0881e-03,  3.1419e-03,\n",
       "                      -4.5901e-04, -1.6145e-03,  1.6310e-03,  1.8206e-03,  8.3910e-04,\n",
       "                       6.8242e-04, -1.3410e-03,  6.8103e-04,  1.2708e-03,  2.8450e-04,\n",
       "                       2.5869e-03,  1.0707e-04,  6.4327e-05,  1.7671e-03, -2.0129e-03,\n",
       "                      -1.8560e-03, -2.4180e-04, -1.8629e-03, -1.4995e-03,  6.7737e-04,\n",
       "                      -8.5262e-04, -6.0399e-04,  1.6931e-03,  2.1784e-03,  1.1098e-03,\n",
       "                       1.0216e-03, -1.1089e-03, -2.7018e-03,  1.1234e-03,  1.8278e-03,\n",
       "                      -1.7061e-03,  4.5633e-04,  8.4204e-04, -1.3034e-03,  1.5979e-03,\n",
       "                       3.6736e-04,  2.8701e-03, -3.6834e-03,  1.7847e-03,  1.6305e-03,\n",
       "                      -1.5567e-03,  2.7016e-03,  2.9293e-03,  1.5380e-03, -2.7369e-03,\n",
       "                      -7.8759e-06,  7.1163e-04,  6.8528e-04, -2.6645e-04, -1.5904e-03,\n",
       "                       1.0195e-03, -2.4039e-03, -2.0806e-04, -1.5456e-03, -6.8009e-04,\n",
       "                      -1.0517e-03, -2.6950e-04, -2.1766e-03, -8.1920e-05, -3.5171e-03,\n",
       "                       1.7170e-03,  1.0195e-03,  8.6754e-04, -3.5845e-03, -1.8818e-03,\n",
       "                       2.9158e-03,  7.9358e-04,  1.1655e-03, -5.5007e-04, -1.6214e-03,\n",
       "                       1.0503e-03, -1.1938e-03, -1.7770e-03,  3.0406e-04,  1.1696e-03,\n",
       "                      -6.9941e-04,  3.5522e-03, -6.7200e-04,  3.6206e-03, -1.6155e-03,\n",
       "                      -1.2714e-04,  3.9540e-03, -2.6591e-03, -1.8175e-03, -2.8097e-03,\n",
       "                       1.6254e-03,  3.3664e-05,  1.5040e-03,  2.1071e-03,  1.5885e-04,\n",
       "                      -5.8692e-04,  1.4273e-03, -1.8397e-03,  4.6336e-03,  4.9601e-04,\n",
       "                       1.9672e-03, -1.7646e-03,  1.2096e-03, -6.5631e-04, -1.0617e-03,\n",
       "                      -3.0868e-03,  7.2848e-04,  1.6563e-03, -4.0415e-03,  7.0546e-05,\n",
       "                      -2.2786e-03,  6.5707e-04,  3.1266e-04, -1.2061e-03, -2.5763e-03,\n",
       "                       2.0127e-03,  2.8253e-03, -8.3781e-04, -1.7494e-03,  7.0285e-04,\n",
       "                       2.4080e-03,  2.9233e-03,  1.5793e-03,  7.3420e-04,  1.0373e-03,\n",
       "                       2.8758e-03, -7.6425e-04, -7.4812e-04,  2.6871e-04, -2.6187e-03,\n",
       "                       4.0995e-04,  3.6620e-03, -6.2353e-05, -3.8682e-03, -1.7416e-03,\n",
       "                      -7.7765e-04,  1.4781e-03,  5.1475e-04, -7.0830e-04,  1.4190e-03,\n",
       "                       8.7530e-06,  1.5957e-04, -3.0286e-04,  9.5570e-04,  2.0586e-03,\n",
       "                      -3.3950e-03,  3.3213e-03,  1.8419e-04, -4.7686e-04, -3.4185e-03,\n",
       "                      -1.2990e-04, -2.5225e-03, -7.5702e-05, -9.4387e-04,  1.4657e-03,\n",
       "                      -1.7333e-03, -4.1498e-03, -2.0604e-03, -3.4823e-04, -9.5471e-05,\n",
       "                       6.5622e-04,  3.2280e-03,  2.8719e-03,  1.1439e-03, -2.6127e-03,\n",
       "                       1.4552e-03, -4.5915e-04,  2.8438e-03,  3.1832e-03, -3.4060e-04,\n",
       "                       2.8292e-03, -2.9171e-04, -4.7096e-03,  1.9523e-03, -8.1116e-04,\n",
       "                      -1.1814e-03,  1.4816e-03,  7.6737e-04, -4.5386e-04, -1.8599e-04,\n",
       "                       3.3315e-03, -1.1037e-03,  2.7790e-03, -1.7423e-03, -2.1437e-03,\n",
       "                       2.6565e-03,  1.9918e-03, -1.8396e-03, -3.9405e-04, -1.1888e-03,\n",
       "                      -1.1219e-03,  3.0707e-03, -1.0311e-03,  3.8089e-03,  1.5058e-03,\n",
       "                       1.4502e-04, -1.7201e-04, -4.7239e-04,  9.2947e-04,  3.3036e-03,\n",
       "                       2.1717e-03, -1.0697e-03,  2.2744e-03,  2.5030e-04,  4.1695e-04,\n",
       "                      -2.9914e-04, -1.5817e-03, -4.5082e-04, -6.4402e-04, -6.7233e-05,\n",
       "                       1.7743e-03,  1.0094e-03,  1.2060e-03,  1.9997e-03,  1.9396e-03,\n",
       "                      -5.7845e-04, -3.3891e-03,  4.5955e-04,  2.7885e-03,  9.2034e-04,\n",
       "                      -2.6154e-03,  2.3598e-03,  2.1128e-03, -4.0450e-04,  2.7576e-03,\n",
       "                      -1.0943e-03,  1.3163e-03,  1.8052e-03, -2.2236e-03, -1.9684e-03,\n",
       "                       4.6789e-04, -4.1862e-04, -4.6908e-04,  8.1592e-04,  6.1381e-04,\n",
       "                      -2.0136e-03,  1.8366e-03,  1.5293e-03, -8.9606e-04, -3.5539e-03,\n",
       "                      -1.2271e-03,  2.1205e-03, -3.0467e-03,  1.7062e-03, -1.3713e-03,\n",
       "                      -1.3806e-03,  8.1670e-04,  8.9146e-04, -4.0484e-04, -2.3157e-03,\n",
       "                      -2.8471e-03, -4.9800e-04,  1.0201e-03, -3.8955e-04,  1.7154e-03,\n",
       "                       4.0343e-04,  1.2511e-03, -1.0663e-03,  5.0573e-03,  1.3353e-03,\n",
       "                      -2.2325e-03,  5.9787e-04,  2.6029e-03,  2.3766e-03,  1.7810e-04,\n",
       "                       1.0429e-03, -1.2016e-03,  9.3334e-04,  3.8050e-03,  3.1848e-04,\n",
       "                       1.5734e-04,  1.1522e-04,  1.0060e-03,  6.8471e-05, -2.2441e-03,\n",
       "                      -2.0246e-03,  1.3180e-03, -2.6064e-03, -1.3630e-03, -1.8851e-03,\n",
       "                       5.1216e-04, -2.7899e-03, -1.4826e-03, -1.6202e-03, -1.0776e-03,\n",
       "                      -3.3202e-03, -2.3606e-03,  1.1621e-03,  4.8774e-04, -1.1435e-03,\n",
       "                       7.2484e-04, -1.1843e-03, -2.0213e-03,  2.3767e-03,  1.8097e-03,\n",
       "                       8.8568e-04, -1.0737e-03,  1.2722e-03, -2.3800e-03, -1.9715e-03,\n",
       "                       3.4783e-04,  1.6229e-03, -5.2220e-04,  2.2582e-04, -3.1544e-03,\n",
       "                      -2.0679e-03,  7.2191e-04, -1.7975e-03,  3.4353e-03, -2.9721e-03,\n",
       "                       1.3420e-03, -2.0939e-03, -9.5144e-04,  1.2197e-03, -3.6529e-04,\n",
       "                      -1.7273e-03, -6.3125e-04])),\n",
       "             ('transformer.resblocks.9.ln_1.weight',\n",
       "              tensor([1.0008, 1.0000, 1.0005, 0.9994, 0.9983, 0.9997, 1.0002, 0.9994, 0.9997,\n",
       "                      1.0020, 0.9992, 1.0001, 1.0014, 1.0013, 0.9965, 0.9994, 1.0011, 0.9990,\n",
       "                      0.9992, 1.0012, 0.9988, 0.9997, 1.0002, 1.0012, 0.9990, 1.0012, 0.9996,\n",
       "                      0.9981, 0.9969, 0.9976, 1.0017, 0.9998, 0.9998, 0.9997, 0.9972, 1.0001,\n",
       "                      1.0028, 1.0006, 0.9983, 0.9962, 1.0027, 0.9976, 1.0013, 0.9993, 1.0005,\n",
       "                      0.9984, 1.0024, 0.9977, 0.9997, 0.9997, 0.9993, 1.0002, 0.9982, 1.0021,\n",
       "                      1.0001, 0.9987, 0.9995, 1.0003, 0.9981, 1.0001, 0.9983, 0.9981, 0.9990,\n",
       "                      0.9991, 1.0023, 0.9963, 0.9985, 0.9993, 0.9975, 1.0011, 0.9994, 0.9963,\n",
       "                      0.9987, 1.0001, 1.0012, 1.0007, 1.0009, 0.9999, 1.0009, 1.0001, 0.9974,\n",
       "                      0.9999, 0.9987, 1.0003, 0.9987, 1.0004, 1.0005, 1.0004, 0.9989, 1.0038,\n",
       "                      0.9991, 0.9996, 0.9989, 0.9974, 0.9966, 0.9986, 1.0003, 1.0005, 0.9997,\n",
       "                      1.0001, 0.9968, 0.9999, 0.9987, 1.0015, 1.0001, 0.9972, 0.9984, 0.9993,\n",
       "                      1.0017, 0.9973, 0.9999, 1.0004, 0.9984, 1.0014, 1.0005, 0.9992, 1.0006,\n",
       "                      0.9976, 0.9979, 0.9974, 0.9993, 1.0010, 1.0015, 1.0015, 1.0020, 0.9984,\n",
       "                      0.9982, 0.9944, 0.9991, 0.9994, 0.9980, 0.9959, 1.0005, 1.0001, 1.0001,\n",
       "                      0.9998, 1.0000, 0.9994, 0.9989, 0.9995, 0.9974, 1.0002, 0.9978, 1.0019,\n",
       "                      0.9996, 0.9979, 0.9989, 1.0006, 0.9990, 0.9977, 0.9989, 0.9990, 0.9987,\n",
       "                      1.0015, 0.9989, 1.0000, 0.9989, 0.9994, 0.9983, 0.9988, 0.9991, 1.0007,\n",
       "                      0.9967, 1.0024, 1.0007, 1.0014, 1.0011, 1.0002, 1.0008, 1.0003, 0.9998,\n",
       "                      0.9992, 0.9970, 1.0006, 0.9988, 0.9987, 1.0007, 0.9996, 1.0000, 1.0004,\n",
       "                      1.0009, 1.0003, 1.0012, 0.9958, 1.0000, 0.9980, 0.9998, 1.0007, 0.9974,\n",
       "                      1.0006, 1.0020, 0.9994, 0.9965, 0.9974, 0.9980, 1.0003, 1.0001, 0.9956,\n",
       "                      0.9997, 0.9984, 0.9990, 0.9974, 1.0004, 1.0011, 0.9994, 0.9996, 0.9996,\n",
       "                      1.0000, 0.9989, 0.9995, 1.0011, 0.9987, 1.0026, 0.9997, 0.9995, 1.0015,\n",
       "                      0.9989, 0.9996, 0.9976, 0.9999, 1.0025, 1.0005, 0.9992, 0.9997, 0.9999,\n",
       "                      0.9964, 0.9998, 0.9993, 0.9997, 0.9955, 1.0014, 1.0019, 0.9959, 1.0005,\n",
       "                      0.9978, 1.0040, 1.0009, 0.9979, 0.9984, 1.0013, 0.9998, 0.9957, 0.9986,\n",
       "                      0.9975, 1.0009, 1.0021, 0.9999, 1.0004, 0.9981, 0.9982, 1.0001, 0.9971,\n",
       "                      0.9978, 1.0008, 1.0008, 0.9996, 1.0003, 0.9999, 1.0023, 1.0006, 0.9996,\n",
       "                      1.0007, 1.0027, 0.9973, 0.9972, 1.0006, 1.0009, 0.9994, 0.9990, 0.9995,\n",
       "                      0.9982, 0.9979, 0.9984, 0.9988, 0.9980, 1.0003, 0.9990, 0.9979, 1.0002,\n",
       "                      0.9987, 0.9995, 0.9983, 1.0006, 0.9976, 1.0007, 1.0013, 0.9981, 0.9999,\n",
       "                      0.9994, 1.0010, 1.0005, 1.0021, 0.9992, 0.9968, 1.0001, 1.0016, 0.9991,\n",
       "                      1.0013, 1.0001, 0.9983, 0.9985, 0.9987, 0.9988, 0.9993, 1.0003, 1.0013,\n",
       "                      1.0000, 1.0003, 1.0010, 0.9977, 0.9991, 0.9977, 1.0001, 1.0009, 0.9998,\n",
       "                      0.9990, 0.9998, 1.0018, 0.9991, 0.9973, 0.9993, 0.9997, 1.0005, 0.9991,\n",
       "                      0.9986, 0.9996, 0.9998, 1.0002, 1.0005, 1.0010, 0.9993, 0.9965, 0.9996,\n",
       "                      0.9976, 0.9981, 1.0005, 0.9997, 1.0005, 1.0011, 1.0028, 0.9985, 0.9980,\n",
       "                      1.0010, 1.0007, 0.9987, 0.9983, 1.0004, 0.9987, 1.0000, 1.0004, 0.9998,\n",
       "                      0.9972, 1.0003, 0.9986, 1.0000, 0.9975, 1.0008, 0.9988, 0.9987, 0.9979,\n",
       "                      1.0004, 0.9990, 0.9977, 0.9976, 0.9991, 0.9998, 0.9962, 0.9987, 1.0016,\n",
       "                      0.9975, 1.0001, 1.0001, 0.9991, 0.9995, 1.0022, 1.0014, 0.9970, 0.9993,\n",
       "                      0.9974, 0.9993, 1.0003, 1.0006, 1.0002, 1.0009, 1.0016, 0.9993, 0.9977,\n",
       "                      0.9989, 0.9976, 1.0005, 1.0002, 0.9981, 0.9972, 0.9994, 0.9959, 0.9980,\n",
       "                      0.9998, 0.9984, 0.9985, 1.0028, 0.9987, 1.0001, 1.0004, 0.9974, 0.9995,\n",
       "                      0.9988, 1.0010, 0.9960, 0.9982, 0.9978, 1.0008, 0.9970, 0.9975, 0.9995,\n",
       "                      1.0002, 0.9973, 0.9991, 1.0003, 1.0015, 0.9985, 1.0009, 0.9979, 0.9991,\n",
       "                      1.0001, 0.9979, 0.9992, 0.9998, 0.9991, 0.9987, 0.9973, 1.0005, 1.0004,\n",
       "                      1.0011, 1.0009, 0.9983, 1.0015, 0.9984, 0.9994, 1.0021, 1.0001, 0.9991,\n",
       "                      0.9994, 0.9993, 0.9983, 0.9991, 0.9982, 1.0001, 0.9966, 0.9976, 1.0025,\n",
       "                      0.9992, 0.9996, 1.0000, 0.9995, 1.0002, 0.9985, 1.0041, 0.9970, 1.0000,\n",
       "                      1.0001, 0.9955, 0.9974, 0.9997, 0.9988, 0.9988, 0.9995, 0.9987, 1.0010,\n",
       "                      0.9973, 0.9973, 0.9985, 0.9986, 0.9999, 0.9989, 1.0022, 1.0004, 1.0004,\n",
       "                      0.9994, 0.9991, 1.0000, 0.9980, 0.9996, 1.0005, 1.0019, 0.9996, 1.0011,\n",
       "                      0.9975, 0.9990, 0.9988, 0.9966, 1.0020, 0.9975, 0.9985, 0.9995, 0.9967,\n",
       "                      0.9984, 0.9982, 0.9991, 0.9996, 0.9960, 0.9964, 0.9995, 1.0001, 0.9976,\n",
       "                      0.9992, 0.9988, 0.9970, 0.9971, 0.9971, 1.0032, 1.0014, 0.9970])),\n",
       "             ('transformer.resblocks.9.ln_1.bias',\n",
       "              tensor([-2.5122e-03, -9.8939e-04,  3.7425e-04,  1.7820e-04,  1.1536e-03,\n",
       "                      -5.3252e-04,  1.1213e-03,  2.1719e-04, -1.0934e-03,  1.7561e-03,\n",
       "                      -5.8584e-04, -5.6394e-04, -2.0281e-03,  1.0530e-03,  2.4026e-03,\n",
       "                      -2.5284e-04,  2.2617e-03, -1.6733e-03,  1.1295e-03, -8.4974e-04,\n",
       "                       1.2684e-03,  1.0822e-03, -2.2196e-04, -1.7618e-03, -1.4690e-03,\n",
       "                       2.2251e-04,  2.0873e-03,  2.3814e-03,  4.0387e-03,  1.8322e-03,\n",
       "                       1.5101e-03, -3.3172e-05,  1.3302e-03,  5.4639e-04, -9.9552e-04,\n",
       "                       1.4876e-03,  3.4256e-03,  2.7663e-03, -1.9083e-05, -7.1345e-04,\n",
       "                       1.4109e-03, -4.9972e-04,  5.7013e-04, -1.1491e-03, -1.6786e-05,\n",
       "                      -2.5613e-05, -1.1724e-03,  1.1895e-03, -1.4677e-03, -1.2281e-03,\n",
       "                      -3.0019e-03,  2.4888e-04, -1.1227e-03,  2.5671e-03, -1.4128e-03,\n",
       "                       1.6486e-03,  2.6760e-03,  1.4965e-03,  1.5820e-03, -3.1236e-03,\n",
       "                       1.3084e-03, -6.7979e-04, -2.6292e-04, -2.8277e-04,  1.1530e-03,\n",
       "                      -2.2884e-03, -4.1678e-04,  9.6123e-04,  2.5520e-03,  1.5925e-03,\n",
       "                      -8.8099e-04, -2.0015e-04,  1.3746e-03, -1.2659e-03, -2.2137e-03,\n",
       "                       2.6484e-03, -2.7943e-03, -7.3754e-04, -4.2957e-04,  1.6504e-03,\n",
       "                      -3.6025e-03,  2.7787e-04,  2.5782e-03,  1.6461e-03,  1.0953e-03,\n",
       "                       1.8185e-03, -2.1410e-03,  8.2708e-04,  2.0014e-03, -7.0549e-04,\n",
       "                       2.6721e-03, -3.3214e-05, -3.0666e-03,  6.2275e-04,  6.9286e-04,\n",
       "                      -2.8118e-04,  1.7571e-03,  2.1345e-03,  8.5836e-04, -1.6451e-03,\n",
       "                      -6.2965e-04, -1.9204e-03, -1.1948e-04, -1.6280e-03, -9.6365e-04,\n",
       "                       2.1883e-03,  2.9282e-03,  1.9840e-03,  8.0503e-04, -1.4672e-03,\n",
       "                       1.4579e-03, -1.6525e-03, -2.1874e-03,  1.3315e-04,  1.4677e-03,\n",
       "                      -7.1084e-04, -3.5540e-04, -3.8359e-03,  8.1082e-04,  5.7810e-04,\n",
       "                      -2.1028e-03,  1.2635e-03, -4.5860e-03, -1.2122e-03, -7.7872e-04,\n",
       "                       1.0386e-03, -7.7895e-04,  3.0909e-03,  4.7952e-04, -2.4783e-03,\n",
       "                       2.9987e-03, -3.8877e-03,  1.2744e-03,  4.5732e-04, -8.5277e-04,\n",
       "                      -1.0369e-04, -8.5490e-04,  8.8838e-04,  8.5809e-04,  1.3134e-03,\n",
       "                       4.5325e-04,  2.3570e-03,  1.1921e-05,  4.1952e-03,  7.2734e-04,\n",
       "                      -7.9702e-04,  7.0547e-04, -7.9735e-05,  2.0398e-03, -7.7074e-04,\n",
       "                      -2.3249e-03,  2.5895e-04, -2.2248e-05,  9.6939e-04,  2.0558e-03,\n",
       "                      -1.1008e-03, -2.1477e-03,  2.0426e-04,  1.7181e-03,  1.9230e-03,\n",
       "                      -2.0209e-03, -1.3397e-04, -3.8503e-03,  1.1349e-03, -1.1507e-03,\n",
       "                      -2.1321e-03,  1.8874e-03, -1.3604e-03,  7.0462e-04,  1.7717e-03,\n",
       "                      -5.4590e-04,  1.7062e-03, -1.2946e-03,  7.2141e-04, -2.2096e-03,\n",
       "                       1.2745e-03, -1.8409e-03,  1.1933e-03,  2.2997e-03, -4.0153e-04,\n",
       "                      -7.9400e-04,  3.1041e-03,  1.8018e-03,  1.7943e-03,  1.4023e-03,\n",
       "                       2.2521e-03, -8.1436e-05,  1.7957e-03, -3.7255e-03, -2.6587e-04,\n",
       "                      -1.8840e-03,  8.6949e-04,  1.8923e-04, -5.0981e-05, -6.1847e-04,\n",
       "                       1.5037e-04,  1.6932e-03, -2.8964e-03, -1.8175e-04, -1.8675e-04,\n",
       "                       1.9735e-03,  1.8190e-03,  5.8971e-04,  1.1452e-03, -3.5918e-03,\n",
       "                       1.4429e-03, -2.7554e-03, -9.9320e-04,  2.6314e-03,  1.1403e-04,\n",
       "                      -1.6787e-03,  3.3700e-03,  2.9769e-05, -1.4824e-03, -8.2636e-04,\n",
       "                      -1.8369e-03,  2.1070e-03, -1.1020e-03,  1.2013e-03, -2.8159e-03,\n",
       "                      -2.0945e-03, -7.3199e-04,  1.4748e-03,  2.0008e-03, -1.2422e-03,\n",
       "                      -2.5011e-03,  8.7002e-04,  1.0096e-03,  1.3674e-03,  9.6742e-04,\n",
       "                      -1.6623e-03,  2.7270e-03, -5.1116e-03,  2.4214e-04, -3.8584e-03,\n",
       "                      -6.3175e-05,  1.7666e-03, -2.1237e-04, -1.9707e-03, -1.0320e-03,\n",
       "                      -1.5284e-03, -3.3974e-03, -6.2389e-04, -3.4507e-03, -1.0319e-03,\n",
       "                       1.7909e-03, -9.6002e-04, -6.0763e-04,  3.5356e-04, -5.6868e-04,\n",
       "                       1.4863e-03,  2.5875e-03,  3.9756e-03,  9.2188e-04, -1.6262e-03,\n",
       "                      -1.2646e-04,  4.3742e-04, -2.0651e-03, -3.8404e-03, -1.9922e-03,\n",
       "                      -1.6365e-03,  4.8256e-05,  5.4465e-04,  5.8185e-04,  1.0491e-03,\n",
       "                       1.3292e-03,  6.0834e-04,  3.1091e-03,  2.9236e-04, -4.0141e-04,\n",
       "                       2.1710e-03, -1.5824e-03,  7.4065e-04,  9.0715e-04,  1.8699e-04,\n",
       "                       6.4879e-04,  3.0180e-04,  1.6260e-03,  1.2363e-03, -6.8325e-04,\n",
       "                       1.4090e-03,  5.4005e-04,  2.4461e-04,  1.2653e-03, -1.3432e-03,\n",
       "                       5.6183e-04,  3.3729e-03,  1.8024e-03,  1.5221e-04, -3.1924e-03,\n",
       "                       1.3799e-03,  2.6436e-03, -2.1234e-03, -2.2320e-03,  1.2597e-03,\n",
       "                      -6.0972e-04, -4.1352e-04, -1.0039e-03,  8.9655e-04, -5.0892e-04,\n",
       "                       5.9679e-04,  1.8297e-03, -2.1961e-04, -8.2165e-04,  1.5423e-05,\n",
       "                      -5.9891e-04,  7.9079e-04, -2.9705e-03, -3.4269e-03,  1.0914e-03,\n",
       "                       1.0275e-03, -5.8274e-04,  7.8863e-04, -3.3397e-04,  2.6644e-04,\n",
       "                      -2.2779e-03,  2.1703e-04, -1.0250e-03,  8.0153e-04, -1.4227e-03,\n",
       "                      -2.0847e-03,  1.1218e-03, -2.4730e-03, -3.8880e-04,  2.2984e-04,\n",
       "                       7.5271e-04, -2.0936e-03,  1.3984e-03, -1.1765e-03, -1.2307e-03,\n",
       "                       1.3247e-03, -2.7106e-03,  3.5718e-04, -1.4638e-03, -1.1326e-03,\n",
       "                      -1.0853e-03,  1.6772e-03,  2.1720e-03, -6.4550e-04,  3.7383e-03,\n",
       "                       7.8948e-04, -1.9897e-03, -1.2806e-03, -1.0001e-03,  1.0326e-03,\n",
       "                       5.2657e-04, -2.9657e-04, -1.4543e-03,  2.3595e-03,  1.2311e-03,\n",
       "                      -2.5627e-03,  1.3710e-03, -1.7807e-03,  2.9020e-03, -5.7158e-04,\n",
       "                       5.5823e-04,  1.3832e-03,  5.5496e-04, -1.4565e-03,  7.7002e-04,\n",
       "                      -1.0058e-03, -3.8155e-04, -4.5505e-04,  3.5841e-05,  2.2066e-03,\n",
       "                       1.3079e-03, -1.8725e-03,  1.8863e-04,  2.4668e-03,  2.0177e-04,\n",
       "                      -7.9645e-04, -1.7289e-03,  1.1578e-03, -2.3165e-03,  1.1315e-03,\n",
       "                       3.1009e-03,  1.9127e-03,  1.0272e-03,  3.8863e-03,  1.7740e-04,\n",
       "                       1.5161e-03,  2.3566e-03, -2.5426e-03, -2.5906e-03, -1.3866e-03,\n",
       "                       2.7244e-03, -2.0403e-03,  5.8892e-04, -1.6296e-03,  1.4828e-03,\n",
       "                      -1.8331e-03,  6.3121e-04,  1.8053e-03,  6.2400e-04, -2.7714e-03,\n",
       "                       5.1312e-04, -5.4314e-04, -1.9850e-03, -8.0483e-04, -1.2862e-03,\n",
       "                       6.5499e-04, -2.3766e-03, -6.3500e-04,  1.5445e-03, -1.7349e-03,\n",
       "                      -1.8822e-03,  2.6612e-03,  7.1173e-04, -1.7081e-03, -1.0238e-03,\n",
       "                       2.0647e-03,  1.8995e-03, -1.8472e-03,  1.9064e-03,  1.3531e-03,\n",
       "                       4.4532e-04,  2.3215e-03, -1.7460e-03,  1.5138e-03, -1.6918e-03,\n",
       "                      -3.1573e-03, -1.3248e-03, -3.6109e-04,  3.1507e-03,  7.4756e-04,\n",
       "                      -1.8573e-03,  7.6462e-04, -5.7291e-04,  3.4348e-03,  8.0063e-05,\n",
       "                      -2.9462e-03, -1.6033e-03, -3.4334e-04, -1.5155e-03,  4.1905e-06,\n",
       "                       2.0799e-04, -1.6815e-03,  1.7354e-04,  2.1625e-03,  3.6212e-03,\n",
       "                       2.2882e-03,  2.6394e-03, -2.0493e-03,  1.2362e-03,  1.0222e-03,\n",
       "                       4.7410e-04, -1.5867e-03, -3.1306e-04, -2.3531e-05,  1.3452e-03,\n",
       "                       1.6324e-03,  1.8940e-03, -1.1418e-03,  1.3039e-03, -1.2169e-03,\n",
       "                       5.6007e-04,  3.3995e-03, -3.4980e-03,  7.7966e-04, -1.2161e-03,\n",
       "                       1.8229e-04,  2.2028e-04,  6.3075e-04, -5.9731e-04,  3.0043e-03,\n",
       "                       8.8150e-04,  2.8817e-03,  2.0601e-03, -1.4818e-03, -1.0599e-03,\n",
       "                      -2.2001e-04,  5.2275e-04, -1.7140e-03, -2.2727e-03, -2.5421e-03,\n",
       "                       2.8810e-04, -1.1355e-04, -6.0503e-06,  1.0779e-03, -2.0491e-03,\n",
       "                       8.7947e-04,  5.1919e-04,  9.0334e-04,  3.7385e-03, -8.7178e-04,\n",
       "                      -2.3767e-03, -2.4082e-03, -3.0285e-04,  1.8024e-03, -3.1669e-03,\n",
       "                       3.3725e-03,  4.1942e-04, -8.7873e-04, -2.1823e-03,  3.0090e-03,\n",
       "                      -1.5718e-03,  1.3291e-03,  3.5033e-03, -3.4828e-03, -3.5906e-04,\n",
       "                       3.6412e-03,  1.0598e-03,  2.1177e-04, -3.3228e-04, -1.0653e-03,\n",
       "                      -3.5435e-03,  1.7310e-03,  1.1530e-03, -6.9369e-04,  6.1169e-04,\n",
       "                      -2.7985e-03,  1.6681e-03])),\n",
       "             ('transformer.resblocks.9.mlp.c_fc.weight',\n",
       "              tensor([[-0.0006,  0.0314, -0.0108,  ..., -0.0067, -0.0381,  0.0177],\n",
       "                      [-0.0528,  0.0002,  0.0136,  ...,  0.0098, -0.0114,  0.0085],\n",
       "                      [-0.0056,  0.0011, -0.0020,  ...,  0.0067, -0.0038, -0.0033],\n",
       "                      ...,\n",
       "                      [-0.0206,  0.0458, -0.0230,  ..., -0.0064,  0.0121,  0.0254],\n",
       "                      [-0.0681, -0.0012, -0.0235,  ...,  0.0080,  0.0134, -0.0090],\n",
       "                      [-0.0054, -0.0433, -0.0105,  ...,  0.0496,  0.0346, -0.0305]])),\n",
       "             ('transformer.resblocks.9.mlp.c_fc.bias',\n",
       "              tensor([ 0.0034, -0.0173, -0.0041,  ..., -0.0034,  0.0182,  0.0225])),\n",
       "             ('transformer.resblocks.9.mlp.c_proj.weight',\n",
       "              tensor([[-0.0143, -0.0116, -0.0003,  ...,  0.0112,  0.0027, -0.0115],\n",
       "                      [-0.0077, -0.0050,  0.0021,  ..., -0.0033,  0.0020,  0.0015],\n",
       "                      [-0.0189,  0.0180,  0.0016,  ..., -0.0055,  0.0022,  0.0061],\n",
       "                      ...,\n",
       "                      [-0.0091,  0.0011, -0.0015,  ...,  0.0011,  0.0097,  0.0037],\n",
       "                      [ 0.0060,  0.0120,  0.0026,  ...,  0.0107,  0.0133, -0.0087],\n",
       "                      [ 0.0072,  0.0102,  0.0122,  ..., -0.0033, -0.0095, -0.0034]])),\n",
       "             ('transformer.resblocks.9.mlp.c_proj.bias',\n",
       "              tensor([ 1.3594e-02, -1.7226e-02,  2.0395e-02, -2.0240e-02, -2.1428e-03,\n",
       "                      -7.5193e-03, -1.6387e-02,  1.5236e-02, -8.7959e-03, -1.5047e-02,\n",
       "                       1.3360e-02, -1.2737e-02,  3.3362e-03,  1.5142e-02,  3.8681e-03,\n",
       "                       1.3630e-02,  6.5661e-03,  1.3491e-03, -1.3587e-02,  8.5095e-03,\n",
       "                       1.1826e-03, -1.0308e-04, -3.6969e-04,  2.4223e-03, -8.4252e-03,\n",
       "                       1.9814e-02, -7.0824e-03,  1.5656e-02, -2.8604e-03, -1.9526e-02,\n",
       "                       1.9474e-02, -1.5642e-02, -1.0373e-02, -1.5389e-02,  9.4754e-03,\n",
       "                      -5.2618e-03, -5.8066e-03,  6.2206e-03,  1.5195e-02,  1.3204e-02,\n",
       "                       1.0041e-02,  1.6556e-02, -1.6670e-02, -6.4945e-03,  4.0241e-04,\n",
       "                       1.1791e-02, -1.2238e-03, -1.4966e-02, -9.7969e-03,  2.1391e-02,\n",
       "                      -1.9899e-02, -9.5723e-03,  1.3905e-02,  4.0122e-03, -1.1189e-02,\n",
       "                       1.5012e-02, -6.4058e-03, -1.6925e-02,  1.4642e-02,  4.6512e-03,\n",
       "                      -1.5929e-02,  1.7722e-02, -7.4997e-03, -1.0098e-02,  1.9114e-02,\n",
       "                      -1.7610e-03,  5.2631e-04, -1.1706e-02, -1.1257e-02, -1.1858e-02,\n",
       "                       1.0617e-02, -3.7388e-03,  1.4976e-02,  1.6722e-02,  6.7129e-03,\n",
       "                       1.6785e-02, -1.3703e-02, -2.1074e-02,  1.9509e-02,  1.9634e-02,\n",
       "                       1.8380e-02,  1.7869e-02,  8.9869e-03,  9.0545e-03,  2.3827e-02,\n",
       "                      -2.0844e-02,  1.9322e-02,  1.9143e-03,  1.8526e-03, -1.4339e-02,\n",
       "                      -2.0559e-02,  1.5990e-02, -9.5902e-03,  7.3212e-03,  1.2260e-02,\n",
       "                       9.2384e-03,  5.5056e-03, -1.0082e-02, -9.4453e-03, -1.0909e-02,\n",
       "                      -1.6439e-02, -2.7312e-03, -1.0364e-02, -2.2580e-02, -3.4481e-03,\n",
       "                      -1.0899e-02,  1.0031e-02,  1.8087e-02,  1.0839e-02,  4.1904e-03,\n",
       "                       3.0860e-03,  1.6423e-02, -1.7986e-02,  2.6333e-03, -2.8135e-03,\n",
       "                      -8.0556e-03,  5.1493e-03,  1.1859e-02, -2.7648e-03, -1.4657e-02,\n",
       "                       1.3619e-02, -1.4954e-02,  7.5382e-03,  1.1790e-02, -3.7155e-03,\n",
       "                      -9.2259e-03,  1.5594e-03, -1.3249e-02, -1.6210e-02, -8.2777e-04,\n",
       "                       1.4089e-02, -4.3871e-03, -1.5737e-02,  6.4903e-03,  1.4789e-02,\n",
       "                       1.6066e-02, -1.0568e-05,  9.6076e-03, -3.6981e-06, -1.5567e-02,\n",
       "                      -1.8483e-02,  1.5384e-02,  1.9381e-02, -7.9926e-03, -3.1353e-04,\n",
       "                       1.3917e-02, -1.3900e-02,  9.9078e-03,  1.5205e-02,  1.7507e-02,\n",
       "                       1.1014e-04,  2.0190e-04,  1.8697e-02, -5.0569e-03,  4.2539e-04,\n",
       "                      -4.4882e-04, -1.8944e-02, -1.3887e-02, -7.0773e-03,  3.7482e-04,\n",
       "                       1.9362e-03, -1.4750e-03,  1.6813e-02,  1.3687e-02,  1.8837e-02,\n",
       "                      -2.0134e-02,  1.5942e-02,  5.6913e-03,  1.1386e-02,  4.8903e-04,\n",
       "                       2.1649e-02, -5.9565e-03, -3.7474e-03,  7.8399e-03,  1.8330e-02,\n",
       "                       9.5177e-03, -2.3265e-02, -4.7455e-03,  1.6928e-03,  1.1531e-02,\n",
       "                       1.5411e-02,  2.0143e-02,  2.5332e-06,  1.0003e-02,  2.3253e-03,\n",
       "                      -9.9702e-03,  2.0217e-02, -4.7755e-03,  1.9110e-02,  1.8942e-02,\n",
       "                       1.6715e-02, -2.7448e-03,  1.4871e-02,  2.2192e-02, -2.0837e-02,\n",
       "                       5.4008e-03, -5.3138e-03,  2.4324e-02,  1.4120e-02,  2.2497e-03,\n",
       "                      -6.8945e-03,  2.3607e-03,  8.3204e-03,  5.2182e-03,  2.1371e-02,\n",
       "                       1.4924e-02,  2.0060e-02,  9.4452e-03,  1.0575e-02,  2.1931e-02,\n",
       "                      -1.8697e-02, -1.0831e-02,  1.1437e-03,  1.5472e-02,  1.7620e-02,\n",
       "                       1.4241e-02,  7.1760e-03, -1.3658e-02,  5.8607e-05,  1.2288e-02,\n",
       "                       5.1768e-03, -8.8724e-03,  1.9115e-02, -8.2624e-03,  1.2833e-02,\n",
       "                       8.8482e-03, -2.9464e-03, -1.6406e-02, -1.7120e-02,  2.2120e-02,\n",
       "                       1.1260e-02, -1.3956e-02,  1.6299e-02, -1.7567e-02,  6.3409e-03,\n",
       "                       1.3943e-02, -1.4379e-02, -2.2223e-02,  1.7414e-02,  1.2647e-02,\n",
       "                       1.5845e-02,  1.2297e-02,  1.6417e-02, -8.7175e-03, -8.4558e-03,\n",
       "                      -1.4193e-02, -1.7891e-03,  9.8042e-03,  2.0722e-02, -2.0180e-02,\n",
       "                       1.4811e-02, -1.5911e-02, -8.0469e-03, -9.0112e-03,  1.6293e-02,\n",
       "                       1.4084e-02, -1.0169e-02, -9.3423e-03, -2.0461e-02, -1.0911e-02,\n",
       "                      -1.0868e-02,  9.9257e-03,  1.3032e-02,  4.3362e-05,  1.6115e-02,\n",
       "                      -1.7012e-02,  1.0948e-02, -1.3658e-02, -6.4709e-03,  6.5198e-03,\n",
       "                       4.0732e-03, -2.1105e-02,  1.3548e-02, -2.4100e-03,  1.1134e-02,\n",
       "                      -4.9582e-03,  1.9514e-02,  1.5629e-02, -7.9230e-03, -6.0405e-03,\n",
       "                      -8.6935e-03,  1.0154e-02, -1.8030e-02, -3.1678e-03, -1.6901e-02,\n",
       "                       1.4517e-02,  8.3004e-03,  1.5924e-02, -1.7876e-02,  1.5331e-03,\n",
       "                       4.8790e-04,  2.2288e-05,  1.8033e-02, -8.9442e-03,  1.4344e-03,\n",
       "                      -1.1872e-02, -1.5878e-02,  1.3143e-02,  6.7049e-05, -6.9156e-03,\n",
       "                      -1.4242e-02, -7.5823e-03,  1.2478e-02,  3.1589e-03, -3.0071e-03,\n",
       "                      -5.0616e-03, -1.8750e-02, -1.7223e-02,  1.0463e-02,  5.9775e-03,\n",
       "                       2.0291e-02, -1.5514e-02, -9.2966e-03,  2.5547e-03,  1.8402e-03,\n",
       "                       5.7912e-03,  1.8962e-02,  1.0023e-02,  5.4823e-03,  8.1768e-03,\n",
       "                      -3.1687e-03, -2.2100e-02, -2.7723e-03, -1.6690e-02, -1.1497e-02,\n",
       "                       1.7647e-02, -8.5479e-03,  5.8864e-03,  8.9731e-03,  1.9278e-02,\n",
       "                      -2.1980e-02, -1.3326e-02, -3.5125e-03, -2.9403e-03,  1.5289e-02,\n",
       "                       4.2017e-03, -1.4387e-02,  1.1379e-02,  4.0174e-03, -3.3218e-03,\n",
       "                       1.5944e-02, -1.7019e-02, -2.1285e-02, -4.2922e-03, -2.4027e-02,\n",
       "                      -2.2246e-03,  1.9490e-02, -7.9420e-04,  5.4589e-03, -9.7497e-03,\n",
       "                       5.4762e-03, -1.2831e-02,  5.5239e-03, -1.8519e-02, -3.8528e-03,\n",
       "                       6.5403e-03, -1.1610e-02, -8.6812e-03, -8.4317e-03,  6.2231e-03,\n",
       "                      -1.5964e-02,  1.2499e-02,  5.0536e-03,  2.1204e-02,  5.5519e-03,\n",
       "                       2.1629e-02, -1.1140e-02, -1.3831e-02, -1.0551e-02, -2.2176e-02,\n",
       "                      -1.6802e-02,  2.9385e-03, -1.0858e-02,  2.1520e-02,  5.9862e-03,\n",
       "                       2.2268e-02,  1.1973e-02,  1.0830e-02, -1.9988e-02, -1.3543e-02,\n",
       "                      -9.5558e-03,  2.3259e-02, -5.4316e-03,  1.2780e-02, -8.9406e-03,\n",
       "                      -1.7963e-02,  5.9804e-03,  1.6221e-02,  2.2968e-02, -1.5127e-02,\n",
       "                       1.5568e-02, -2.7219e-03, -7.7408e-03, -8.4211e-03,  5.1994e-03,\n",
       "                      -5.6847e-04,  1.9874e-02,  1.7312e-02, -1.9000e-03,  1.9371e-02,\n",
       "                      -1.1433e-02,  1.2477e-03, -6.6515e-04, -6.8153e-03, -1.5535e-03,\n",
       "                      -4.3611e-03, -1.1390e-02, -1.5340e-02,  7.9800e-03, -1.2029e-02,\n",
       "                      -2.1725e-02,  6.5865e-03,  1.6466e-02,  9.6584e-03,  1.4444e-02,\n",
       "                      -1.7767e-03, -1.7567e-03,  2.1185e-02,  5.4345e-03,  7.4546e-03,\n",
       "                      -2.0941e-03,  8.7991e-03,  1.7344e-02, -1.7742e-02,  5.4863e-03,\n",
       "                      -1.6250e-03, -6.1045e-03, -2.1026e-02,  8.5542e-04, -2.0880e-02,\n",
       "                      -1.5680e-02,  2.3630e-02, -3.7485e-03, -1.0180e-02,  2.5059e-03,\n",
       "                      -1.6203e-02, -8.2968e-03, -3.8120e-03,  7.3993e-04, -1.3936e-02,\n",
       "                       1.8106e-02, -1.1041e-02, -1.1721e-02, -8.4967e-04, -1.4239e-03,\n",
       "                      -9.3056e-03, -4.2496e-03,  1.3104e-02,  7.5932e-03, -1.6619e-02,\n",
       "                       1.8735e-02,  6.7773e-03,  1.6958e-02, -5.8603e-03,  2.0846e-02,\n",
       "                      -1.2743e-02,  1.3565e-02,  2.7620e-03, -3.3447e-03,  1.1606e-02,\n",
       "                       8.3236e-03, -1.0174e-02,  2.4563e-03, -9.7122e-03,  4.9369e-03,\n",
       "                      -2.1259e-02, -1.0895e-02,  2.6110e-03,  2.1857e-03,  1.1191e-02,\n",
       "                       3.3355e-03,  6.1735e-03,  1.2886e-02, -2.0394e-02, -6.8282e-03,\n",
       "                       6.0724e-03,  1.4388e-03, -3.5327e-04, -9.0956e-03, -5.0415e-03,\n",
       "                      -8.9782e-03, -6.5209e-03, -2.0016e-02,  5.5579e-05,  1.7024e-02,\n",
       "                       3.1646e-03, -8.0243e-03,  1.7393e-04,  2.1378e-02, -1.6309e-02,\n",
       "                      -2.7070e-03, -1.5489e-02,  2.0334e-02,  1.5863e-02,  1.3246e-02,\n",
       "                      -1.0777e-02,  9.4537e-03,  1.2505e-02, -1.5084e-02, -2.7012e-03,\n",
       "                       1.6442e-02,  1.8037e-02,  1.0675e-02,  1.4757e-02, -1.8811e-02,\n",
       "                      -1.1369e-02, -1.7856e-02,  4.9899e-03,  1.9372e-02, -8.8217e-03,\n",
       "                      -2.2119e-02, -8.1893e-03])),\n",
       "             ('transformer.resblocks.9.ln_2.weight',\n",
       "              tensor([0.9993, 0.9997, 1.0005, 1.0023, 1.0036, 1.0011, 1.0011, 0.9970, 0.9979,\n",
       "                      1.0029, 0.9978, 1.0008, 1.0004, 1.0028, 0.9957, 0.9970, 1.0030, 1.0030,\n",
       "                      1.0005, 0.9969, 0.9992, 1.0011, 1.0015, 0.9998, 1.0021, 1.0031, 0.9992,\n",
       "                      0.9990, 1.0017, 1.0017, 1.0043, 1.0033, 1.0013, 1.0015, 1.0051, 1.0000,\n",
       "                      1.0009, 1.0036, 1.0025, 1.0027, 1.0011, 1.0004, 1.0003, 0.9992, 0.9985,\n",
       "                      0.9989, 0.9984, 1.0005, 1.0011, 0.9974, 1.0011, 1.0007, 0.9976, 0.9977,\n",
       "                      1.0012, 1.0016, 1.0015, 1.0003, 1.0005, 1.0026, 0.9994, 1.0020, 1.0000,\n",
       "                      1.0002, 1.0008, 1.0009, 0.9993, 0.9990, 1.0004, 0.9999, 1.0042, 1.0017,\n",
       "                      0.9984, 1.0010, 0.9991, 1.0028, 0.9983, 0.9985, 1.0007, 1.0019, 1.0007,\n",
       "                      1.0031, 0.9985, 0.9992, 0.9999, 1.0010, 0.9985, 1.0044, 1.0035, 0.9987,\n",
       "                      1.0015, 1.0009, 0.9988, 1.0003, 1.0016, 0.9968, 1.0005, 1.0015, 1.0013,\n",
       "                      0.9994, 1.0012, 0.9997, 0.9998, 0.9973, 1.0001, 0.9980, 0.9978, 0.9991,\n",
       "                      0.9999, 1.0030, 0.9991, 1.0012, 1.0035, 0.9984, 1.0021, 0.9984, 0.9998,\n",
       "                      1.0016, 1.0010, 0.9995, 1.0019, 0.9981, 1.0007, 1.0009, 1.0033, 1.0022,\n",
       "                      0.9955, 1.0021, 1.0011, 1.0038, 1.0026, 0.9976, 0.9987, 1.0000, 1.0000,\n",
       "                      1.0036, 1.0013, 0.9962, 1.0019, 1.0029, 0.9991, 0.9993, 1.0015, 0.9986,\n",
       "                      1.0007, 0.9992, 0.9971, 1.0024, 1.0045, 1.0041, 0.9994, 0.9941, 0.9922,\n",
       "                      1.0029, 1.0037, 1.0009, 1.0009, 0.9977, 0.9999, 0.9993, 0.9996, 1.0009,\n",
       "                      0.9983, 1.0037, 1.0012, 0.9995, 1.0018, 0.9996, 0.9995, 1.0008, 0.9995,\n",
       "                      0.9979, 1.0012, 1.0013, 0.9984, 1.0003, 0.9998, 1.0004, 1.0037, 0.9987,\n",
       "                      1.0018, 1.0035, 0.9992, 0.9997, 0.9957, 0.9990, 0.9975, 0.9970, 1.0028,\n",
       "                      0.9998, 1.0075, 1.0027, 0.9986, 0.9984, 1.0022, 0.9994, 1.0011, 1.0002,\n",
       "                      1.0001, 0.9986, 1.0020, 1.0003, 1.0020, 1.0005, 1.0004, 1.0009, 1.0012,\n",
       "                      1.0067, 0.9971, 1.0020, 0.9993, 0.9982, 1.0012, 0.9997, 0.9969, 1.0003,\n",
       "                      0.9981, 0.9988, 1.0010, 1.0015, 1.0008, 1.0011, 0.9991, 0.9990, 0.9983,\n",
       "                      1.0041, 0.9998, 1.0014, 0.9982, 0.9997, 1.0004, 0.9972, 0.9999, 1.0009,\n",
       "                      0.9986, 1.0017, 0.9981, 0.9986, 1.0016, 1.0008, 1.0006, 0.9987, 0.9995,\n",
       "                      1.0027, 1.0013, 1.0030, 0.9990, 1.0030, 1.0021, 1.0019, 1.0002, 0.9972,\n",
       "                      1.0018, 1.0027, 1.0012, 0.9999, 1.0010, 0.9989, 1.0008, 0.9949, 1.0022,\n",
       "                      1.0019, 1.0017, 0.9972, 0.9974, 1.0002, 0.9989, 1.0006, 0.9992, 1.0033,\n",
       "                      1.0026, 0.9996, 1.0023, 1.0016, 0.9990, 0.9992, 1.0016, 1.0010, 0.9957,\n",
       "                      0.9968, 1.0040, 0.9996, 0.9984, 0.9986, 1.0025, 0.9983, 1.0010, 1.0007,\n",
       "                      0.9994, 1.0001, 1.0029, 1.0004, 0.9978, 0.9974, 1.0009, 1.0012, 0.9989,\n",
       "                      0.9987, 1.0020, 0.9986, 0.9999, 1.0014, 1.0020, 0.9964, 0.9992, 0.9973,\n",
       "                      0.9998, 1.0021, 0.9970, 1.0001, 1.0031, 0.9992, 1.0042, 1.0001, 1.0023,\n",
       "                      0.9955, 0.9992, 0.9981, 1.0022, 0.9988, 0.9957, 1.0014, 1.0000, 0.9999,\n",
       "                      1.0002, 0.9983, 1.0001, 1.0032, 0.9962, 1.0021, 1.0008, 1.0011, 1.0000,\n",
       "                      1.0003, 0.9988, 1.0022, 1.0020, 1.0027, 1.0000, 0.9987, 0.9970, 0.9984,\n",
       "                      0.9990, 1.0047, 0.9996, 1.0034, 1.0001, 1.0022, 1.0000, 1.0000, 0.9963,\n",
       "                      0.9991, 1.0012, 1.0028, 0.9999, 1.0041, 0.9974, 1.0023, 1.0019, 0.9980,\n",
       "                      1.0015, 0.9997, 0.9975, 0.9974, 1.0032, 0.9954, 1.0027, 0.9962, 0.9980,\n",
       "                      0.9988, 1.0011, 0.9997, 1.0033, 0.9988, 1.0011, 0.9956, 1.0007, 1.0001,\n",
       "                      0.9990, 1.0009, 0.9989, 0.9993, 0.9986, 0.9988, 0.9983, 0.9998, 0.9994,\n",
       "                      0.9978, 0.9970, 0.9999, 1.0019, 1.0013, 0.9973, 0.9992, 0.9983, 0.9996,\n",
       "                      1.0002, 1.0005, 0.9959, 1.0020, 1.0019, 0.9988, 1.0021, 1.0016, 1.0010,\n",
       "                      0.9992, 0.9977, 0.9988, 1.0028, 1.0044, 1.0020, 0.9987, 0.9975, 0.9979,\n",
       "                      0.9966, 1.0015, 0.9997, 0.9989, 1.0009, 0.9960, 0.9986, 1.0051, 1.0006,\n",
       "                      1.0013, 0.9976, 1.0025, 1.0032, 1.0044, 1.0005, 1.0004, 1.0012, 1.0024,\n",
       "                      1.0001, 1.0034, 0.9986, 0.9991, 0.9987, 0.9990, 1.0014, 1.0027, 0.9979,\n",
       "                      0.9987, 1.0048, 1.0029, 1.0019, 0.9990, 1.0012, 0.9978, 1.0021, 1.0008,\n",
       "                      0.9980, 0.9985, 0.9993, 0.9973, 1.0007, 1.0005, 0.9999, 1.0000, 0.9992,\n",
       "                      1.0000, 1.0015, 0.9972, 1.0018, 0.9965, 1.0010, 1.0009, 1.0005, 0.9973,\n",
       "                      1.0045, 1.0016, 0.9996, 1.0033, 0.9991, 1.0011, 0.9990, 0.9998, 1.0012,\n",
       "                      0.9968, 1.0014, 0.9974, 1.0024, 1.0015, 1.0031, 1.0019, 0.9980, 1.0038,\n",
       "                      1.0019, 0.9998, 0.9996, 1.0007, 0.9992, 0.9991, 1.0011, 1.0003, 0.9993,\n",
       "                      0.9982, 1.0017, 1.0020, 1.0026, 0.9993, 0.9990, 1.0024, 0.9979, 0.9967,\n",
       "                      0.9984, 1.0027, 1.0009, 1.0009, 1.0005, 1.0022, 0.9997, 0.9997])),\n",
       "             ('transformer.resblocks.9.ln_2.bias',\n",
       "              tensor([-5.5749e-04, -4.1211e-04, -2.8044e-03, -1.0517e-03, -1.6044e-03,\n",
       "                      -5.5340e-04,  1.6584e-03, -3.5715e-03,  2.6560e-03,  1.8588e-03,\n",
       "                       3.3799e-03,  4.4106e-05,  4.1381e-04, -5.7794e-04,  4.1814e-03,\n",
       "                      -4.9387e-03, -1.3386e-03, -1.8319e-03,  2.2506e-03,  1.9474e-03,\n",
       "                      -8.0880e-04, -1.0092e-04,  2.6778e-04,  5.0670e-04, -1.3236e-04,\n",
       "                       1.9031e-03,  3.0015e-03,  1.7771e-03,  8.4792e-04, -1.0094e-03,\n",
       "                       1.4071e-03,  1.6518e-03, -5.5169e-04, -1.9924e-04, -3.0334e-04,\n",
       "                       6.8834e-04, -1.1921e-03,  2.6033e-03, -6.2475e-04,  9.7145e-05,\n",
       "                      -3.6210e-04, -6.5486e-04,  4.1352e-04, -2.8761e-03,  1.7699e-03,\n",
       "                      -2.1539e-03,  1.9296e-03,  1.7648e-03,  6.8627e-04, -3.0086e-03,\n",
       "                      -5.2093e-04, -9.3607e-05,  8.8312e-04, -3.1518e-03, -2.1537e-04,\n",
       "                       2.4236e-03,  3.8924e-03,  5.3821e-04, -6.6797e-04, -1.5477e-03,\n",
       "                       2.8371e-04, -1.0662e-04, -3.5944e-03, -1.2687e-03, -1.2562e-03,\n",
       "                       1.8613e-03, -1.5427e-03,  1.6325e-03, -5.5508e-04, -2.4465e-03,\n",
       "                      -1.4390e-03,  5.4111e-04,  2.3316e-03,  3.7558e-03, -9.2957e-04,\n",
       "                       3.8946e-04,  9.6784e-04,  2.9230e-03,  1.4733e-03, -1.1559e-03,\n",
       "                      -8.5728e-04, -2.9673e-04, -1.2877e-04, -2.9998e-03, -2.0808e-03,\n",
       "                      -2.0063e-03,  2.2658e-03,  1.6258e-03,  2.6026e-03,  2.3840e-03,\n",
       "                      -1.0167e-03,  4.8551e-04,  1.2393e-03,  2.8773e-03,  1.9352e-03,\n",
       "                       8.5326e-04, -5.2992e-04,  1.0706e-03,  3.8426e-04, -1.1010e-03,\n",
       "                      -1.3095e-04, -1.6846e-03, -2.8150e-04,  2.3757e-03,  1.7161e-03,\n",
       "                       1.9557e-03, -3.6834e-04,  2.2939e-03,  3.5606e-04,  2.8412e-03,\n",
       "                      -1.1017e-04,  3.0302e-03, -5.3214e-04, -8.2918e-04, -5.3436e-04,\n",
       "                       1.7110e-03, -2.7237e-03, -1.2040e-03, -1.4687e-03,  1.5883e-03,\n",
       "                      -1.1238e-03,  4.5475e-03, -1.0491e-03,  1.7668e-04, -1.2850e-03,\n",
       "                      -5.4206e-04,  1.8809e-03,  7.4313e-04, -2.1741e-03, -1.8516e-03,\n",
       "                      -1.6452e-03, -2.1261e-03, -1.6123e-03,  7.6620e-04,  1.1826e-03,\n",
       "                      -1.0763e-03,  4.6858e-03, -3.6808e-03, -7.8054e-04, -2.2259e-04,\n",
       "                      -5.1787e-04,  6.3676e-04, -3.3278e-04,  4.0974e-04, -1.0663e-03,\n",
       "                      -2.0518e-03,  5.2541e-03, -1.7915e-03, -2.8863e-04,  1.7986e-03,\n",
       "                       6.5056e-04, -4.5874e-03, -6.4949e-03, -5.3402e-04,  1.5140e-03,\n",
       "                      -6.7780e-04,  8.4175e-04,  1.7944e-03,  4.1805e-04,  2.0145e-03,\n",
       "                       3.6608e-04, -1.7059e-03, -8.8780e-04, -9.7762e-04,  2.3283e-03,\n",
       "                      -1.5762e-03,  1.7227e-03,  3.5074e-04,  1.3306e-03, -1.7679e-03,\n",
       "                      -1.1815e-03, -5.9922e-04,  4.1861e-03, -9.5113e-04, -1.7220e-03,\n",
       "                      -1.1510e-03,  2.0442e-03, -1.8265e-03,  2.6749e-03,  3.6754e-03,\n",
       "                      -3.8728e-04,  4.4735e-04,  3.0526e-03,  1.4380e-03, -2.2504e-03,\n",
       "                      -1.6353e-03, -1.8387e-03, -1.0914e-03, -1.7545e-03,  3.7279e-03,\n",
       "                      -9.3949e-04, -2.7544e-03,  8.5122e-05, -2.1478e-03,  5.6890e-04,\n",
       "                       2.0646e-03, -7.6713e-04, -1.0705e-03, -2.5379e-03, -1.3928e-03,\n",
       "                       2.0380e-03, -1.2098e-04, -7.7709e-04, -7.5926e-04, -1.4637e-03,\n",
       "                      -3.2327e-04, -2.9372e-04, -3.2355e-03, -2.4065e-03,  1.3447e-03,\n",
       "                      -4.5314e-03,  2.9737e-03,  7.0683e-04, -3.5248e-03,  2.2605e-03,\n",
       "                      -1.5839e-04, -3.3054e-03,  2.4731e-03, -8.9606e-04,  4.7368e-04,\n",
       "                       5.7189e-04,  1.5856e-03, -8.8232e-04, -4.1741e-03, -5.7111e-04,\n",
       "                       1.3882e-03,  1.2523e-03,  2.2187e-03, -2.7666e-03, -5.6515e-04,\n",
       "                      -1.1056e-03,  4.0920e-04,  5.5645e-04, -2.6890e-07,  1.3750e-03,\n",
       "                       1.2764e-03, -3.3592e-03,  1.4702e-03, -1.8449e-03,  9.3627e-04,\n",
       "                      -3.9069e-04, -2.1435e-03,  1.0757e-03,  2.3956e-03, -2.1908e-03,\n",
       "                      -1.9065e-03, -2.3276e-03,  1.5186e-03, -1.0714e-03, -8.2754e-04,\n",
       "                      -3.2380e-03,  2.1789e-03, -2.1829e-03,  6.5601e-04, -1.6306e-03,\n",
       "                       4.9999e-04,  1.1701e-03,  9.5754e-04,  2.0962e-03,  3.8227e-03,\n",
       "                      -1.7517e-03,  1.4964e-03, -1.0277e-04,  2.5202e-03,  3.4690e-03,\n",
       "                       1.9838e-04, -2.8063e-03, -2.4474e-03, -1.3255e-03,  1.3972e-03,\n",
       "                      -7.6243e-04,  1.4962e-03,  2.4423e-03, -2.1230e-03, -2.9975e-03,\n",
       "                       1.5865e-03, -5.6143e-04,  1.0553e-03, -4.8098e-03,  3.6923e-03,\n",
       "                       6.5861e-04, -1.5145e-03,  1.8683e-03,  6.6021e-04, -2.6210e-04,\n",
       "                      -5.7583e-03, -1.5456e-03, -1.1932e-03, -1.4412e-03,  1.0369e-03,\n",
       "                       6.3070e-05, -1.5332e-03,  2.2768e-03, -4.3977e-04,  2.7144e-03,\n",
       "                       1.4577e-04,  2.4949e-03,  8.7985e-04,  2.7972e-04,  1.4485e-03,\n",
       "                      -4.6226e-05,  9.2324e-04, -1.3782e-03,  3.2847e-03,  1.5472e-04,\n",
       "                       4.0678e-03, -1.2568e-03,  1.0262e-06,  3.0940e-03,  1.2163e-03,\n",
       "                      -1.4167e-03, -1.7332e-03,  3.3465e-04, -1.9496e-03,  2.1236e-03,\n",
       "                      -3.4209e-03, -1.2902e-03, -1.4199e-03, -2.9823e-03,  3.8949e-04,\n",
       "                      -6.1164e-03,  1.1319e-03, -2.1129e-03, -1.4114e-03,  6.7488e-04,\n",
       "                       3.5312e-04, -2.2939e-03, -1.9650e-04,  2.1808e-03, -3.2960e-03,\n",
       "                       6.6366e-04, -2.1148e-03,  5.1817e-04,  1.3287e-04, -3.0006e-03,\n",
       "                      -2.2174e-03,  2.7887e-03,  9.7138e-04, -1.7554e-03, -1.5364e-03,\n",
       "                       4.0408e-03, -5.7394e-03,  7.9068e-04, -2.6170e-03, -1.9097e-04,\n",
       "                       4.9549e-06, -1.8044e-03, -6.9058e-04,  1.4852e-03, -2.3187e-03,\n",
       "                       3.2104e-03,  1.0168e-03, -4.8989e-04, -3.6284e-03,  8.6304e-07,\n",
       "                       1.2524e-03, -3.0302e-03,  4.9111e-04, -2.0895e-03,  3.8519e-04,\n",
       "                      -2.8860e-03,  1.4034e-03, -1.7666e-03, -2.8999e-03,  1.7249e-03,\n",
       "                      -2.0438e-03, -1.6434e-05,  3.3648e-03, -1.0071e-03,  1.9655e-03,\n",
       "                       2.0948e-03, -1.2413e-03,  1.9860e-03,  5.0596e-04,  1.5928e-03,\n",
       "                      -3.9778e-03,  4.6118e-04, -1.6076e-03,  1.2231e-03,  2.0342e-03,\n",
       "                      -5.0050e-04,  1.2253e-03, -3.8353e-04,  1.6145e-03,  1.0301e-03,\n",
       "                       5.2218e-04,  2.1574e-04, -1.5473e-04, -3.6991e-03,  6.7734e-04,\n",
       "                      -3.9397e-03, -1.2296e-03,  2.4118e-03,  1.7323e-03, -4.7462e-03,\n",
       "                      -2.0600e-03,  1.2640e-03, -1.7850e-03, -3.2372e-04, -2.9933e-03,\n",
       "                      -3.3485e-03,  4.6514e-03, -3.0244e-03, -2.4402e-03, -3.1875e-03,\n",
       "                       4.1013e-04, -6.3235e-04, -6.6369e-04,  2.6767e-03,  5.1697e-04,\n",
       "                       7.6436e-04,  8.0444e-04, -2.0256e-03, -1.3848e-03, -1.0380e-03,\n",
       "                       9.3165e-04, -3.2589e-03, -1.5325e-03,  7.7822e-04, -3.5403e-03,\n",
       "                       2.0883e-04,  1.4412e-03, -1.2379e-03,  2.0899e-03, -1.9173e-04,\n",
       "                      -3.4207e-03,  3.1557e-04,  2.6580e-03, -7.9417e-04,  3.9794e-03,\n",
       "                       1.6692e-04, -1.4896e-03, -1.5604e-03,  8.9962e-04,  3.2486e-03,\n",
       "                      -2.1991e-03, -2.3503e-03,  7.1663e-03,  2.1845e-04,  1.6212e-03,\n",
       "                       2.3254e-03, -9.5259e-04,  9.5586e-05, -9.5357e-04,  4.4992e-04,\n",
       "                       6.6284e-04,  6.1950e-04, -1.4900e-03,  3.2355e-04,  5.5849e-05,\n",
       "                      -1.5439e-03, -1.3622e-03,  9.6995e-04, -1.2059e-03,  1.8832e-03,\n",
       "                      -7.5228e-04, -3.1080e-04, -4.7897e-04,  8.5365e-04, -9.7744e-04,\n",
       "                       3.5703e-04,  4.3449e-04, -1.8340e-03, -4.8342e-03, -2.0941e-03,\n",
       "                       5.1766e-04,  3.6497e-04, -3.1338e-03,  3.0714e-03,  1.1050e-03,\n",
       "                       1.4459e-03, -2.6391e-03,  1.3796e-03,  1.6647e-03,  1.9713e-03,\n",
       "                      -1.5378e-03,  6.5785e-04,  3.4982e-03,  3.2832e-03,  3.3696e-03,\n",
       "                      -3.7221e-04,  5.4414e-04,  1.8457e-03,  6.5083e-04,  1.7166e-03,\n",
       "                      -4.6843e-03,  8.8315e-04,  2.7736e-03, -2.3002e-03,  1.9482e-03,\n",
       "                      -1.4735e-04,  7.2377e-04,  1.8763e-03,  2.7661e-04,  1.5549e-04,\n",
       "                      -1.0190e-03,  2.0048e-03,  3.2481e-03, -1.3517e-03,  5.7289e-04,\n",
       "                       3.4193e-03, -1.5626e-03, -7.4929e-04, -2.1262e-03,  1.5078e-03,\n",
       "                       2.6660e-05,  3.1715e-04, -1.0075e-03,  2.1404e-03,  1.1772e-03,\n",
       "                       6.4624e-04, -4.5349e-04])),\n",
       "             ('transformer.resblocks.10.attn.in_proj_weight',\n",
       "              tensor([[-0.0539,  0.0054, -0.0994,  ...,  0.0307, -0.0360,  0.0297],\n",
       "                      [-0.0427,  0.0114,  0.0219,  ..., -0.0798,  0.0696,  0.0651],\n",
       "                      [-0.0008,  0.0612,  0.0027,  ...,  0.0156, -0.0502,  0.0107],\n",
       "                      ...,\n",
       "                      [ 0.0247,  0.0354, -0.0269,  ..., -0.0531,  0.0904, -0.0508],\n",
       "                      [ 0.0043, -0.0074, -0.0476,  ...,  0.0100,  0.0110,  0.0444],\n",
       "                      [ 0.0151, -0.0415,  0.0336,  ...,  0.0736, -0.0694,  0.0109]])),\n",
       "             ('transformer.resblocks.10.attn.in_proj_bias',\n",
       "              tensor([ 0.0045,  0.0009, -0.0027,  ..., -0.0025,  0.0010,  0.0014])),\n",
       "             ('transformer.resblocks.10.attn.out_proj.weight',\n",
       "              tensor([[ 0.0150, -0.0070,  0.0091,  ...,  0.0024, -0.0024, -0.0116],\n",
       "                      [-0.0008,  0.0047, -0.0181,  ..., -0.0012, -0.0080, -0.0031],\n",
       "                      [ 0.0102, -0.0049, -0.0115,  ...,  0.0094, -0.0061,  0.0048],\n",
       "                      ...,\n",
       "                      [ 0.0050, -0.0079, -0.0094,  ..., -0.0050, -0.0062, -0.0033],\n",
       "                      [ 0.0019,  0.0025,  0.0191,  ..., -0.0006, -0.0062,  0.0051],\n",
       "                      [ 0.0056, -0.0009, -0.0102,  ...,  0.0111, -0.0002,  0.0056]])),\n",
       "             ('transformer.resblocks.10.attn.out_proj.bias',\n",
       "              tensor([-6.3260e-04, -3.9287e-03, -6.7339e-05, -4.8920e-04, -8.9896e-04,\n",
       "                      -2.7963e-03, -1.0158e-07,  2.4510e-03, -3.3942e-03,  1.8975e-03,\n",
       "                      -4.7905e-03, -2.6485e-03, -5.3362e-04, -1.5688e-03, -5.8677e-03,\n",
       "                       2.4729e-03, -2.4579e-04,  1.9634e-03, -8.9797e-04, -9.1079e-04,\n",
       "                      -1.5368e-04,  7.9583e-04,  3.0159e-04, -6.3327e-04, -5.2529e-04,\n",
       "                      -1.5601e-03, -1.3011e-03, -1.2957e-03, -9.3796e-04, -1.9270e-03,\n",
       "                       7.3179e-04, -8.3103e-05,  2.5582e-03, -3.2864e-03, -2.3480e-04,\n",
       "                       1.0395e-03,  1.1735e-03, -1.4647e-04,  1.0656e-03, -3.0226e-03,\n",
       "                       1.5150e-03,  2.2294e-03,  1.6605e-03,  1.5003e-03, -4.0951e-03,\n",
       "                       1.7749e-03, -3.4306e-03,  5.0724e-04, -3.5923e-04,  1.9962e-03,\n",
       "                      -8.2687e-04, -3.7491e-04, -1.1265e-03,  2.6733e-03,  2.9149e-03,\n",
       "                       9.9839e-04, -1.5968e-03, -9.4432e-04, -1.6354e-04, -8.9736e-04,\n",
       "                      -1.9240e-03,  2.7448e-04,  8.1259e-04,  7.4235e-04, -1.5415e-04,\n",
       "                      -1.0540e-03,  2.1865e-03,  3.1312e-04, -1.4815e-03,  1.8373e-03,\n",
       "                      -4.7423e-04, -3.5503e-04, -2.3351e-03,  6.8914e-05, -9.1528e-04,\n",
       "                       1.8633e-03, -1.7334e-03, -2.0510e-03, -2.6318e-03,  2.1967e-03,\n",
       "                       1.6321e-03, -2.0439e-03,  7.3833e-04,  2.0162e-03,  2.1245e-03,\n",
       "                       2.7523e-04, -7.2014e-04,  5.1708e-04,  2.2451e-04, -2.6814e-03,\n",
       "                       2.4319e-04, -4.8551e-04,  1.7266e-03, -2.2306e-03, -2.0851e-03,\n",
       "                      -4.3507e-03, -2.6034e-04, -1.4311e-03, -1.9751e-03,  7.3053e-04,\n",
       "                       6.1141e-04, -1.5725e-03,  2.9594e-04, -2.8317e-04, -6.5659e-04,\n",
       "                      -2.8361e-03, -3.7651e-03, -9.7380e-04,  7.8998e-04, -7.8386e-04,\n",
       "                       2.7403e-03,  6.4979e-04,  3.0953e-03,  1.2619e-03, -1.6381e-03,\n",
       "                      -4.9790e-03,  1.5214e-03,  1.6977e-03, -2.9717e-03, -1.0481e-03,\n",
       "                       4.9457e-04, -2.2298e-03,  1.9103e-03, -7.8558e-04, -1.3702e-03,\n",
       "                       8.3541e-04, -1.4960e-03, -2.2337e-03, -9.4927e-04,  4.2146e-05,\n",
       "                       1.3291e-05,  3.6688e-03, -3.3189e-04, -2.5187e-03, -6.8568e-04,\n",
       "                      -1.0310e-03, -1.4821e-03,  2.6169e-03, -1.1869e-03,  6.1228e-04,\n",
       "                       4.7294e-04,  1.8212e-03,  1.2772e-03,  3.1710e-03,  1.5733e-03,\n",
       "                       2.4635e-03, -2.7822e-03,  1.2101e-03,  6.7675e-04,  9.2402e-04,\n",
       "                       1.8778e-03,  5.0203e-03,  6.7854e-03,  1.9251e-03, -6.7921e-04,\n",
       "                      -7.3465e-05,  8.1236e-04, -2.0310e-03,  2.6329e-04,  2.0251e-04,\n",
       "                      -1.9560e-03, -1.0168e-03,  3.4360e-03,  5.7041e-04, -1.5082e-03,\n",
       "                       1.5826e-03,  2.0935e-03, -3.1233e-04, -4.3019e-04, -1.7386e-03,\n",
       "                       1.1847e-03, -2.1496e-04, -2.2969e-03,  1.5530e-04,  2.0476e-03,\n",
       "                       9.6740e-04, -1.6315e-03,  6.0017e-04, -4.1385e-05, -1.3086e-03,\n",
       "                      -2.3263e-03, -7.3829e-04,  1.2289e-03, -3.2008e-03,  4.9947e-03,\n",
       "                      -2.3969e-04,  5.4399e-04, -3.6870e-04,  1.7008e-03, -2.6616e-03,\n",
       "                       7.8865e-04,  6.3357e-04, -7.4134e-04,  2.1120e-03, -1.8364e-03,\n",
       "                      -1.6539e-03,  1.8697e-03,  2.9691e-03,  4.1256e-03,  3.4328e-03,\n",
       "                      -6.3266e-04, -1.6193e-03,  1.4494e-03,  1.8227e-03,  1.1089e-03,\n",
       "                       4.3403e-04, -1.0454e-03,  6.1637e-04,  1.3804e-03,  4.4347e-04,\n",
       "                       2.6873e-03,  2.1722e-06,  4.0990e-04,  2.0018e-03, -1.9650e-03,\n",
       "                      -1.5848e-03,  5.4953e-07, -1.8939e-03, -1.4691e-03,  8.9181e-04,\n",
       "                      -7.0730e-04, -5.6974e-04,  1.6067e-03,  2.6653e-03,  9.4016e-04,\n",
       "                       7.3921e-04, -1.2279e-03, -2.9442e-03,  1.6078e-03,  1.8166e-03,\n",
       "                      -1.3689e-03,  3.8161e-04,  7.2724e-04, -1.4769e-03,  1.4314e-03,\n",
       "                       5.0540e-05,  3.1004e-03, -3.7069e-03,  1.7856e-03,  1.5290e-03,\n",
       "                      -1.8083e-03,  2.8387e-03,  2.8542e-03,  1.2114e-03, -2.1943e-03,\n",
       "                      -5.2383e-06,  9.1777e-04,  4.2570e-04, -3.9100e-05, -1.4808e-03,\n",
       "                       1.1267e-03, -2.6707e-03, -8.4496e-05, -1.5320e-03, -5.7638e-04,\n",
       "                      -9.8431e-04, -2.2040e-04, -1.6939e-03,  4.5149e-05, -3.4464e-03,\n",
       "                       1.7170e-03,  6.5787e-04,  8.6333e-04, -3.6305e-03, -2.0797e-03,\n",
       "                       2.8367e-03,  9.0189e-04,  9.7247e-04, -3.7318e-04, -1.6311e-03,\n",
       "                       1.1952e-03, -1.1213e-03, -1.9451e-03,  5.4884e-04,  1.3651e-03,\n",
       "                      -7.9554e-04,  3.4386e-03, -6.5344e-04,  3.8616e-03, -1.7414e-03,\n",
       "                      -2.1015e-04,  3.7602e-03, -2.7630e-03, -1.8240e-03, -2.7185e-03,\n",
       "                       1.8454e-03,  1.0524e-04,  1.3290e-03,  1.9968e-03,  7.2484e-05,\n",
       "                      -6.7541e-04,  1.4433e-03, -1.8791e-03,  4.5555e-03,  3.9128e-04,\n",
       "                       2.0268e-03, -1.8569e-03,  1.2478e-03, -5.5091e-04, -1.3660e-03,\n",
       "                      -2.8754e-03,  4.5611e-04,  1.6820e-03, -4.0388e-03,  3.7378e-05,\n",
       "                      -2.6397e-03,  8.7532e-04,  3.1421e-04, -1.1594e-03, -2.8801e-03,\n",
       "                       2.0174e-03,  2.8578e-03, -9.9452e-04, -1.6315e-03,  4.2734e-04,\n",
       "                       2.6302e-03,  2.8875e-03,  1.6268e-03,  9.2449e-04,  9.2110e-04,\n",
       "                       3.3335e-03, -7.9777e-04, -6.1892e-04,  2.2932e-04, -2.4714e-03,\n",
       "                       4.2326e-04,  3.7544e-03, -2.0372e-04, -3.9295e-03, -1.3825e-03,\n",
       "                      -6.3038e-04,  1.5314e-03,  5.5260e-04, -6.2721e-04,  1.5029e-03,\n",
       "                       5.9815e-04,  8.5813e-05, -6.9491e-04,  1.0286e-03,  2.1877e-03,\n",
       "                      -3.4912e-03,  3.6484e-03,  3.1922e-04, -2.4522e-04, -3.1397e-03,\n",
       "                      -1.0072e-04, -2.4266e-03,  5.6995e-05, -1.0822e-03,  1.4311e-03,\n",
       "                      -2.1056e-03, -4.1267e-03, -1.6500e-03,  1.5718e-04,  2.0384e-04,\n",
       "                       2.4818e-04,  3.1711e-03,  2.7470e-03,  1.2010e-03, -2.4969e-03,\n",
       "                       1.7570e-03, -5.0345e-04,  3.0786e-03,  3.0051e-03, -7.0112e-04,\n",
       "                       2.7913e-03, -2.8337e-04, -5.0900e-03,  1.7914e-03, -1.0496e-03,\n",
       "                      -1.0723e-03,  1.3714e-03,  6.2495e-04, -6.0036e-04, -5.0472e-04,\n",
       "                       3.5993e-03, -1.0172e-03,  2.7931e-03, -1.7508e-03, -2.5092e-03,\n",
       "                       2.4518e-03,  1.5634e-03, -1.5199e-03, -5.4316e-04, -1.3281e-03,\n",
       "                      -1.1894e-03,  2.9938e-03, -1.2881e-03,  4.0413e-03,  1.2418e-03,\n",
       "                       7.9658e-04, -1.4516e-04, -6.0413e-04,  7.5762e-04,  3.3948e-03,\n",
       "                       2.3681e-03, -1.1993e-03,  2.0766e-03,  3.5109e-04,  5.2748e-04,\n",
       "                      -1.1173e-04, -1.7273e-03, -2.3340e-04, -3.1815e-04,  2.7127e-04,\n",
       "                       1.5826e-03,  8.9212e-04,  1.2972e-03,  1.3682e-03,  1.9024e-03,\n",
       "                      -8.6800e-04, -3.3071e-03,  6.4382e-04,  2.8649e-03,  7.8792e-04,\n",
       "                      -2.8390e-03,  2.5081e-03,  2.0716e-03, -5.8641e-04,  2.5989e-03,\n",
       "                      -8.9173e-04,  1.2151e-03,  2.1602e-03, -2.4366e-03, -1.8430e-03,\n",
       "                       6.8932e-04, -4.5770e-04, -8.3074e-04,  8.7846e-04,  3.4729e-04,\n",
       "                      -1.7575e-03,  2.0494e-03,  1.8419e-03, -8.2430e-04, -3.5840e-03,\n",
       "                      -7.7612e-04,  2.0077e-03, -3.4009e-03,  1.6983e-03, -1.2595e-03,\n",
       "                      -1.4844e-03,  6.7994e-04,  8.3147e-04, -3.5101e-04, -2.1605e-03,\n",
       "                      -2.8665e-03, -4.5050e-04,  1.3811e-03, -1.7893e-04,  1.5803e-03,\n",
       "                       1.1605e-04,  1.0981e-03, -1.1139e-03,  4.8440e-03,  1.0167e-03,\n",
       "                      -2.2597e-03,  3.9135e-04,  2.3380e-03,  2.0318e-03,  1.9895e-05,\n",
       "                       8.5990e-04, -9.4509e-04,  1.1297e-03,  3.8484e-03,  2.9166e-04,\n",
       "                       3.2413e-05,  7.0772e-05,  1.2020e-03, -5.6244e-04, -2.3816e-03,\n",
       "                      -2.0115e-03,  1.3418e-03, -2.4104e-03, -1.4397e-03, -1.9255e-03,\n",
       "                       5.2059e-04, -2.8245e-03, -1.6346e-03, -1.6944e-03, -1.1542e-03,\n",
       "                      -3.1341e-03, -2.3919e-03,  8.8986e-04,  5.4135e-04, -1.2610e-03,\n",
       "                       1.3880e-03, -1.3883e-03, -1.9709e-03,  2.2784e-03,  1.3850e-03,\n",
       "                       7.8504e-04, -1.0989e-03,  9.8098e-04, -2.2975e-03, -1.9143e-03,\n",
       "                       4.2583e-04,  1.3932e-03, -6.8232e-04,  5.0244e-04, -3.2959e-03,\n",
       "                      -2.2384e-03,  9.7778e-04, -1.5690e-03,  3.8964e-03, -3.1630e-03,\n",
       "                       1.4521e-03, -1.9605e-03, -8.8142e-04,  9.8274e-04, -5.5861e-04,\n",
       "                      -1.5946e-03, -1.5927e-04])),\n",
       "             ('transformer.resblocks.10.ln_1.weight',\n",
       "              tensor([0.9992, 1.0015, 1.0029, 1.0030, 1.0011, 1.0010, 0.9972, 0.9994, 0.9987,\n",
       "                      1.0030, 1.0004, 1.0004, 1.0019, 0.9995, 0.9985, 0.9974, 0.9988, 1.0013,\n",
       "                      0.9977, 1.0013, 0.9970, 0.9981, 1.0015, 1.0022, 0.9979, 0.9993, 0.9970,\n",
       "                      0.9985, 0.9997, 0.9965, 1.0009, 0.9986, 0.9995, 0.9985, 0.9994, 1.0026,\n",
       "                      0.9991, 1.0008, 0.9977, 0.9985, 1.0017, 1.0021, 0.9984, 0.9994, 0.9997,\n",
       "                      0.9979, 1.0004, 0.9994, 1.0002, 0.9965, 0.9998, 1.0018, 0.9999, 0.9990,\n",
       "                      1.0001, 0.9967, 1.0000, 1.0008, 0.9996, 0.9995, 1.0008, 0.9992, 0.9979,\n",
       "                      0.9997, 0.9986, 0.9984, 0.9993, 0.9997, 0.9979, 1.0005, 0.9997, 0.9981,\n",
       "                      0.9994, 1.0000, 0.9951, 0.9993, 0.9970, 1.0006, 0.9997, 0.9970, 0.9993,\n",
       "                      1.0006, 1.0012, 0.9983, 1.0003, 0.9962, 0.9997, 0.9998, 0.9997, 0.9961,\n",
       "                      0.9959, 1.0008, 0.9991, 1.0004, 0.9963, 1.0004, 1.0002, 0.9987, 0.9975,\n",
       "                      0.9972, 1.0018, 0.9999, 1.0012, 1.0001, 0.9996, 1.0003, 0.9983, 1.0027,\n",
       "                      0.9986, 1.0003, 0.9997, 0.9991, 0.9982, 1.0000, 1.0001, 0.9972, 0.9998,\n",
       "                      1.0026, 0.9989, 0.9982, 1.0000, 0.9979, 0.9995, 1.0010, 0.9984, 0.9985,\n",
       "                      1.0017, 1.0006, 0.9981, 1.0008, 1.0009, 1.0008, 1.0001, 1.0018, 0.9965,\n",
       "                      1.0014, 0.9986, 0.9987, 0.9967, 1.0003, 0.9998, 1.0006, 0.9989, 1.0021,\n",
       "                      0.9981, 1.0004, 0.9974, 1.0030, 0.9988, 1.0006, 1.0012, 0.9979, 0.9996,\n",
       "                      1.0000, 0.9984, 1.0006, 1.0001, 0.9993, 1.0014, 1.0009, 1.0033, 0.9989,\n",
       "                      0.9986, 1.0001, 0.9992, 1.0000, 0.9990, 0.9972, 1.0006, 0.9993, 1.0006,\n",
       "                      0.9971, 1.0006, 1.0018, 1.0001, 1.0010, 1.0016, 0.9970, 0.9986, 0.9989,\n",
       "                      1.0002, 1.0022, 0.9972, 0.9996, 0.9979, 1.0013, 1.0017, 1.0007, 0.9983,\n",
       "                      0.9995, 1.0007, 0.9998, 1.0024, 0.9968, 1.0000, 0.9994, 0.9973, 0.9987,\n",
       "                      0.9978, 0.9984, 0.9980, 0.9979, 0.9998, 0.9983, 1.0033, 0.9998, 1.0022,\n",
       "                      0.9978, 0.9998, 1.0009, 0.9994, 0.9999, 0.9991, 0.9978, 0.9989, 0.9998,\n",
       "                      1.0003, 0.9996, 0.9991, 0.9983, 1.0026, 1.0019, 0.9978, 1.0013, 1.0004,\n",
       "                      1.0012, 1.0004, 0.9982, 1.0001, 1.0000, 0.9963, 0.9985, 1.0000, 0.9993,\n",
       "                      1.0002, 0.9980, 0.9995, 0.9993, 0.9993, 0.9994, 0.9990, 0.9999, 0.9989,\n",
       "                      1.0001, 0.9992, 0.9969, 0.9991, 1.0008, 1.0004, 1.0001, 0.9996, 0.9977,\n",
       "                      0.9991, 1.0004, 1.0005, 0.9982, 1.0002, 1.0010, 0.9971, 1.0014, 0.9999,\n",
       "                      1.0009, 1.0006, 0.9990, 0.9989, 0.9989, 0.9994, 1.0016, 0.9967, 0.9993,\n",
       "                      0.9995, 0.9995, 0.9970, 0.9988, 0.9990, 0.9981, 0.9987, 1.0025, 0.9977,\n",
       "                      1.0002, 0.9975, 1.0028, 0.9985, 1.0002, 0.9977, 1.0015, 0.9966, 0.9989,\n",
       "                      0.9999, 0.9995, 1.0029, 1.0000, 0.9995, 1.0000, 0.9996, 1.0018, 0.9980,\n",
       "                      1.0026, 0.9989, 0.9972, 1.0006, 1.0013, 1.0000, 1.0011, 1.0014, 0.9986,\n",
       "                      0.9964, 0.9996, 1.0000, 0.9965, 0.9992, 1.0003, 1.0037, 1.0006, 0.9996,\n",
       "                      0.9995, 0.9997, 0.9973, 0.9989, 1.0011, 0.9989, 0.9971, 0.9980, 1.0002,\n",
       "                      1.0014, 0.9997, 1.0006, 1.0019, 0.9992, 1.0000, 0.9966, 0.9972, 0.9971,\n",
       "                      0.9995, 0.9999, 1.0013, 1.0008, 0.9998, 0.9998, 0.9979, 0.9986, 1.0000,\n",
       "                      0.9979, 0.9998, 1.0003, 1.0015, 1.0006, 0.9985, 1.0008, 0.9982, 0.9997,\n",
       "                      0.9989, 1.0003, 0.9994, 0.9987, 1.0011, 1.0009, 0.9982, 0.9979, 0.9988,\n",
       "                      1.0013, 1.0003, 0.9946, 1.0018, 1.0018, 1.0001, 0.9990, 1.0005, 1.0024,\n",
       "                      0.9985, 0.9991, 1.0002, 0.9980, 0.9995, 0.9976, 0.9999, 0.9990, 1.0015,\n",
       "                      0.9981, 0.9986, 1.0012, 1.0015, 1.0013, 1.0005, 0.9975, 1.0025, 1.0005,\n",
       "                      0.9954, 0.9984, 0.9991, 0.9985, 0.9982, 1.0004, 0.9980, 0.9986, 0.9999,\n",
       "                      1.0008, 1.0032, 1.0016, 0.9982, 1.0019, 1.0008, 1.0001, 1.0030, 1.0008,\n",
       "                      1.0003, 0.9986, 0.9988, 1.0003, 0.9997, 1.0016, 1.0002, 1.0003, 0.9997,\n",
       "                      0.9990, 0.9968, 0.9968, 0.9998, 0.9964, 1.0000, 0.9992, 0.9988, 0.9989,\n",
       "                      0.9987, 1.0010, 0.9988, 1.0002, 0.9986, 0.9972, 1.0015, 1.0001, 0.9981,\n",
       "                      0.9966, 0.9981, 0.9993, 1.0017, 1.0012, 0.9998, 1.0007, 1.0006, 0.9995,\n",
       "                      0.9974, 0.9984, 0.9955, 1.0017, 0.9972, 0.9987, 0.9999, 1.0002, 0.9998,\n",
       "                      1.0016, 0.9976, 1.0010, 0.9975, 0.9978, 0.9985, 0.9972, 0.9996, 1.0003,\n",
       "                      1.0007, 0.9980, 1.0024, 0.9984, 1.0017, 0.9992, 0.9978, 0.9972, 0.9992,\n",
       "                      1.0010, 0.9975, 0.9992, 0.9999, 0.9997, 0.9977, 0.9980, 0.9991, 0.9988,\n",
       "                      0.9983, 1.0012, 1.0002, 1.0002, 1.0031, 1.0008, 0.9993, 0.9992, 1.0004,\n",
       "                      0.9980, 1.0002, 1.0015, 0.9990, 1.0007, 0.9982, 0.9990, 0.9989, 1.0015,\n",
       "                      1.0003, 1.0014, 0.9986, 0.9963, 0.9989, 0.9994, 1.0007, 0.9996, 0.9961,\n",
       "                      0.9993, 0.9978, 0.9987, 0.9975, 0.9983, 1.0001, 0.9999, 1.0008])),\n",
       "             ('transformer.resblocks.10.ln_1.bias',\n",
       "              tensor([-9.9415e-04, -1.9775e-03, -1.3741e-03, -1.5857e-03,  2.6872e-03,\n",
       "                      -2.3776e-03,  2.3129e-03,  9.5021e-04,  2.4397e-04, -1.3059e-03,\n",
       "                      -4.0490e-04,  1.9555e-03, -8.5591e-04, -2.1160e-03,  2.5304e-04,\n",
       "                       1.6384e-04,  1.8117e-03, -5.5078e-04,  4.8822e-03,  4.2876e-04,\n",
       "                       4.2839e-04,  1.7427e-03,  6.7393e-04, -2.8038e-03, -1.3539e-03,\n",
       "                       2.4423e-03,  7.5139e-04,  1.6033e-03,  8.4112e-04,  1.4960e-03,\n",
       "                       1.3520e-03,  2.7450e-03, -1.7678e-03,  6.2237e-04, -1.5436e-03,\n",
       "                       1.0279e-03, -1.4448e-03, -1.4090e-03, -6.9912e-04,  1.2784e-03,\n",
       "                      -1.2879e-03,  1.0438e-03, -4.3677e-04, -6.3388e-04,  1.3675e-05,\n",
       "                      -5.6728e-05, -1.3294e-04,  1.0408e-03, -3.1418e-04, -1.8171e-03,\n",
       "                      -2.1187e-03, -3.4902e-03, -2.7068e-03, -5.5977e-04, -3.9761e-05,\n",
       "                       1.2750e-03,  1.2388e-03, -4.2194e-04,  1.1657e-03,  1.4493e-03,\n",
       "                      -1.2587e-03, -1.3785e-03,  7.0069e-04, -7.1522e-04,  2.5265e-04,\n",
       "                       5.1564e-04, -1.5385e-03,  6.4422e-04,  1.4564e-03, -7.0054e-04,\n",
       "                      -2.6808e-03,  1.5122e-03, -2.6378e-05,  2.4847e-04,  1.1392e-03,\n",
       "                       3.9637e-04,  2.3130e-03, -5.6130e-04,  1.2200e-03, -2.3356e-03,\n",
       "                       9.3529e-05, -1.2950e-03,  1.9015e-03, -1.1259e-03, -3.1419e-04,\n",
       "                      -8.3963e-06, -3.4697e-04, -3.2302e-04, -1.5475e-03,  1.8451e-03,\n",
       "                       2.6801e-03, -2.8239e-03, -1.7194e-03,  2.7544e-03,  1.7746e-03,\n",
       "                      -7.0541e-04,  1.3742e-03,  8.7869e-04,  1.8391e-03, -1.8395e-03,\n",
       "                       1.8047e-03,  7.1108e-04,  1.3909e-03, -1.6081e-03, -1.2113e-03,\n",
       "                      -1.7049e-03,  4.1389e-04, -4.8279e-04,  2.0520e-03,  1.2357e-03,\n",
       "                       1.3491e-03,  4.9081e-04, -1.2375e-03, -2.5969e-03, -8.3264e-05,\n",
       "                       1.8576e-03,  4.1338e-04,  2.2712e-03, -7.0090e-04,  2.7435e-04,\n",
       "                       8.7774e-04,  5.1829e-04,  5.2658e-04,  1.6592e-03,  1.0462e-03,\n",
       "                       3.2777e-04, -1.0522e-03, -2.8628e-03,  1.2513e-03,  3.8291e-04,\n",
       "                      -2.3823e-04,  2.4332e-03,  1.0271e-03,  1.4177e-04,  2.5962e-03,\n",
       "                      -5.2365e-04,  3.5534e-04, -2.0795e-03,  1.1183e-03,  1.1510e-03,\n",
       "                      -1.0378e-03, -1.6345e-03,  4.7651e-04,  3.2391e-03, -2.4508e-03,\n",
       "                       1.0021e-03,  5.4756e-04, -3.2235e-03, -2.1560e-03, -5.7201e-04,\n",
       "                      -1.0279e-03, -1.6737e-03,  2.5303e-04,  8.8441e-04, -2.3402e-03,\n",
       "                      -6.4849e-04,  9.0588e-04,  1.1169e-03,  3.2142e-04,  1.2995e-03,\n",
       "                      -1.3169e-03, -1.3423e-03, -6.0876e-04, -1.9883e-03,  8.7247e-04,\n",
       "                      -1.5361e-03,  3.6370e-04,  2.6841e-03,  3.6943e-04,  8.6904e-04,\n",
       "                       1.6430e-03,  5.0749e-04,  1.4537e-03,  3.1283e-03,  4.8850e-04,\n",
       "                       2.8254e-03, -2.6608e-04, -3.7150e-04,  1.4948e-03, -3.4710e-04,\n",
       "                       3.0418e-04,  1.0722e-03, -5.2602e-04,  2.4023e-04, -1.6077e-03,\n",
       "                       1.2393e-03,  1.9385e-03,  6.9724e-04, -2.9769e-03, -1.1280e-03,\n",
       "                      -1.4477e-03,  1.3329e-03,  2.8148e-03, -3.1733e-03,  2.5408e-03,\n",
       "                       1.5125e-03, -1.6401e-03, -5.6176e-04, -2.4488e-04, -4.7521e-04,\n",
       "                       1.6149e-03,  2.1487e-03,  5.2828e-04, -4.1726e-04, -2.4243e-03,\n",
       "                       9.1514e-04, -2.8484e-03,  1.7034e-03,  1.4293e-03, -2.2761e-03,\n",
       "                       1.4173e-03,  2.0870e-05, -3.3125e-03, -6.6482e-04, -2.6052e-04,\n",
       "                      -7.8096e-04,  3.6947e-04, -9.0579e-04,  1.6623e-04, -4.3571e-03,\n",
       "                      -2.0377e-03, -8.8964e-04,  1.3552e-03, -2.2947e-03,  1.3682e-03,\n",
       "                       5.3546e-04, -4.9484e-05,  1.6371e-03, -1.5888e-03, -9.4008e-04,\n",
       "                      -3.2265e-03,  5.0345e-04, -1.8609e-05,  2.7146e-03, -2.5054e-04,\n",
       "                       2.5336e-03,  1.2244e-03,  3.4265e-04, -1.1018e-04,  4.9525e-04,\n",
       "                       2.9271e-03, -1.0692e-03,  8.9129e-04,  6.5677e-04, -1.0479e-03,\n",
       "                       1.1349e-03, -1.5658e-03,  3.9711e-04, -4.6767e-04, -7.0058e-04,\n",
       "                       8.1901e-04,  1.4106e-03,  2.2082e-03, -5.5146e-04, -5.1527e-04,\n",
       "                      -2.8423e-04, -8.3456e-04, -2.5226e-03, -3.7941e-03, -2.2952e-03,\n",
       "                       4.4364e-04,  1.8913e-03,  1.1365e-04,  1.1048e-03,  4.0961e-04,\n",
       "                      -1.2124e-03,  1.3495e-03,  4.2716e-03, -2.7358e-03, -1.1690e-04,\n",
       "                       6.4994e-04, -8.3854e-04,  1.4822e-03, -4.6280e-04, -4.9893e-05,\n",
       "                       1.1245e-03, -1.6820e-03, -5.2633e-04, -1.2794e-03, -1.2401e-03,\n",
       "                       1.0029e-03,  3.3251e-03, -3.5965e-05, -2.4525e-03,  2.5564e-03,\n",
       "                       5.7483e-04,  8.3643e-04,  2.2845e-04,  1.4234e-03, -1.5449e-03,\n",
       "                       1.5445e-03,  7.6540e-05, -6.5042e-04,  1.0662e-03,  1.1134e-03,\n",
       "                      -2.8271e-03,  3.8371e-04, -1.0553e-03, -1.7391e-03,  3.3728e-03,\n",
       "                      -1.3506e-03,  1.7460e-03, -5.7472e-04, -7.6961e-04,  1.8839e-04,\n",
       "                      -8.2674e-04, -1.4044e-03,  1.1560e-04, -1.0098e-03,  3.6846e-03,\n",
       "                      -6.4597e-06, -7.6904e-04, -2.3679e-05,  7.9825e-04, -4.1816e-04,\n",
       "                       6.3422e-04,  4.9220e-04,  1.0015e-03,  7.5933e-04, -2.0318e-04,\n",
       "                       2.2918e-04, -9.5292e-04,  1.2599e-03, -2.2400e-04, -1.8858e-03,\n",
       "                       2.2897e-04,  1.1295e-04,  1.3577e-03,  6.4152e-04, -2.2152e-03,\n",
       "                      -3.2186e-03,  4.8972e-04, -1.8233e-03,  1.5134e-04,  1.5132e-03,\n",
       "                      -2.5683e-03,  1.9214e-04,  7.9505e-04,  6.3837e-04, -1.7069e-03,\n",
       "                       1.1924e-03, -4.1760e-04, -1.5223e-03,  4.9876e-04, -1.6455e-03,\n",
       "                       7.4388e-04, -1.6612e-03, -1.8480e-03,  8.8139e-06,  7.6613e-04,\n",
       "                       1.7329e-04,  6.6968e-04, -2.1502e-03, -1.5917e-03, -2.6370e-03,\n",
       "                       2.9090e-03,  1.9269e-03, -3.0020e-04,  1.2811e-03, -8.8765e-04,\n",
       "                       6.3140e-05, -1.2638e-03,  2.6087e-04,  2.0719e-03,  1.8609e-03,\n",
       "                       8.6630e-04, -4.4889e-04,  4.3447e-04,  9.1817e-04,  1.0425e-03,\n",
       "                      -8.1733e-04,  9.3793e-04, -1.2304e-04,  2.4427e-03,  2.8424e-03,\n",
       "                       2.7896e-07,  1.0088e-03,  2.0528e-03,  1.3243e-03,  1.7079e-03,\n",
       "                       1.6683e-03,  1.3568e-03, -1.8779e-03,  4.8018e-04,  1.7685e-03,\n",
       "                       1.3277e-03,  1.4215e-03,  5.0929e-03, -6.6529e-04,  7.6894e-05,\n",
       "                      -3.6582e-03, -1.3874e-04, -7.9859e-04,  1.4838e-03,  2.2588e-05,\n",
       "                      -1.1876e-03, -6.0438e-05,  2.9264e-03, -1.3854e-03,  1.2643e-03,\n",
       "                      -1.0417e-03,  5.6348e-04, -4.5113e-04, -1.1988e-03, -1.0007e-03,\n",
       "                      -1.0517e-04,  1.9357e-03,  4.6626e-04,  1.5509e-03, -1.2898e-03,\n",
       "                       1.1851e-03, -9.6423e-04, -1.8821e-03, -2.7212e-03,  1.2928e-03,\n",
       "                       2.0899e-03,  8.5594e-04,  9.7380e-04,  2.9330e-03,  2.1130e-03,\n",
       "                      -2.1141e-03, -2.7620e-04, -2.5958e-03,  1.3840e-03, -2.4986e-03,\n",
       "                       5.7699e-04, -2.0751e-03,  2.8778e-03,  7.9816e-04, -6.9356e-04,\n",
       "                      -1.6762e-03, -1.2543e-04, -1.6500e-03, -3.0840e-04,  4.0721e-04,\n",
       "                      -1.6026e-03,  2.4432e-03, -1.0577e-03, -5.6972e-04, -1.7911e-03,\n",
       "                      -1.5218e-03,  1.1592e-03,  7.1482e-04,  2.2315e-03, -1.4828e-03,\n",
       "                       5.5001e-04, -1.3691e-03, -2.3283e-03, -3.0337e-03,  7.0268e-04,\n",
       "                       2.8750e-03,  3.6475e-03, -1.1605e-03, -2.1559e-03, -6.3032e-04,\n",
       "                      -7.3716e-05,  8.1393e-04,  1.1064e-03,  1.7870e-03,  2.4454e-03,\n",
       "                       1.2239e-03, -2.6177e-03,  2.0420e-03, -3.5726e-06,  1.1269e-03,\n",
       "                       2.8703e-04, -1.7234e-04,  1.6845e-04,  2.7127e-03,  4.4602e-03,\n",
       "                       2.3719e-04,  6.7302e-04, -8.3345e-04, -2.6957e-03,  7.6921e-04,\n",
       "                       8.2398e-04,  7.3653e-04,  4.2633e-04, -1.6386e-03, -1.5784e-03,\n",
       "                       5.3341e-04, -8.3545e-04,  1.5549e-03,  3.0356e-04,  1.9220e-03,\n",
       "                      -6.5229e-04,  9.3381e-05, -7.7934e-04,  1.4206e-03,  2.2182e-03,\n",
       "                       4.8684e-04,  8.1972e-04,  9.9901e-04, -7.8199e-05, -7.9466e-04,\n",
       "                       6.0801e-05, -4.6150e-04, -1.8883e-04, -1.3336e-03,  4.6406e-04,\n",
       "                      -3.3025e-04,  2.8354e-04, -9.6466e-04, -2.4269e-03,  2.8595e-04,\n",
       "                      -2.3958e-03, -1.2800e-04,  8.3624e-04, -1.3307e-03,  5.2730e-04,\n",
       "                       1.6032e-04, -2.3728e-03])),\n",
       "             ('transformer.resblocks.10.mlp.c_fc.weight',\n",
       "              tensor([[-0.0189, -0.0201,  0.0080,  ..., -0.0131, -0.0101, -0.0397],\n",
       "                      [ 0.0005,  0.0228,  0.0195,  ...,  0.0247,  0.0133,  0.0579],\n",
       "                      [ 0.0120,  0.0168,  0.0285,  ..., -0.0044, -0.0115, -0.0329],\n",
       "                      ...,\n",
       "                      [ 0.0029, -0.0386,  0.0193,  ..., -0.0099, -0.0173, -0.0468],\n",
       "                      [ 0.0087,  0.0082, -0.0261,  ..., -0.0054, -0.0561, -0.0129],\n",
       "                      [ 0.0182,  0.0166, -0.0199,  ..., -0.0101, -0.0162, -0.0016]])),\n",
       "             ('transformer.resblocks.10.mlp.c_fc.bias',\n",
       "              tensor([ 0.0094,  0.0381,  0.0335,  ...,  0.0020, -0.0267,  0.0232])),\n",
       "             ('transformer.resblocks.10.mlp.c_proj.weight',\n",
       "              tensor([[-0.0002, -0.0061, -0.0085,  ...,  0.0115, -0.0021,  0.0029],\n",
       "                      [-0.0097, -0.0067, -0.0164,  ..., -0.0018,  0.0015,  0.0099],\n",
       "                      [-0.0008,  0.0052,  0.0019,  ..., -0.0044,  0.0052, -0.0077],\n",
       "                      ...,\n",
       "                      [ 0.0037,  0.0059, -0.0098,  ...,  0.0020, -0.0073, -0.0039],\n",
       "                      [ 0.0007,  0.0072,  0.0176,  ..., -0.0090,  0.0041,  0.0079],\n",
       "                      [ 0.0095, -0.0002,  0.0007,  ..., -0.0159, -0.0143,  0.0213]])),\n",
       "             ('transformer.resblocks.10.mlp.c_proj.bias',\n",
       "              tensor([-6.7612e-03, -2.1360e-02,  1.1146e-02, -1.4758e-02,  5.0379e-03,\n",
       "                      -9.0624e-03, -1.2899e-02, -4.6039e-03, -4.9047e-03, -9.5331e-03,\n",
       "                       1.0399e-02,  1.2807e-02, -4.4603e-03, -2.0197e-02,  3.0146e-03,\n",
       "                       1.2543e-02,  3.0944e-03, -8.9894e-03,  2.0835e-02, -4.0097e-03,\n",
       "                      -1.4596e-02, -3.9653e-03,  1.8883e-03,  2.6451e-03, -6.8659e-03,\n",
       "                       1.5906e-03,  1.3912e-02, -3.8458e-03,  3.8153e-03,  1.4426e-02,\n",
       "                      -2.1150e-02, -1.9220e-02,  6.1637e-03, -1.6689e-02,  2.5870e-03,\n",
       "                      -2.7908e-03, -1.8013e-02,  4.0774e-03, -3.2451e-03,  1.1176e-03,\n",
       "                      -2.6235e-03,  2.6236e-03,  1.2458e-02, -1.1332e-02, -1.0844e-02,\n",
       "                       1.7084e-03,  9.3346e-03,  1.7856e-02, -2.2023e-02, -5.3503e-03,\n",
       "                      -1.0853e-02,  5.1309e-03,  1.8797e-02, -7.7754e-03,  4.3655e-03,\n",
       "                       1.1348e-03,  8.6801e-03,  1.3154e-03, -6.9718e-03, -2.2121e-02,\n",
       "                      -2.1277e-02, -1.1062e-02,  1.8974e-02,  1.5301e-02,  1.7646e-02,\n",
       "                       3.1334e-03,  1.9670e-02,  1.5407e-02, -2.3587e-02,  1.3219e-02,\n",
       "                      -1.5758e-03, -1.5078e-02,  1.4359e-02,  6.0737e-03, -1.4258e-02,\n",
       "                       2.1047e-02, -2.0280e-02, -1.8711e-02,  5.0648e-03,  8.4968e-03,\n",
       "                      -1.0021e-03, -2.3241e-02,  1.9645e-02,  2.3621e-02, -1.7370e-03,\n",
       "                       3.9542e-03, -5.9763e-03, -2.1101e-02,  1.7948e-02, -1.7031e-02,\n",
       "                       2.1823e-02,  2.0891e-02, -1.5062e-02, -1.1349e-02,  4.0250e-03,\n",
       "                      -8.4717e-03,  3.3682e-03, -5.5021e-03,  8.4672e-05,  6.6207e-03,\n",
       "                      -3.3984e-04, -1.2764e-02,  1.1790e-02, -1.1971e-02, -1.4008e-03,\n",
       "                       9.2700e-03, -1.5405e-02, -2.1692e-02, -2.0357e-02,  1.1410e-02,\n",
       "                       3.4115e-03, -7.0914e-03, -1.4491e-02, -1.9380e-02, -2.3472e-02,\n",
       "                      -1.5038e-02, -8.1617e-03, -1.7429e-02,  1.0432e-02, -1.8268e-02,\n",
       "                      -1.7342e-02,  2.1876e-03,  2.4096e-02, -1.5402e-02, -9.6808e-03,\n",
       "                      -1.6566e-03,  2.0159e-02, -1.7281e-03, -1.9793e-02,  1.8789e-02,\n",
       "                      -1.4356e-02,  2.4680e-02,  1.7352e-02,  1.0460e-02, -1.0518e-02,\n",
       "                       5.8786e-04,  1.3271e-02, -5.4716e-03,  2.0705e-02,  1.3290e-03,\n",
       "                       6.2833e-03, -1.5057e-02, -1.5216e-02, -8.3800e-03, -2.5305e-03,\n",
       "                      -9.2847e-03, -8.4093e-03, -5.9625e-03,  1.4493e-02,  6.9458e-03,\n",
       "                       1.6145e-02, -1.4880e-02, -9.4566e-03, -3.9223e-03,  2.0259e-02,\n",
       "                       1.0910e-02, -6.6954e-03, -6.9850e-03, -2.0843e-02,  2.0406e-02,\n",
       "                       1.6312e-03, -1.0935e-02,  1.6885e-02,  3.2187e-03, -1.4734e-02,\n",
       "                       1.2120e-02,  1.9017e-02,  2.0378e-02,  1.0801e-02,  1.6094e-02,\n",
       "                      -7.8860e-04, -6.3625e-03, -6.7676e-03, -6.4482e-03,  3.4907e-05,\n",
       "                       2.0766e-02, -1.7021e-02,  1.9176e-02, -8.1118e-03,  7.8025e-03,\n",
       "                      -1.1732e-02,  1.1389e-02,  7.0204e-03,  1.4765e-02,  1.1089e-02,\n",
       "                       1.5158e-02,  1.9285e-02,  1.7168e-02,  9.2698e-03,  1.8717e-02,\n",
       "                      -1.6294e-02,  1.5936e-02, -1.5559e-02, -1.3084e-02,  1.0220e-02,\n",
       "                      -1.2020e-02,  1.8017e-02,  1.2824e-02,  8.2793e-03,  8.4648e-03,\n",
       "                       2.1320e-03, -1.2150e-02, -7.2262e-03, -1.8774e-03, -1.0501e-02,\n",
       "                       6.9164e-03,  2.8926e-03,  1.7234e-02,  4.4027e-03, -2.0061e-02,\n",
       "                      -1.5797e-02, -2.0343e-02, -1.7731e-02, -1.0118e-02, -1.2135e-02,\n",
       "                       1.2329e-02, -1.1744e-03,  1.2190e-02, -1.7600e-02, -7.6817e-03,\n",
       "                       1.5409e-03, -1.6810e-02, -8.6093e-03,  2.2101e-03,  1.5642e-02,\n",
       "                      -9.0062e-03, -5.5135e-03,  1.4103e-02, -7.1824e-03, -1.2483e-02,\n",
       "                       4.6931e-04,  2.4001e-03,  1.2826e-02, -1.2197e-02, -1.1187e-02,\n",
       "                       1.3173e-02,  2.5393e-03, -1.1898e-02, -1.4968e-02,  6.2173e-03,\n",
       "                      -2.1410e-02, -2.0219e-03,  2.2009e-02, -1.0282e-02, -5.1792e-03,\n",
       "                      -1.3656e-02,  2.0808e-02, -1.4718e-02,  1.0054e-04, -2.2260e-03,\n",
       "                       1.0427e-02, -1.2069e-02, -1.9751e-03,  5.5218e-03,  1.4405e-02,\n",
       "                      -1.2857e-02,  1.3744e-02,  8.3661e-03,  6.1601e-03, -2.1919e-02,\n",
       "                       1.9787e-02, -1.4582e-02, -8.8261e-03,  5.1688e-03, -1.7377e-02,\n",
       "                       1.3395e-02, -4.5048e-03,  2.2587e-02,  9.2478e-03,  3.3452e-03,\n",
       "                       2.3190e-02,  1.0840e-02, -4.9847e-03,  2.4972e-03,  1.9548e-02,\n",
       "                       1.7393e-02,  7.3377e-03, -4.3966e-03, -3.1972e-03,  1.3271e-02,\n",
       "                       1.6616e-02,  2.0889e-02,  1.8536e-02, -1.1599e-02, -4.5058e-03,\n",
       "                      -1.5489e-02,  4.1109e-03, -1.2684e-02, -1.3598e-02,  1.3119e-02,\n",
       "                      -1.0594e-02, -2.6925e-03, -1.7046e-02,  1.9443e-02,  1.2495e-02,\n",
       "                       1.7255e-02, -1.1229e-02,  6.9277e-03,  1.1028e-02, -4.7521e-03,\n",
       "                      -2.1009e-03, -2.3787e-03,  7.5366e-03, -1.1076e-02, -8.5909e-03,\n",
       "                       4.3559e-03, -2.0461e-02,  6.1921e-03,  6.0738e-03, -8.4324e-03,\n",
       "                       9.7696e-03, -5.0706e-03,  1.8672e-02,  3.0693e-03,  1.6910e-02,\n",
       "                      -1.1190e-02,  1.4843e-02,  1.9388e-02, -2.7777e-03,  2.1054e-02,\n",
       "                       7.6350e-03, -1.3121e-02,  2.1302e-02,  9.0722e-03,  8.6436e-03,\n",
       "                       1.2687e-02, -8.5548e-03, -1.6205e-03,  4.4523e-03,  1.5204e-02,\n",
       "                      -1.9182e-02,  1.1733e-02,  1.4133e-02,  1.2070e-02,  1.9718e-02,\n",
       "                      -1.9029e-02,  9.5350e-03, -1.2263e-02,  7.8960e-03,  3.1764e-03,\n",
       "                      -1.6900e-02, -1.5983e-02,  7.2940e-03, -1.8634e-02,  1.4923e-02,\n",
       "                      -5.7464e-04, -7.1694e-03,  1.5658e-02, -3.4160e-03, -1.1537e-03,\n",
       "                      -5.8062e-03,  1.6270e-02, -1.6886e-02,  8.1161e-03, -1.8818e-02,\n",
       "                      -1.6568e-02, -1.7117e-02, -1.6869e-02,  1.0791e-02, -9.2023e-03,\n",
       "                      -1.5265e-02, -4.2641e-03, -1.5392e-02,  1.8730e-03,  1.8271e-02,\n",
       "                       4.4518e-03,  1.2131e-02,  9.5541e-03, -1.0942e-02,  1.7045e-02,\n",
       "                       3.1143e-04,  1.4255e-02,  1.3066e-02,  8.6095e-03,  1.9172e-02,\n",
       "                      -1.6623e-02, -3.0855e-03,  1.1400e-03,  2.4079e-03, -1.5742e-02,\n",
       "                       1.3011e-02,  1.1311e-02,  1.9796e-02, -8.0281e-03, -1.1853e-02,\n",
       "                      -2.3682e-03, -1.6970e-02, -8.1477e-03,  6.5933e-03, -2.0462e-02,\n",
       "                      -9.7651e-04,  3.8298e-03,  1.5594e-03,  1.8276e-02, -1.5366e-02,\n",
       "                      -8.0065e-03,  2.0082e-02,  1.1651e-03,  1.4533e-02,  2.0911e-02,\n",
       "                      -2.1675e-02, -2.7179e-04,  1.9043e-02,  4.4656e-07, -1.0346e-02,\n",
       "                       2.1373e-02,  5.7397e-03,  4.5241e-04,  1.8111e-02,  7.6155e-04,\n",
       "                       1.2172e-03, -2.5152e-02, -3.1773e-03, -1.8472e-02,  9.9405e-03,\n",
       "                       1.5825e-02, -1.7535e-02,  1.3172e-02,  1.0870e-02,  2.3914e-02,\n",
       "                       1.7794e-02, -1.4588e-02, -1.8739e-03,  2.0715e-03,  1.6479e-03,\n",
       "                       1.5389e-02,  2.0549e-02,  1.7625e-02, -3.5832e-03,  7.5174e-03,\n",
       "                       6.4270e-03,  1.3880e-02,  1.0675e-02,  4.9342e-03, -1.7928e-02,\n",
       "                      -2.2002e-02, -1.0747e-02,  2.1020e-03, -1.0903e-03,  1.3900e-02,\n",
       "                      -2.1344e-02,  1.2260e-02, -1.4112e-02,  1.9226e-02, -1.9388e-02,\n",
       "                       9.7985e-03, -1.8544e-02, -4.3599e-03, -2.9058e-03, -1.8104e-02,\n",
       "                       8.6703e-03, -1.7501e-02, -6.5228e-03, -7.5996e-03, -1.5691e-02,\n",
       "                       4.8630e-03, -1.6756e-02,  8.8199e-03, -1.1813e-02, -6.3473e-03,\n",
       "                       1.5345e-03,  5.4082e-03,  1.4628e-03,  2.1383e-02,  5.4880e-03,\n",
       "                      -5.5453e-03, -1.9356e-02,  9.1005e-03,  2.1309e-02,  6.5287e-03,\n",
       "                      -1.4622e-02,  1.6444e-02,  8.9382e-03, -5.5319e-03,  1.8434e-03,\n",
       "                      -1.6061e-04,  1.6659e-02, -2.3274e-03, -1.5014e-02, -2.1675e-02,\n",
       "                      -2.0695e-02,  2.7593e-03, -7.5221e-03,  5.0526e-03, -2.2990e-02,\n",
       "                       1.1529e-02, -1.0792e-02,  4.9466e-03, -8.5439e-03,  1.1151e-02,\n",
       "                      -2.6547e-03, -1.8233e-02,  1.9357e-03, -2.3990e-02, -4.5851e-03,\n",
       "                       1.1271e-02,  1.0918e-02, -1.9183e-02,  1.7876e-02, -6.0656e-03,\n",
       "                       9.7950e-03,  2.1243e-02,  1.1696e-02,  1.2113e-02, -2.1143e-02,\n",
       "                      -9.2467e-03,  4.1766e-03,  1.9939e-02, -1.6974e-02, -1.3754e-03,\n",
       "                       1.1687e-02, -2.5524e-03])),\n",
       "             ('transformer.resblocks.10.ln_2.weight',\n",
       "              tensor([0.9971, 0.9989, 0.9992, 1.0023, 1.0022, 0.9975, 1.0025, 0.9992, 0.9986,\n",
       "                      1.0002, 0.9979, 0.9992, 0.9997, 1.0008, 0.9939, 0.9977, 1.0005, 1.0017,\n",
       "                      0.9995, 0.9999, 1.0020, 0.9993, 1.0036, 1.0009, 0.9973, 1.0000, 0.9994,\n",
       "                      0.9987, 1.0008, 1.0003, 1.0022, 1.0025, 0.9972, 0.9969, 1.0010, 0.9991,\n",
       "                      1.0008, 1.0018, 1.0024, 0.9985, 0.9987, 0.9999, 1.0004, 0.9994, 0.9960,\n",
       "                      1.0003, 0.9965, 1.0005, 1.0007, 0.9998, 0.9994, 1.0031, 1.0022, 0.9985,\n",
       "                      0.9952, 1.0001, 1.0030, 1.0036, 1.0017, 1.0020, 0.9982, 1.0014, 1.0027,\n",
       "                      1.0009, 1.0028, 0.9983, 0.9990, 1.0017, 0.9984, 1.0027, 1.0037, 0.9997,\n",
       "                      0.9959, 1.0006, 1.0004, 0.9980, 0.9971, 0.9989, 0.9983, 1.0007, 0.9976,\n",
       "                      1.0012, 1.0021, 0.9979, 1.0000, 1.0008, 1.0006, 1.0002, 1.0029, 1.0006,\n",
       "                      1.0015, 1.0030, 1.0023, 1.0006, 1.0021, 0.9957, 1.0041, 0.9991, 1.0003,\n",
       "                      0.9993, 0.9995, 1.0007, 0.9987, 0.9975, 1.0035, 0.9967, 0.9950, 1.0011,\n",
       "                      0.9985, 1.0004, 0.9978, 0.9998, 0.9979, 1.0017, 0.9989, 0.9969, 0.9995,\n",
       "                      1.0000, 0.9942, 0.9995, 0.9998, 0.9992, 0.9999, 0.9983, 1.0007, 0.9974,\n",
       "                      0.9991, 1.0023, 1.0007, 1.0027, 0.9996, 0.9965, 1.0021, 0.9990, 0.9980,\n",
       "                      1.0010, 1.0002, 1.0002, 0.9989, 1.0003, 0.9989, 0.9966, 1.0015, 0.9999,\n",
       "                      0.9943, 0.9990, 0.9990, 1.0039, 1.0034, 1.0030, 1.0015, 0.9936, 0.9947,\n",
       "                      0.9989, 1.0031, 1.0062, 1.0011, 0.9998, 0.9993, 1.0019, 1.0014, 0.9976,\n",
       "                      0.9998, 1.0014, 0.9985, 0.9998, 0.9996, 0.9998, 1.0020, 0.9973, 1.0013,\n",
       "                      0.9942, 1.0020, 1.0002, 0.9955, 1.0008, 1.0019, 1.0023, 1.0021, 0.9988,\n",
       "                      1.0008, 1.0023, 1.0010, 0.9945, 0.9960, 1.0017, 0.9965, 0.9979, 0.9981,\n",
       "                      0.9990, 1.0044, 0.9979, 0.9976, 0.9966, 0.9973, 1.0012, 0.9992, 0.9977,\n",
       "                      0.9985, 0.9988, 1.0033, 0.9978, 0.9990, 0.9997, 1.0063, 1.0000, 1.0033,\n",
       "                      1.0031, 0.9992, 0.9968, 0.9987, 1.0008, 0.9983, 1.0007, 0.9958, 1.0000,\n",
       "                      0.9969, 0.9986, 0.9999, 1.0025, 1.0007, 1.0000, 1.0003, 1.0012, 1.0007,\n",
       "                      1.0009, 0.9996, 0.9963, 1.0003, 1.0011, 0.9988, 1.0015, 0.9988, 1.0034,\n",
       "                      0.9998, 1.0016, 1.0016, 0.9974, 1.0001, 1.0001, 1.0008, 0.9981, 0.9970,\n",
       "                      0.9998, 1.0011, 1.0009, 1.0022, 1.0003, 0.9995, 0.9996, 1.0003, 0.9996,\n",
       "                      1.0005, 0.9987, 1.0017, 1.0005, 1.0004, 0.9985, 0.9990, 0.9959, 1.0007,\n",
       "                      1.0022, 1.0031, 0.9976, 0.9992, 1.0017, 1.0016, 0.9980, 0.9997, 0.9990,\n",
       "                      1.0007, 0.9986, 0.9980, 0.9971, 0.9998, 0.9990, 0.9964, 1.0022, 1.0000,\n",
       "                      0.9984, 1.0011, 0.9969, 0.9976, 1.0014, 0.9980, 0.9982, 0.9967, 0.9991,\n",
       "                      1.0013, 0.9991, 0.9987, 0.9970, 0.9928, 0.9965, 1.0032, 1.0015, 0.9965,\n",
       "                      1.0024, 1.0015, 1.0011, 0.9992, 1.0004, 0.9991, 0.9951, 1.0020, 0.9988,\n",
       "                      1.0016, 1.0026, 1.0016, 0.9961, 1.0013, 0.9975, 1.0038, 1.0007, 1.0021,\n",
       "                      0.9998, 0.9992, 0.9992, 1.0027, 1.0009, 0.9972, 0.9988, 1.0027, 0.9979,\n",
       "                      1.0010, 0.9988, 0.9972, 0.9996, 0.9969, 0.9979, 1.0040, 1.0019, 1.0008,\n",
       "                      1.0017, 1.0021, 1.0007, 1.0031, 0.9969, 0.9979, 0.9965, 1.0003, 0.9991,\n",
       "                      0.9995, 1.0007, 1.0003, 0.9989, 0.9968, 0.9995, 1.0005, 1.0019, 0.9997,\n",
       "                      0.9967, 1.0018, 1.0016, 1.0064, 0.9997, 0.9998, 0.9997, 0.9979, 0.9970,\n",
       "                      0.9991, 0.9988, 0.9960, 0.9971, 1.0009, 0.9997, 1.0003, 0.9959, 0.9997,\n",
       "                      1.0018, 1.0024, 1.0007, 1.0002, 0.9989, 0.9990, 0.9972, 1.0004, 0.9990,\n",
       "                      0.9996, 1.0000, 0.9991, 0.9983, 0.9988, 0.9998, 1.0000, 0.9981, 0.9992,\n",
       "                      0.9982, 0.9938, 1.0030, 1.0004, 1.0018, 1.0008, 1.0002, 0.9995, 0.9982,\n",
       "                      1.0010, 0.9981, 0.9983, 0.9999, 1.0007, 0.9994, 1.0013, 0.9983, 1.0017,\n",
       "                      0.9995, 0.9984, 0.9989, 0.9996, 1.0024, 1.0005, 0.9977, 0.9989, 0.9986,\n",
       "                      1.0000, 0.9984, 0.9997, 1.0001, 0.9977, 0.9990, 1.0010, 1.0029, 0.9989,\n",
       "                      0.9989, 1.0000, 1.0013, 1.0031, 1.0035, 0.9986, 1.0008, 0.9996, 1.0018,\n",
       "                      1.0029, 0.9978, 0.9975, 1.0028, 0.9968, 0.9942, 1.0012, 1.0021, 0.9979,\n",
       "                      0.9992, 1.0021, 1.0006, 0.9972, 0.9964, 1.0010, 0.9990, 1.0006, 1.0019,\n",
       "                      1.0023, 0.9983, 1.0009, 0.9951, 1.0022, 0.9991, 1.0020, 0.9999, 0.9978,\n",
       "                      1.0001, 1.0024, 1.0000, 0.9970, 1.0016, 1.0021, 0.9985, 1.0009, 0.9977,\n",
       "                      1.0026, 0.9986, 0.9964, 1.0017, 0.9962, 1.0022, 0.9997, 1.0013, 0.9975,\n",
       "                      0.9969, 1.0008, 1.0020, 1.0001, 1.0014, 1.0025, 1.0013, 1.0002, 1.0001,\n",
       "                      1.0000, 0.9961, 0.9968, 0.9983, 0.9984, 1.0001, 1.0004, 0.9998, 1.0067,\n",
       "                      1.0003, 0.9988, 0.9970, 1.0001, 0.9969, 1.0000, 0.9999, 0.9955, 0.9964,\n",
       "                      1.0011, 1.0041, 0.9986, 0.9995, 0.9993, 1.0034, 0.9988, 0.9992])),\n",
       "             ('transformer.resblocks.10.ln_2.bias',\n",
       "              tensor([ 9.4319e-04, -1.1011e-03, -4.1306e-04,  9.0315e-04,  5.1397e-04,\n",
       "                       1.6224e-03,  1.1033e-03, -2.5706e-03,  2.3567e-03, -1.9270e-03,\n",
       "                       1.6880e-03,  1.9130e-03, -8.3972e-04,  2.8846e-04,  3.7829e-03,\n",
       "                      -4.7556e-03,  5.1423e-04, -2.3822e-03,  1.6426e-03,  2.5589e-03,\n",
       "                       1.0164e-03,  2.9103e-03,  1.1049e-04, -2.5546e-03,  1.7884e-03,\n",
       "                       1.9986e-03,  2.3278e-03, -1.3856e-05,  1.6401e-03,  1.0990e-03,\n",
       "                      -3.0430e-03,  1.7285e-03, -1.3804e-03,  2.1369e-03,  1.5128e-03,\n",
       "                      -1.2321e-03,  6.0889e-04,  2.4306e-04,  9.4199e-04,  1.3706e-03,\n",
       "                      -2.0006e-03,  6.9366e-04,  7.5318e-04, -2.6593e-03,  1.4915e-03,\n",
       "                      -7.3043e-04,  2.2133e-03,  2.4686e-03, -5.1234e-04, -1.4904e-03,\n",
       "                       7.6917e-04, -2.8154e-03,  9.2745e-04,  1.2960e-04, -2.2757e-03,\n",
       "                      -8.3793e-04,  2.1103e-03, -1.2578e-03,  2.3910e-03,  1.1606e-03,\n",
       "                       1.8085e-03,  5.9870e-04, -2.2929e-03, -8.7712e-05, -4.7409e-04,\n",
       "                       2.5759e-03,  2.1514e-04,  7.2536e-04,  1.4384e-03,  3.0130e-03,\n",
       "                       7.4082e-04,  2.3501e-03,  2.7633e-03,  6.3077e-04,  6.7286e-04,\n",
       "                       9.5403e-05,  1.4537e-03,  3.1300e-03,  1.2908e-03, -2.2608e-03,\n",
       "                      -4.8473e-04,  1.7159e-03, -2.2875e-03, -3.9770e-03, -1.5632e-03,\n",
       "                       1.4846e-03,  3.1487e-05,  9.3596e-04, -2.4216e-03,  8.4297e-04,\n",
       "                      -8.6678e-04, -8.1240e-05,  8.7737e-04,  2.6092e-03, -1.4930e-05,\n",
       "                       2.8733e-03, -6.7679e-04,  1.4449e-04, -5.5102e-04, -2.0507e-03,\n",
       "                      -1.9133e-03,  1.0345e-03, -3.0641e-03,  2.0598e-03, -1.3047e-03,\n",
       "                       2.5320e-03,  2.3534e-03,  9.5969e-04, -1.8242e-03,  9.6357e-04,\n",
       "                      -6.1576e-04, -1.3145e-03, -1.5649e-03,  1.1963e-03,  1.2828e-03,\n",
       "                       1.6899e-03,  1.2432e-04,  1.2478e-03,  4.2290e-03,  2.3870e-03,\n",
       "                      -8.1029e-04,  2.6917e-03, -3.4236e-03,  1.3119e-03,  1.3831e-03,\n",
       "                       1.2375e-03,  1.9813e-03, -1.7459e-03, -5.2285e-04,  1.2376e-03,\n",
       "                       1.3399e-03, -2.7195e-03,  9.0275e-04, -5.1896e-04,  3.9006e-04,\n",
       "                       1.7420e-03,  9.9844e-04, -6.8844e-04, -1.0172e-04, -1.8414e-04,\n",
       "                      -2.1149e-03, -4.4232e-03, -2.5098e-03,  7.0122e-04, -3.8272e-03,\n",
       "                       2.1732e-04,  2.1587e-03,  1.4058e-04, -1.9675e-03,  2.2764e-03,\n",
       "                       2.3104e-03, -4.0913e-03, -5.5931e-03, -8.0706e-04,  1.2348e-03,\n",
       "                      -1.5060e-03, -1.3480e-03,  5.7410e-04,  3.1701e-04, -2.4922e-03,\n",
       "                       8.4266e-04, -9.6577e-04, -1.6108e-03, -5.2010e-04,  2.6752e-03,\n",
       "                       1.4674e-03,  2.2184e-04, -1.6611e-03,  2.0249e-03,  4.3062e-03,\n",
       "                       4.6748e-04,  6.5753e-04,  1.3322e-03, -3.8697e-03, -2.9013e-03,\n",
       "                      -7.0262e-04,  6.7708e-05, -3.7760e-04, -1.0789e-03,  2.7997e-03,\n",
       "                       3.7861e-04, -9.2202e-04, -5.6221e-04,  7.3973e-03, -1.7114e-03,\n",
       "                       1.2686e-03, -1.9088e-03, -1.1481e-03, -9.1188e-04, -7.5835e-04,\n",
       "                      -1.0263e-03, -1.0020e-03,  2.7631e-04, -2.8430e-03,  1.3588e-03,\n",
       "                       1.1316e-03, -1.8161e-03, -1.5721e-03,  4.6034e-04, -3.2353e-04,\n",
       "                       7.0729e-04,  3.8722e-03, -2.5803e-03, -9.0362e-04, -3.8067e-04,\n",
       "                      -6.5643e-04, -2.3429e-03, -2.8433e-03, -3.8945e-03,  9.1724e-04,\n",
       "                      -3.2433e-03,  1.4864e-03,  7.8332e-04,  1.2301e-03,  3.9174e-03,\n",
       "                       2.6310e-03,  1.5239e-03, -1.7815e-03,  2.7031e-03, -5.5441e-04,\n",
       "                       3.6558e-03, -1.1062e-03, -1.9269e-03,  4.6353e-04, -1.9774e-03,\n",
       "                      -3.5088e-03,  9.3537e-04,  4.2008e-03, -1.1224e-03,  5.8956e-04,\n",
       "                       2.4905e-03, -7.0181e-04, -1.3637e-03, -1.0644e-03,  2.9768e-04,\n",
       "                      -2.5690e-03,  1.9697e-03,  3.2834e-03, -3.7126e-03, -1.7376e-03,\n",
       "                       9.3684e-04, -1.3938e-03, -1.1617e-03,  8.3585e-04,  1.3097e-03,\n",
       "                       1.8601e-04,  4.2044e-04, -1.4912e-03,  1.5417e-03, -3.8432e-04,\n",
       "                      -1.9028e-03,  1.4216e-03, -1.2027e-03,  5.6731e-04,  5.6723e-04,\n",
       "                       8.3806e-04,  2.2599e-04,  1.2043e-03, -3.3171e-04,  4.5038e-03,\n",
       "                       1.5668e-03,  9.1970e-04,  8.7517e-04,  2.1634e-03,  1.9465e-03,\n",
       "                       6.1342e-04, -8.9267e-04, -2.8696e-04,  6.4223e-04,  6.4734e-04,\n",
       "                      -6.2647e-05,  2.5913e-03,  1.1431e-03, -1.6633e-03, -1.5234e-03,\n",
       "                       8.5165e-04, -3.3718e-03,  1.5065e-03, -6.1128e-05,  2.3113e-03,\n",
       "                      -1.0782e-03, -3.7876e-03,  1.6923e-03,  1.1163e-03,  2.1797e-03,\n",
       "                      -1.1780e-04, -1.6110e-03, -5.1391e-04, -5.9931e-04,  1.1511e-03,\n",
       "                      -1.4444e-03, -4.3732e-04,  4.5947e-03, -2.3169e-03, -1.8526e-03,\n",
       "                      -1.7006e-03,  2.1757e-03, -1.3071e-03, -5.6087e-04, -1.5078e-03,\n",
       "                       8.7375e-04, -6.4494e-04,  8.9942e-04,  2.4799e-03, -6.7486e-04,\n",
       "                       5.7266e-05, -6.3590e-04, -9.8441e-04,  1.3653e-04,  3.5593e-03,\n",
       "                      -2.5386e-03, -7.7998e-04, -2.8090e-03,  1.3513e-04, -1.6909e-03,\n",
       "                       1.0038e-03, -4.1958e-04, -2.6717e-03,  5.3584e-04, -1.3405e-03,\n",
       "                      -1.9035e-05,  1.9492e-03, -2.1654e-03, -3.1816e-03,  6.9859e-04,\n",
       "                       1.2455e-03, -2.3734e-03,  2.0021e-03,  3.3205e-03,  7.8482e-04,\n",
       "                       1.6673e-03, -1.3600e-03,  2.9760e-03,  1.5648e-03,  2.1863e-03,\n",
       "                      -1.5059e-04, -4.3845e-05,  2.9503e-04, -3.2271e-03, -4.5751e-03,\n",
       "                      -3.6879e-04, -3.0963e-03, -1.9033e-03, -4.0339e-04,  8.1788e-04,\n",
       "                       4.0474e-04, -1.8558e-03,  2.0896e-04,  3.0813e-03, -9.0667e-04,\n",
       "                       7.6153e-05,  3.0224e-03, -1.3300e-03, -1.1595e-03,  1.6639e-03,\n",
       "                      -3.9266e-04, -1.2197e-03,  1.6682e-04, -1.2233e-03,  3.1094e-03,\n",
       "                       1.2583e-03, -1.2810e-04, -1.8465e-03, -1.1580e-03,  2.7607e-03,\n",
       "                       5.8715e-04,  9.4030e-05,  2.7139e-03, -8.8838e-04,  1.4289e-03,\n",
       "                      -8.5031e-04, -1.7320e-03, -2.6036e-04,  2.5164e-04,  2.5235e-03,\n",
       "                      -1.1820e-03,  8.8695e-04, -6.3877e-04,  7.5874e-04,  2.8799e-04,\n",
       "                      -1.6275e-03, -7.4891e-04,  1.2983e-04,  5.7566e-04, -3.1519e-04,\n",
       "                       3.1056e-03, -9.3240e-04, -2.7687e-03, -5.7927e-03, -1.7165e-04,\n",
       "                       4.5648e-04,  1.6804e-03, -1.9442e-03,  2.4028e-03, -3.7522e-03,\n",
       "                      -2.5657e-03,  6.1634e-04, -7.5359e-04,  2.3811e-03, -2.9739e-03,\n",
       "                      -1.3373e-03,  2.7964e-03,  2.2074e-04,  6.3440e-05, -1.1358e-03,\n",
       "                      -5.7543e-04, -1.8763e-03, -1.6960e-03, -5.7034e-04,  1.4205e-03,\n",
       "                      -6.5679e-04,  2.4580e-04, -2.8152e-03, -2.8093e-03, -2.4708e-03,\n",
       "                       1.2991e-03, -2.6179e-03, -1.8648e-03,  1.6833e-03, -6.0705e-04,\n",
       "                       2.8087e-03, -5.3100e-04, -7.5612e-04,  2.9216e-04, -3.1981e-04,\n",
       "                       1.0401e-03,  7.3690e-04,  8.6590e-04,  8.3784e-06, -3.3442e-04,\n",
       "                      -9.0430e-06, -2.3700e-03,  4.5447e-03, -1.1232e-03,  1.2121e-03,\n",
       "                      -1.5161e-04, -3.5837e-03,  5.8128e-03,  6.3078e-04, -8.5300e-04,\n",
       "                       1.5564e-03, -3.9702e-04, -1.4768e-03,  1.7114e-03,  2.9502e-03,\n",
       "                       3.2885e-03, -7.4400e-04,  1.6480e-03,  1.0266e-03, -1.2263e-04,\n",
       "                      -1.1216e-03, -6.0948e-04,  1.2585e-04, -4.4200e-03,  2.1197e-03,\n",
       "                       1.4033e-03, -1.6150e-03, -1.3564e-03, -6.1141e-04,  3.7962e-04,\n",
       "                       1.1217e-03, -1.4570e-04, -1.6096e-04,  2.2367e-04,  2.1000e-03,\n",
       "                       4.6327e-04, -4.9778e-04, -4.9325e-04, -2.9351e-04,  1.2688e-03,\n",
       "                       3.0356e-03,  2.3364e-03,  4.7003e-03,  1.7233e-04,  1.5557e-03,\n",
       "                       6.8073e-04,  1.7653e-03,  1.7757e-03,  9.7133e-04, -1.3475e-03,\n",
       "                      -1.7969e-04, -1.1925e-03, -8.1685e-04,  4.9904e-04, -5.2342e-04,\n",
       "                      -1.8908e-03,  1.1778e-03,  3.2716e-03, -1.1979e-03, -2.3539e-03,\n",
       "                      -1.1480e-04,  1.7577e-03,  5.4853e-04,  1.8714e-03,  4.4883e-04,\n",
       "                       2.1141e-03, -1.7361e-03,  2.8116e-03,  9.1313e-04,  9.7210e-04,\n",
       "                       3.1038e-03,  1.6608e-03,  7.6906e-04, -3.7036e-03, -3.0977e-04,\n",
       "                      -1.3317e-03,  4.3571e-04,  8.0508e-06, -1.5553e-03, -8.0996e-04,\n",
       "                       3.5751e-04, -2.5971e-03])),\n",
       "             ('transformer.resblocks.11.attn.in_proj_weight',\n",
       "              tensor([[ 0.0498,  0.0044,  0.0030,  ..., -0.0054,  0.0678,  0.0146],\n",
       "                      [ 0.0186, -0.0386,  0.0581,  ...,  0.0236,  0.0419,  0.0042],\n",
       "                      [-0.0175, -0.0078, -0.0464,  ...,  0.0154, -0.0101,  0.0857],\n",
       "                      ...,\n",
       "                      [-0.0490, -0.0939, -0.0157,  ...,  0.0141,  0.0372,  0.0527],\n",
       "                      [-0.0021, -0.0304, -0.0332,  ..., -0.0220, -0.0558, -0.0110],\n",
       "                      [ 0.0596,  0.0105, -0.0014,  ...,  0.0536, -0.0678, -0.1278]])),\n",
       "             ('transformer.resblocks.11.attn.in_proj_bias',\n",
       "              tensor([-0.0043, -0.0037, -0.0013,  ...,  0.0005, -0.0005, -0.0006])),\n",
       "             ('transformer.resblocks.11.attn.out_proj.weight',\n",
       "              tensor([[-0.0005,  0.0041,  0.0013,  ..., -0.0054,  0.0123, -0.0028],\n",
       "                      [ 0.0017, -0.0057,  0.0052,  ...,  0.0033,  0.0056,  0.0015],\n",
       "                      [-0.0030, -0.0152, -0.0075,  ...,  0.0141,  0.0019,  0.0021],\n",
       "                      ...,\n",
       "                      [-0.0125,  0.0020, -0.0121,  ...,  0.0122, -0.0016, -0.0032],\n",
       "                      [ 0.0112, -0.0017,  0.0019,  ..., -0.0062, -0.0071, -0.0130],\n",
       "                      [-0.0128, -0.0131, -0.0101,  ..., -0.0016, -0.0034, -0.0016]])),\n",
       "             ('transformer.resblocks.11.attn.out_proj.bias',\n",
       "              tensor([-6.1307e-04, -3.6574e-03,  9.4274e-05, -5.8925e-04, -8.1612e-04,\n",
       "                      -2.9101e-03, -2.1117e-04,  2.3305e-03, -3.3000e-03,  2.0599e-03,\n",
       "                      -4.6993e-03, -2.8116e-03, -2.1607e-04, -1.7117e-03, -5.7799e-03,\n",
       "                       2.6406e-03, -2.8711e-04,  2.2004e-03, -7.9807e-04, -1.0512e-03,\n",
       "                       6.9636e-05,  7.1716e-04, -9.0024e-07, -3.9175e-04, -3.6867e-04,\n",
       "                      -1.6626e-03, -1.3128e-03, -1.0045e-03, -9.8053e-04, -1.9204e-03,\n",
       "                       8.9550e-04, -2.7436e-04,  2.3420e-03, -3.1536e-03, -4.3493e-04,\n",
       "                       7.8072e-04,  9.5668e-04,  4.8882e-06,  9.9848e-04, -2.8403e-03,\n",
       "                       1.5520e-03,  1.8349e-03,  1.3358e-03,  1.7437e-03, -4.0047e-03,\n",
       "                       1.7167e-03, -3.2732e-03,  3.4141e-04, -2.0531e-04,  1.9773e-03,\n",
       "                      -6.8713e-04, -2.0607e-04, -1.0366e-03,  2.6564e-03,  3.1000e-03,\n",
       "                       9.8657e-04, -1.7684e-03, -8.9718e-04, -3.4195e-04, -1.0984e-03,\n",
       "                      -1.9793e-03,  4.2754e-05,  9.6157e-04,  9.3942e-04, -4.1077e-04,\n",
       "                      -1.1305e-03,  2.0169e-03,  2.7213e-04, -1.6228e-03,  1.5825e-03,\n",
       "                      -4.7582e-04, -4.2402e-04, -2.3761e-03,  4.3416e-04, -8.1769e-04,\n",
       "                       1.7573e-03, -1.7906e-03, -2.0992e-03, -2.5998e-03,  2.0429e-03,\n",
       "                       1.5524e-03, -2.1359e-03,  6.4895e-04,  1.8840e-03,  2.1646e-03,\n",
       "                       1.9097e-04, -6.1629e-04,  3.1447e-04,  4.9333e-04, -2.8256e-03,\n",
       "                       2.4081e-04, -4.7996e-04,  1.7428e-03, -2.3840e-03, -1.9953e-03,\n",
       "                      -4.6885e-03, -2.9814e-04, -1.2916e-03, -1.9301e-03,  6.8604e-04,\n",
       "                       6.9785e-04, -1.6439e-03,  5.4703e-04, -3.4719e-04, -7.4387e-04,\n",
       "                      -2.8522e-03, -3.8702e-03, -9.8405e-04,  7.0981e-04, -7.4797e-04,\n",
       "                       2.5283e-03,  8.7343e-04,  3.1796e-03,  1.2968e-03, -1.7139e-03,\n",
       "                      -5.1082e-03,  1.4771e-03,  1.5002e-03, -3.2085e-03, -1.1394e-03,\n",
       "                       4.3702e-04, -2.5204e-03,  2.0422e-03, -9.1192e-04, -1.2953e-03,\n",
       "                       8.0362e-04, -1.4289e-03, -2.1629e-03, -5.7332e-04, -6.6584e-05,\n",
       "                       4.4601e-05,  3.6214e-03, -3.8837e-04, -2.3573e-03, -5.7275e-04,\n",
       "                      -1.2083e-03, -1.2175e-03,  2.5392e-03, -9.8369e-04,  3.7277e-04,\n",
       "                       3.8401e-04,  1.8235e-03,  1.1897e-03,  2.9038e-03,  1.7257e-03,\n",
       "                       2.6859e-03, -2.9167e-03,  1.4266e-03,  7.7532e-04,  7.6357e-04,\n",
       "                       1.6605e-03,  4.8700e-03,  6.7527e-03,  1.8777e-03, -9.5170e-04,\n",
       "                       2.4939e-04,  9.0350e-04, -1.9289e-03,  2.2763e-04,  4.6760e-04,\n",
       "                      -2.0536e-03, -9.9389e-04,  3.5421e-03,  7.1679e-04, -1.3649e-03,\n",
       "                       1.6041e-03,  2.1514e-03, -2.7405e-04, -5.4420e-04, -1.9262e-03,\n",
       "                       8.8213e-04, -2.7904e-04, -2.1465e-03,  3.7782e-04,  2.1732e-03,\n",
       "                       8.9260e-04, -1.3981e-03,  6.1911e-04,  2.8448e-04, -1.3456e-03,\n",
       "                      -2.1119e-03, -5.1913e-04,  1.1659e-03, -3.5745e-03,  5.0017e-03,\n",
       "                      -6.2118e-04,  2.9809e-04, -2.0274e-04,  1.3475e-03, -2.1762e-03,\n",
       "                       8.4702e-04,  6.0780e-04, -5.7421e-04,  2.2679e-03, -1.9290e-03,\n",
       "                      -1.6629e-03,  1.8586e-03,  2.8865e-03,  4.1193e-03,  3.2702e-03,\n",
       "                      -4.9061e-04, -1.5429e-03,  1.7147e-03,  1.7653e-03,  1.1446e-03,\n",
       "                       5.2444e-04, -6.6764e-04,  8.6961e-04,  1.5784e-03, -1.0669e-05,\n",
       "                       2.7713e-03, -2.7869e-04,  3.5126e-04,  1.6228e-03, -2.0179e-03,\n",
       "                      -1.6401e-03, -3.1919e-05, -1.5830e-03, -1.7464e-03,  8.9347e-04,\n",
       "                      -7.7297e-04, -4.9292e-04,  1.7415e-03,  2.6150e-03,  1.0812e-03,\n",
       "                       8.3922e-04, -1.3055e-03, -3.0227e-03,  1.8230e-03,  1.6116e-03,\n",
       "                      -1.6997e-03,  4.8045e-04,  7.4819e-04, -1.5740e-03,  1.4083e-03,\n",
       "                       2.4658e-04,  3.0157e-03, -3.6321e-03,  1.8412e-03,  1.7803e-03,\n",
       "                      -1.6841e-03,  2.8845e-03,  2.7693e-03,  1.1585e-03, -2.2356e-03,\n",
       "                      -1.8962e-04,  1.1696e-03,  4.0823e-04,  1.2095e-04, -1.3910e-03,\n",
       "                       1.1776e-03, -2.4523e-03,  6.0162e-05, -1.7733e-03, -6.2943e-04,\n",
       "                      -1.0267e-03, -3.3448e-04, -1.5289e-03, -5.7311e-05, -3.3190e-03,\n",
       "                       1.2684e-03,  7.9925e-04,  6.4520e-04, -3.4171e-03, -2.0394e-03,\n",
       "                       2.6248e-03,  8.8969e-04,  8.5840e-04, -5.5856e-04, -1.4385e-03,\n",
       "                       1.4097e-03, -1.0841e-03, -1.8661e-03,  3.6238e-04,  1.2662e-03,\n",
       "                      -9.1180e-04,  3.5138e-03, -6.4541e-04,  3.6452e-03, -1.9008e-03,\n",
       "                      -3.0750e-04,  3.8222e-03, -2.8715e-03, -1.7870e-03, -2.6644e-03,\n",
       "                       1.7335e-03,  2.4592e-04,  1.2406e-03,  1.8864e-03, -5.6365e-05,\n",
       "                      -6.1862e-04,  1.3524e-03, -2.0674e-03,  4.5989e-03,  3.3695e-04,\n",
       "                       2.2533e-03, -1.6725e-03,  1.2814e-03, -6.5799e-04, -1.1115e-03,\n",
       "                      -2.5908e-03,  4.1151e-04,  1.6967e-03, -4.2660e-03,  1.7582e-04,\n",
       "                      -2.2874e-03,  1.0142e-03,  2.4880e-04, -9.8649e-04, -2.9773e-03,\n",
       "                       2.4090e-03,  3.0693e-03, -7.5058e-04, -1.4768e-03,  3.4140e-04,\n",
       "                       2.6190e-03,  2.6788e-03,  1.9331e-03,  7.4008e-04,  9.1520e-04,\n",
       "                       3.2885e-03, -9.7735e-04, -4.8932e-04,  3.3945e-04, -2.5895e-03,\n",
       "                       1.0235e-04,  3.7346e-03, -2.9834e-04, -3.9280e-03, -1.3054e-03,\n",
       "                      -6.8013e-04,  1.6286e-03,  1.6212e-04, -3.6418e-04,  1.3549e-03,\n",
       "                       8.9221e-04,  9.3825e-05, -8.1893e-04,  1.1338e-03,  2.1637e-03,\n",
       "                      -3.5909e-03,  3.6835e-03,  1.7461e-05, -8.0283e-05, -2.8630e-03,\n",
       "                      -2.6147e-04, -2.3816e-03,  5.0801e-05, -1.2357e-03,  1.3713e-03,\n",
       "                      -1.9173e-03, -4.1687e-03, -1.1991e-03,  1.2907e-04,  2.6217e-04,\n",
       "                       1.9723e-04,  3.2069e-03,  2.6860e-03,  1.0786e-03, -2.5750e-03,\n",
       "                       1.7140e-03, -3.9003e-04,  2.9720e-03,  3.0807e-03, -9.0390e-04,\n",
       "                       2.4975e-03, -4.7610e-04, -5.2628e-03,  1.7836e-03, -1.0939e-03,\n",
       "                      -7.3550e-04,  1.3498e-03,  6.3755e-04, -5.5248e-04, -8.1054e-04,\n",
       "                       3.4884e-03, -9.9264e-04,  2.6401e-03, -1.5001e-03, -2.5614e-03,\n",
       "                       2.3819e-03,  1.5915e-03, -1.3568e-03, -5.2693e-04, -1.1756e-03,\n",
       "                      -1.3128e-03,  2.9693e-03, -1.2633e-03,  4.2119e-03,  1.0854e-03,\n",
       "                       7.4133e-04, -2.9262e-04, -5.0562e-04,  6.7299e-04,  3.4650e-03,\n",
       "                       2.3092e-03, -1.2751e-03,  2.1182e-03,  3.2620e-04,  7.9269e-04,\n",
       "                      -3.6169e-05, -1.9447e-03, -3.3942e-04, -3.3115e-04,  3.8719e-04,\n",
       "                       1.5565e-03,  1.0319e-03,  1.3472e-03,  1.0527e-03,  1.9855e-03,\n",
       "                      -7.3544e-04, -3.0130e-03,  1.0354e-03,  2.8018e-03,  7.7776e-04,\n",
       "                      -2.5087e-03,  2.5314e-03,  2.4331e-03, -6.8949e-04,  2.3592e-03,\n",
       "                      -9.0878e-04,  1.3038e-03,  2.2246e-03, -2.2909e-03, -1.5993e-03,\n",
       "                       6.6656e-04, -4.4202e-04, -1.0714e-03,  4.6306e-04,  3.8703e-04,\n",
       "                      -1.7569e-03,  2.1925e-03,  1.4458e-03, -5.7107e-04, -3.3667e-03,\n",
       "                      -8.2389e-04,  2.0416e-03, -3.8814e-03,  1.3675e-03, -1.0583e-03,\n",
       "                      -1.3533e-03,  7.9757e-04,  1.0114e-03, -3.5379e-04, -2.1520e-03,\n",
       "                      -3.0899e-03, -2.8630e-04,  1.4022e-03, -3.7177e-04,  1.4066e-03,\n",
       "                       1.6040e-04,  8.9915e-04, -9.9204e-04,  4.7907e-03,  1.0108e-03,\n",
       "                      -2.3068e-03,  3.4026e-04,  2.2319e-03,  1.8390e-03, -3.3234e-05,\n",
       "                       6.6459e-04, -9.2274e-04,  1.0256e-03,  3.5714e-03,  5.9844e-05,\n",
       "                      -8.2478e-05,  1.2418e-04,  1.5298e-03, -2.4991e-04, -2.2609e-03,\n",
       "                      -2.2773e-03,  1.0898e-03, -2.5370e-03, -1.6571e-03, -1.8880e-03,\n",
       "                       4.6699e-04, -2.7852e-03, -1.4314e-03, -1.9110e-03, -1.0312e-03,\n",
       "                      -2.9944e-03, -2.2199e-03,  9.3676e-04,  4.7950e-04, -1.0095e-03,\n",
       "                       1.5182e-03, -1.4212e-03, -2.0956e-03,  2.3985e-03,  1.4522e-03,\n",
       "                       8.1667e-04, -1.3526e-03,  6.8264e-04, -2.2825e-03, -1.9343e-03,\n",
       "                       3.2226e-04,  1.4901e-03, -8.0225e-04,  5.7267e-04, -3.1164e-03,\n",
       "                      -2.2821e-03,  1.1399e-03, -1.4882e-03,  3.8727e-03, -2.9056e-03,\n",
       "                       1.4199e-03, -1.6977e-03, -7.9910e-04,  9.9063e-04, -8.0954e-04,\n",
       "                      -1.5874e-03, -1.4881e-05])),\n",
       "             ('transformer.resblocks.11.ln_1.weight',\n",
       "              tensor([0.9967, 0.9996, 0.9994, 0.9988, 0.9988, 0.9989, 0.9999, 0.9984, 0.9961,\n",
       "                      0.9982, 0.9997, 0.9957, 0.9996, 0.9968, 0.9984, 0.9990, 0.9980, 0.9986,\n",
       "                      1.0017, 1.0002, 0.9977, 1.0013, 1.0028, 0.9979, 0.9998, 0.9983, 0.9999,\n",
       "                      1.0004, 1.0000, 0.9978, 0.9972, 1.0000, 1.0000, 1.0012, 1.0004, 1.0018,\n",
       "                      1.0014, 0.9973, 1.0006, 0.9981, 0.9994, 1.0016, 0.9998, 0.9968, 0.9999,\n",
       "                      1.0010, 1.0017, 1.0002, 0.9965, 0.9995, 1.0000, 0.9983, 1.0006, 0.9972,\n",
       "                      0.9987, 1.0008, 0.9970, 1.0006, 0.9996, 1.0013, 0.9983, 0.9997, 0.9991,\n",
       "                      0.9984, 0.9995, 0.9997, 0.9983, 1.0013, 0.9973, 1.0008, 0.9985, 1.0017,\n",
       "                      0.9987, 1.0000, 1.0009, 0.9994, 0.9969, 1.0013, 0.9982, 1.0007, 0.9982,\n",
       "                      1.0010, 0.9981, 1.0026, 1.0001, 0.9994, 0.9991, 1.0001, 0.9995, 0.9983,\n",
       "                      1.0017, 0.9990, 0.9983, 0.9997, 1.0001, 0.9968, 1.0020, 0.9996, 0.9990,\n",
       "                      0.9990, 1.0000, 0.9980, 0.9978, 0.9993, 0.9992, 0.9987, 0.9965, 1.0008,\n",
       "                      1.0002, 0.9962, 1.0007, 1.0007, 0.9988, 0.9982, 1.0007, 0.9982, 0.9997,\n",
       "                      0.9975, 0.9982, 0.9974, 1.0003, 0.9998, 1.0007, 0.9999, 1.0008, 1.0001,\n",
       "                      0.9987, 0.9973, 1.0005, 0.9989, 0.9974, 0.9988, 0.9948, 1.0011, 0.9990,\n",
       "                      0.9988, 1.0001, 0.9998, 0.9990, 1.0003, 1.0004, 0.9987, 0.9996, 0.9999,\n",
       "                      1.0013, 0.9960, 0.9959, 0.9990, 0.9967, 0.9998, 0.9980, 1.0012, 0.9979,\n",
       "                      0.9982, 1.0008, 0.9994, 0.9982, 1.0019, 1.0004, 1.0001, 0.9982, 0.9974,\n",
       "                      0.9969, 0.9966, 0.9994, 0.9970, 0.9983, 0.9970, 0.9977, 0.9983, 0.9997,\n",
       "                      0.9981, 1.0015, 0.9980, 0.9992, 1.0021, 1.0016, 0.9993, 1.0018, 0.9980,\n",
       "                      1.0001, 0.9977, 1.0000, 0.9975, 0.9996, 0.9995, 0.9991, 0.9996, 1.0005,\n",
       "                      1.0013, 1.0057, 0.9985, 0.9957, 0.9991, 1.0018, 0.9991, 0.9992, 0.9970,\n",
       "                      0.9991, 0.9986, 1.0009, 1.0025, 0.9968, 0.9993, 0.9997, 0.9969, 1.0003,\n",
       "                      1.0003, 1.0001, 0.9998, 0.9983, 0.9969, 1.0014, 0.9999, 0.9981, 0.9997,\n",
       "                      0.9983, 0.9998, 0.9970, 0.9980, 0.9992, 1.0007, 0.9996, 1.0001, 0.9962,\n",
       "                      1.0005, 0.9994, 0.9989, 0.9967, 0.9990, 0.9964, 0.9990, 0.9993, 0.9980,\n",
       "                      1.0025, 0.9971, 0.9979, 1.0009, 1.0008, 0.9990, 0.9983, 0.9992, 1.0006,\n",
       "                      0.9988, 0.9982, 1.0020, 0.9982, 0.9995, 1.0014, 0.9983, 0.9987, 1.0007,\n",
       "                      0.9982, 0.9996, 0.9985, 1.0006, 0.9988, 0.9976, 1.0007, 0.9998, 1.0016,\n",
       "                      0.9976, 1.0019, 0.9988, 0.9990, 0.9988, 0.9972, 1.0032, 1.0026, 1.0001,\n",
       "                      1.0000, 0.9990, 0.9999, 0.9985, 1.0002, 0.9989, 0.9977, 1.0012, 0.9995,\n",
       "                      0.9988, 0.9985, 1.0015, 0.9959, 0.9974, 0.9990, 0.9974, 0.9985, 0.9975,\n",
       "                      0.9996, 0.9990, 0.9980, 1.0004, 0.9980, 0.9982, 1.0018, 1.0013, 0.9987,\n",
       "                      0.9986, 0.9996, 1.0005, 0.9994, 1.0007, 0.9997, 0.9965, 1.0023, 1.0003,\n",
       "                      0.9967, 1.0003, 0.9982, 0.9983, 0.9988, 0.9975, 1.0014, 1.0011, 1.0012,\n",
       "                      0.9980, 0.9990, 0.9967, 0.9981, 0.9999, 0.9975, 0.9994, 0.9976, 0.9980,\n",
       "                      0.9961, 0.9996, 1.0000, 0.9962, 0.9975, 1.0021, 1.0008, 0.9962, 0.9991,\n",
       "                      1.0023, 0.9988, 0.9993, 0.9964, 0.9997, 0.9990, 1.0000, 0.9957, 0.9982,\n",
       "                      0.9997, 1.0014, 1.0004, 0.9979, 0.9993, 1.0014, 0.9992, 0.9997, 0.9991,\n",
       "                      0.9986, 0.9994, 1.0020, 0.9994, 0.9989, 0.9979, 0.9976, 1.0023, 0.9978,\n",
       "                      0.9996, 0.9996, 1.0022, 0.9973, 1.0007, 0.9990, 1.0020, 0.9987, 0.9983,\n",
       "                      0.9997, 0.9992, 0.9994, 0.9995, 0.9963, 1.0010, 1.0003, 1.0018, 1.0003,\n",
       "                      0.9997, 0.9982, 0.9986, 0.9994, 0.9999, 0.9992, 0.9987, 0.9973, 0.9980,\n",
       "                      0.9981, 0.9982, 0.9996, 0.9986, 0.9961, 1.0013, 0.9971, 0.9976, 0.9984,\n",
       "                      0.9983, 0.9997, 0.9988, 1.0041, 0.9962, 0.9984, 1.0011, 0.9972, 0.9989,\n",
       "                      0.9996, 0.9967, 0.9991, 1.0004, 0.9963, 1.0016, 1.0001, 0.9972, 1.0030,\n",
       "                      0.9991, 1.0021, 1.0019, 0.9975, 1.0002, 0.9978, 0.9978, 0.9958, 1.0000,\n",
       "                      0.9998, 0.9995, 1.0012, 0.9993, 0.9973, 1.0010, 0.9991, 0.9992, 0.9988,\n",
       "                      1.0011, 0.9990, 1.0004, 0.9977, 1.0011, 0.9972, 1.0005, 1.0011, 1.0008,\n",
       "                      0.9978, 1.0015, 1.0001, 0.9994, 0.9978, 0.9960, 0.9995, 1.0009, 0.9993,\n",
       "                      0.9992, 0.9993, 1.0019, 1.0004, 0.9984, 0.9986, 1.0011, 0.9994, 1.0007,\n",
       "                      0.9996, 0.9992, 0.9996, 0.9986, 0.9995, 0.9984, 0.9982, 0.9979, 0.9965,\n",
       "                      0.9973, 1.0006, 0.9969, 0.9994, 0.9992, 1.0011, 0.9983, 1.0004, 0.9990,\n",
       "                      1.0015, 0.9962, 0.9981, 0.9994, 0.9987, 1.0011, 0.9979, 0.9992, 1.0012,\n",
       "                      1.0021, 0.9990, 0.9989, 0.9986, 0.9970, 0.9989, 0.9993, 0.9989, 0.9979,\n",
       "                      0.9990, 0.9996, 0.9976, 0.9991, 1.0014, 0.9974, 0.9980, 0.9985, 1.0002,\n",
       "                      1.0007, 0.9992, 0.9990, 1.0012, 0.9974, 1.0007, 0.9970, 0.9988])),\n",
       "             ('transformer.resblocks.11.ln_1.bias',\n",
       "              tensor([ 8.5350e-04, -4.0553e-04, -2.3098e-03,  1.4118e-03, -1.2114e-03,\n",
       "                       6.1052e-04,  1.0598e-03, -5.2449e-04,  3.6446e-03,  7.8534e-04,\n",
       "                       2.3566e-04,  9.7907e-04, -1.4788e-03,  1.9354e-03,  1.0784e-03,\n",
       "                       7.0782e-05,  2.5819e-04,  6.9128e-05, -1.5798e-03,  8.4279e-04,\n",
       "                      -1.9047e-03, -8.7463e-04,  3.4064e-03,  2.0428e-03, -1.2467e-03,\n",
       "                       8.4808e-04,  4.3806e-04, -1.2455e-03,  9.1996e-04,  1.4874e-03,\n",
       "                      -5.4677e-04,  1.0875e-03,  1.9447e-03, -2.0532e-03,  1.5436e-03,\n",
       "                       2.6085e-03,  1.1067e-03, -2.8558e-03,  5.1450e-04,  9.6653e-04,\n",
       "                       1.0061e-03,  3.3079e-04,  1.9685e-04, -2.5151e-03, -8.2159e-05,\n",
       "                      -4.8368e-05, -1.7576e-03,  9.7286e-04,  7.7674e-04, -6.3242e-04,\n",
       "                      -1.8707e-04,  1.2040e-03,  1.9887e-04, -1.3321e-03, -1.1068e-03,\n",
       "                       1.2865e-06,  1.1207e-03,  7.9685e-04, -5.4571e-04,  1.4464e-03,\n",
       "                       2.0661e-04,  1.5943e-03, -4.9476e-04, -1.9084e-03,  3.9773e-03,\n",
       "                      -1.2024e-03,  9.3883e-04, -2.0249e-03,  1.2405e-03,  7.1458e-04,\n",
       "                      -9.0703e-04, -1.1533e-03,  6.7367e-04, -1.5869e-03, -7.5450e-04,\n",
       "                       2.4092e-03,  1.4089e-03, -8.2965e-04,  2.6894e-03,  1.9521e-03,\n",
       "                      -3.1120e-04,  1.6551e-03,  2.7230e-03,  1.9527e-03, -1.0323e-03,\n",
       "                       4.3283e-04,  3.1833e-06,  7.9087e-04, -8.6369e-04,  9.2222e-04,\n",
       "                       1.0681e-03,  1.7933e-03, -8.9983e-04,  6.0008e-04,  1.3808e-04,\n",
       "                       1.8505e-03,  2.4858e-03,  7.8816e-04,  9.1776e-04,  1.3450e-03,\n",
       "                       5.6509e-04, -2.0841e-04,  8.5804e-05,  1.4412e-03,  2.8642e-03,\n",
       "                       1.4927e-03,  3.3789e-03,  1.6608e-03,  2.1093e-03, -7.9570e-04,\n",
       "                       5.8940e-04, -1.1315e-03, -7.3942e-05, -2.8622e-03, -5.2850e-04,\n",
       "                       1.9310e-04,  8.2419e-04,  1.3283e-03,  6.3459e-04,  1.5388e-03,\n",
       "                       1.3810e-03,  1.5871e-03, -4.7149e-04,  2.8788e-04, -1.5563e-03,\n",
       "                       1.1302e-03, -1.0162e-03,  6.8705e-04, -1.4291e-03, -9.0635e-05,\n",
       "                      -7.1465e-04, -4.1854e-04, -1.0732e-03,  1.5604e-03, -5.3325e-04,\n",
       "                       7.2316e-04, -1.1681e-03,  1.0952e-03, -1.0318e-03,  5.9614e-04,\n",
       "                       6.9886e-04,  8.7893e-05,  1.6467e-03,  8.7703e-05,  8.5345e-04,\n",
       "                      -3.4485e-03,  2.9051e-03, -3.5393e-03,  3.1243e-04,  1.3796e-03,\n",
       "                      -8.9630e-04,  2.2099e-03, -1.5523e-03,  6.4597e-04,  1.8260e-03,\n",
       "                      -2.2803e-03, -8.8748e-04,  1.4860e-04,  6.2053e-04,  4.3220e-05,\n",
       "                      -1.2215e-04,  2.8351e-05, -2.9477e-03, -2.4923e-03, -3.0766e-03,\n",
       "                      -2.2842e-03, -1.2200e-03,  3.5565e-03,  1.3862e-03,  3.9823e-04,\n",
       "                       3.9893e-04,  2.1956e-03, -8.7553e-04, -1.6220e-03, -1.5367e-03,\n",
       "                       1.4003e-03, -1.4386e-03, -9.7413e-04, -3.0908e-03,  4.9918e-04,\n",
       "                       7.0266e-05, -1.1899e-03,  6.6532e-04,  2.0455e-03,  4.2514e-04,\n",
       "                       1.3639e-03,  4.5049e-03, -2.4037e-03,  1.7996e-03, -1.7106e-03,\n",
       "                      -1.1898e-03, -5.1154e-04, -3.2597e-03, -1.0336e-03, -5.4293e-04,\n",
       "                       3.0892e-04, -7.6051e-04, -2.0390e-03, -1.1565e-05, -7.0700e-04,\n",
       "                      -8.9150e-04, -2.1264e-03, -3.9420e-03, -8.1553e-04, -2.4629e-04,\n",
       "                      -2.5589e-03, -7.1153e-04, -1.4290e-03, -2.1489e-04,  2.5039e-03,\n",
       "                      -9.2830e-04,  3.9339e-03,  2.8050e-04,  1.0175e-03, -6.8391e-05,\n",
       "                       9.8461e-04,  2.2965e-05, -5.1491e-04,  3.1527e-04, -1.3711e-03,\n",
       "                      -4.4604e-04, -3.9419e-04, -5.0820e-04, -2.8997e-04, -2.9449e-03,\n",
       "                      -2.0211e-04,  1.0029e-03,  1.3983e-03, -4.9013e-04,  7.5602e-04,\n",
       "                       1.6147e-03, -1.0182e-03,  6.2044e-05,  2.2145e-03, -2.8578e-04,\n",
       "                       1.3985e-03, -1.0562e-03, -2.0809e-04,  6.0030e-04, -2.5673e-03,\n",
       "                       4.7650e-04, -8.3326e-04,  1.7524e-03,  4.0987e-05,  2.3987e-03,\n",
       "                       2.1092e-03, -4.2250e-03, -5.3865e-04, -2.1563e-03, -1.4434e-04,\n",
       "                       9.5235e-06, -1.9976e-03, -2.0843e-04,  2.1368e-03,  6.0015e-04,\n",
       "                       4.3650e-04,  2.1439e-03,  6.3076e-04,  1.2233e-03, -4.9349e-04,\n",
       "                       2.8168e-03, -3.2836e-03,  1.5567e-03,  7.9935e-04,  3.1545e-04,\n",
       "                      -9.7512e-04,  8.2765e-04,  2.0575e-03,  1.4290e-03, -2.1692e-03,\n",
       "                      -1.5188e-03, -1.2204e-03, -1.2461e-03,  2.7568e-03,  8.0376e-04,\n",
       "                       1.1905e-03, -7.3522e-04, -1.4594e-03,  2.4971e-04,  1.1067e-03,\n",
       "                       2.1823e-03,  5.1672e-04,  1.3004e-03,  7.3124e-05,  1.2255e-03,\n",
       "                      -2.1425e-03, -1.6807e-03, -2.1490e-03, -6.6701e-04, -5.4254e-04,\n",
       "                       7.7487e-04,  7.7949e-04,  6.3522e-04, -4.7054e-05,  2.2825e-03,\n",
       "                      -2.2354e-03,  1.8095e-04,  1.0890e-05,  1.5139e-03,  4.1823e-04,\n",
       "                      -1.6198e-04,  9.1554e-04, -2.2295e-03,  2.2842e-03, -8.3136e-04,\n",
       "                      -1.6444e-03, -1.7380e-03,  6.9269e-04,  2.5997e-04,  8.4464e-04,\n",
       "                      -3.6355e-03, -2.4790e-03, -5.6614e-04, -2.8343e-03,  1.3720e-03,\n",
       "                      -2.0064e-03, -1.4523e-04, -2.0559e-03,  1.9553e-04,  7.9707e-04,\n",
       "                      -1.6817e-03, -1.4618e-04,  2.3730e-03, -1.2682e-03,  1.3473e-03,\n",
       "                       2.3565e-03,  1.0364e-03, -3.8208e-04,  2.1532e-03, -4.6680e-04,\n",
       "                      -1.2675e-03, -7.6855e-04,  1.2423e-03, -3.0196e-03, -1.5206e-03,\n",
       "                      -2.9065e-03,  7.4112e-04, -7.6306e-04,  1.3342e-03, -5.8006e-05,\n",
       "                       3.1593e-03, -2.7390e-04,  2.0336e-03, -1.1344e-03,  1.2272e-04,\n",
       "                       2.7946e-03,  5.7808e-04, -1.3402e-03,  1.8120e-04, -3.9004e-04,\n",
       "                       5.4774e-04,  1.1216e-03, -1.6380e-03,  1.7656e-03, -5.8055e-04,\n",
       "                       1.7187e-05, -7.0764e-04, -9.5021e-04,  1.2897e-03,  1.2219e-03,\n",
       "                       9.1709e-05, -1.1839e-03,  3.2251e-03, -1.9613e-03, -1.0290e-04,\n",
       "                       9.5640e-05,  1.6570e-03,  3.6368e-04,  1.8418e-04, -6.9857e-04,\n",
       "                      -2.4648e-03,  8.0842e-04, -7.7720e-04,  2.8328e-03,  2.6979e-03,\n",
       "                       1.4022e-04, -1.5300e-04,  1.9427e-03, -4.9085e-04,  1.7530e-03,\n",
       "                      -8.6579e-04, -2.0974e-03, -3.3034e-04,  4.7285e-04,  9.5170e-04,\n",
       "                      -9.9818e-04, -6.9971e-04,  2.5193e-03, -2.4274e-03,  2.8432e-04,\n",
       "                      -1.1715e-03,  7.3637e-04,  1.6590e-03, -6.7182e-05, -2.5570e-03,\n",
       "                       9.7380e-04, -1.3243e-03, -1.3226e-03, -2.5704e-03, -1.3401e-03,\n",
       "                      -1.5648e-03,  1.6328e-03,  8.7413e-05,  1.1962e-03, -7.4965e-04,\n",
       "                      -5.9334e-04, -1.0422e-03,  9.2931e-04,  7.1702e-04, -3.7558e-03,\n",
       "                      -1.4633e-03, -1.2952e-03, -2.6032e-03,  1.4270e-03, -4.1196e-04,\n",
       "                      -2.3988e-03,  1.5716e-03, -4.0696e-03,  4.6199e-04, -6.6887e-04,\n",
       "                      -8.4726e-04, -4.0843e-04, -1.7597e-03,  6.9180e-04, -1.1771e-03,\n",
       "                      -8.6710e-04, -6.5236e-04,  3.4086e-03,  3.5302e-03,  3.8444e-05,\n",
       "                       5.4927e-04,  2.1400e-03,  1.1871e-03, -9.3708e-04, -9.1291e-04,\n",
       "                       2.7760e-03,  1.0582e-03,  2.6595e-03,  1.5984e-03, -1.7568e-03,\n",
       "                      -1.5213e-03, -1.5706e-03, -1.7108e-03, -9.3493e-04, -1.4168e-03,\n",
       "                       2.4080e-03, -2.4141e-03, -1.2418e-03,  1.0346e-03,  1.1440e-03,\n",
       "                      -7.0461e-04,  1.5788e-03, -1.2386e-03,  1.5952e-03, -2.4293e-03,\n",
       "                       3.7233e-04,  4.8609e-04, -1.1014e-04,  2.2198e-04,  2.1560e-04,\n",
       "                       1.9886e-04,  6.8531e-04,  6.8138e-04,  1.9850e-05,  1.8829e-04,\n",
       "                       8.3638e-04,  5.9747e-04, -3.3410e-03, -2.5439e-03,  7.8688e-04,\n",
       "                       2.2627e-03, -6.2571e-04,  1.0042e-03,  1.6240e-03,  1.1841e-03,\n",
       "                      -2.8853e-04,  1.5279e-03, -1.6912e-03,  2.9764e-03,  1.1197e-03,\n",
       "                      -5.9715e-04,  8.2240e-04,  6.5285e-06,  1.4478e-03, -5.2025e-04,\n",
       "                       3.1417e-04, -9.3276e-04,  2.5340e-03,  3.4883e-04, -6.6602e-04,\n",
       "                       3.2115e-04,  2.2237e-03,  7.3812e-04,  2.4144e-04,  3.7268e-04,\n",
       "                      -4.6308e-04, -5.9365e-04,  1.0747e-04, -2.6376e-03, -2.5713e-03,\n",
       "                       1.3161e-03, -2.2740e-03, -6.7332e-04,  7.6425e-04, -1.1303e-03,\n",
       "                      -1.1583e-04, -8.0556e-04,  3.3144e-05, -5.5231e-04,  2.8222e-03,\n",
       "                       2.8214e-03,  2.2180e-03])),\n",
       "             ('transformer.resblocks.11.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0310,  0.0396,  0.0546,  ..., -0.0299, -0.0166,  0.0002],\n",
       "                      [-0.0375, -0.0682,  0.0015,  ...,  0.0015, -0.0228, -0.0148],\n",
       "                      [-0.0208, -0.0211,  0.0334,  ...,  0.0130,  0.0273,  0.0153],\n",
       "                      ...,\n",
       "                      [ 0.0435,  0.0410,  0.0331,  ...,  0.0350, -0.0156, -0.0174],\n",
       "                      [ 0.0466,  0.0565, -0.0074,  ..., -0.0667, -0.0107,  0.0084],\n",
       "                      [-0.0271, -0.0015, -0.0162,  ...,  0.0317, -0.0070, -0.0246]])),\n",
       "             ('transformer.resblocks.11.mlp.c_fc.bias',\n",
       "              tensor([ 0.0024, -0.0001,  0.0009,  ..., -0.0348, -0.0396,  0.0250])),\n",
       "             ('transformer.resblocks.11.mlp.c_proj.weight',\n",
       "              tensor([[ 2.5465e-03, -3.7949e-04, -2.6675e-03,  ..., -9.9703e-03,\n",
       "                        7.6195e-03,  2.7008e-03],\n",
       "                      [ 4.8455e-04, -1.3234e-02,  6.6279e-03,  ...,  1.4343e-03,\n",
       "                        2.8288e-02,  7.4191e-03],\n",
       "                      [ 4.1279e-05, -2.0517e-03,  1.9585e-02,  ..., -4.5030e-03,\n",
       "                        9.5562e-04,  1.2278e-02],\n",
       "                      ...,\n",
       "                      [-6.1257e-04, -7.1217e-04,  1.1816e-02,  ...,  4.3275e-03,\n",
       "                       -5.3084e-03,  8.5324e-03],\n",
       "                      [-1.2689e-02, -1.0179e-02, -2.6734e-03,  ..., -8.9791e-03,\n",
       "                        5.3729e-03, -2.4627e-03],\n",
       "                      [-2.1042e-03,  5.1673e-03, -8.5973e-03,  ...,  4.1561e-03,\n",
       "                        7.9764e-04, -5.0466e-03]])),\n",
       "             ('transformer.resblocks.11.mlp.c_proj.bias',\n",
       "              tensor([ 7.8059e-03, -6.7684e-03,  4.8165e-03, -2.1274e-02,  2.4418e-03,\n",
       "                       1.2978e-02, -2.0554e-02,  1.8395e-02,  1.7405e-02, -4.2448e-03,\n",
       "                      -1.9380e-02, -2.7079e-05,  2.3861e-03,  8.1349e-04,  1.0557e-02,\n",
       "                       8.1757e-03, -9.7690e-03,  1.7552e-02, -3.7141e-03, -1.9175e-02,\n",
       "                       1.6119e-02, -6.6262e-03,  9.6593e-03,  1.9932e-02, -8.1995e-03,\n",
       "                      -2.0746e-02,  1.2763e-02,  1.3391e-02,  6.2244e-04, -4.6520e-03,\n",
       "                       1.5078e-02,  2.0767e-02, -1.3260e-02, -1.1176e-02,  1.0006e-02,\n",
       "                      -1.9298e-02, -1.0042e-02,  8.6082e-03, -1.4057e-02, -1.9688e-02,\n",
       "                      -1.0883e-02, -1.5628e-02,  1.4808e-02,  1.8623e-02,  1.3685e-02,\n",
       "                      -4.8580e-03, -1.2165e-02,  2.0312e-02, -1.2727e-02,  2.2169e-02,\n",
       "                       3.7957e-03,  1.2975e-02, -1.4678e-02,  1.6062e-02, -1.0194e-02,\n",
       "                       8.0011e-04,  4.0542e-03, -1.6990e-02, -1.0290e-02, -3.9652e-03,\n",
       "                       1.3348e-02,  1.1630e-02,  1.6878e-02,  5.4103e-03, -7.2393e-03,\n",
       "                      -1.1974e-02, -4.5495e-03, -1.9809e-02,  1.4104e-02,  2.1058e-02,\n",
       "                       1.5301e-02,  1.7439e-02, -2.9784e-04,  1.7567e-02,  3.7462e-03,\n",
       "                      -1.3567e-02, -2.3166e-03,  1.7690e-02, -2.2576e-02, -1.8889e-02,\n",
       "                       1.0715e-02,  9.4540e-03, -8.8843e-03, -1.2389e-02, -3.2369e-03,\n",
       "                       9.3671e-03, -2.0408e-02,  1.8689e-02, -5.2168e-03, -2.4731e-02,\n",
       "                      -1.1929e-02, -1.0335e-02,  1.4147e-02, -1.7585e-02, -5.5298e-03,\n",
       "                      -2.4891e-02, -1.9104e-02,  4.3463e-03, -2.0077e-02, -1.0211e-02,\n",
       "                       1.3563e-02, -9.7103e-03, -7.2459e-03, -1.6283e-02, -5.6669e-03,\n",
       "                       1.2894e-02,  7.7622e-03,  6.3426e-03, -6.3388e-03, -8.2652e-03,\n",
       "                      -1.0259e-03, -7.0508e-04,  1.8952e-02,  1.3244e-04,  1.4883e-02,\n",
       "                       4.0846e-03,  1.6736e-02, -2.0021e-02,  1.2064e-02, -1.0474e-02,\n",
       "                       8.9004e-03, -1.1282e-02, -1.0282e-03, -1.9644e-02, -7.2577e-03,\n",
       "                       2.8651e-03,  1.9043e-02, -2.0865e-02, -4.1233e-03,  4.8687e-03,\n",
       "                      -1.1221e-02,  1.7094e-02,  1.1620e-02, -1.1246e-02, -1.6870e-02,\n",
       "                       1.6602e-02,  6.2921e-03,  4.5725e-03, -1.7709e-02,  6.4209e-03,\n",
       "                       1.9247e-04, -1.8386e-02,  1.6373e-03, -1.3577e-02,  9.8031e-03,\n",
       "                      -6.5075e-03, -6.4100e-04, -2.0522e-02, -4.1226e-03,  1.1725e-02,\n",
       "                      -1.8529e-02, -2.2467e-03, -1.1835e-02,  1.1258e-02, -1.4773e-02,\n",
       "                       1.8815e-02, -1.1854e-02,  1.6534e-02,  9.2462e-03, -1.0941e-02,\n",
       "                      -2.3188e-02,  1.6616e-02, -7.2992e-03, -1.9656e-03, -2.1585e-02,\n",
       "                      -7.4440e-03, -1.5047e-03, -5.8892e-04,  7.9263e-03, -3.0065e-03,\n",
       "                       1.6428e-02,  1.5416e-02,  4.3309e-04, -4.8473e-03, -1.5487e-02,\n",
       "                       6.6165e-03,  1.1197e-02,  5.6385e-03,  1.3670e-02,  1.8410e-02,\n",
       "                       1.2306e-02, -1.6325e-02,  5.5292e-03, -1.9657e-02,  2.2453e-02,\n",
       "                      -1.3900e-03,  1.4550e-02, -2.1464e-02, -1.2873e-02,  4.8021e-03,\n",
       "                      -1.1066e-02,  1.7732e-02, -9.7927e-03,  2.3905e-02,  5.1433e-03,\n",
       "                      -2.1107e-02, -1.6887e-02, -5.3340e-03, -1.4991e-02, -2.9875e-03,\n",
       "                      -9.3649e-03,  3.6940e-03, -6.1127e-03, -1.8895e-02, -4.3294e-03,\n",
       "                      -1.4832e-02, -1.9515e-02, -9.0503e-03, -2.0132e-02, -1.2808e-02,\n",
       "                       2.5475e-03,  1.3028e-02,  6.5088e-03,  1.9616e-02,  9.2075e-03,\n",
       "                      -8.2167e-03, -9.3317e-03,  2.0552e-02, -1.7947e-02, -1.4605e-02,\n",
       "                       6.5469e-04,  7.7913e-03, -5.7432e-03, -9.1179e-03, -3.8517e-03,\n",
       "                       4.3553e-03, -1.8577e-02, -1.7749e-02,  2.2226e-02, -1.4861e-02,\n",
       "                       1.5981e-02, -1.7599e-02, -1.3008e-02,  1.7054e-03,  2.0955e-02,\n",
       "                       1.0505e-02,  2.1506e-02, -2.0347e-02, -1.4175e-02, -1.9836e-02,\n",
       "                      -1.1741e-02,  2.0089e-02,  7.4478e-03,  5.5944e-03,  5.2383e-03,\n",
       "                       2.8691e-04, -1.3780e-03, -1.0986e-02, -4.4297e-03, -1.1609e-03,\n",
       "                       1.4363e-02,  1.7721e-02,  1.9677e-02,  3.0447e-04, -1.7577e-02,\n",
       "                       7.2378e-05,  3.6566e-03,  1.0219e-02,  1.5042e-02, -6.4096e-04,\n",
       "                      -1.4273e-03, -2.0681e-02, -1.7413e-02,  1.2005e-02,  3.6465e-03,\n",
       "                       2.0629e-02, -6.4656e-03,  3.7906e-03,  1.8074e-02,  5.2258e-03,\n",
       "                       1.5188e-02,  1.2143e-02, -1.5547e-02, -1.0441e-02,  1.1958e-02,\n",
       "                      -1.6347e-02, -4.9654e-03, -3.4588e-03,  2.1227e-02,  1.1487e-02,\n",
       "                      -1.3227e-02,  8.6380e-03, -6.3567e-04,  1.9426e-02,  1.8317e-02,\n",
       "                       1.8115e-02,  1.2087e-02, -1.1607e-02,  1.3772e-02, -1.9341e-02,\n",
       "                       3.5576e-03,  1.5979e-02, -2.4509e-04,  4.4515e-03,  2.1339e-02,\n",
       "                       3.2078e-04, -1.3397e-02,  9.6334e-03,  2.2506e-05,  1.1733e-03,\n",
       "                      -1.7951e-02, -1.1148e-02, -1.6718e-02, -2.3859e-02, -3.3470e-03,\n",
       "                      -1.7271e-02, -9.1672e-03,  2.0425e-02,  4.0629e-03, -1.8058e-02,\n",
       "                      -1.8454e-02,  1.1486e-03, -1.9359e-02, -1.3711e-02,  9.7342e-03,\n",
       "                       1.9616e-02,  2.2741e-02, -1.3586e-02,  3.8614e-03,  9.0154e-04,\n",
       "                      -4.3203e-03, -1.0144e-02,  1.3845e-02,  7.8192e-03, -2.3472e-02,\n",
       "                      -1.7240e-02, -1.3283e-02, -1.5310e-02, -2.2727e-02, -1.1245e-02,\n",
       "                       1.3402e-02, -1.9837e-02, -4.0660e-03,  4.1243e-03,  8.1692e-03,\n",
       "                       1.2101e-02,  4.7849e-03, -3.5544e-03,  8.7933e-03, -3.2156e-03,\n",
       "                       6.5059e-03,  9.2210e-03,  1.7684e-02, -8.1528e-03,  1.2708e-02,\n",
       "                       1.7384e-02,  4.0938e-03,  1.7805e-02, -1.5850e-02, -1.0676e-02,\n",
       "                      -7.1960e-03, -8.3627e-03,  5.6384e-03,  5.4330e-03, -1.7794e-02,\n",
       "                       9.2508e-03,  1.6079e-03,  1.7253e-02, -1.5535e-02,  1.0681e-02,\n",
       "                       1.0217e-02,  1.3847e-02,  5.6421e-03,  3.6191e-03,  5.7098e-03,\n",
       "                       2.4288e-02, -9.4012e-03, -5.7810e-03, -3.3109e-03, -4.5738e-03,\n",
       "                       1.7468e-02,  8.3858e-03,  2.2494e-02,  9.6926e-03,  1.3419e-02,\n",
       "                      -1.7063e-02, -1.1732e-02, -1.4067e-02, -1.8558e-02, -8.3969e-03,\n",
       "                       1.2212e-02,  2.8085e-03, -1.8921e-02, -8.0657e-03, -1.5252e-02,\n",
       "                      -1.1243e-03, -8.4910e-03,  2.3449e-03,  1.8612e-02, -4.1493e-03,\n",
       "                       9.7985e-03, -8.8795e-03,  1.1164e-02,  1.2600e-02,  1.0207e-02,\n",
       "                       4.5804e-04, -7.7078e-03,  2.6026e-03,  2.2445e-02, -1.6276e-02,\n",
       "                       1.9055e-03, -1.7648e-02, -9.7186e-03, -6.0908e-03, -1.0894e-02,\n",
       "                       1.8260e-02,  3.3537e-03, -1.5953e-02, -1.2446e-02, -1.9479e-02,\n",
       "                      -2.6383e-03,  1.8081e-02,  1.5139e-02,  5.3843e-03,  6.4566e-03,\n",
       "                      -1.2655e-02,  1.8952e-02, -1.0880e-02, -2.0677e-02,  9.6777e-03,\n",
       "                       1.4595e-03, -1.8374e-02,  1.1515e-02, -9.4409e-03, -8.7682e-03,\n",
       "                       1.8678e-02,  4.6089e-03,  8.1795e-03,  1.4457e-02,  1.5770e-02,\n",
       "                      -1.5130e-02,  8.8661e-03,  1.3909e-02, -2.2432e-02, -1.3569e-02,\n",
       "                      -1.2236e-02,  1.4600e-03, -9.7343e-03, -5.4822e-03, -1.8493e-02,\n",
       "                       8.9499e-03,  4.5954e-03,  1.5243e-02,  1.2676e-02,  7.2456e-04,\n",
       "                       6.2876e-03,  2.8562e-03,  2.3246e-02, -3.2994e-03, -6.7068e-03,\n",
       "                       2.2111e-02,  6.8604e-03,  1.3780e-02,  1.1401e-02, -2.0415e-02,\n",
       "                       6.1419e-03,  1.1405e-02,  5.2876e-04, -4.8821e-03, -7.4070e-03,\n",
       "                       1.9600e-02,  1.5214e-02,  1.5660e-02, -1.3991e-02,  2.0971e-02,\n",
       "                      -1.0660e-02, -7.3005e-03,  1.1792e-02,  3.7189e-03, -1.6910e-02,\n",
       "                      -1.9782e-03, -9.8628e-03,  2.3644e-03, -1.4569e-03,  1.8793e-02,\n",
       "                       1.9638e-02, -1.5005e-03, -2.0225e-02, -9.2218e-03,  2.6407e-03,\n",
       "                      -8.2866e-03, -7.6439e-03, -8.1562e-03, -4.3113e-03, -1.0339e-02,\n",
       "                       1.0909e-02, -9.3785e-03, -2.1205e-02,  2.2186e-02,  2.2953e-03,\n",
       "                      -1.6553e-02,  1.0278e-02, -1.0113e-02, -8.2417e-03, -2.1735e-02,\n",
       "                       1.1317e-02,  1.6405e-02,  4.3104e-03,  1.0270e-02, -9.5406e-03,\n",
       "                      -1.9895e-03, -1.3216e-02, -6.6550e-03,  2.0222e-02,  1.2216e-02,\n",
       "                      -9.1252e-03,  1.3144e-02, -1.6353e-02, -1.6656e-02,  6.2260e-03,\n",
       "                       1.4584e-02,  1.2829e-02])),\n",
       "             ('transformer.resblocks.11.ln_2.weight',\n",
       "              tensor([0.9997, 0.9974, 1.0010, 1.0010, 1.0021, 0.9986, 0.9993, 0.9968, 0.9977,\n",
       "                      0.9995, 0.9962, 1.0004, 1.0004, 0.9996, 0.9944, 0.9961, 1.0028, 1.0000,\n",
       "                      1.0011, 1.0001, 1.0014, 1.0008, 1.0035, 1.0032, 0.9992, 0.9979, 1.0017,\n",
       "                      0.9980, 0.9992, 0.9975, 0.9993, 0.9993, 0.9981, 0.9973, 0.9998, 1.0020,\n",
       "                      0.9999, 1.0021, 1.0017, 0.9999, 0.9984, 0.9972, 0.9989, 1.0005, 0.9965,\n",
       "                      0.9997, 0.9961, 0.9992, 0.9986, 1.0008, 1.0008, 1.0041, 1.0009, 0.9974,\n",
       "                      0.9963, 0.9998, 0.9973, 1.0018, 1.0018, 1.0020, 0.9981, 1.0025, 0.9985,\n",
       "                      1.0020, 1.0020, 0.9998, 0.9979, 1.0010, 1.0003, 0.9972, 1.0004, 1.0009,\n",
       "                      0.9982, 0.9997, 1.0006, 0.9985, 0.9977, 0.9970, 0.9969, 0.9987, 0.9989,\n",
       "                      1.0008, 0.9991, 0.9989, 0.9997, 1.0019, 0.9990, 1.0002, 1.0020, 0.9985,\n",
       "                      0.9994, 1.0036, 0.9991, 0.9978, 0.9975, 0.9971, 0.9959, 0.9981, 1.0015,\n",
       "                      0.9996, 1.0011, 1.0019, 0.9979, 0.9991, 1.0001, 1.0000, 0.9979, 1.0008,\n",
       "                      0.9961, 1.0019, 0.9980, 0.9988, 0.9979, 0.9976, 1.0010, 0.9951, 0.9979,\n",
       "                      0.9981, 0.9943, 0.9979, 1.0015, 0.9984, 0.9988, 1.0013, 0.9990, 0.9993,\n",
       "                      0.9990, 1.0008, 1.0005, 1.0027, 0.9990, 0.9958, 1.0027, 0.9969, 1.0007,\n",
       "                      1.0022, 0.9980, 0.9991, 1.0028, 0.9990, 0.9952, 0.9991, 1.0033, 0.9990,\n",
       "                      0.9979, 0.9995, 0.9956, 1.0014, 0.9996, 1.0011, 0.9991, 0.9924, 0.9966,\n",
       "                      1.0028, 1.0003, 1.0046, 1.0002, 1.0002, 1.0044, 1.0004, 1.0005, 0.9948,\n",
       "                      0.9950, 1.0016, 0.9971, 0.9982, 0.9991, 0.9983, 1.0016, 0.9970, 0.9999,\n",
       "                      0.9961, 0.9972, 0.9986, 0.9972, 0.9999, 0.9983, 1.0017, 0.9984, 0.9972,\n",
       "                      1.0004, 1.0006, 0.9988, 0.9984, 0.9957, 0.9996, 0.9955, 0.9976, 0.9998,\n",
       "                      0.9963, 1.0030, 0.9982, 0.9979, 0.9958, 0.9984, 0.9997, 1.0001, 0.9987,\n",
       "                      0.9982, 0.9988, 1.0012, 1.0001, 1.0002, 0.9978, 1.0026, 0.9988, 0.9998,\n",
       "                      1.0009, 0.9980, 0.9993, 0.9930, 1.0004, 1.0002, 0.9952, 0.9970, 0.9980,\n",
       "                      0.9998, 0.9984, 0.9976, 1.0013, 0.9985, 1.0010, 0.9986, 0.9994, 0.9991,\n",
       "                      1.0008, 1.0008, 0.9999, 1.0014, 1.0006, 0.9983, 1.0006, 1.0012, 0.9964,\n",
       "                      0.9985, 0.9981, 0.9985, 0.9982, 0.9997, 0.9993, 0.9974, 1.0011, 0.9956,\n",
       "                      1.0031, 0.9980, 1.0034, 1.0031, 0.9964, 1.0016, 1.0034, 0.9973, 1.0017,\n",
       "                      1.0020, 0.9998, 0.9998, 1.0023, 1.0006, 0.9961, 0.9975, 0.9952, 1.0013,\n",
       "                      1.0005, 1.0019, 0.9993, 0.9982, 0.9992, 0.9998, 1.0007, 0.9963, 1.0016,\n",
       "                      0.9998, 0.9987, 0.9994, 1.0003, 0.9991, 1.0015, 0.9952, 1.0043, 0.9974,\n",
       "                      0.9998, 1.0011, 0.9978, 0.9975, 1.0018, 0.9989, 1.0001, 0.9995, 0.9987,\n",
       "                      0.9985, 1.0022, 1.0042, 0.9991, 0.9977, 0.9960, 1.0024, 0.9993, 0.9971,\n",
       "                      1.0022, 1.0010, 1.0009, 0.9977, 0.9974, 0.9995, 0.9953, 0.9980, 0.9973,\n",
       "                      1.0004, 1.0030, 0.9985, 0.9991, 1.0010, 0.9955, 1.0022, 1.0012, 1.0026,\n",
       "                      0.9988, 0.9988, 0.9995, 1.0035, 0.9993, 0.9994, 0.9997, 1.0011, 1.0012,\n",
       "                      0.9979, 0.9984, 0.9963, 1.0009, 0.9942, 0.9970, 1.0015, 1.0008, 0.9996,\n",
       "                      1.0028, 1.0004, 1.0008, 1.0048, 1.0014, 0.9977, 0.9974, 0.9985, 0.9989,\n",
       "                      0.9979, 1.0021, 0.9975, 1.0015, 0.9991, 1.0033, 0.9990, 0.9996, 0.9974,\n",
       "                      0.9975, 0.9990, 1.0010, 1.0014, 1.0041, 0.9981, 0.9979, 1.0033, 0.9977,\n",
       "                      1.0022, 0.9979, 0.9988, 0.9991, 0.9980, 0.9968, 1.0000, 1.0000, 0.9960,\n",
       "                      1.0010, 0.9994, 0.9984, 1.0008, 1.0012, 0.9993, 0.9995, 0.9993, 0.9994,\n",
       "                      0.9976, 0.9992, 0.9992, 1.0009, 0.9984, 0.9986, 0.9950, 0.9989, 0.9993,\n",
       "                      0.9980, 0.9980, 1.0009, 0.9990, 0.9998, 0.9995, 0.9974, 0.9975, 1.0013,\n",
       "                      0.9999, 0.9989, 1.0022, 1.0013, 1.0008, 0.9952, 1.0027, 0.9999, 1.0029,\n",
       "                      1.0018, 0.9981, 0.9987, 0.9984, 1.0001, 0.9983, 0.9992, 1.0011, 0.9970,\n",
       "                      0.9964, 0.9959, 0.9978, 0.9950, 0.9984, 0.9971, 1.0031, 1.0005, 1.0007,\n",
       "                      0.9997, 0.9993, 0.9996, 0.9994, 1.0029, 0.9975, 0.9969, 0.9997, 0.9972,\n",
       "                      1.0018, 1.0014, 0.9967, 0.9999, 0.9945, 0.9960, 1.0000, 1.0007, 0.9983,\n",
       "                      1.0018, 0.9999, 0.9998, 0.9987, 0.9974, 1.0002, 1.0012, 0.9991, 1.0012,\n",
       "                      1.0011, 0.9953, 1.0013, 0.9952, 1.0006, 0.9984, 0.9994, 0.9994, 0.9993,\n",
       "                      0.9973, 0.9990, 0.9997, 1.0005, 0.9959, 1.0021, 0.9956, 1.0014, 0.9978,\n",
       "                      1.0015, 0.9973, 1.0011, 0.9993, 0.9974, 1.0009, 0.9983, 1.0008, 1.0016,\n",
       "                      1.0009, 0.9955, 1.0011, 1.0003, 1.0020, 1.0041, 1.0018, 1.0002, 0.9996,\n",
       "                      1.0020, 0.9975, 0.9990, 0.9971, 0.9989, 0.9958, 0.9991, 0.9988, 1.0045,\n",
       "                      0.9994, 0.9984, 0.9971, 1.0023, 0.9979, 0.9976, 0.9985, 0.9978, 0.9941,\n",
       "                      0.9977, 0.9991, 1.0009, 1.0015, 0.9995, 1.0023, 0.9966, 0.9993])),\n",
       "             ('transformer.resblocks.11.ln_2.bias',\n",
       "              tensor([-9.2171e-05,  2.3245e-03, -2.5286e-03,  1.1751e-04,  6.1468e-04,\n",
       "                       9.1659e-04,  1.6652e-03, -1.7608e-03,  1.6653e-03, -1.5656e-03,\n",
       "                       4.0853e-03,  7.6892e-04,  3.6977e-04,  1.7856e-03,  5.2387e-03,\n",
       "                      -3.5321e-03, -2.0611e-03,  6.9932e-04,  2.1912e-04,  1.1983e-03,\n",
       "                       9.1557e-04,  1.0814e-03, -8.1391e-04, -3.7289e-03,  2.3849e-04,\n",
       "                       2.2580e-03,  1.1371e-03,  6.6655e-04, -7.6684e-04,  2.2305e-03,\n",
       "                      -1.8129e-03,  1.8469e-03, -1.3942e-03,  2.7165e-03, -8.3769e-04,\n",
       "                       2.3276e-03,  3.1315e-04,  1.0981e-03,  1.4670e-03,  3.9822e-04,\n",
       "                      -1.8483e-03, -1.7777e-03, -1.6186e-03, -4.6709e-04,  2.7923e-03,\n",
       "                      -3.9020e-03,  1.8355e-03,  2.0013e-03,  1.0375e-03,  1.7968e-03,\n",
       "                      -2.1721e-04, -8.2363e-04,  5.0791e-04, -1.1754e-03, -3.0912e-03,\n",
       "                       7.6119e-04,  2.2861e-03,  2.5340e-03,  1.0770e-03,  2.1413e-03,\n",
       "                       9.4712e-04,  1.2887e-04, -1.2102e-03, -1.7100e-03, -1.2286e-03,\n",
       "                       4.4232e-04, -2.1889e-03,  8.9302e-04,  1.3094e-03, -8.6080e-05,\n",
       "                      -3.3240e-03, -1.6966e-03,  2.0083e-03,  1.0988e-03,  4.0449e-04,\n",
       "                      -9.0735e-04,  2.4216e-03,  9.4698e-04,  1.4057e-03, -1.0857e-03,\n",
       "                       5.5574e-04,  2.3167e-03,  5.0983e-04, -1.0315e-03, -2.5100e-03,\n",
       "                       1.2075e-03,  7.1479e-05,  4.0756e-04,  1.4235e-03,  1.8173e-03,\n",
       "                      -2.8282e-03, -1.9570e-03, -1.6700e-03,  4.0417e-03,  3.3642e-03,\n",
       "                       1.1848e-03,  3.4451e-03,  5.1966e-03,  5.0167e-04, -6.8504e-04,\n",
       "                      -4.3664e-04, -3.8428e-04,  8.2531e-04,  1.6733e-03, -2.7625e-03,\n",
       "                       5.5106e-04,  9.1059e-04,  1.4512e-03, -1.0620e-03,  4.9388e-04,\n",
       "                      -3.5434e-03,  1.1701e-03, -1.9972e-03, -1.7702e-03,  2.3132e-03,\n",
       "                       3.3218e-03, -7.7897e-04, -5.9661e-04,  4.7489e-03,  7.3759e-04,\n",
       "                       5.0462e-04,  3.4107e-03, -1.7017e-03, -5.4685e-04,  4.5933e-04,\n",
       "                      -3.8725e-04,  2.5816e-03,  1.2914e-03,  1.5613e-03,  1.0439e-03,\n",
       "                       2.7442e-03, -3.4417e-03,  4.9424e-03,  1.1204e-03,  8.4351e-04,\n",
       "                       3.3198e-04,  1.6340e-03, -7.2540e-04, -1.9715e-03, -3.1226e-03,\n",
       "                       7.5849e-04, -3.3499e-03,  1.0331e-03,  3.4092e-04, -3.1514e-03,\n",
       "                      -2.1392e-03,  3.3864e-03, -3.2697e-03, -1.4708e-03,  5.1692e-04,\n",
       "                      -2.0173e-04, -5.1683e-03, -1.0817e-03,  2.4919e-03,  3.9161e-04,\n",
       "                      -1.0140e-03, -5.7440e-04,  2.3239e-03, -2.5594e-03,  1.0829e-03,\n",
       "                      -2.0855e-03,  1.2900e-03, -3.8772e-03, -2.8343e-04,  2.0062e-03,\n",
       "                      -2.4824e-04,  2.2458e-03, -2.0727e-03,  7.4631e-04,  1.8631e-03,\n",
       "                      -2.1996e-03,  5.7635e-04,  2.1548e-03, -7.2379e-04, -5.2743e-04,\n",
       "                      -1.4149e-03,  1.6434e-03, -3.3675e-03,  5.6042e-04,  3.1483e-03,\n",
       "                       2.7889e-04,  6.7214e-04, -1.5909e-06,  2.3001e-03, -2.6008e-03,\n",
       "                       2.0758e-04, -1.9960e-03,  5.5267e-04, -4.3111e-04,  2.3651e-03,\n",
       "                      -2.6400e-03, -1.3039e-03, -9.9156e-04, -1.9391e-03,  5.4435e-04,\n",
       "                       2.1446e-03, -2.1112e-03, -1.7523e-03, -2.1472e-03, -1.4686e-03,\n",
       "                       3.2168e-03,  1.4686e-03, -1.2903e-03, -1.8539e-03, -5.9825e-05,\n",
       "                      -1.3978e-03,  2.2311e-04, -4.0663e-03, -4.1651e-03,  1.5266e-03,\n",
       "                      -5.3019e-03,  1.1459e-03, -1.6627e-03, -3.4435e-03,  4.1361e-03,\n",
       "                       1.2480e-03, -2.6683e-03, -1.0607e-04,  1.9415e-03, -5.9185e-04,\n",
       "                       1.4659e-03, -9.5756e-04, -2.0094e-03, -1.0423e-03, -1.9688e-05,\n",
       "                      -2.6813e-03, -9.2601e-05,  2.2010e-03, -3.0188e-04, -8.6796e-04,\n",
       "                      -5.3953e-04,  1.2730e-03, -1.2175e-03,  3.5194e-03, -3.3288e-03,\n",
       "                       2.5416e-04,  4.3367e-04,  1.9406e-03, -2.0360e-03, -1.1122e-03,\n",
       "                       2.9402e-03,  4.4010e-04, -4.2312e-03,  2.3999e-03,  1.4227e-03,\n",
       "                      -1.1881e-03, -5.0213e-04, -7.8805e-04,  3.8510e-04,  2.8105e-04,\n",
       "                      -7.1787e-04,  1.1158e-03, -7.4329e-04,  3.1336e-05, -1.5552e-03,\n",
       "                       1.5907e-03,  1.0762e-03,  3.2088e-03,  1.0918e-03,  1.6801e-03,\n",
       "                      -2.7365e-03, -9.4626e-04, -2.6877e-03,  1.7914e-03,  1.6958e-03,\n",
       "                       1.3214e-03, -2.8774e-03, -1.2675e-03, -1.9547e-03, -5.7688e-04,\n",
       "                      -3.6114e-04,  1.4179e-03,  2.6042e-03, -1.1038e-04,  1.7168e-04,\n",
       "                       4.1265e-04, -2.5306e-03,  1.4745e-03, -1.4366e-03, -6.4306e-04,\n",
       "                      -1.3299e-03, -1.8163e-03,  1.0569e-04, -8.1596e-06,  2.3360e-03,\n",
       "                      -1.0643e-04,  1.2727e-03, -2.7514e-03, -1.7935e-03, -7.9210e-05,\n",
       "                      -5.0522e-05, -3.7508e-04,  1.1399e-03, -3.0157e-03,  6.1218e-04,\n",
       "                      -1.1488e-04,  1.1813e-03, -7.4506e-04, -3.9212e-04, -4.1452e-05,\n",
       "                       3.1212e-03,  8.6914e-04,  3.7392e-04,  5.6951e-03,  2.4014e-03,\n",
       "                       1.2022e-04, -6.4385e-04, -2.7745e-03,  1.6233e-03,  2.4405e-03,\n",
       "                      -1.9462e-03, -2.1211e-03,  1.0305e-03,  7.2707e-04,  1.8082e-03,\n",
       "                      -1.8603e-03, -2.4790e-03, -3.0350e-03, -1.0066e-03, -2.9173e-04,\n",
       "                      -1.8943e-03, -2.6156e-04,  1.0180e-04,  3.6826e-04,  1.8906e-03,\n",
       "                      -2.2783e-04, -5.2962e-03,  4.0976e-04,  6.2308e-03,  1.8716e-03,\n",
       "                       1.6769e-03, -1.3873e-04, -1.3953e-03, -1.1003e-03,  1.2023e-03,\n",
       "                      -2.0240e-03, -1.8754e-03,  1.8610e-03, -3.0433e-03, -1.4023e-03,\n",
       "                       1.3851e-03, -3.3300e-03, -1.9962e-03,  6.3550e-04,  3.4396e-03,\n",
       "                       4.3165e-04, -2.7738e-03, -1.1873e-03, -4.1863e-04, -2.6837e-03,\n",
       "                       1.4505e-03,  1.2778e-03,  1.6877e-03, -3.8724e-03, -1.3982e-03,\n",
       "                      -2.8292e-05,  4.5798e-04, -1.6804e-03, -1.5673e-03,  3.7500e-03,\n",
       "                       1.3212e-03,  2.7515e-03, -2.3027e-03,  5.1456e-04, -7.2435e-04,\n",
       "                      -5.1493e-04,  6.5173e-04, -9.4453e-04, -3.1557e-03,  6.6357e-05,\n",
       "                       8.9922e-04, -1.0649e-03, -2.1796e-04,  1.6256e-03,  1.5850e-04,\n",
       "                      -1.1828e-03, -2.7274e-03, -3.1144e-03,  1.8091e-03,  2.5838e-03,\n",
       "                      -7.4063e-04,  1.8139e-03, -2.1774e-04,  1.1401e-03,  1.7254e-03,\n",
       "                       1.6392e-03,  1.0405e-03, -1.3267e-03, -2.6610e-03,  3.3199e-04,\n",
       "                      -8.3956e-04, -2.5140e-04,  9.4884e-04, -7.0185e-04, -3.9826e-03,\n",
       "                      -3.8692e-04,  1.3275e-03, -5.1760e-04, -1.7745e-03, -2.5861e-03,\n",
       "                      -7.9282e-04,  4.2725e-03,  2.0328e-03,  3.1885e-04, -5.4196e-03,\n",
       "                       9.0832e-04, -9.8965e-04, -7.1237e-04, -7.9394e-04, -4.0242e-04,\n",
       "                       1.5157e-03, -1.1693e-03, -9.2504e-04, -2.7676e-03, -2.0621e-03,\n",
       "                       3.3709e-03, -1.9068e-03, -5.7099e-03,  1.1957e-03, -1.3727e-03,\n",
       "                       5.0074e-04, -7.6585e-04, -4.4034e-04,  1.8018e-03,  6.8863e-04,\n",
       "                      -1.1611e-03, -6.6003e-04,  1.9869e-03, -3.3360e-05,  3.6407e-04,\n",
       "                       6.8094e-04, -1.3515e-03,  2.3942e-03,  1.4480e-03,  3.0516e-03,\n",
       "                      -1.2085e-03, -3.0605e-03,  2.6941e-03, -3.5929e-04, -7.9206e-04,\n",
       "                       5.5701e-04,  2.3800e-03, -1.2455e-03,  2.6391e-04, -1.0827e-03,\n",
       "                       2.6116e-03,  8.9695e-04,  7.0486e-04,  1.6989e-03, -1.5297e-03,\n",
       "                      -2.1714e-03, -3.7061e-03,  4.5507e-04, -3.3481e-03,  1.0737e-03,\n",
       "                       2.5644e-03, -1.5410e-03, -1.6700e-03, -2.0569e-03, -2.0672e-03,\n",
       "                      -2.6137e-03, -1.3048e-03, -9.7882e-04, -4.3731e-03, -7.6241e-04,\n",
       "                      -6.1405e-04,  1.4389e-05,  2.1839e-03,  3.0282e-03,  2.6193e-03,\n",
       "                       1.5050e-04, -1.2774e-03,  2.1831e-03,  1.7305e-03,  2.1129e-03,\n",
       "                       1.1459e-04, -1.0496e-03,  6.3619e-04,  3.2412e-03,  1.4457e-04,\n",
       "                       8.1311e-05,  3.1259e-04,  1.0690e-03,  4.1531e-04,  5.4812e-04,\n",
       "                      -1.6570e-03, -7.3482e-04,  3.2951e-03, -1.6994e-03, -1.7189e-03,\n",
       "                      -2.3941e-03,  1.5859e-03, -3.8135e-03,  1.4151e-03, -6.1553e-04,\n",
       "                      -1.2044e-03,  1.5257e-03,  3.7460e-03,  1.0863e-03,  1.1519e-04,\n",
       "                       2.6475e-03,  3.2696e-03, -5.1644e-04, -3.9798e-03,  1.0509e-03,\n",
       "                      -1.2800e-03,  1.0257e-03, -1.4032e-03, -5.9605e-04, -5.1611e-04,\n",
       "                       3.4505e-03,  3.3926e-03])),\n",
       "             ('token_embedding.weight',\n",
       "              tensor([[ 0.0029, -0.0087, -0.0036,  ..., -0.0200, -0.0224, -0.0124],\n",
       "                      [ 0.0055, -0.0065,  0.0246,  ...,  0.0183, -0.0117, -0.0036],\n",
       "                      [-0.0341,  0.0144,  0.0161,  ...,  0.0064,  0.0342,  0.0076],\n",
       "                      ...,\n",
       "                      [-0.0059, -0.0220,  0.0202,  ...,  0.0036,  0.0180,  0.0201],\n",
       "                      [ 0.0003,  0.0123,  0.0106,  ...,  0.0036,  0.0065,  0.0084],\n",
       "                      [-0.0155, -0.0130,  0.0088,  ..., -0.0018,  0.0092,  0.0331]])),\n",
       "             ('ln_final.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('ln_final.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_gnn = '/data/LPJ/ICML25/all_checkpoints/graph_tower/pretrain_unified_lr_8e3_gnn_projector_without_lora/clip_gt_arxiv/model_weights.pkl'\n",
    "pretrained_gnn = torch.load(pretrained_gnn, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_gnn = {f'gnn.{k}': v for k, v in pretrained_gnn.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gnn.W_pos': tensor([[ 5.9814e-03,  1.3916e-02, -1.9646e-04, -1.7944e-02,  9.3384e-03,\n",
       "          -3.8910e-03, -2.8229e-03,  7.7248e-05,  1.6235e-02,  1.1444e-03,\n",
       "          -7.6294e-03,  1.0681e-02, -1.3489e-02,  5.1880e-03,  1.4832e-02,\n",
       "          -4.4250e-03, -4.6997e-03, -3.6926e-03, -1.5991e-02,  4.8828e-03,\n",
       "          -9.7046e-03, -1.2939e-02, -1.2756e-02,  9.9487e-03, -1.5991e-02,\n",
       "           8.7280e-03,  6.8970e-03,  1.2207e-03,  5.7678e-03, -3.0060e-03,\n",
       "          -1.0132e-02,  9.7656e-03,  1.2329e-02, -1.4191e-03, -7.6294e-03,\n",
       "          -1.1169e-02,  1.8433e-02,  8.1787e-03, -8.2397e-04,  7.7515e-03,\n",
       "          -7.3547e-03, -1.9653e-02, -5.3711e-03,  1.9165e-02,  5.9509e-03,\n",
       "          -1.6968e-02, -2.8992e-03, -1.0437e-02,  1.3000e-02,  4.8828e-03,\n",
       "          -1.4343e-03, -1.4526e-02,  4.3945e-03, -4.6082e-03,  5.9307e-06,\n",
       "          -5.9814e-03, -1.3428e-02, -7.8125e-03, -2.1667e-03,  1.1658e-02,\n",
       "          -1.1658e-02, -4.0588e-03, -1.1108e-02, -1.6251e-03, -3.1128e-03,\n",
       "           1.4709e-02,  3.4790e-03, -1.0315e-02, -1.0620e-02, -3.6163e-03,\n",
       "          -1.3489e-02, -9.7046e-03,  1.5869e-02, -1.2451e-02, -4.7913e-03,\n",
       "           1.0254e-02, -1.7166e-03, -5.1880e-04, -7.5378e-03, -4.6387e-03,\n",
       "          -1.0437e-02,  1.1475e-02,  5.9509e-03,  1.1536e-02,  1.0757e-03,\n",
       "          -1.0315e-02, -6.5613e-03,  1.1780e-02,  1.8799e-02,  1.6113e-02,\n",
       "          -1.9531e-02, -1.9775e-02, -2.4872e-03, -1.0223e-03,  1.8921e-02,\n",
       "          -8.5449e-03,  1.2573e-02,  3.6774e-03,  1.9043e-02,  6.2561e-04,\n",
       "           9.8267e-03, -4.2725e-03,  5.9814e-03,  3.2349e-03, -8.6670e-03,\n",
       "          -1.6357e-02,  4.4556e-03,  7.8201e-04,  6.4392e-03,  3.1090e-04,\n",
       "           1.5625e-02, -2.8839e-03, -1.7944e-02,  5.5237e-03,  1.8311e-02,\n",
       "           5.3406e-03,  1.9775e-02, -1.0071e-02, -1.1963e-02,  8.1177e-03,\n",
       "           8.6060e-03, -8.4229e-03, -5.0545e-05, -4.0283e-03, -1.6846e-02,\n",
       "          -3.1891e-03, -2.0020e-02, -3.3722e-03]], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.qTrans': tensor([[ 0.0718, -0.0708, -0.0214,  ..., -0.0938, -0.1406, -0.0003],\n",
       "         [ 0.0898,  0.1543, -0.1050,  ...,  0.0723,  0.0271, -0.0742],\n",
       "         [ 0.0928,  0.1021, -0.0131,  ..., -0.1377,  0.1177, -0.0058],\n",
       "         ...,\n",
       "         [-0.0364, -0.0146, -0.0801,  ..., -0.0298, -0.0231,  0.0084],\n",
       "         [-0.1030, -0.1152,  0.0986,  ..., -0.1250, -0.0530, -0.1187],\n",
       "         [-0.1289, -0.0742,  0.0317,  ..., -0.1426, -0.0376, -0.1143]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.kTrans': tensor([[-0.0469, -0.0801, -0.1396,  ...,  0.0574,  0.0574,  0.0439],\n",
       "         [ 0.0106,  0.1416, -0.0381,  ..., -0.0786,  0.1167, -0.0791],\n",
       "         [ 0.1104, -0.1387, -0.0264,  ..., -0.1309, -0.0713, -0.0967],\n",
       "         ...,\n",
       "         [ 0.0618,  0.1455, -0.1094,  ...,  0.1377, -0.1099,  0.1533],\n",
       "         [-0.0649, -0.0820,  0.1040,  ...,  0.1099, -0.0505, -0.1172],\n",
       "         [ 0.0146, -0.0183,  0.1299,  ...,  0.0574, -0.1221, -0.0061]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.vTrans': tensor([[ 0.0359,  0.0645, -0.0347,  ...,  0.0208,  0.0737,  0.0776],\n",
       "         [-0.1260, -0.0996,  0.0752,  ..., -0.0820,  0.0542, -0.0479],\n",
       "         [ 0.0708, -0.0119,  0.0354,  ...,  0.1484,  0.1250,  0.0918],\n",
       "         ...,\n",
       "         [-0.0942,  0.1475,  0.1377,  ..., -0.0432,  0.0552, -0.1230],\n",
       "         [ 0.0432,  0.1455, -0.0452,  ..., -0.0884, -0.0342, -0.1357],\n",
       "         [ 0.1318,  0.0574, -0.1074,  ...,  0.0488,  0.0275, -0.0327]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.norm.weight': tensor([1.0000, 0.9922, 1.0000, 0.9961, 0.9961, 1.0000, 1.0078, 1.0000, 1.0078,\n",
       "         0.9922, 0.9922, 1.0000, 1.0000, 1.0078, 1.0000, 0.9961, 0.9961, 0.9961,\n",
       "         0.9922, 0.9961, 1.0078, 0.9961, 0.9961, 1.0000, 0.9922, 0.9883, 0.9922,\n",
       "         1.0000, 0.9961, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 0.9883, 0.9922,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 0.9922, 1.0000, 1.0000, 1.0000, 0.9922,\n",
       "         1.0000, 0.9883, 0.9961, 1.0000, 1.0000, 0.9961, 0.9844, 1.0078, 0.9961,\n",
       "         0.9961, 1.0000, 1.0000, 0.9922, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         0.9961, 0.9922, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 0.9961, 0.9961,\n",
       "         1.0000, 1.0000, 0.9922, 0.9961, 1.0000, 0.9922, 1.0078, 1.0000, 0.9922,\n",
       "         0.9961, 1.0000, 0.9883, 0.9961, 0.9883, 1.0000, 0.9922, 0.9922, 1.0000,\n",
       "         1.0078, 0.9922, 0.9961, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 1.0078,\n",
       "         0.9922, 1.0078, 0.9961, 0.9922, 1.0078, 1.0000, 0.9961, 0.9922, 1.0078,\n",
       "         1.0000, 1.0000, 1.0078, 0.9922, 0.9961, 0.9961, 0.9961, 0.9961, 1.0078,\n",
       "         0.9961, 0.9922, 0.9961, 0.9961, 0.9883, 1.0078, 0.9961, 0.9961, 0.9961,\n",
       "         1.0000, 1.0000], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.norm.bias': tensor([ 1.0498e-02,  4.3701e-02, -1.9043e-02,  1.2146e-02,  1.2085e-02,\n",
       "         -8.5449e-03,  5.2734e-02, -7.5073e-03,  1.3489e-02, -2.7832e-02,\n",
       "         -1.0315e-02, -2.4292e-02,  1.7822e-02,  3.0396e-02,  1.5991e-02,\n",
       "          2.5757e-02,  1.6708e-03, -1.2512e-02, -1.9165e-02, -2.0874e-02,\n",
       "         -3.0396e-02,  4.1016e-02, -1.3550e-02, -2.7008e-03, -2.6001e-02,\n",
       "          7.9956e-03,  3.7384e-03,  1.5747e-02,  2.4902e-02, -5.7983e-03,\n",
       "          6.7749e-03, -2.2430e-03,  5.7373e-03,  1.0300e-03, -1.9379e-03,\n",
       "          3.4943e-03,  6.7444e-03,  2.3193e-02, -2.5391e-02, -4.2114e-03,\n",
       "          1.8188e-02,  2.5787e-03, -1.7944e-02, -1.2207e-03, -4.5166e-03,\n",
       "          2.8320e-02,  1.1169e-02, -8.4229e-03,  1.4343e-02, -7.0190e-04,\n",
       "         -5.2185e-03,  1.3580e-03, -1.9531e-02,  2.3926e-02, -3.8818e-02,\n",
       "         -3.4912e-02, -6.2012e-02,  1.4877e-03,  3.5248e-03,  2.7084e-04,\n",
       "         -1.4343e-02,  4.6875e-02, -1.2268e-02, -1.2512e-02,  1.5015e-02,\n",
       "         -5.9814e-03,  3.8574e-02,  1.9165e-02, -6.1523e-02, -2.9175e-02,\n",
       "          7.6599e-03, -3.4424e-02,  1.8799e-02,  7.8735e-03,  4.1748e-02,\n",
       "         -2.1057e-03,  6.3782e-03,  1.3184e-02,  3.0029e-02,  5.8594e-03,\n",
       "         -4.0233e-06,  9.5215e-03,  3.3936e-02,  2.9297e-03, -3.0670e-03,\n",
       "          8.5449e-03,  2.8931e-02,  2.9419e-02, -1.4526e-02, -1.5747e-02,\n",
       "         -9.0942e-03,  3.4912e-02,  8.0566e-03, -6.2256e-03,  9.3994e-03,\n",
       "         -6.2561e-03, -1.5030e-03, -2.6489e-02,  2.4048e-02,  5.0964e-03,\n",
       "         -1.3184e-02, -1.5747e-02,  1.8463e-03, -6.9275e-03,  4.3640e-03,\n",
       "          9.2163e-03,  7.0572e-04, -1.8082e-03, -1.2207e-03,  8.2397e-03,\n",
       "         -2.8076e-02, -2.6123e-02,  4.6997e-03,  9.5825e-03,  4.6387e-03,\n",
       "         -1.1292e-02, -5.9570e-02,  1.5076e-02, -2.9297e-02, -1.0376e-03,\n",
       "          7.1716e-03, -1.3794e-02, -3.7842e-02,  4.5166e-02,  6.4392e-03,\n",
       "          3.0823e-03, -5.0049e-03,  4.1016e-02], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.qTrans': tensor([[ 0.0649, -0.0203, -0.1367,  ..., -0.1318,  0.0184, -0.0649],\n",
       "         [ 0.1187, -0.0059,  0.1250,  ...,  0.0131, -0.1196,  0.1089],\n",
       "         [ 0.0047, -0.1367,  0.0508,  ..., -0.0796, -0.1426,  0.0491],\n",
       "         ...,\n",
       "         [-0.0928, -0.0505,  0.1064,  ...,  0.1396,  0.0415,  0.0703],\n",
       "         [-0.0188, -0.0576,  0.0249,  ...,  0.0168, -0.0322,  0.0413],\n",
       "         [ 0.1260, -0.1465,  0.0491,  ...,  0.0417, -0.0583,  0.1035]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.kTrans': tensor([[-0.1191, -0.0608, -0.1445,  ..., -0.0786, -0.0791, -0.0134],\n",
       "         [-0.0111, -0.0330, -0.1055,  ...,  0.0089, -0.1025, -0.0889],\n",
       "         [ 0.1001, -0.1309, -0.0942,  ...,  0.0028, -0.0067, -0.0884],\n",
       "         ...,\n",
       "         [ 0.0942, -0.0884,  0.0571,  ..., -0.1445, -0.0786,  0.0586],\n",
       "         [-0.1357,  0.1260, -0.0084,  ..., -0.1172,  0.0371, -0.0403],\n",
       "         [-0.0576,  0.1396, -0.1250,  ...,  0.0786, -0.1177, -0.0474]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.vTrans': tensor([[-0.0408,  0.1328,  0.0315,  ...,  0.0090,  0.0894,  0.1133],\n",
       "         [ 0.1406, -0.0500, -0.1318,  ...,  0.0060, -0.1060,  0.0102],\n",
       "         [ 0.1040,  0.0143, -0.0801,  ..., -0.0043, -0.1514, -0.0327],\n",
       "         ...,\n",
       "         [-0.0422, -0.0068, -0.0669,  ..., -0.1387,  0.0217,  0.0854],\n",
       "         [ 0.1260,  0.0156, -0.0583,  ..., -0.0972, -0.0159, -0.0128],\n",
       "         [-0.0179,  0.1094, -0.0491,  ..., -0.1465, -0.0972, -0.0603]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.norm.weight': tensor([1.0000, 0.9961, 0.9922, 1.0000, 0.9961, 1.0000, 0.9922, 0.9922, 1.0078,\n",
       "         1.0000, 1.0078, 1.0078, 0.9961, 0.9922, 1.0000, 1.0078, 0.9961, 0.9961,\n",
       "         0.9922, 0.9883, 1.0000, 0.9961, 0.9961, 0.9922, 0.9961, 1.0000, 0.9961,\n",
       "         0.9883, 0.9961, 0.9961, 1.0000, 0.9922, 1.0000, 0.9883, 0.9922, 1.0000,\n",
       "         1.0000, 1.0000, 1.0156, 1.0000, 1.0000, 1.0000, 1.0000, 0.9922, 0.9883,\n",
       "         1.0078, 0.9922, 0.9961, 1.0000, 1.0078, 0.9961, 0.9883, 1.0156, 1.0078,\n",
       "         0.9961, 0.9961, 1.0156, 0.9883, 1.0078, 1.0156, 1.0000, 1.0078, 1.0000,\n",
       "         1.0000, 0.9961, 1.0000, 0.9961, 0.9961, 1.0000, 0.9961, 0.9922, 1.0000,\n",
       "         1.0000, 1.0000, 0.9922, 0.9922, 0.9961, 0.9844, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.9883, 0.9883, 1.0078, 1.0078, 0.9844, 1.0000, 0.9961,\n",
       "         1.0000, 0.9922, 0.9922, 1.0078, 0.9961, 1.0000, 0.9883, 0.9922, 1.0000,\n",
       "         0.9922, 0.9922, 1.0000, 0.9961, 1.0000, 1.0000, 0.9961, 0.9922, 0.9844,\n",
       "         1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 1.0000, 1.0078, 1.0000, 1.0078,\n",
       "         1.0000, 0.9922, 1.0078, 0.9961, 0.9922, 0.9922, 0.9922, 0.9922, 1.0078,\n",
       "         1.0000, 0.9961], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.norm.bias': tensor([ 0.0236,  0.0430, -0.0012,  0.0520,  0.0029,  0.0006,  0.0320,  0.0132,\n",
       "         -0.0026, -0.0437, -0.0159,  0.0222,  0.0058,  0.0183,  0.0309,  0.0147,\n",
       "         -0.0055, -0.0002, -0.0214, -0.0101, -0.0303,  0.0339, -0.0269,  0.0189,\n",
       "         -0.0048, -0.0294,  0.0121,  0.0009,  0.0238, -0.0139,  0.0106,  0.0007,\n",
       "         -0.0023, -0.0121, -0.0126,  0.0079, -0.0006,  0.0388, -0.0166, -0.0016,\n",
       "          0.0214,  0.0195, -0.0236,  0.0082, -0.0014,  0.0210,  0.0193, -0.0066,\n",
       "          0.0096,  0.0013, -0.0227, -0.0048, -0.0151,  0.0040, -0.0302, -0.0064,\n",
       "         -0.0598, -0.0145, -0.0354,  0.0031,  0.0248,  0.0126, -0.0354, -0.0114,\n",
       "         -0.0019, -0.0081,  0.0299,  0.0066, -0.0374, -0.0243,  0.0140,  0.0053,\n",
       "         -0.0262, -0.0026,  0.0393, -0.0337, -0.0111,  0.0089,  0.0306, -0.0124,\n",
       "          0.0006, -0.0096,  0.0432, -0.0081,  0.0012,  0.0430,  0.0332,  0.0227,\n",
       "         -0.0239,  0.0106, -0.0190,  0.0393,  0.0080, -0.0106,  0.0118, -0.0023,\n",
       "         -0.0020,  0.0031,  0.0069,  0.0094, -0.0118, -0.0253,  0.0164,  0.0030,\n",
       "          0.0038, -0.0182, -0.0021, -0.0417,  0.0079,  0.0006, -0.0344, -0.0366,\n",
       "          0.0061, -0.0025,  0.0111, -0.0105, -0.0574,  0.0172, -0.0062,  0.0234,\n",
       "          0.0294, -0.0258, -0.0146,  0.0210,  0.0050,  0.0220, -0.0117,  0.0254],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.qTrans': tensor([[ 0.0021, -0.0435,  0.0034,  ..., -0.0391,  0.0515,  0.0447],\n",
       "         [-0.0635, -0.1260, -0.0281,  ...,  0.0840, -0.1445, -0.1406],\n",
       "         [-0.0854,  0.0457,  0.0403,  ...,  0.0007, -0.1016, -0.0566],\n",
       "         ...,\n",
       "         [-0.0037, -0.1484, -0.0212,  ..., -0.0496,  0.1133, -0.0796],\n",
       "         [ 0.0513, -0.0825, -0.1289,  ...,  0.0332, -0.0231,  0.0815],\n",
       "         [ 0.0347, -0.0461,  0.1143,  ..., -0.0320, -0.0320,  0.0723]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.kTrans': tensor([[-0.1396,  0.1270,  0.0093,  ..., -0.0195, -0.0879,  0.0415],\n",
       "         [-0.0820,  0.0659, -0.0383,  ..., -0.1172, -0.0869, -0.0427],\n",
       "         [ 0.0623, -0.0408,  0.0977,  ..., -0.0225, -0.0194, -0.0248],\n",
       "         ...,\n",
       "         [-0.0732, -0.1328,  0.1260,  ..., -0.0396, -0.0299,  0.0282],\n",
       "         [-0.1377, -0.1328, -0.1279,  ...,  0.0525, -0.0371,  0.0552],\n",
       "         [-0.0376,  0.0889,  0.0747,  ...,  0.0786,  0.0471, -0.1494]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.vTrans': tensor([[-0.0312,  0.1455,  0.0806,  ...,  0.0669, -0.0723, -0.0938],\n",
       "         [ 0.0161,  0.0332, -0.0437,  ...,  0.0173, -0.0053,  0.0674],\n",
       "         [-0.0583,  0.0510,  0.0203,  ...,  0.1445,  0.0884, -0.0325],\n",
       "         ...,\n",
       "         [-0.1040, -0.0315, -0.1592,  ...,  0.0508,  0.0879, -0.1406],\n",
       "         [-0.0986, -0.0055, -0.0003,  ...,  0.1289,  0.0309, -0.0879],\n",
       "         [-0.1533, -0.0845,  0.1021,  ..., -0.0415,  0.0601,  0.0371]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.norm.weight': tensor([1.0000, 1.0000, 0.9922, 1.0000, 0.9961, 0.9961, 1.0078, 1.0000, 0.9922,\n",
       "         1.0000, 1.0000, 1.0000, 0.9922, 0.9961, 1.0000, 1.0000, 0.9961, 0.9961,\n",
       "         1.0000, 0.9922, 1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 0.9961, 1.0000,\n",
       "         0.9961, 0.9961, 1.0078, 0.9961, 0.9922, 1.0078, 1.0000, 0.9961, 0.9961,\n",
       "         1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 1.0078, 1.0000, 0.9961, 0.9922,\n",
       "         0.9961, 0.9922, 1.0078, 1.0000, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000,\n",
       "         0.9922, 0.9961, 0.9961, 0.9961, 1.0078, 1.0078, 0.9961, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.9922, 1.0078, 0.9961, 1.0000, 1.0000, 1.0000, 0.9961,\n",
       "         1.0000, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.9961, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 0.9922,\n",
       "         1.0000, 1.0000, 1.0000, 1.0078, 0.9961, 1.0000, 0.9961, 1.0000, 0.9961,\n",
       "         0.9961, 0.9961, 0.9961, 1.0000, 0.9961, 1.0078, 0.9961, 1.0078, 0.9961,\n",
       "         0.9961, 1.0000, 1.0000, 1.0000, 0.9961, 1.0000, 1.0000, 1.0000, 1.0078,\n",
       "         0.9922, 0.9961, 1.0078, 0.9922, 1.0078, 1.0000, 1.0000, 0.9883, 1.0000,\n",
       "         1.0000, 1.0000], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.norm.bias': tensor([ 0.0311,  0.0374, -0.0128,  0.0496, -0.0131,  0.0020,  0.0214,  0.0035,\n",
       "          0.0070, -0.0605, -0.0067,  0.0156,  0.0344,  0.0186,  0.0376, -0.0192,\n",
       "         -0.0249,  0.0222,  0.0089,  0.0029, -0.0126,  0.0344, -0.0317,  0.0150,\n",
       "         -0.0018, -0.0374, -0.0442,  0.0152,  0.0292,  0.0020,  0.0311, -0.0020,\n",
       "          0.0302, -0.0043, -0.0034,  0.0069, -0.0242,  0.0232, -0.0065, -0.0067,\n",
       "          0.0094,  0.0146, -0.0334, -0.0067,  0.0067,  0.0327, -0.0383, -0.0286,\n",
       "          0.0369, -0.0014, -0.0469, -0.0135, -0.0302,  0.0378, -0.0156,  0.0046,\n",
       "         -0.0503, -0.0312, -0.0393,  0.0054,  0.0051,  0.0203, -0.0508, -0.0109,\n",
       "         -0.0007, -0.0009,  0.0359, -0.0030, -0.0317, -0.0126,  0.0500, -0.0060,\n",
       "         -0.0111, -0.0393,  0.0630, -0.0160, -0.0337, -0.0045,  0.0233, -0.0198,\n",
       "          0.0238, -0.0271,  0.0281, -0.0337,  0.0048,  0.0464,  0.0261,  0.0461,\n",
       "         -0.0131,  0.0337, -0.0059,  0.0299,  0.0248, -0.0229,  0.0442, -0.0020,\n",
       "         -0.0034, -0.0040,  0.0145,  0.0113, -0.0275, -0.0312,  0.0205,  0.0116,\n",
       "          0.0085, -0.0383, -0.0452, -0.0635,  0.0500,  0.0031, -0.0417, -0.0537,\n",
       "         -0.0045, -0.0271,  0.0168,  0.0071, -0.0564,  0.0294, -0.0381,  0.0157,\n",
       "          0.0195, -0.0070,  0.0067,  0.0023,  0.0157,  0.0505, -0.0244, -0.0049],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.W_P.weight': tensor([[-0.0850, -0.0825, -0.0144,  ..., -0.0262,  0.0889,  0.0226],\n",
       "         [-0.0771,  0.0210,  0.0669,  ..., -0.0092, -0.0908, -0.0167],\n",
       "         [-0.0292,  0.0608,  0.0234,  ...,  0.0815,  0.0713, -0.0134],\n",
       "         ...,\n",
       "         [-0.0212, -0.0111, -0.0532,  ...,  0.0723, -0.0208,  0.0684],\n",
       "         [-0.0344,  0.0132,  0.0669,  ...,  0.0327,  0.0525,  0.0116],\n",
       "         [-0.0786,  0.0879, -0.0547,  ..., -0.0259,  0.0728,  0.0898]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.W_P.bias': tensor([-8.7402e-02,  7.8125e-02,  1.3809e-03, -6.4453e-02,  9.4727e-02,\n",
       "         -9.3262e-02,  1.1475e-01, -6.5918e-02,  7.2266e-02, -4.3457e-02,\n",
       "          2.4658e-02, -2.6855e-02, -1.6174e-03,  9.7168e-02,  6.1340e-03,\n",
       "         -5.3711e-02, -1.6235e-02, -1.0059e-01, -9.3262e-02,  3.0670e-03,\n",
       "         -1.2598e-01, -6.4941e-02, -9.7656e-02,  1.0254e-01, -1.7822e-02,\n",
       "          9.5825e-03,  1.4954e-02,  8.0566e-02, -8.4839e-03, -6.7383e-02,\n",
       "         -3.6926e-03, -2.6367e-02,  5.2490e-02,  4.1748e-02, -7.1777e-02,\n",
       "         -3.7598e-02, -7.0801e-02,  6.5430e-02, -6.3965e-02, -9.0332e-02,\n",
       "          7.2754e-02, -2.0630e-02, -1.0889e-01,  5.1758e-02, -5.7129e-02,\n",
       "          5.9326e-02, -3.1982e-02, -1.0529e-03,  4.3213e-02, -3.9551e-02,\n",
       "         -6.7383e-02,  8.8867e-02, -8.6060e-03,  8.5449e-02, -7.6660e-02,\n",
       "         -3.9795e-02, -6.6406e-02, -6.2012e-02,  6.2561e-03,  7.2266e-02,\n",
       "         -9.2285e-02,  4.6387e-02, -3.3447e-02, -5.9814e-02, -4.1504e-03,\n",
       "         -5.5664e-02,  5.0293e-02,  5.2979e-02, -1.0645e-01, -8.3008e-02,\n",
       "          2.1606e-02, -7.8735e-03, -8.5449e-03,  6.2500e-02,  8.2031e-02,\n",
       "          4.5654e-02,  2.6001e-02, -3.4668e-02, -3.5156e-02, -4.5898e-02,\n",
       "         -7.2266e-02,  3.9062e-02, -3.7670e-05,  5.8105e-02, -6.2012e-02,\n",
       "          5.4688e-02, -2.7588e-02,  5.3467e-02,  3.2471e-02, -8.7402e-02,\n",
       "         -8.5938e-02,  1.1377e-01,  8.1177e-03, -5.4443e-02, -9.2773e-02,\n",
       "         -7.1777e-02, -3.2715e-02,  6.7383e-02,  1.2793e-01, -2.1973e-02,\n",
       "          1.5869e-02, -5.2979e-02,  9.5703e-02, -1.6968e-02,  1.0864e-02,\n",
       "          4.4922e-02, -1.2390e-02, -5.6152e-02,  7.5195e-02,  8.6670e-03,\n",
       "         -9.7656e-02, -5.0049e-02,  5.0537e-02, -8.0078e-02, -6.4453e-02,\n",
       "          1.3855e-02, -3.4912e-02, -2.4780e-02, -1.8066e-02,  6.8848e-02,\n",
       "          4.8584e-02,  8.6914e-02, -1.2402e-01, -2.3926e-02, -7.8735e-03,\n",
       "         -2.1606e-02, -1.0498e-01,  1.1670e-01], dtype=torch.bfloat16),\n",
       " 'gnn.inverW_P.weight': tensor([[ 0.0161, -0.0608,  0.0537,  ...,  0.0474, -0.0452, -0.0249],\n",
       "         [ 0.0544, -0.0815,  0.0400,  ..., -0.0288, -0.0168, -0.0474],\n",
       "         [ 0.0098,  0.0698,  0.0815,  ..., -0.0408, -0.0820,  0.0962],\n",
       "         ...,\n",
       "         [-0.0317, -0.0674,  0.0820,  ...,  0.0588, -0.0091,  0.0228],\n",
       "         [ 0.0513, -0.0043, -0.0278,  ...,  0.0588, -0.0364,  0.0041],\n",
       "         [ 0.0669, -0.0613, -0.0182,  ..., -0.0605, -0.0732, -0.0498]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.inverW_P.bias': tensor([-1.1133e-01,  3.0762e-02, -6.6406e-02, -5.1270e-02,  1.0498e-01,\n",
       "          6.5918e-02,  7.5684e-03,  3.2616e-04, -9.0790e-04, -6.4453e-02,\n",
       "          7.0801e-02, -5.2734e-02, -8.2493e-05, -2.1729e-02,  6.9336e-02,\n",
       "          3.7109e-02, -9.1309e-02,  8.2031e-02,  7.5195e-02, -1.0059e-01,\n",
       "          2.0630e-02,  7.6660e-02, -5.6641e-02,  6.9824e-02,  8.6914e-02,\n",
       "         -6.1035e-02,  6.8848e-02,  2.9419e-02,  8.1543e-02, -6.0547e-02,\n",
       "          5.0049e-02,  5.4688e-02, -5.6885e-02, -2.4536e-02,  1.3306e-02,\n",
       "          8.1177e-03,  7.1777e-02, -1.0010e-01,  2.9419e-02,  7.2266e-02,\n",
       "         -1.1816e-01,  6.8359e-02, -3.5645e-02, -2.6978e-02,  5.3955e-02,\n",
       "         -5.7373e-02,  8.3008e-02,  8.0078e-02,  2.7832e-02,  2.1606e-02,\n",
       "          5.2979e-02, -6.8848e-02, -3.3691e-02,  1.0254e-01,  3.3936e-02,\n",
       "          6.2988e-02,  5.4016e-03,  1.0071e-02, -1.1963e-01,  5.2246e-02,\n",
       "         -1.0840e-01, -6.5918e-02, -6.2500e-02, -5.1758e-02,  2.7466e-02,\n",
       "         -2.6550e-03,  7.0312e-02,  2.5391e-02,  2.4902e-02,  1.3962e-03,\n",
       "         -8.6426e-02,  1.0107e-01, -8.2520e-02, -3.9307e-02, -8.8379e-02,\n",
       "         -8.8379e-02,  5.1880e-03, -9.2285e-02, -2.9175e-02,  7.9590e-02,\n",
       "         -5.5420e-02,  9.7168e-02, -9.7168e-02, -1.5625e-02, -1.6632e-03,\n",
       "         -3.1250e-02,  4.1992e-02, -1.5869e-02, -1.9043e-02, -1.0059e-01,\n",
       "          8.8867e-02, -1.8799e-02, -4.5166e-02,  2.4414e-02,  8.0078e-02,\n",
       "         -6.8359e-02,  7.8613e-02,  5.0537e-02, -4.7119e-02, -2.1484e-02,\n",
       "          3.6621e-02,  1.5625e-02,  2.0264e-02, -2.8809e-02, -1.0872e-04,\n",
       "         -2.4048e-02, -7.3242e-02, -1.1865e-01, -1.0742e-02, -9.5703e-02,\n",
       "          9.8145e-02,  7.4707e-02, -9.5703e-02,  4.5166e-02, -6.8848e-02,\n",
       "         -5.3955e-02, -4.1016e-02,  9.7168e-02,  9.9121e-02,  2.7466e-02,\n",
       "          6.7383e-02, -1.0254e-01,  8.1543e-02, -2.5757e-02,  9.0332e-02,\n",
       "         -5.6458e-03,  2.5146e-02, -7.8125e-02], dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in graph:\n",
    "    if key not in pretrained_gnn:\n",
    "        pretrained_gnn[key] = graph[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gnn.W_pos': tensor([[ 5.9814e-03,  1.3916e-02, -1.9646e-04, -1.7944e-02,  9.3384e-03,\n",
       "          -3.8910e-03, -2.8229e-03,  7.7248e-05,  1.6235e-02,  1.1444e-03,\n",
       "          -7.6294e-03,  1.0681e-02, -1.3489e-02,  5.1880e-03,  1.4832e-02,\n",
       "          -4.4250e-03, -4.6997e-03, -3.6926e-03, -1.5991e-02,  4.8828e-03,\n",
       "          -9.7046e-03, -1.2939e-02, -1.2756e-02,  9.9487e-03, -1.5991e-02,\n",
       "           8.7280e-03,  6.8970e-03,  1.2207e-03,  5.7678e-03, -3.0060e-03,\n",
       "          -1.0132e-02,  9.7656e-03,  1.2329e-02, -1.4191e-03, -7.6294e-03,\n",
       "          -1.1169e-02,  1.8433e-02,  8.1787e-03, -8.2397e-04,  7.7515e-03,\n",
       "          -7.3547e-03, -1.9653e-02, -5.3711e-03,  1.9165e-02,  5.9509e-03,\n",
       "          -1.6968e-02, -2.8992e-03, -1.0437e-02,  1.3000e-02,  4.8828e-03,\n",
       "          -1.4343e-03, -1.4526e-02,  4.3945e-03, -4.6082e-03,  5.9307e-06,\n",
       "          -5.9814e-03, -1.3428e-02, -7.8125e-03, -2.1667e-03,  1.1658e-02,\n",
       "          -1.1658e-02, -4.0588e-03, -1.1108e-02, -1.6251e-03, -3.1128e-03,\n",
       "           1.4709e-02,  3.4790e-03, -1.0315e-02, -1.0620e-02, -3.6163e-03,\n",
       "          -1.3489e-02, -9.7046e-03,  1.5869e-02, -1.2451e-02, -4.7913e-03,\n",
       "           1.0254e-02, -1.7166e-03, -5.1880e-04, -7.5378e-03, -4.6387e-03,\n",
       "          -1.0437e-02,  1.1475e-02,  5.9509e-03,  1.1536e-02,  1.0757e-03,\n",
       "          -1.0315e-02, -6.5613e-03,  1.1780e-02,  1.8799e-02,  1.6113e-02,\n",
       "          -1.9531e-02, -1.9775e-02, -2.4872e-03, -1.0223e-03,  1.8921e-02,\n",
       "          -8.5449e-03,  1.2573e-02,  3.6774e-03,  1.9043e-02,  6.2561e-04,\n",
       "           9.8267e-03, -4.2725e-03,  5.9814e-03,  3.2349e-03, -8.6670e-03,\n",
       "          -1.6357e-02,  4.4556e-03,  7.8201e-04,  6.4392e-03,  3.1090e-04,\n",
       "           1.5625e-02, -2.8839e-03, -1.7944e-02,  5.5237e-03,  1.8311e-02,\n",
       "           5.3406e-03,  1.9775e-02, -1.0071e-02, -1.1963e-02,  8.1177e-03,\n",
       "           8.6060e-03, -8.4229e-03, -5.0545e-05, -4.0283e-03, -1.6846e-02,\n",
       "          -3.1891e-03, -2.0020e-02, -3.3722e-03]], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.qTrans': tensor([[ 0.0718, -0.0708, -0.0214,  ..., -0.0938, -0.1406, -0.0003],\n",
       "         [ 0.0898,  0.1543, -0.1050,  ...,  0.0723,  0.0271, -0.0742],\n",
       "         [ 0.0928,  0.1021, -0.0131,  ..., -0.1377,  0.1177, -0.0058],\n",
       "         ...,\n",
       "         [-0.0364, -0.0146, -0.0801,  ..., -0.0298, -0.0231,  0.0084],\n",
       "         [-0.1030, -0.1152,  0.0986,  ..., -0.1250, -0.0530, -0.1187],\n",
       "         [-0.1289, -0.0742,  0.0317,  ..., -0.1426, -0.0376, -0.1143]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.kTrans': tensor([[-0.0469, -0.0801, -0.1396,  ...,  0.0574,  0.0574,  0.0439],\n",
       "         [ 0.0106,  0.1416, -0.0381,  ..., -0.0786,  0.1167, -0.0791],\n",
       "         [ 0.1104, -0.1387, -0.0264,  ..., -0.1309, -0.0713, -0.0967],\n",
       "         ...,\n",
       "         [ 0.0618,  0.1455, -0.1094,  ...,  0.1377, -0.1099,  0.1533],\n",
       "         [-0.0649, -0.0820,  0.1040,  ...,  0.1099, -0.0505, -0.1172],\n",
       "         [ 0.0146, -0.0183,  0.1299,  ...,  0.0574, -0.1221, -0.0061]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.vTrans': tensor([[ 0.0359,  0.0645, -0.0347,  ...,  0.0208,  0.0737,  0.0776],\n",
       "         [-0.1260, -0.0996,  0.0752,  ..., -0.0820,  0.0542, -0.0479],\n",
       "         [ 0.0708, -0.0119,  0.0354,  ...,  0.1484,  0.1250,  0.0918],\n",
       "         ...,\n",
       "         [-0.0942,  0.1475,  0.1377,  ..., -0.0432,  0.0552, -0.1230],\n",
       "         [ 0.0432,  0.1455, -0.0452,  ..., -0.0884, -0.0342, -0.1357],\n",
       "         [ 0.1318,  0.0574, -0.1074,  ...,  0.0488,  0.0275, -0.0327]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.norm.weight': tensor([1.0000, 0.9922, 1.0000, 0.9961, 0.9961, 1.0000, 1.0078, 1.0000, 1.0078,\n",
       "         0.9922, 0.9922, 1.0000, 1.0000, 1.0078, 1.0000, 0.9961, 0.9961, 0.9961,\n",
       "         0.9922, 0.9961, 1.0078, 0.9961, 0.9961, 1.0000, 0.9922, 0.9883, 0.9922,\n",
       "         1.0000, 0.9961, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 0.9883, 0.9922,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 0.9922, 1.0000, 1.0000, 1.0000, 0.9922,\n",
       "         1.0000, 0.9883, 0.9961, 1.0000, 1.0000, 0.9961, 0.9844, 1.0078, 0.9961,\n",
       "         0.9961, 1.0000, 1.0000, 0.9922, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         0.9961, 0.9922, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 0.9961, 0.9961,\n",
       "         1.0000, 1.0000, 0.9922, 0.9961, 1.0000, 0.9922, 1.0078, 1.0000, 0.9922,\n",
       "         0.9961, 1.0000, 0.9883, 0.9961, 0.9883, 1.0000, 0.9922, 0.9922, 1.0000,\n",
       "         1.0078, 0.9922, 0.9961, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 1.0078,\n",
       "         0.9922, 1.0078, 0.9961, 0.9922, 1.0078, 1.0000, 0.9961, 0.9922, 1.0078,\n",
       "         1.0000, 1.0000, 1.0078, 0.9922, 0.9961, 0.9961, 0.9961, 0.9961, 1.0078,\n",
       "         0.9961, 0.9922, 0.9961, 0.9961, 0.9883, 1.0078, 0.9961, 0.9961, 0.9961,\n",
       "         1.0000, 1.0000], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.0.norm.bias': tensor([ 1.0498e-02,  4.3701e-02, -1.9043e-02,  1.2146e-02,  1.2085e-02,\n",
       "         -8.5449e-03,  5.2734e-02, -7.5073e-03,  1.3489e-02, -2.7832e-02,\n",
       "         -1.0315e-02, -2.4292e-02,  1.7822e-02,  3.0396e-02,  1.5991e-02,\n",
       "          2.5757e-02,  1.6708e-03, -1.2512e-02, -1.9165e-02, -2.0874e-02,\n",
       "         -3.0396e-02,  4.1016e-02, -1.3550e-02, -2.7008e-03, -2.6001e-02,\n",
       "          7.9956e-03,  3.7384e-03,  1.5747e-02,  2.4902e-02, -5.7983e-03,\n",
       "          6.7749e-03, -2.2430e-03,  5.7373e-03,  1.0300e-03, -1.9379e-03,\n",
       "          3.4943e-03,  6.7444e-03,  2.3193e-02, -2.5391e-02, -4.2114e-03,\n",
       "          1.8188e-02,  2.5787e-03, -1.7944e-02, -1.2207e-03, -4.5166e-03,\n",
       "          2.8320e-02,  1.1169e-02, -8.4229e-03,  1.4343e-02, -7.0190e-04,\n",
       "         -5.2185e-03,  1.3580e-03, -1.9531e-02,  2.3926e-02, -3.8818e-02,\n",
       "         -3.4912e-02, -6.2012e-02,  1.4877e-03,  3.5248e-03,  2.7084e-04,\n",
       "         -1.4343e-02,  4.6875e-02, -1.2268e-02, -1.2512e-02,  1.5015e-02,\n",
       "         -5.9814e-03,  3.8574e-02,  1.9165e-02, -6.1523e-02, -2.9175e-02,\n",
       "          7.6599e-03, -3.4424e-02,  1.8799e-02,  7.8735e-03,  4.1748e-02,\n",
       "         -2.1057e-03,  6.3782e-03,  1.3184e-02,  3.0029e-02,  5.8594e-03,\n",
       "         -4.0233e-06,  9.5215e-03,  3.3936e-02,  2.9297e-03, -3.0670e-03,\n",
       "          8.5449e-03,  2.8931e-02,  2.9419e-02, -1.4526e-02, -1.5747e-02,\n",
       "         -9.0942e-03,  3.4912e-02,  8.0566e-03, -6.2256e-03,  9.3994e-03,\n",
       "         -6.2561e-03, -1.5030e-03, -2.6489e-02,  2.4048e-02,  5.0964e-03,\n",
       "         -1.3184e-02, -1.5747e-02,  1.8463e-03, -6.9275e-03,  4.3640e-03,\n",
       "          9.2163e-03,  7.0572e-04, -1.8082e-03, -1.2207e-03,  8.2397e-03,\n",
       "         -2.8076e-02, -2.6123e-02,  4.6997e-03,  9.5825e-03,  4.6387e-03,\n",
       "         -1.1292e-02, -5.9570e-02,  1.5076e-02, -2.9297e-02, -1.0376e-03,\n",
       "          7.1716e-03, -1.3794e-02, -3.7842e-02,  4.5166e-02,  6.4392e-03,\n",
       "          3.0823e-03, -5.0049e-03,  4.1016e-02], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.qTrans': tensor([[ 0.0649, -0.0203, -0.1367,  ..., -0.1318,  0.0184, -0.0649],\n",
       "         [ 0.1187, -0.0059,  0.1250,  ...,  0.0131, -0.1196,  0.1089],\n",
       "         [ 0.0047, -0.1367,  0.0508,  ..., -0.0796, -0.1426,  0.0491],\n",
       "         ...,\n",
       "         [-0.0928, -0.0505,  0.1064,  ...,  0.1396,  0.0415,  0.0703],\n",
       "         [-0.0188, -0.0576,  0.0249,  ...,  0.0168, -0.0322,  0.0413],\n",
       "         [ 0.1260, -0.1465,  0.0491,  ...,  0.0417, -0.0583,  0.1035]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.kTrans': tensor([[-0.1191, -0.0608, -0.1445,  ..., -0.0786, -0.0791, -0.0134],\n",
       "         [-0.0111, -0.0330, -0.1055,  ...,  0.0089, -0.1025, -0.0889],\n",
       "         [ 0.1001, -0.1309, -0.0942,  ...,  0.0028, -0.0067, -0.0884],\n",
       "         ...,\n",
       "         [ 0.0942, -0.0884,  0.0571,  ..., -0.1445, -0.0786,  0.0586],\n",
       "         [-0.1357,  0.1260, -0.0084,  ..., -0.1172,  0.0371, -0.0403],\n",
       "         [-0.0576,  0.1396, -0.1250,  ...,  0.0786, -0.1177, -0.0474]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.vTrans': tensor([[-0.0408,  0.1328,  0.0315,  ...,  0.0090,  0.0894,  0.1133],\n",
       "         [ 0.1406, -0.0500, -0.1318,  ...,  0.0060, -0.1060,  0.0102],\n",
       "         [ 0.1040,  0.0143, -0.0801,  ..., -0.0043, -0.1514, -0.0327],\n",
       "         ...,\n",
       "         [-0.0422, -0.0068, -0.0669,  ..., -0.1387,  0.0217,  0.0854],\n",
       "         [ 0.1260,  0.0156, -0.0583,  ..., -0.0972, -0.0159, -0.0128],\n",
       "         [-0.0179,  0.1094, -0.0491,  ..., -0.1465, -0.0972, -0.0603]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.norm.weight': tensor([1.0000, 0.9961, 0.9922, 1.0000, 0.9961, 1.0000, 0.9922, 0.9922, 1.0078,\n",
       "         1.0000, 1.0078, 1.0078, 0.9961, 0.9922, 1.0000, 1.0078, 0.9961, 0.9961,\n",
       "         0.9922, 0.9883, 1.0000, 0.9961, 0.9961, 0.9922, 0.9961, 1.0000, 0.9961,\n",
       "         0.9883, 0.9961, 0.9961, 1.0000, 0.9922, 1.0000, 0.9883, 0.9922, 1.0000,\n",
       "         1.0000, 1.0000, 1.0156, 1.0000, 1.0000, 1.0000, 1.0000, 0.9922, 0.9883,\n",
       "         1.0078, 0.9922, 0.9961, 1.0000, 1.0078, 0.9961, 0.9883, 1.0156, 1.0078,\n",
       "         0.9961, 0.9961, 1.0156, 0.9883, 1.0078, 1.0156, 1.0000, 1.0078, 1.0000,\n",
       "         1.0000, 0.9961, 1.0000, 0.9961, 0.9961, 1.0000, 0.9961, 0.9922, 1.0000,\n",
       "         1.0000, 1.0000, 0.9922, 0.9922, 0.9961, 0.9844, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.9883, 0.9883, 1.0078, 1.0078, 0.9844, 1.0000, 0.9961,\n",
       "         1.0000, 0.9922, 0.9922, 1.0078, 0.9961, 1.0000, 0.9883, 0.9922, 1.0000,\n",
       "         0.9922, 0.9922, 1.0000, 0.9961, 1.0000, 1.0000, 0.9961, 0.9922, 0.9844,\n",
       "         1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 1.0000, 1.0078, 1.0000, 1.0078,\n",
       "         1.0000, 0.9922, 1.0078, 0.9961, 0.9922, 0.9922, 0.9922, 0.9922, 1.0078,\n",
       "         1.0000, 0.9961], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.1.norm.bias': tensor([ 0.0236,  0.0430, -0.0012,  0.0520,  0.0029,  0.0006,  0.0320,  0.0132,\n",
       "         -0.0026, -0.0437, -0.0159,  0.0222,  0.0058,  0.0183,  0.0309,  0.0147,\n",
       "         -0.0055, -0.0002, -0.0214, -0.0101, -0.0303,  0.0339, -0.0269,  0.0189,\n",
       "         -0.0048, -0.0294,  0.0121,  0.0009,  0.0238, -0.0139,  0.0106,  0.0007,\n",
       "         -0.0023, -0.0121, -0.0126,  0.0079, -0.0006,  0.0388, -0.0166, -0.0016,\n",
       "          0.0214,  0.0195, -0.0236,  0.0082, -0.0014,  0.0210,  0.0193, -0.0066,\n",
       "          0.0096,  0.0013, -0.0227, -0.0048, -0.0151,  0.0040, -0.0302, -0.0064,\n",
       "         -0.0598, -0.0145, -0.0354,  0.0031,  0.0248,  0.0126, -0.0354, -0.0114,\n",
       "         -0.0019, -0.0081,  0.0299,  0.0066, -0.0374, -0.0243,  0.0140,  0.0053,\n",
       "         -0.0262, -0.0026,  0.0393, -0.0337, -0.0111,  0.0089,  0.0306, -0.0124,\n",
       "          0.0006, -0.0096,  0.0432, -0.0081,  0.0012,  0.0430,  0.0332,  0.0227,\n",
       "         -0.0239,  0.0106, -0.0190,  0.0393,  0.0080, -0.0106,  0.0118, -0.0023,\n",
       "         -0.0020,  0.0031,  0.0069,  0.0094, -0.0118, -0.0253,  0.0164,  0.0030,\n",
       "          0.0038, -0.0182, -0.0021, -0.0417,  0.0079,  0.0006, -0.0344, -0.0366,\n",
       "          0.0061, -0.0025,  0.0111, -0.0105, -0.0574,  0.0172, -0.0062,  0.0234,\n",
       "          0.0294, -0.0258, -0.0146,  0.0210,  0.0050,  0.0220, -0.0117,  0.0254],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.qTrans': tensor([[ 0.0021, -0.0435,  0.0034,  ..., -0.0391,  0.0515,  0.0447],\n",
       "         [-0.0635, -0.1260, -0.0281,  ...,  0.0840, -0.1445, -0.1406],\n",
       "         [-0.0854,  0.0457,  0.0403,  ...,  0.0007, -0.1016, -0.0566],\n",
       "         ...,\n",
       "         [-0.0037, -0.1484, -0.0212,  ..., -0.0496,  0.1133, -0.0796],\n",
       "         [ 0.0513, -0.0825, -0.1289,  ...,  0.0332, -0.0231,  0.0815],\n",
       "         [ 0.0347, -0.0461,  0.1143,  ..., -0.0320, -0.0320,  0.0723]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.kTrans': tensor([[-0.1396,  0.1270,  0.0093,  ..., -0.0195, -0.0879,  0.0415],\n",
       "         [-0.0820,  0.0659, -0.0383,  ..., -0.1172, -0.0869, -0.0427],\n",
       "         [ 0.0623, -0.0408,  0.0977,  ..., -0.0225, -0.0194, -0.0248],\n",
       "         ...,\n",
       "         [-0.0732, -0.1328,  0.1260,  ..., -0.0396, -0.0299,  0.0282],\n",
       "         [-0.1377, -0.1328, -0.1279,  ...,  0.0525, -0.0371,  0.0552],\n",
       "         [-0.0376,  0.0889,  0.0747,  ...,  0.0786,  0.0471, -0.1494]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.vTrans': tensor([[-0.0312,  0.1455,  0.0806,  ...,  0.0669, -0.0723, -0.0938],\n",
       "         [ 0.0161,  0.0332, -0.0437,  ...,  0.0173, -0.0053,  0.0674],\n",
       "         [-0.0583,  0.0510,  0.0203,  ...,  0.1445,  0.0884, -0.0325],\n",
       "         ...,\n",
       "         [-0.1040, -0.0315, -0.1592,  ...,  0.0508,  0.0879, -0.1406],\n",
       "         [-0.0986, -0.0055, -0.0003,  ...,  0.1289,  0.0309, -0.0879],\n",
       "         [-0.1533, -0.0845,  0.1021,  ..., -0.0415,  0.0601,  0.0371]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.norm.weight': tensor([1.0000, 1.0000, 0.9922, 1.0000, 0.9961, 0.9961, 1.0078, 1.0000, 0.9922,\n",
       "         1.0000, 1.0000, 1.0000, 0.9922, 0.9961, 1.0000, 1.0000, 0.9961, 0.9961,\n",
       "         1.0000, 0.9922, 1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 0.9961, 1.0000,\n",
       "         0.9961, 0.9961, 1.0078, 0.9961, 0.9922, 1.0078, 1.0000, 0.9961, 0.9961,\n",
       "         1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 1.0078, 1.0000, 0.9961, 0.9922,\n",
       "         0.9961, 0.9922, 1.0078, 1.0000, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000,\n",
       "         0.9922, 0.9961, 0.9961, 0.9961, 1.0078, 1.0078, 0.9961, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.9922, 1.0078, 0.9961, 1.0000, 1.0000, 1.0000, 0.9961,\n",
       "         1.0000, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.9961, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 0.9922,\n",
       "         1.0000, 1.0000, 1.0000, 1.0078, 0.9961, 1.0000, 0.9961, 1.0000, 0.9961,\n",
       "         0.9961, 0.9961, 0.9961, 1.0000, 0.9961, 1.0078, 0.9961, 1.0078, 0.9961,\n",
       "         0.9961, 1.0000, 1.0000, 1.0000, 0.9961, 1.0000, 1.0000, 1.0000, 1.0078,\n",
       "         0.9922, 0.9961, 1.0078, 0.9922, 1.0078, 1.0000, 1.0000, 0.9883, 1.0000,\n",
       "         1.0000, 1.0000], dtype=torch.bfloat16),\n",
       " 'gnn.gtLayers.2.norm.bias': tensor([ 0.0311,  0.0374, -0.0128,  0.0496, -0.0131,  0.0020,  0.0214,  0.0035,\n",
       "          0.0070, -0.0605, -0.0067,  0.0156,  0.0344,  0.0186,  0.0376, -0.0192,\n",
       "         -0.0249,  0.0222,  0.0089,  0.0029, -0.0126,  0.0344, -0.0317,  0.0150,\n",
       "         -0.0018, -0.0374, -0.0442,  0.0152,  0.0292,  0.0020,  0.0311, -0.0020,\n",
       "          0.0302, -0.0043, -0.0034,  0.0069, -0.0242,  0.0232, -0.0065, -0.0067,\n",
       "          0.0094,  0.0146, -0.0334, -0.0067,  0.0067,  0.0327, -0.0383, -0.0286,\n",
       "          0.0369, -0.0014, -0.0469, -0.0135, -0.0302,  0.0378, -0.0156,  0.0046,\n",
       "         -0.0503, -0.0312, -0.0393,  0.0054,  0.0051,  0.0203, -0.0508, -0.0109,\n",
       "         -0.0007, -0.0009,  0.0359, -0.0030, -0.0317, -0.0126,  0.0500, -0.0060,\n",
       "         -0.0111, -0.0393,  0.0630, -0.0160, -0.0337, -0.0045,  0.0233, -0.0198,\n",
       "          0.0238, -0.0271,  0.0281, -0.0337,  0.0048,  0.0464,  0.0261,  0.0461,\n",
       "         -0.0131,  0.0337, -0.0059,  0.0299,  0.0248, -0.0229,  0.0442, -0.0020,\n",
       "         -0.0034, -0.0040,  0.0145,  0.0113, -0.0275, -0.0312,  0.0205,  0.0116,\n",
       "          0.0085, -0.0383, -0.0452, -0.0635,  0.0500,  0.0031, -0.0417, -0.0537,\n",
       "         -0.0045, -0.0271,  0.0168,  0.0071, -0.0564,  0.0294, -0.0381,  0.0157,\n",
       "          0.0195, -0.0070,  0.0067,  0.0023,  0.0157,  0.0505, -0.0244, -0.0049],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.W_P.weight': tensor([[-0.0850, -0.0825, -0.0144,  ..., -0.0262,  0.0889,  0.0226],\n",
       "         [-0.0771,  0.0210,  0.0669,  ..., -0.0092, -0.0908, -0.0167],\n",
       "         [-0.0292,  0.0608,  0.0234,  ...,  0.0815,  0.0713, -0.0134],\n",
       "         ...,\n",
       "         [-0.0212, -0.0111, -0.0532,  ...,  0.0723, -0.0208,  0.0684],\n",
       "         [-0.0344,  0.0132,  0.0669,  ...,  0.0327,  0.0525,  0.0116],\n",
       "         [-0.0786,  0.0879, -0.0547,  ..., -0.0259,  0.0728,  0.0898]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.W_P.bias': tensor([-8.7402e-02,  7.8125e-02,  1.3809e-03, -6.4453e-02,  9.4727e-02,\n",
       "         -9.3262e-02,  1.1475e-01, -6.5918e-02,  7.2266e-02, -4.3457e-02,\n",
       "          2.4658e-02, -2.6855e-02, -1.6174e-03,  9.7168e-02,  6.1340e-03,\n",
       "         -5.3711e-02, -1.6235e-02, -1.0059e-01, -9.3262e-02,  3.0670e-03,\n",
       "         -1.2598e-01, -6.4941e-02, -9.7656e-02,  1.0254e-01, -1.7822e-02,\n",
       "          9.5825e-03,  1.4954e-02,  8.0566e-02, -8.4839e-03, -6.7383e-02,\n",
       "         -3.6926e-03, -2.6367e-02,  5.2490e-02,  4.1748e-02, -7.1777e-02,\n",
       "         -3.7598e-02, -7.0801e-02,  6.5430e-02, -6.3965e-02, -9.0332e-02,\n",
       "          7.2754e-02, -2.0630e-02, -1.0889e-01,  5.1758e-02, -5.7129e-02,\n",
       "          5.9326e-02, -3.1982e-02, -1.0529e-03,  4.3213e-02, -3.9551e-02,\n",
       "         -6.7383e-02,  8.8867e-02, -8.6060e-03,  8.5449e-02, -7.6660e-02,\n",
       "         -3.9795e-02, -6.6406e-02, -6.2012e-02,  6.2561e-03,  7.2266e-02,\n",
       "         -9.2285e-02,  4.6387e-02, -3.3447e-02, -5.9814e-02, -4.1504e-03,\n",
       "         -5.5664e-02,  5.0293e-02,  5.2979e-02, -1.0645e-01, -8.3008e-02,\n",
       "          2.1606e-02, -7.8735e-03, -8.5449e-03,  6.2500e-02,  8.2031e-02,\n",
       "          4.5654e-02,  2.6001e-02, -3.4668e-02, -3.5156e-02, -4.5898e-02,\n",
       "         -7.2266e-02,  3.9062e-02, -3.7670e-05,  5.8105e-02, -6.2012e-02,\n",
       "          5.4688e-02, -2.7588e-02,  5.3467e-02,  3.2471e-02, -8.7402e-02,\n",
       "         -8.5938e-02,  1.1377e-01,  8.1177e-03, -5.4443e-02, -9.2773e-02,\n",
       "         -7.1777e-02, -3.2715e-02,  6.7383e-02,  1.2793e-01, -2.1973e-02,\n",
       "          1.5869e-02, -5.2979e-02,  9.5703e-02, -1.6968e-02,  1.0864e-02,\n",
       "          4.4922e-02, -1.2390e-02, -5.6152e-02,  7.5195e-02,  8.6670e-03,\n",
       "         -9.7656e-02, -5.0049e-02,  5.0537e-02, -8.0078e-02, -6.4453e-02,\n",
       "          1.3855e-02, -3.4912e-02, -2.4780e-02, -1.8066e-02,  6.8848e-02,\n",
       "          4.8584e-02,  8.6914e-02, -1.2402e-01, -2.3926e-02, -7.8735e-03,\n",
       "         -2.1606e-02, -1.0498e-01,  1.1670e-01], dtype=torch.bfloat16),\n",
       " 'gnn.inverW_P.weight': tensor([[ 0.0161, -0.0608,  0.0537,  ...,  0.0474, -0.0452, -0.0249],\n",
       "         [ 0.0544, -0.0815,  0.0400,  ..., -0.0288, -0.0168, -0.0474],\n",
       "         [ 0.0098,  0.0698,  0.0815,  ..., -0.0408, -0.0820,  0.0962],\n",
       "         ...,\n",
       "         [-0.0317, -0.0674,  0.0820,  ...,  0.0588, -0.0091,  0.0228],\n",
       "         [ 0.0513, -0.0043, -0.0278,  ...,  0.0588, -0.0364,  0.0041],\n",
       "         [ 0.0669, -0.0613, -0.0182,  ..., -0.0605, -0.0732, -0.0498]],\n",
       "        dtype=torch.bfloat16),\n",
       " 'gnn.inverW_P.bias': tensor([-1.1133e-01,  3.0762e-02, -6.6406e-02, -5.1270e-02,  1.0498e-01,\n",
       "          6.5918e-02,  7.5684e-03,  3.2616e-04, -9.0790e-04, -6.4453e-02,\n",
       "          7.0801e-02, -5.2734e-02, -8.2493e-05, -2.1729e-02,  6.9336e-02,\n",
       "          3.7109e-02, -9.1309e-02,  8.2031e-02,  7.5195e-02, -1.0059e-01,\n",
       "          2.0630e-02,  7.6660e-02, -5.6641e-02,  6.9824e-02,  8.6914e-02,\n",
       "         -6.1035e-02,  6.8848e-02,  2.9419e-02,  8.1543e-02, -6.0547e-02,\n",
       "          5.0049e-02,  5.4688e-02, -5.6885e-02, -2.4536e-02,  1.3306e-02,\n",
       "          8.1177e-03,  7.1777e-02, -1.0010e-01,  2.9419e-02,  7.2266e-02,\n",
       "         -1.1816e-01,  6.8359e-02, -3.5645e-02, -2.6978e-02,  5.3955e-02,\n",
       "         -5.7373e-02,  8.3008e-02,  8.0078e-02,  2.7832e-02,  2.1606e-02,\n",
       "          5.2979e-02, -6.8848e-02, -3.3691e-02,  1.0254e-01,  3.3936e-02,\n",
       "          6.2988e-02,  5.4016e-03,  1.0071e-02, -1.1963e-01,  5.2246e-02,\n",
       "         -1.0840e-01, -6.5918e-02, -6.2500e-02, -5.1758e-02,  2.7466e-02,\n",
       "         -2.6550e-03,  7.0312e-02,  2.5391e-02,  2.4902e-02,  1.3962e-03,\n",
       "         -8.6426e-02,  1.0107e-01, -8.2520e-02, -3.9307e-02, -8.8379e-02,\n",
       "         -8.8379e-02,  5.1880e-03, -9.2285e-02, -2.9175e-02,  7.9590e-02,\n",
       "         -5.5420e-02,  9.7168e-02, -9.7168e-02, -1.5625e-02, -1.6632e-03,\n",
       "         -3.1250e-02,  4.1992e-02, -1.5869e-02, -1.9043e-02, -1.0059e-01,\n",
       "          8.8867e-02, -1.8799e-02, -4.5166e-02,  2.4414e-02,  8.0078e-02,\n",
       "         -6.8359e-02,  7.8613e-02,  5.0537e-02, -4.7119e-02, -2.1484e-02,\n",
       "          3.6621e-02,  1.5625e-02,  2.0264e-02, -2.8809e-02, -1.0872e-04,\n",
       "         -2.4048e-02, -7.3242e-02, -1.1865e-01, -1.0742e-02, -9.5703e-02,\n",
       "          9.8145e-02,  7.4707e-02, -9.5703e-02,  4.5166e-02, -6.8848e-02,\n",
       "         -5.3955e-02, -4.1016e-02,  9.7168e-02,  9.9121e-02,  2.7466e-02,\n",
       "          6.7383e-02, -1.0254e-01,  8.1543e-02, -2.5757e-02,  9.0332e-02,\n",
       "         -5.6458e-03,  2.5146e-02, -7.8125e-02], dtype=torch.bfloat16),\n",
       " 'positional_embedding': tensor([[ 0.0058, -0.0168,  0.0063,  ..., -0.0139, -0.0081,  0.0112],\n",
       "         [ 0.0232, -0.0001,  0.0147,  ...,  0.0132, -0.0007,  0.0020],\n",
       "         [ 0.0133,  0.0132,  0.0058,  ...,  0.0047, -0.0026,  0.0081],\n",
       "         ...,\n",
       "         [-0.0152,  0.0029, -0.0169,  ..., -0.0090,  0.0010, -0.0025],\n",
       "         [-0.0162, -0.0187,  0.0087,  ...,  0.0078,  0.0164, -0.0023],\n",
       "         [-0.0097, -0.0164, -0.0059,  ..., -0.0043,  0.0037,  0.0037]]),\n",
       " 'text_projection': tensor([[-0.0410, -0.0377, -0.0631,  ..., -0.0065, -0.0226,  0.0741],\n",
       "         [-0.0365, -0.0054, -0.0592,  ...,  0.0389, -0.0064,  0.0429],\n",
       "         [ 0.0242, -0.0850, -0.0273,  ...,  0.0583,  0.0585,  0.0480],\n",
       "         ...,\n",
       "         [-0.0469, -0.0222,  0.0058,  ..., -0.0335, -0.0138,  0.0039],\n",
       "         [ 0.0486, -0.0557,  0.0617,  ...,  0.0045, -0.0933, -0.0408],\n",
       "         [-0.0480,  0.0018,  0.0018,  ...,  0.0597, -0.0146, -0.0121]]),\n",
       " 'transformer.resblocks.0.attn.in_proj_weight': tensor([[-0.0386, -0.0164,  0.0539,  ...,  0.0473, -0.0142,  0.0294],\n",
       "         [-0.1036, -0.0155,  0.0263,  ...,  0.0997, -0.0942,  0.0818],\n",
       "         [-0.0279, -0.0485, -0.0516,  ..., -0.0584, -0.0111, -0.0295],\n",
       "         ...,\n",
       "         [ 0.0253,  0.0657,  0.0075,  ...,  0.0373,  0.0687,  0.0145],\n",
       "         [ 0.0336, -0.0426,  0.0012,  ...,  0.0046,  0.0415,  0.0268],\n",
       "         [-0.0350, -0.1482,  0.0063,  ..., -0.0491,  0.0799, -0.0633]]),\n",
       " 'transformer.resblocks.0.attn.in_proj_bias': tensor([ 0.0040,  0.0027,  0.0017,  ..., -0.0003,  0.0015, -0.0008]),\n",
       " 'transformer.resblocks.0.attn.out_proj.weight': tensor([[ 0.0020, -0.0054,  0.0113,  ...,  0.0046,  0.0014, -0.0005],\n",
       "         [-0.0046, -0.0104, -0.0114,  ...,  0.0081, -0.0010,  0.0026],\n",
       "         [-0.0075, -0.0157, -0.0159,  ..., -0.0241, -0.0126,  0.0025],\n",
       "         ...,\n",
       "         [ 0.0123,  0.0151,  0.0187,  ..., -0.0049,  0.0095,  0.0021],\n",
       "         [-0.0165,  0.0040,  0.0126,  ..., -0.0096,  0.0125, -0.0073],\n",
       "         [ 0.0067,  0.0091, -0.0133,  ...,  0.0202,  0.0022, -0.0105]]),\n",
       " 'transformer.resblocks.0.attn.out_proj.bias': tensor([ 8.9692e-04,  1.0220e-03, -2.0629e-04,  1.6931e-03,  8.9451e-04,\n",
       "         -4.8473e-04,  5.6180e-04,  6.8366e-04, -7.0256e-04, -3.6743e-04,\n",
       "         -7.4765e-04, -7.3229e-04, -4.4554e-04,  5.7694e-04,  9.9435e-04,\n",
       "          4.0542e-04,  1.9019e-04,  6.2458e-04,  1.7963e-03, -4.4144e-04,\n",
       "          7.2305e-04,  1.6039e-03,  1.1330e-03, -1.9102e-04,  2.2421e-03,\n",
       "          8.5972e-04,  2.5603e-04, -7.0506e-04, -4.1549e-04,  1.0301e-03,\n",
       "         -4.5542e-04, -9.2871e-04,  1.4805e-04, -1.7179e-03,  5.0727e-04,\n",
       "          1.6532e-03,  5.7939e-04,  9.4943e-04, -5.2340e-04, -4.6331e-04,\n",
       "          1.7569e-04, -2.7131e-04, -7.6733e-04, -3.0273e-04, -8.1773e-04,\n",
       "          1.5065e-03,  9.7191e-05,  2.8187e-04,  1.0993e-04,  8.8506e-05,\n",
       "         -7.5200e-05, -2.9801e-05,  1.5703e-03,  2.5936e-05,  5.7550e-04,\n",
       "          7.3876e-04,  4.2168e-04,  8.5444e-04, -3.8489e-04,  2.7007e-04,\n",
       "          3.2627e-04, -2.5476e-05, -4.5499e-04, -9.0727e-04, -1.2014e-03,\n",
       "         -2.2536e-04, -4.5173e-04,  1.0406e-03, -9.6512e-04,  1.9338e-05,\n",
       "         -1.1766e-03, -5.6536e-04, -1.4194e-03,  8.6014e-05, -3.5373e-04,\n",
       "          3.4790e-04,  1.2651e-04, -4.5867e-04, -1.3185e-03,  6.7173e-04,\n",
       "         -1.3936e-03, -1.1014e-03,  9.8046e-04,  5.0388e-04,  2.3145e-05,\n",
       "         -3.2738e-04, -6.5935e-04,  2.2038e-04,  4.2638e-04, -3.6696e-04,\n",
       "         -1.1963e-03,  2.0712e-03,  2.3618e-05,  6.7208e-04, -4.6925e-04,\n",
       "          8.8791e-04, -7.3709e-04,  3.0648e-04, -4.8083e-06,  3.3078e-04,\n",
       "         -1.1549e-03,  3.4765e-05, -6.1867e-04, -7.6668e-06,  6.9569e-04,\n",
       "         -1.6938e-03, -2.6221e-04, -3.8009e-04,  2.8414e-04,  1.4151e-03,\n",
       "          8.8072e-05,  5.6230e-05,  8.5553e-04,  1.1832e-05,  7.4261e-04,\n",
       "          6.0962e-04, -4.5861e-05,  7.8123e-04,  2.8068e-04,  7.7428e-04,\n",
       "          6.5349e-05,  1.1633e-03, -3.2152e-04,  2.9833e-04, -7.2344e-05,\n",
       "          1.7998e-05, -5.4800e-04,  5.8205e-05, -8.3471e-04, -2.9114e-04,\n",
       "          3.5986e-04, -9.6127e-04, -2.5775e-04,  2.0157e-04, -6.2095e-04,\n",
       "          2.9256e-04, -1.9428e-03,  2.0363e-04,  4.7808e-05,  7.1667e-04,\n",
       "          3.0340e-04, -8.5887e-04,  5.0873e-04,  5.3221e-04, -1.2960e-03,\n",
       "         -7.9078e-05,  3.6400e-04, -2.4539e-03, -5.8637e-04, -5.0800e-04,\n",
       "          1.2770e-03,  1.8153e-04,  1.0105e-03,  9.7379e-04,  2.0904e-04,\n",
       "         -6.4616e-04, -7.9483e-04, -1.3308e-03, -8.3434e-04,  1.4739e-03,\n",
       "         -3.3451e-04,  2.1672e-04,  1.4554e-03,  4.3463e-04,  2.8649e-04,\n",
       "          5.5627e-04, -5.6611e-04,  1.2173e-03, -9.1645e-04,  2.7396e-04,\n",
       "         -1.1560e-03, -1.4724e-03,  6.3512e-04,  2.0867e-03,  3.0078e-04,\n",
       "         -7.8669e-04, -9.7139e-04, -9.6531e-05,  9.7628e-04, -4.5166e-04,\n",
       "          3.9560e-04,  1.1330e-04,  7.8455e-04, -9.3623e-04, -4.6543e-04,\n",
       "         -7.5056e-04, -1.8352e-04,  3.7556e-04,  6.6008e-04,  1.6193e-03,\n",
       "          1.6577e-03,  2.1819e-04,  1.3395e-04, -6.9964e-04,  3.1236e-04,\n",
       "          5.8553e-04,  6.9310e-05, -1.0884e-03, -5.2378e-04, -1.2921e-03,\n",
       "         -1.3960e-04, -3.7702e-04, -1.0043e-04,  3.2043e-05, -1.1581e-04,\n",
       "          4.9428e-04,  6.1434e-04, -1.3726e-03, -7.0124e-04, -1.7059e-03,\n",
       "          5.1525e-04,  2.9169e-04,  7.3414e-04,  9.8541e-04,  1.1007e-04,\n",
       "         -9.6796e-04,  2.6266e-04, -4.4513e-04, -7.2059e-04,  2.9118e-04,\n",
       "          1.3963e-04,  2.3316e-04, -7.6686e-04,  7.2668e-04,  9.6914e-04,\n",
       "          9.9880e-04, -6.6108e-04,  1.3415e-03, -3.0407e-04, -5.6631e-04,\n",
       "         -3.9831e-05, -2.2778e-04,  6.1089e-04,  1.2600e-05,  5.2760e-04,\n",
       "         -9.3144e-04,  1.2101e-03, -1.0319e-03,  7.4840e-04,  1.5777e-04,\n",
       "          3.0675e-04,  6.1750e-04, -1.3482e-03, -3.5096e-04,  1.7628e-03,\n",
       "         -1.1044e-03,  4.8034e-04,  1.2709e-04, -1.5143e-03, -2.0024e-03,\n",
       "          1.9095e-03, -2.7955e-04, -2.8318e-04,  2.9668e-06, -6.8586e-04,\n",
       "          2.2817e-05,  4.3179e-04, -8.5359e-05,  1.6266e-03, -4.5430e-04,\n",
       "         -4.9097e-04, -6.2658e-04,  6.1421e-04,  5.5195e-04, -2.8340e-04,\n",
       "         -1.9695e-04,  2.8017e-04,  9.0482e-05, -1.1321e-03, -9.0262e-04,\n",
       "          5.7476e-04,  8.5642e-05,  8.9081e-04,  3.7390e-04, -7.6151e-04,\n",
       "          3.8985e-04, -1.4566e-04, -1.1038e-03,  1.1649e-04,  1.6464e-04,\n",
       "         -1.2837e-04,  9.5553e-04,  1.5119e-03, -1.2641e-03,  5.7717e-04,\n",
       "          3.0475e-04, -4.4462e-04,  1.1369e-04,  4.0737e-04, -2.0025e-03,\n",
       "         -1.3861e-03, -4.9279e-04, -7.4467e-04,  1.0534e-03,  2.3929e-04,\n",
       "          3.6871e-04, -3.9376e-04,  9.7215e-04,  9.3228e-04,  3.9301e-04,\n",
       "         -7.7462e-04, -9.0844e-04, -1.8968e-04,  2.0062e-03,  4.3904e-04,\n",
       "         -7.0972e-05,  4.2055e-04,  3.1657e-04, -1.0156e-06, -5.9397e-04,\n",
       "          5.2881e-04, -2.7621e-04, -3.7214e-06,  1.2336e-03,  1.0860e-04,\n",
       "         -1.0385e-04, -3.6195e-04, -2.7200e-04,  5.6270e-04, -1.0502e-03,\n",
       "         -6.3616e-04,  5.0637e-04, -9.7316e-04, -5.0709e-04,  1.3640e-04,\n",
       "         -5.6896e-04,  6.1610e-04,  3.6764e-04,  3.4348e-04, -7.7506e-04,\n",
       "         -6.3178e-04, -5.0729e-04, -9.3212e-04, -5.0660e-04, -3.2749e-04,\n",
       "          5.8207e-04, -1.0015e-03,  6.4061e-04, -4.7054e-04, -3.4890e-05,\n",
       "         -1.3166e-03,  4.6117e-04,  2.4520e-04,  9.1687e-04,  5.1324e-05,\n",
       "         -1.1110e-03, -1.3344e-03,  2.0561e-04, -2.8262e-04,  9.8221e-04,\n",
       "         -9.5720e-04,  3.5476e-04, -4.2815e-04,  8.5939e-04, -1.6153e-04,\n",
       "          7.6452e-04, -4.2771e-04, -1.8201e-03,  4.8649e-04, -6.2209e-04,\n",
       "          5.8773e-04,  9.9585e-04, -5.1314e-05,  2.5152e-04,  9.8854e-04,\n",
       "          4.7113e-04, -8.9438e-04, -5.1264e-04, -9.8020e-04, -3.2317e-04,\n",
       "         -3.7521e-04, -3.9020e-05,  8.0912e-04,  7.5333e-04, -5.4069e-04,\n",
       "          1.9214e-04,  6.4748e-04, -1.5575e-03, -4.0697e-04, -7.8428e-04,\n",
       "          3.5700e-04,  4.6012e-04, -5.3387e-04,  2.7970e-04,  2.2902e-04,\n",
       "         -1.0648e-04,  9.0122e-04, -8.2274e-04, -9.8687e-04, -2.6759e-04,\n",
       "         -1.5707e-04, -8.3946e-04, -6.0048e-05,  1.0207e-03, -5.4866e-05,\n",
       "          8.5741e-04,  2.9920e-03, -3.2679e-04,  1.4097e-04,  2.5292e-04,\n",
       "         -4.7795e-04,  3.0938e-04,  6.6889e-04, -8.0228e-04, -4.9483e-05,\n",
       "         -2.4723e-04,  9.9546e-04, -1.5574e-04, -8.8235e-04, -2.0734e-04,\n",
       "          7.9217e-04,  6.7424e-04, -2.4781e-05, -5.5328e-04, -8.6280e-05,\n",
       "         -3.9576e-05, -5.3599e-05, -1.4427e-03,  4.6976e-04,  4.7721e-04,\n",
       "          1.3103e-03, -1.7787e-03, -1.3363e-04, -7.9303e-04,  4.0881e-04,\n",
       "          2.1592e-03, -1.9176e-03,  6.1342e-05, -6.6012e-04, -1.1078e-03,\n",
       "         -1.7160e-05,  1.1319e-04,  2.8143e-04,  9.6312e-04, -2.5835e-05,\n",
       "          4.6238e-05, -3.6271e-04, -5.8414e-05, -9.0819e-04,  5.5315e-04,\n",
       "          8.2721e-05, -1.7111e-03, -1.7543e-03,  6.1506e-04,  6.6007e-04,\n",
       "          1.2193e-03, -5.4004e-04,  4.4432e-04,  1.0531e-03, -5.6289e-04,\n",
       "          9.6702e-04,  5.0796e-04, -1.8214e-03, -3.0902e-05,  2.9076e-04,\n",
       "          2.3231e-04, -2.8966e-04,  3.1071e-04,  8.2733e-04, -1.2972e-03,\n",
       "          2.4762e-04, -6.6570e-04,  5.2558e-04, -9.0817e-04,  1.5767e-03,\n",
       "         -9.8624e-04, -6.8600e-04, -5.7675e-04,  2.0363e-05, -6.8184e-05,\n",
       "         -1.3518e-03,  3.0835e-04, -7.3340e-04,  2.0453e-04,  7.8527e-04,\n",
       "         -1.4559e-03,  1.1291e-03, -1.0113e-03,  1.2377e-03,  3.6430e-04,\n",
       "         -6.6615e-04,  9.0753e-04, -4.1210e-04,  1.9458e-04, -1.2175e-03,\n",
       "         -1.0939e-03,  6.6910e-05,  8.6588e-04, -1.7949e-03,  4.8105e-04,\n",
       "         -4.4849e-04, -1.2561e-03, -5.4309e-05, -4.8665e-04,  1.1672e-04,\n",
       "          3.0257e-04, -1.0169e-05, -9.1882e-05, -1.2537e-03, -6.7055e-05,\n",
       "          1.3950e-04,  2.6962e-04,  4.0142e-04,  6.3397e-04,  6.3286e-04,\n",
       "          1.3265e-03,  2.0296e-04,  4.3784e-04,  1.3011e-04, -2.4116e-04,\n",
       "         -2.4917e-04, -3.7785e-04]),\n",
       " 'transformer.resblocks.0.ln_1.weight': tensor([0.9995, 1.0022, 1.0002, 0.9986, 1.0021, 0.9989, 0.9986, 1.0004, 1.0009,\n",
       "         0.9940, 0.9996, 0.9994, 1.0000, 1.0024, 0.9969, 0.9965, 0.9979, 0.9983,\n",
       "         1.0003, 0.9995, 0.9982, 1.0002, 0.9964, 0.9991, 1.0008, 0.9999, 1.0042,\n",
       "         0.9987, 1.0008, 0.9991, 0.9994, 0.9986, 0.9920, 0.9992, 0.9978, 0.9962,\n",
       "         0.9992, 0.9984, 0.9969, 1.0009, 0.9991, 1.0005, 0.9972, 1.0013, 1.0002,\n",
       "         0.9969, 0.9969, 1.0021, 0.9977, 0.9978, 0.9987, 0.9990, 0.9972, 1.0025,\n",
       "         0.9989, 0.9979, 0.9983, 1.0006, 1.0001, 1.0002, 0.9994, 0.9989, 1.0016,\n",
       "         0.9990, 0.9961, 0.9991, 1.0012, 0.9991, 0.9992, 1.0022, 1.0011, 0.9988,\n",
       "         1.0014, 0.9959, 1.0002, 0.9994, 0.9999, 1.0008, 1.0009, 0.9957, 0.9990,\n",
       "         1.0010, 1.0001, 1.0010, 0.9977, 1.0002, 1.0019, 0.9990, 1.0000, 1.0007,\n",
       "         1.0030, 0.9983, 1.0005, 1.0025, 0.9986, 1.0015, 0.9950, 1.0016, 0.9994,\n",
       "         0.9995, 1.0002, 0.9998, 0.9980, 0.9971, 0.9985, 0.9950, 1.0029, 0.9965,\n",
       "         1.0036, 0.9983, 0.9995, 1.0022, 1.0021, 0.9981, 1.0019, 1.0018, 1.0032,\n",
       "         0.9997, 1.0003, 0.9985, 0.9972, 1.0024, 0.9979, 0.9984, 0.9997, 1.0026,\n",
       "         0.9982, 1.0010, 0.9968, 1.0001, 0.9980, 1.0002, 0.9973, 0.9973, 1.0011,\n",
       "         0.9972, 0.9994, 0.9974, 0.9998, 0.9984, 0.9977, 1.0027, 0.9963, 1.0000,\n",
       "         0.9975, 0.9986, 1.0006, 0.9957, 1.0001, 1.0020, 1.0055, 0.9982, 0.9969,\n",
       "         0.9972, 0.9982, 1.0010, 1.0013, 0.9981, 1.0008, 0.9983, 1.0002, 1.0005,\n",
       "         1.0005, 0.9997, 1.0014, 1.0008, 0.9955, 1.0039, 0.9977, 0.9981, 1.0017,\n",
       "         0.9993, 1.0025, 1.0019, 1.0006, 0.9973, 1.0033, 1.0007, 1.0012, 1.0021,\n",
       "         0.9990, 1.0015, 1.0014, 0.9990, 1.0029, 0.9974, 1.0029, 1.0002, 1.0020,\n",
       "         0.9981, 0.9986, 1.0004, 0.9942, 0.9971, 1.0026, 0.9999, 0.9975, 0.9993,\n",
       "         0.9998, 0.9950, 1.0017, 0.9977, 1.0008, 0.9977, 1.0047, 1.0018, 0.9968,\n",
       "         0.9958, 1.0008, 0.9981, 1.0019, 1.0008, 0.9938, 0.9990, 0.9986, 0.9960,\n",
       "         0.9989, 0.9976, 0.9966, 1.0034, 0.9975, 1.0012, 1.0003, 1.0020, 0.9988,\n",
       "         0.9992, 0.9975, 1.0052, 0.9966, 0.9990, 0.9988, 0.9997, 0.9991, 0.9960,\n",
       "         1.0006, 0.9988, 0.9991, 0.9960, 1.0022, 0.9944, 1.0012, 0.9997, 0.9981,\n",
       "         1.0010, 0.9973, 0.9993, 0.9998, 1.0038, 0.9965, 0.9997, 0.9980, 1.0036,\n",
       "         0.9970, 0.9997, 0.9972, 0.9998, 0.9918, 1.0003, 1.0001, 0.9988, 1.0009,\n",
       "         0.9969, 0.9999, 0.9992, 0.9974, 0.9983, 0.9954, 1.0016, 0.9978, 0.9981,\n",
       "         1.0039, 1.0003, 0.9947, 0.9966, 1.0011, 0.9997, 1.0012, 1.0006, 0.9998,\n",
       "         1.0025, 0.9956, 0.9985, 0.9993, 1.0012, 0.9984, 0.9974, 0.9979, 1.0005,\n",
       "         1.0010, 0.9996, 1.0022, 0.9981, 1.0022, 1.0007, 1.0009, 1.0003, 0.9986,\n",
       "         0.9962, 0.9985, 0.9984, 1.0022, 0.9978, 0.9980, 0.9995, 1.0009, 1.0005,\n",
       "         1.0012, 1.0032, 1.0000, 1.0012, 0.9997, 1.0025, 1.0033, 1.0009, 0.9968,\n",
       "         0.9990, 1.0013, 1.0002, 0.9977, 1.0001, 0.9988, 1.0010, 0.9969, 0.9957,\n",
       "         0.9979, 1.0045, 0.9981, 0.9981, 1.0001, 0.9982, 1.0017, 1.0025, 1.0027,\n",
       "         1.0014, 0.9999, 1.0022, 0.9991, 1.0006, 0.9972, 0.9996, 0.9980, 0.9997,\n",
       "         1.0015, 1.0024, 0.9999, 0.9988, 1.0061, 0.9974, 1.0002, 0.9972, 0.9989,\n",
       "         1.0005, 0.9973, 1.0007, 0.9995, 0.9971, 1.0047, 1.0017, 0.9983, 1.0042,\n",
       "         0.9959, 0.9992, 0.9979, 0.9990, 0.9984, 0.9971, 1.0006, 0.9993, 1.0018,\n",
       "         1.0007, 0.9991, 1.0011, 0.9998, 1.0005, 0.9978, 1.0028, 1.0025, 0.9992,\n",
       "         0.9999, 1.0020, 0.9983, 0.9999, 0.9956, 1.0028, 1.0003, 1.0036, 0.9989,\n",
       "         0.9989, 0.9989, 0.9987, 0.9983, 0.9992, 0.9993, 0.9958, 0.9961, 1.0009,\n",
       "         0.9941, 1.0000, 0.9978, 0.9989, 0.9969, 0.9991, 0.9973, 0.9920, 0.9958,\n",
       "         1.0027, 1.0005, 0.9972, 0.9977, 1.0022, 1.0002, 1.0002, 0.9970, 0.9975,\n",
       "         0.9974, 1.0011, 0.9959, 0.9969, 0.9976, 0.9993, 0.9998, 0.9991, 0.9965,\n",
       "         0.9961, 1.0001, 1.0001, 1.0005, 0.9986, 0.9992, 0.9970, 0.9998, 1.0020,\n",
       "         0.9987, 0.9982, 0.9974, 0.9958, 0.9987, 0.9969, 0.9980, 1.0066, 0.9976,\n",
       "         0.9991, 0.9979, 1.0031, 1.0023, 0.9989, 0.9960, 1.0021, 1.0019, 1.0012,\n",
       "         1.0007, 0.9995, 1.0014, 0.9998, 0.9988, 1.0013, 0.9993, 1.0011, 1.0030,\n",
       "         0.9999, 1.0021, 1.0012, 0.9965, 1.0025, 0.9968, 0.9988, 0.9999, 1.0013,\n",
       "         1.0016, 0.9977, 1.0025, 1.0004, 1.0043, 1.0013, 0.9985, 0.9995, 0.9964,\n",
       "         0.9969, 0.9993, 1.0000, 0.9983, 1.0025, 0.9992, 0.9984, 1.0005, 0.9990,\n",
       "         0.9993, 1.0021, 0.9964, 1.0030, 0.9967, 1.0005, 1.0014, 0.9943, 1.0012,\n",
       "         0.9994, 1.0001, 0.9984, 0.9987, 1.0006, 0.9957, 1.0015, 0.9985, 1.0028,\n",
       "         0.9980, 0.9976, 1.0009, 1.0008, 0.9950, 0.9992, 0.9967, 0.9962]),\n",
       " 'transformer.resblocks.0.ln_1.bias': tensor([-5.4751e-04, -6.3205e-04,  2.4731e-04, -2.5029e-04,  9.3515e-04,\n",
       "         -1.4771e-04,  7.9424e-04, -2.7111e-04, -7.5258e-04,  1.8667e-03,\n",
       "          5.8203e-04,  1.1020e-03,  3.1211e-04, -4.2563e-04, -2.8050e-03,\n",
       "          1.8054e-03,  4.9505e-04, -3.2166e-04, -1.4422e-03, -2.3268e-04,\n",
       "         -4.4786e-04,  5.0874e-04,  7.1012e-04, -5.4490e-04,  2.0383e-03,\n",
       "          1.1511e-03,  1.6098e-03, -8.7192e-04, -1.2520e-03,  4.2318e-04,\n",
       "          1.2740e-03, -9.1562e-04,  4.4365e-04, -1.3052e-04, -1.1739e-03,\n",
       "          1.8795e-03, -2.2700e-04,  6.2491e-04, -2.4644e-04, -2.8358e-04,\n",
       "          4.2929e-04, -1.7085e-03,  1.6371e-03, -1.9492e-04,  2.1556e-03,\n",
       "         -9.7771e-04,  1.1916e-03, -2.9786e-04, -5.9186e-04,  2.0103e-03,\n",
       "          6.0074e-04,  3.3451e-04,  3.1776e-04, -6.4521e-04, -4.4765e-05,\n",
       "         -6.8813e-04,  4.2328e-04, -1.2276e-04, -1.0339e-03,  7.5170e-05,\n",
       "         -1.8452e-04, -2.0868e-04,  1.5871e-03,  7.0354e-04,  1.6007e-03,\n",
       "         -1.4893e-04,  5.1100e-04,  8.4957e-04,  2.9422e-04, -1.9823e-03,\n",
       "          2.9141e-04,  9.2138e-04,  2.3704e-04,  1.2817e-04, -4.9750e-04,\n",
       "         -4.8842e-04,  5.4538e-04, -3.9898e-04, -9.4801e-04, -6.3521e-04,\n",
       "          2.3519e-04,  5.5830e-04, -7.5273e-04, -5.9472e-04,  1.0329e-03,\n",
       "          1.1925e-04, -6.8955e-05, -6.3308e-04,  7.7143e-04, -2.9183e-06,\n",
       "          1.1040e-03, -1.6041e-04, -9.0543e-04, -1.5607e-04, -3.3894e-04,\n",
       "         -4.2831e-04, -1.6782e-03,  8.8799e-04, -8.1726e-04, -7.2051e-06,\n",
       "          5.1052e-04, -2.1384e-04,  1.5802e-04, -2.2532e-04, -4.7976e-04,\n",
       "          9.1574e-04, -3.7162e-05, -1.1764e-03,  1.4123e-03, -7.0820e-04,\n",
       "         -1.4037e-03,  1.2010e-04, -2.8382e-04,  5.2317e-04,  1.4617e-04,\n",
       "         -8.2410e-04,  5.8531e-04, -6.7709e-04,  1.2631e-03, -1.5452e-03,\n",
       "         -4.2793e-04,  7.5399e-04, -3.8328e-04, -1.2614e-03,  5.2640e-04,\n",
       "         -1.0699e-03, -2.9101e-04,  9.7183e-04, -3.5844e-05, -1.6933e-03,\n",
       "         -1.5612e-04,  6.6371e-04, -6.4428e-04,  3.1999e-03, -4.7080e-04,\n",
       "         -1.1538e-03, -9.9621e-04, -1.4167e-03,  8.1094e-04, -1.3568e-04,\n",
       "          2.1201e-03, -5.0259e-04, -4.8985e-04, -3.9589e-04,  1.6822e-03,\n",
       "          1.4761e-04,  7.4229e-04,  5.8008e-04,  6.9285e-04, -5.9647e-04,\n",
       "         -6.6239e-04, -8.9916e-04,  8.3800e-04, -6.8753e-04, -9.5630e-04,\n",
       "          1.9257e-04,  1.1618e-03, -7.1904e-04, -8.1378e-05, -2.1677e-04,\n",
       "          1.5376e-04,  2.0885e-03, -1.5145e-03,  4.9579e-04,  4.6564e-04,\n",
       "         -4.6662e-04,  1.3407e-03, -3.4093e-04, -4.9467e-04, -2.0867e-03,\n",
       "         -7.2311e-04, -1.2607e-05, -2.3145e-03,  3.4383e-04,  6.4175e-04,\n",
       "         -8.4211e-04, -2.1518e-04, -9.5534e-05, -1.0080e-03, -1.3168e-03,\n",
       "         -1.7182e-03, -9.9167e-04,  1.3056e-03,  8.3837e-04,  2.7253e-04,\n",
       "          9.4831e-04, -1.1955e-03,  1.0804e-03, -9.1070e-04,  1.2740e-03,\n",
       "          6.0381e-04,  1.0610e-03, -7.1478e-04, -2.0287e-03, -8.1867e-04,\n",
       "          5.0587e-05, -8.8001e-04, -6.7607e-04,  6.7934e-05, -1.9803e-03,\n",
       "         -1.2457e-03,  8.3991e-05,  1.3584e-03,  6.7928e-04,  3.6176e-04,\n",
       "         -1.3673e-03, -4.2773e-04,  1.2768e-03, -6.7806e-04,  6.6743e-04,\n",
       "         -1.8548e-03,  5.5036e-05, -1.2901e-03,  3.0876e-04,  1.6158e-04,\n",
       "          7.4943e-04,  7.8373e-04, -4.4758e-05,  1.2602e-03, -1.4129e-04,\n",
       "          1.3060e-03,  1.2375e-03, -8.1887e-04, -6.4267e-04,  9.2448e-04,\n",
       "          1.6900e-03, -1.9577e-04,  2.4147e-04,  1.4096e-03, -7.4620e-04,\n",
       "         -5.0680e-05,  5.8029e-04,  1.5810e-04, -1.6483e-03, -6.9959e-04,\n",
       "          4.7785e-04, -9.0646e-04, -4.2626e-04,  9.9619e-04, -1.6812e-03,\n",
       "         -1.5177e-03, -2.3788e-04,  3.5252e-04,  4.2822e-05,  3.7543e-05,\n",
       "          1.1176e-03,  1.1316e-03, -7.5654e-04, -1.8871e-03, -1.0461e-03,\n",
       "         -8.0501e-04, -5.1176e-04, -1.4165e-03,  9.8751e-05,  1.6705e-03,\n",
       "         -7.6119e-04,  1.2396e-03,  1.0924e-03,  1.3067e-04,  1.1660e-03,\n",
       "          4.9933e-05, -9.9141e-05,  1.6192e-03,  2.4641e-04,  1.4763e-03,\n",
       "          2.0062e-04, -1.0280e-03,  1.1334e-03,  1.8847e-03,  1.7101e-04,\n",
       "          3.4573e-04, -6.4573e-04, -9.2285e-04,  1.2433e-03,  1.2647e-03,\n",
       "          1.2370e-04,  9.4361e-05, -2.5353e-04,  2.0876e-03,  3.1602e-04,\n",
       "          1.4285e-03,  1.3261e-03, -1.5643e-03, -3.4570e-04, -2.3742e-05,\n",
       "          5.8343e-04,  9.9129e-05,  1.6019e-03, -2.8607e-04, -9.4064e-04,\n",
       "         -1.7512e-03, -6.3208e-04,  1.4633e-03,  1.6213e-04, -8.3521e-04,\n",
       "          1.0014e-03,  2.7541e-03,  1.2333e-03, -2.9373e-03, -2.0969e-05,\n",
       "          9.2416e-04,  2.2858e-04, -3.2426e-05,  8.7978e-04, -1.0958e-03,\n",
       "         -1.9178e-04,  5.0769e-04, -1.1216e-04, -4.5908e-04, -1.2439e-03,\n",
       "          8.9423e-04, -8.0550e-05,  6.0153e-04,  7.2287e-04, -1.9168e-03,\n",
       "          6.3732e-04,  3.6415e-05,  1.8133e-03, -1.1423e-03,  1.4897e-03,\n",
       "          1.0922e-03, -4.7427e-05,  7.2533e-04, -1.3933e-03,  6.0719e-04,\n",
       "         -1.2789e-03, -1.6752e-03,  1.2111e-03, -2.7376e-03,  1.6756e-03,\n",
       "         -8.4430e-04, -1.2605e-04,  4.4555e-04, -6.0973e-04,  4.7874e-04,\n",
       "         -8.5913e-04, -1.1758e-03, -1.4299e-04,  2.6330e-04,  1.6569e-04,\n",
       "          4.8088e-04, -1.8481e-04, -6.4562e-04, -6.1054e-04,  5.3066e-04,\n",
       "          1.4467e-03, -1.5239e-03, -1.2992e-04, -1.0153e-03,  1.2774e-03,\n",
       "         -1.7388e-04, -3.2916e-05, -1.4547e-03, -4.9248e-04,  6.6070e-04,\n",
       "          5.7510e-05,  3.7327e-04, -1.0139e-03,  5.7490e-04, -8.7747e-04,\n",
       "          1.1639e-03,  1.1691e-04,  1.7719e-03, -9.5812e-04, -5.3698e-04,\n",
       "         -9.1345e-04, -7.4393e-04, -9.6320e-04, -1.8803e-05, -6.9145e-04,\n",
       "          8.6969e-04, -2.3226e-04, -4.2062e-04,  4.0347e-04, -1.9000e-03,\n",
       "         -7.5886e-04,  8.6211e-05,  3.4179e-04,  1.9096e-04,  1.3308e-03,\n",
       "          1.8810e-04, -8.4474e-04,  2.6152e-03,  7.5242e-04, -1.3709e-03,\n",
       "          1.1048e-03,  3.4841e-04, -2.1902e-04, -6.0064e-04,  3.1104e-04,\n",
       "         -5.6785e-04,  7.3023e-05, -4.2189e-04, -2.2720e-04,  5.0906e-04,\n",
       "          3.2464e-04, -9.6423e-04,  3.8380e-04,  3.1409e-04,  2.5591e-04,\n",
       "         -4.2938e-04, -9.9443e-04, -4.3689e-04, -5.0597e-05, -5.5932e-04,\n",
       "         -5.8075e-04, -6.5576e-04,  1.9603e-04, -7.0269e-04,  2.2896e-04,\n",
       "         -3.9596e-04, -1.5449e-03,  8.5928e-04, -8.8209e-04, -8.3056e-04,\n",
       "         -2.5425e-04, -4.8838e-04,  7.4449e-04,  5.4131e-04, -1.2240e-05,\n",
       "         -4.3031e-04, -1.4784e-03, -4.9087e-04, -1.1400e-03,  2.6109e-04,\n",
       "          5.7867e-04, -1.4400e-03,  1.1436e-03,  1.7917e-05, -5.3836e-04,\n",
       "          7.1042e-04, -4.0966e-05, -8.9320e-04, -1.2206e-05,  4.5473e-04,\n",
       "          4.0509e-05,  9.3379e-04,  5.7039e-04, -2.4544e-04, -1.1600e-03,\n",
       "         -2.6492e-03,  3.8176e-04,  9.9591e-04, -1.2813e-03,  1.4187e-04,\n",
       "          4.7575e-04, -8.9878e-04,  5.8561e-04, -8.1034e-05,  9.2909e-05,\n",
       "          1.9626e-04, -4.3689e-04, -5.8935e-05,  3.3462e-04,  1.0309e-03,\n",
       "         -5.4731e-04,  2.5183e-04,  1.0651e-03, -1.4338e-03, -4.2308e-05,\n",
       "         -3.2292e-04,  7.8388e-04,  2.4041e-05, -2.3490e-04, -7.3837e-04,\n",
       "          1.2757e-03,  9.4370e-04,  5.8075e-05, -3.9978e-04, -1.0236e-04,\n",
       "         -7.9390e-04, -8.8893e-04,  3.3053e-04, -1.2809e-03,  1.7651e-03,\n",
       "          6.9574e-05,  1.2843e-03,  1.1798e-03,  3.9029e-04, -5.2935e-04,\n",
       "         -1.1238e-03,  3.6029e-05, -7.4881e-04, -3.4884e-04, -6.8033e-04,\n",
       "          8.0448e-04, -3.8789e-04,  1.3864e-04, -5.9311e-04, -1.1513e-04,\n",
       "          1.1144e-03,  4.8031e-04,  5.5154e-04,  3.5446e-04, -5.5865e-04,\n",
       "          3.2409e-04,  6.0854e-06,  9.7965e-04, -1.4917e-03, -8.3525e-04,\n",
       "         -7.4206e-06, -2.6484e-04,  1.4139e-03,  1.1752e-03, -1.8416e-03,\n",
       "         -5.6810e-04, -2.2367e-04, -5.9258e-04,  1.6745e-03, -1.4960e-03,\n",
       "         -1.7515e-03, -9.5662e-04]),\n",
       " 'transformer.resblocks.0.mlp.c_fc.weight': tensor([[-0.0359,  0.0661,  0.0342,  ...,  0.0385,  0.0177, -0.0454],\n",
       "         [ 0.0107, -0.0155, -0.0448,  ...,  0.0338, -0.0019,  0.0206],\n",
       "         [-0.0245, -0.0037,  0.0214,  ...,  0.0491,  0.0357,  0.0146],\n",
       "         ...,\n",
       "         [-0.0290, -0.0237,  0.0343,  ...,  0.0106, -0.0111,  0.0119],\n",
       "         [ 0.0049,  0.0364, -0.0086,  ..., -0.0130, -0.0504,  0.0180],\n",
       "         [-0.0190, -0.0314,  0.0071,  ...,  0.0198, -0.0189, -0.0213]]),\n",
       " 'transformer.resblocks.0.mlp.c_fc.bias': tensor([-0.0141,  0.0244, -0.0103,  ...,  0.0170,  0.0081, -0.0238]),\n",
       " 'transformer.resblocks.0.mlp.c_proj.weight': tensor([[-0.0002,  0.0121,  0.0159,  ...,  0.0038, -0.0046, -0.0163],\n",
       "         [-0.0149,  0.0068,  0.0001,  ..., -0.0207, -0.0084, -0.0029],\n",
       "         [ 0.0108,  0.0002,  0.0077,  ..., -0.0021, -0.0158, -0.0043],\n",
       "         ...,\n",
       "         [-0.0033, -0.0118,  0.0077,  ..., -0.0051,  0.0148,  0.0063],\n",
       "         [-0.0106,  0.0052,  0.0140,  ..., -0.0012,  0.0049, -0.0104],\n",
       "         [ 0.0013,  0.0026, -0.0051,  ...,  0.0163,  0.0134,  0.0004]]),\n",
       " 'transformer.resblocks.0.mlp.c_proj.bias': tensor([ 7.2207e-03,  7.0620e-04,  7.3272e-03, -1.9631e-02,  9.2378e-04,\n",
       "          1.6839e-02,  1.6696e-02,  1.3302e-02,  7.3129e-03,  1.9667e-02,\n",
       "          1.2005e-02, -3.1761e-03, -1.4670e-03, -1.0488e-03,  5.5552e-03,\n",
       "          2.2032e-02, -2.8537e-04,  7.4668e-03, -1.4519e-02, -1.5367e-02,\n",
       "         -1.4335e-03, -1.0426e-03, -1.3584e-02, -3.7857e-04,  5.9556e-03,\n",
       "         -9.2862e-03, -1.6773e-02, -6.3463e-03,  2.2184e-02,  4.1932e-03,\n",
       "         -2.1609e-02, -1.0560e-02, -8.5329e-03, -8.8175e-03,  1.0353e-02,\n",
       "          6.0896e-03,  1.3533e-02,  1.5808e-02, -4.7661e-03,  2.0424e-02,\n",
       "          1.3330e-02,  1.7033e-02,  1.3163e-02,  1.2422e-02, -1.3651e-02,\n",
       "         -1.9497e-02, -6.0461e-03,  1.7404e-02,  6.4414e-03, -7.0163e-03,\n",
       "          2.1560e-02,  1.6599e-02,  2.3045e-03, -6.9857e-03,  1.6309e-02,\n",
       "          6.7711e-03,  8.7089e-03, -5.5778e-03,  3.3965e-03,  1.4899e-02,\n",
       "          4.9925e-03,  1.2006e-02, -1.2858e-02,  5.1845e-04, -5.3306e-03,\n",
       "         -5.6828e-03, -4.8649e-03, -1.5420e-02, -2.1049e-02,  1.5184e-02,\n",
       "          5.3192e-03,  1.9760e-02, -1.4244e-02,  1.3180e-02, -9.2257e-04,\n",
       "          1.5334e-03, -1.6974e-03,  2.1403e-02,  7.9118e-03,  2.4889e-03,\n",
       "         -1.2258e-02, -1.9011e-02, -3.9968e-03,  3.0647e-03, -3.9830e-03,\n",
       "          1.8063e-02, -1.3944e-02,  1.1742e-02, -2.0356e-02,  1.2612e-02,\n",
       "         -1.8373e-02, -1.4095e-02,  1.8039e-02,  1.0695e-02,  8.1946e-03,\n",
       "          8.1278e-03, -6.2443e-03,  6.7737e-03, -1.3897e-04, -2.0368e-02,\n",
       "         -1.6097e-02,  1.4575e-02,  2.8098e-03, -1.1195e-03,  9.6635e-03,\n",
       "         -3.2161e-03, -1.2853e-03,  1.4689e-02,  1.9965e-02,  1.9891e-02,\n",
       "          2.5545e-04,  1.2330e-02, -1.8914e-02,  4.2655e-03,  1.7515e-02,\n",
       "         -9.7167e-03, -1.1418e-02,  2.4581e-03,  1.5573e-02, -1.9548e-02,\n",
       "         -1.0486e-02,  1.6981e-02,  1.5960e-02, -1.6400e-02,  1.2553e-02,\n",
       "         -2.1639e-02, -2.0060e-02,  5.9889e-04,  1.9173e-02, -6.7214e-03,\n",
       "         -5.0513e-03, -1.1987e-02,  3.5219e-03, -1.8555e-02,  1.7163e-02,\n",
       "          1.8073e-02, -3.5548e-03, -1.8542e-02,  6.4161e-03, -1.3931e-02,\n",
       "         -1.4134e-02, -5.5467e-03,  6.7812e-03,  5.4703e-03, -1.4593e-02,\n",
       "          1.5974e-02, -3.5024e-03, -2.1150e-02, -1.8211e-02, -1.2226e-02,\n",
       "         -8.6529e-03,  4.4564e-03, -1.5282e-02, -1.9344e-02, -2.1603e-02,\n",
       "          3.2161e-03,  2.7551e-03, -8.3407e-03, -1.5545e-02,  8.1392e-03,\n",
       "          2.0939e-02,  1.1274e-02, -7.8433e-03,  3.6509e-03, -9.5915e-03,\n",
       "          2.3781e-03, -6.2474e-05, -5.9122e-03,  6.7563e-04,  1.0878e-02,\n",
       "         -1.0919e-02, -8.2868e-03, -1.3629e-02,  1.9804e-02, -8.3799e-03,\n",
       "         -1.4535e-02,  8.9886e-03,  1.2905e-02,  4.3568e-03,  2.0987e-02,\n",
       "          1.6997e-02,  1.6835e-02,  4.5416e-03, -1.5500e-02,  2.9985e-03,\n",
       "          1.9033e-02, -8.9178e-04,  3.9554e-03,  1.4787e-02,  9.4070e-03,\n",
       "         -1.3238e-02,  1.8726e-03,  6.8654e-03,  6.2252e-03, -1.9372e-02,\n",
       "         -9.1580e-03, -3.9882e-03,  1.9133e-02, -1.1506e-02,  1.5946e-02,\n",
       "         -1.5601e-03,  3.1210e-03, -3.8600e-03,  1.6233e-02,  1.7407e-02,\n",
       "          2.0733e-03, -2.5077e-03, -1.6050e-02,  1.5100e-02, -1.0315e-02,\n",
       "          1.2988e-03,  9.0751e-03, -1.3956e-02,  1.7573e-02, -5.1375e-03,\n",
       "         -1.8220e-02, -6.7211e-03,  6.3529e-03,  9.9898e-03,  9.8469e-03,\n",
       "          6.0387e-03, -1.0132e-05, -1.1346e-02, -1.9190e-02,  1.6172e-02,\n",
       "         -4.5991e-03,  1.7746e-02, -3.9752e-03,  2.0148e-02,  1.5940e-02,\n",
       "         -1.4625e-02,  1.7676e-02,  3.4034e-03,  8.5695e-04, -9.8230e-03,\n",
       "         -1.6755e-02, -1.3493e-02,  1.0931e-02, -1.9621e-03, -3.9915e-03,\n",
       "          1.0899e-02, -2.0201e-04,  2.0950e-02, -1.2365e-02,  1.8094e-03,\n",
       "         -8.1404e-03, -1.0113e-02,  2.9354e-03,  3.5886e-03,  1.9085e-02,\n",
       "         -4.1159e-03, -8.2959e-03,  7.4783e-03,  1.8343e-02, -8.8452e-03,\n",
       "         -2.0784e-02,  1.9906e-02, -1.3878e-02,  1.0271e-02, -4.2954e-03,\n",
       "         -1.8187e-02, -8.8326e-04, -1.8707e-02, -9.7680e-03,  1.1452e-03,\n",
       "         -5.3221e-03,  1.4428e-03,  1.3252e-02, -1.7347e-02, -1.2227e-02,\n",
       "          1.2144e-02, -8.8869e-03,  9.9531e-03, -2.1589e-02, -1.1998e-02,\n",
       "         -9.2310e-03, -2.7346e-03,  1.6668e-02, -6.1619e-03,  7.0895e-03,\n",
       "         -1.1378e-02,  1.0639e-02, -6.1479e-03,  3.0756e-03, -1.2246e-02,\n",
       "         -9.8790e-03,  1.4231e-02,  9.3705e-04, -6.4303e-03, -6.0419e-03,\n",
       "          1.2228e-02, -1.7026e-02, -1.1299e-02,  6.5989e-03,  1.8022e-02,\n",
       "          1.8650e-02,  4.8607e-04, -1.2267e-02,  4.1052e-03,  2.2371e-02,\n",
       "         -8.3488e-03, -1.1308e-02,  1.1254e-02, -2.4858e-03, -1.6502e-02,\n",
       "         -2.0400e-02,  7.3094e-03,  2.0393e-02, -1.1072e-02,  1.7949e-02,\n",
       "         -2.1197e-02, -1.9588e-02,  1.9413e-02,  1.9341e-02,  2.1540e-02,\n",
       "         -8.7008e-03, -1.9456e-02, -1.3362e-02,  5.8306e-03, -1.7109e-02,\n",
       "          3.1295e-03,  2.0644e-02, -1.2378e-02,  7.3432e-03, -1.9111e-02,\n",
       "         -7.5158e-03,  8.5229e-03, -2.0925e-02, -1.7603e-02,  8.8000e-03,\n",
       "          5.9470e-03, -1.8201e-02,  4.5629e-03, -1.1167e-02, -1.9085e-03,\n",
       "         -1.3983e-02,  3.9556e-03,  1.0437e-02,  1.9501e-02, -4.6114e-04,\n",
       "         -5.4119e-03,  1.2890e-02,  2.1684e-07,  2.7218e-03,  1.3256e-02,\n",
       "          9.5864e-03,  2.9975e-03, -1.8665e-02,  1.0767e-02, -3.5088e-03,\n",
       "         -1.5140e-02, -1.2344e-02, -2.3043e-02, -1.6030e-02,  1.0534e-02,\n",
       "          2.1310e-02, -1.0388e-02, -1.0218e-02,  1.7451e-03, -4.6040e-03,\n",
       "         -3.2498e-03, -7.0064e-03, -1.6388e-02, -7.4716e-03, -1.7249e-02,\n",
       "          1.7766e-02, -1.5467e-02,  1.6683e-02, -6.1399e-03, -1.5028e-02,\n",
       "         -1.2503e-02, -1.0907e-02, -1.3351e-02,  1.9914e-02, -1.0891e-03,\n",
       "          1.4321e-02,  1.5763e-02,  2.8837e-03, -3.4586e-03,  1.8424e-02,\n",
       "          6.8106e-03,  3.9128e-03, -1.9220e-02, -1.3973e-02, -3.0981e-03,\n",
       "         -1.8909e-02,  2.2215e-02, -2.0379e-02, -1.4219e-03, -1.0782e-02,\n",
       "         -1.6361e-02,  2.0082e-02, -2.1159e-02,  1.4662e-02,  1.9757e-02,\n",
       "         -1.4131e-02, -4.8634e-04, -1.2539e-02, -1.1837e-02, -5.3688e-03,\n",
       "          1.4233e-02, -6.4317e-04,  7.5841e-03, -2.9773e-03,  1.6259e-02,\n",
       "          1.5935e-03,  1.6137e-02, -1.5701e-02,  1.5742e-02, -7.5596e-03,\n",
       "         -6.1406e-03,  5.7377e-03, -1.5881e-02,  7.0901e-03, -5.5137e-03,\n",
       "         -3.8385e-03, -6.5413e-03, -7.4370e-03,  1.3644e-02,  6.2876e-03,\n",
       "          2.1727e-02, -1.7081e-02, -1.1268e-02,  1.2542e-02, -1.1952e-02,\n",
       "          8.1027e-03, -1.0288e-02,  1.0803e-02, -9.1592e-03,  1.3981e-03,\n",
       "          1.3204e-02, -7.5699e-03,  1.1708e-02, -1.1636e-02,  1.9146e-02,\n",
       "          2.1737e-02, -1.3548e-02,  1.7269e-02, -6.0336e-03,  1.8076e-02,\n",
       "          2.0619e-02, -1.8411e-02, -2.2402e-02, -9.3753e-03,  5.0485e-03,\n",
       "          2.1244e-02,  8.4447e-03,  1.0778e-02,  2.4914e-03,  1.8134e-03,\n",
       "         -8.3484e-03, -1.0609e-02,  6.5596e-03, -4.9852e-03,  1.1028e-02,\n",
       "          1.2404e-02, -1.6705e-02, -1.2807e-02,  1.9169e-02, -1.3017e-02,\n",
       "         -4.4374e-03,  1.7131e-03, -1.5701e-03, -1.1530e-02, -1.1059e-02,\n",
       "         -2.8568e-04,  1.3386e-02,  1.3440e-02, -1.1498e-02, -1.4431e-02,\n",
       "         -1.0302e-03,  4.6806e-03,  1.6207e-02,  1.8796e-03,  1.1543e-02,\n",
       "          1.2308e-02,  2.1544e-02,  4.2095e-03, -1.1749e-02, -7.3108e-03,\n",
       "         -9.0701e-03,  2.0581e-03, -1.3348e-02,  2.0722e-02,  6.2828e-03,\n",
       "         -1.3011e-02,  1.4758e-02,  1.9482e-02, -1.6900e-02, -2.0595e-02,\n",
       "          3.7032e-04,  6.1473e-03,  2.6948e-03,  6.1192e-05,  1.5237e-02,\n",
       "         -2.1110e-02,  1.2804e-02, -9.0139e-03,  1.4740e-02, -3.8695e-03,\n",
       "          1.5604e-02,  2.9332e-04,  1.6603e-02, -4.4222e-03, -1.3833e-02,\n",
       "          1.9074e-02,  4.6578e-03,  2.1044e-02, -1.7052e-02, -1.7813e-02,\n",
       "          1.9240e-02,  1.4541e-02]),\n",
       " 'transformer.resblocks.0.ln_2.weight': tensor([1.0054, 0.9997, 1.0031, 0.9985, 1.0018, 1.0021, 1.0011, 1.0021, 1.0021,\n",
       "         0.9998, 1.0031, 1.0000, 1.0022, 1.0052, 0.9997, 1.0014, 1.0021, 1.0017,\n",
       "         1.0017, 1.0008, 1.0026, 0.9997, 1.0006, 1.0001, 1.0023, 1.0029, 1.0010,\n",
       "         1.0006, 1.0003, 1.0033, 0.9996, 1.0002, 1.0049, 1.0011, 1.0026, 1.0012,\n",
       "         1.0038, 1.0012, 1.0027, 1.0004, 1.0017, 1.0030, 0.9992, 1.0006, 0.9972,\n",
       "         1.0011, 1.0026, 1.0027, 1.0032, 1.0023, 1.0025, 1.0016, 0.9984, 0.9981,\n",
       "         1.0024, 1.0035, 1.0012, 1.0004, 1.0044, 1.0022, 1.0033, 0.9984, 1.0000,\n",
       "         1.0044, 1.0007, 1.0033, 1.0009, 1.0006, 1.0018, 0.9992, 1.0002, 0.9992,\n",
       "         1.0026, 1.0038, 1.0020, 1.0005, 0.9996, 1.0021, 0.9984, 1.0005, 1.0004,\n",
       "         0.9998, 1.0054, 0.9997, 1.0005, 1.0027, 1.0000, 1.0042, 1.0039, 1.0045,\n",
       "         1.0029, 1.0015, 1.0015, 1.0028, 1.0015, 1.0030, 1.0052, 0.9967, 0.9989,\n",
       "         1.0023, 1.0020, 1.0018, 1.0006, 0.9995, 1.0005, 1.0028, 0.9999, 1.0017,\n",
       "         1.0008, 1.0015, 0.9992, 1.0015, 1.0031, 1.0024, 1.0021, 1.0007, 1.0022,\n",
       "         1.0011, 1.0020, 1.0001, 1.0010, 1.0024, 1.0022, 1.0039, 1.0049, 1.0009,\n",
       "         1.0027, 1.0013, 1.0018, 1.0020, 1.0018, 1.0025, 0.9987, 1.0021, 1.0040,\n",
       "         0.9985, 0.9999, 0.9992, 1.0016, 1.0001, 1.0000, 0.9977, 1.0017, 1.0034,\n",
       "         1.0038, 1.0028, 1.0026, 1.0001, 1.0021, 1.0038, 1.0024, 1.0015, 1.0029,\n",
       "         0.9994, 0.9990, 1.0012, 1.0004, 0.9989, 1.0014, 1.0046, 1.0042, 0.9986,\n",
       "         1.0013, 0.9990, 1.0010, 1.0045, 1.0032, 1.0026, 1.0004, 0.9990, 1.0035,\n",
       "         0.9997, 1.0028, 0.9966, 1.0011, 1.0026, 1.0006, 1.0044, 1.0030, 1.0031,\n",
       "         1.0035, 1.0011, 1.0018, 1.0005, 1.0026, 1.0015, 1.0009, 1.0014, 1.0004,\n",
       "         0.9985, 1.0029, 1.0015, 1.0042, 1.0050, 1.0024, 1.0032, 1.0025, 1.0051,\n",
       "         1.0013, 1.0008, 1.0015, 1.0019, 1.0012, 0.9999, 1.0009, 1.0017, 1.0000,\n",
       "         1.0017, 1.0020, 1.0002, 1.0042, 1.0027, 1.0037, 1.0034, 1.0008, 0.9995,\n",
       "         1.0033, 1.0018, 1.0031, 1.0014, 1.0032, 1.0021, 1.0023, 1.0020, 0.9994,\n",
       "         1.0040, 1.0021, 1.0000, 1.0003, 1.0016, 1.0051, 1.0023, 0.9987, 1.0018,\n",
       "         1.0024, 1.0024, 1.0006, 1.0016, 1.0032, 1.0027, 1.0019, 1.0026, 1.0006,\n",
       "         1.0022, 0.9992, 1.0026, 1.0032, 1.0009, 0.9987, 1.0012, 1.0007, 1.0016,\n",
       "         1.0009, 1.0026, 1.0013, 0.9971, 1.0044, 0.9988, 0.9987, 1.0024, 1.0044,\n",
       "         1.0038, 1.0021, 1.0006, 1.0010, 1.0012, 1.0003, 1.0010, 1.0019, 1.0026,\n",
       "         1.0030, 0.9999, 1.0027, 1.0045, 1.0014, 0.9999, 1.0044, 1.0042, 1.0048,\n",
       "         0.9991, 1.0015, 0.9999, 0.9987, 1.0032, 1.0010, 1.0018, 1.0005, 0.9999,\n",
       "         1.0009, 1.0019, 1.0016, 1.0016, 1.0024, 1.0015, 1.0009, 1.0019, 1.0012,\n",
       "         1.0027, 1.0032, 1.0021, 1.0019, 1.0009, 1.0017, 1.0030, 0.9980, 1.0029,\n",
       "         1.0039, 0.9994, 1.0018, 1.0044, 0.9979, 1.0009, 0.9999, 1.0048, 1.0016,\n",
       "         1.0034, 1.0023, 1.0030, 1.0030, 1.0041, 1.0006, 1.0024, 1.0017, 1.0005,\n",
       "         1.0003, 1.0011, 0.9979, 1.0011, 1.0027, 1.0026, 1.0005, 0.9997, 1.0004,\n",
       "         0.9980, 1.0035, 1.0009, 1.0028, 0.9990, 1.0026, 0.9997, 1.0021, 0.9999,\n",
       "         1.0003, 1.0022, 0.9982, 0.9996, 0.9998, 1.0042, 1.0042, 1.0015, 1.0018,\n",
       "         1.0012, 1.0020, 1.0048, 1.0031, 1.0016, 1.0026, 1.0018, 1.0012, 1.0033,\n",
       "         1.0015, 1.0011, 1.0040, 1.0028, 0.9985, 1.0010, 1.0017, 1.0038, 0.9989,\n",
       "         1.0030, 1.0052, 1.0036, 1.0009, 1.0024, 1.0018, 1.0001, 0.9980, 1.0020,\n",
       "         1.0014, 1.0004, 1.0004, 1.0032, 1.0012, 0.9999, 1.0000, 1.0042, 1.0028,\n",
       "         1.0024, 1.0022, 1.0042, 1.0022, 1.0016, 1.0011, 0.9999, 1.0024, 0.9994,\n",
       "         1.0006, 1.0016, 1.0030, 1.0017, 1.0016, 1.0040, 1.0044, 1.0007, 1.0006,\n",
       "         1.0013, 1.0024, 0.9985, 1.0031, 1.0015, 1.0050, 1.0007, 0.9996, 0.9990,\n",
       "         1.0019, 1.0005, 1.0028, 1.0005, 1.0034, 1.0038, 1.0038, 1.0038, 1.0044,\n",
       "         1.0035, 1.0002, 1.0005, 1.0008, 1.0012, 1.0000, 1.0001, 1.0020, 1.0016,\n",
       "         1.0019, 1.0027, 1.0031, 1.0038, 1.0015, 1.0034, 1.0011, 1.0034, 1.0022,\n",
       "         1.0020, 0.9970, 1.0046, 1.0001, 1.0019, 1.0016, 1.0019, 1.0026, 1.0018,\n",
       "         0.9997, 1.0004, 0.9986, 1.0008, 1.0038, 1.0020, 1.0012, 0.9987, 1.0014,\n",
       "         1.0073, 1.0014, 1.0039, 1.0024, 1.0023, 1.0007, 1.0025, 1.0022, 1.0022,\n",
       "         1.0025, 1.0044, 0.9971, 1.0013, 1.0014, 1.0008, 1.0021, 0.9995, 1.0015,\n",
       "         0.9996, 1.0008, 1.0020, 0.9979, 1.0016, 1.0029, 1.0000, 1.0002, 1.0001,\n",
       "         1.0024, 1.0044, 1.0010, 1.0028, 1.0052, 1.0011, 1.0033, 1.0014, 0.9997,\n",
       "         1.0032, 1.0012, 1.0033, 1.0030, 1.0047, 1.0002, 1.0030, 1.0005, 1.0003,\n",
       "         0.9984, 1.0004, 0.9986, 1.0018, 1.0046, 1.0034, 1.0035, 1.0051]),\n",
       " 'transformer.resblocks.0.ln_2.bias': tensor([ 7.4647e-04,  1.0557e-03, -1.5635e-05,  1.9297e-03,  3.9092e-04,\n",
       "          7.2086e-05,  1.1176e-03,  3.0559e-04, -4.3475e-04, -3.8109e-04,\n",
       "         -2.5239e-04, -2.2653e-04, -3.9979e-04, -5.4019e-04, -2.3611e-04,\n",
       "         -5.1506e-04,  1.3634e-03,  1.4026e-03,  9.2857e-04, -5.4396e-04,\n",
       "          7.4458e-04,  2.0645e-03,  7.7347e-04,  5.0741e-04,  1.4401e-03,\n",
       "          9.4586e-04, -2.0964e-05, -1.1220e-04, -8.5382e-04,  1.2012e-03,\n",
       "         -3.7650e-04, -6.3698e-04, -1.0370e-03, -1.2347e-03,  1.1867e-03,\n",
       "          1.8296e-03, -1.8656e-04,  3.5024e-04,  1.0154e-03, -7.5326e-04,\n",
       "          5.6149e-04,  2.2174e-04, -9.9872e-04, -5.7211e-04, -9.6418e-06,\n",
       "          3.9277e-04, -4.2970e-04,  7.8828e-04,  2.4397e-05,  1.0354e-03,\n",
       "         -5.2518e-04, -3.4078e-04,  1.2099e-03, -8.9934e-04,  9.8530e-04,\n",
       "          8.6576e-04,  3.9017e-04,  5.9374e-04, -5.5099e-04, -9.7114e-05,\n",
       "          3.3525e-04, -3.6922e-04, -1.2361e-03, -6.7222e-04, -1.4511e-03,\n",
       "         -1.0097e-03,  1.5597e-04,  1.6299e-04, -8.3220e-04,  2.6716e-05,\n",
       "         -1.4822e-04, -1.0887e-03, -9.7998e-04, -6.4691e-05,  8.0753e-05,\n",
       "         -2.9962e-04, -3.5611e-04, -2.7400e-04, -9.3792e-04,  4.6763e-04,\n",
       "         -9.5150e-04, -5.5524e-04,  2.0619e-04,  5.0534e-04, -1.2465e-03,\n",
       "          3.4640e-04, -9.8893e-04,  6.0752e-04,  2.8378e-04, -1.1258e-03,\n",
       "         -1.0782e-03,  6.5810e-04, -1.7564e-04,  1.6457e-04, -9.1057e-05,\n",
       "          3.6818e-04, -5.7068e-04,  6.9802e-04, -2.5397e-04, -2.5286e-05,\n",
       "         -1.0651e-03,  6.4650e-05, -3.6961e-04,  1.1076e-03,  1.4685e-03,\n",
       "         -9.4514e-04, -1.6235e-03, -3.2903e-04, -1.5857e-04,  1.2200e-03,\n",
       "         -2.4702e-04, -2.5925e-04,  1.3007e-04, -2.9594e-04, -2.3967e-05,\n",
       "          7.2063e-05,  4.3811e-04,  1.1376e-05,  5.1287e-04,  1.1695e-03,\n",
       "          2.3695e-04,  1.7088e-04, -8.0467e-04, -4.3272e-05, -4.6062e-04,\n",
       "          1.2716e-04, -5.2372e-04,  9.2247e-04,  5.8748e-05, -7.9394e-04,\n",
       "          6.3126e-04, -4.9539e-04, -7.5031e-04, -7.8104e-04, -3.6285e-05,\n",
       "         -4.9551e-04, -1.3514e-03,  4.0917e-04,  4.4030e-04,  1.4770e-03,\n",
       "          1.1259e-04,  2.2008e-04, -6.7289e-04,  4.3932e-04, -1.2237e-03,\n",
       "         -6.3431e-04,  1.3445e-04, -2.0907e-03, -5.7570e-04, -7.3988e-04,\n",
       "          1.1449e-03, -3.8937e-04,  8.1403e-04,  1.8288e-04, -4.1718e-04,\n",
       "         -4.0757e-04,  1.4481e-04, -1.0438e-03, -4.5608e-05,  6.9886e-04,\n",
       "          1.4510e-04,  4.5770e-04,  1.0915e-03,  6.5790e-04, -9.9259e-05,\n",
       "          1.6932e-03,  1.4867e-04,  1.2962e-03, -4.1148e-04,  5.2249e-04,\n",
       "         -3.7678e-04, -1.2029e-03,  8.0948e-04,  1.8392e-03, -5.1726e-04,\n",
       "         -4.4200e-04, -1.8118e-03,  4.5643e-04,  1.2138e-03,  7.5822e-05,\n",
       "          5.4669e-04,  5.6028e-04,  1.0576e-03, -4.1723e-04, -7.5170e-04,\n",
       "         -1.7064e-04, -2.6822e-04,  3.1883e-04,  9.6700e-04,  2.0745e-03,\n",
       "          8.1444e-04, -5.5409e-05, -4.5794e-05,  5.2131e-05, -8.6547e-04,\n",
       "          5.9942e-05, -7.1140e-04, -4.9541e-04, -2.0509e-04, -8.6433e-04,\n",
       "         -5.4577e-04, -3.1480e-04, -5.3878e-05,  1.8506e-04, -7.1211e-04,\n",
       "          3.8275e-04, -5.2027e-05, -8.1062e-04, -9.2703e-05, -2.1119e-04,\n",
       "          6.3292e-04, -7.9076e-04, -6.2347e-04,  1.1419e-03,  2.1932e-04,\n",
       "          1.9534e-04,  5.8341e-04, -1.9895e-04, -1.3395e-03,  8.8627e-06,\n",
       "         -2.6801e-05, -9.2326e-04, -1.7047e-04,  4.4346e-04,  9.0408e-04,\n",
       "          6.9046e-04, -6.3836e-04,  1.2625e-03, -6.2081e-04, -6.5297e-04,\n",
       "          1.1019e-04, -8.0510e-06, -7.8183e-05,  4.1650e-04,  3.7296e-05,\n",
       "         -1.1018e-03,  7.7554e-04, -1.0508e-03,  6.5695e-04,  7.3508e-04,\n",
       "          3.0773e-04,  5.9150e-04, -6.9202e-04, -9.0096e-04,  1.1301e-03,\n",
       "         -2.1322e-03,  1.7216e-04, -1.6400e-04, -1.7816e-03, -1.6013e-03,\n",
       "          6.7041e-04, -3.1617e-04, -6.4954e-04, -2.7078e-04, -1.5073e-03,\n",
       "         -7.3666e-04, -2.9272e-04,  5.3770e-04,  1.4357e-04,  5.9831e-04,\n",
       "         -8.7619e-04, -2.9664e-05,  9.8934e-04, -7.5947e-04, -1.1957e-03,\n",
       "         -5.1284e-05, -5.2494e-04,  5.1394e-04, -4.0699e-04, -6.0896e-04,\n",
       "          2.6462e-04, -6.8929e-04,  1.2983e-03,  1.2234e-03, -7.2371e-04,\n",
       "          7.0965e-04,  9.1013e-05, -1.2538e-03,  8.1124e-04, -2.0524e-04,\n",
       "         -6.7497e-05,  4.1960e-04,  6.7336e-04, -6.0755e-04, -1.3339e-04,\n",
       "         -1.3702e-04,  2.1783e-04,  1.4983e-04,  6.8922e-04, -1.6094e-03,\n",
       "         -1.2838e-03, -9.0465e-04, -6.9029e-04,  3.0028e-04, -9.9102e-05,\n",
       "          4.6423e-04, -6.5015e-04,  7.9616e-04,  2.8565e-05,  7.6609e-05,\n",
       "         -3.4084e-04, -1.0389e-03, -2.5791e-04,  1.4793e-03, -1.4785e-04,\n",
       "         -6.2543e-04, -2.6956e-04, -1.6445e-04,  2.8691e-05, -1.7995e-04,\n",
       "          4.7813e-05, -1.0160e-04, -2.4113e-04,  8.8937e-04,  2.1180e-04,\n",
       "         -6.2256e-04,  5.9421e-04,  4.2427e-04,  5.4475e-04, -1.3738e-03,\n",
       "         -2.7111e-04,  2.2765e-04, -2.0052e-03, -1.1306e-05, -4.6809e-04,\n",
       "         -3.1379e-04,  3.2302e-04,  1.5051e-03,  3.6393e-04, -1.1681e-04,\n",
       "         -5.0376e-04,  1.6749e-04, -4.8635e-04,  1.3298e-04, -8.3420e-05,\n",
       "          6.9548e-04, -4.9221e-04,  5.9193e-04, -8.1806e-05, -2.6152e-04,\n",
       "         -3.3420e-04,  1.0156e-03,  5.2809e-05,  8.0571e-04,  4.7198e-04,\n",
       "          8.6140e-05, -1.1324e-03, -2.2691e-04,  2.7394e-04, -3.1905e-05,\n",
       "         -1.2116e-03,  8.5600e-04,  1.1413e-04,  6.7877e-04,  3.0773e-04,\n",
       "          1.3553e-03, -3.5754e-04, -1.0672e-03,  5.6350e-04, -9.4885e-04,\n",
       "          6.8292e-04,  1.0028e-03, -8.4046e-04,  6.6224e-04,  1.0876e-04,\n",
       "          3.2958e-05, -9.6555e-04,  4.8662e-04, -8.3416e-04,  2.8414e-06,\n",
       "         -8.8772e-04,  5.0459e-04,  4.1091e-04, -5.1385e-04, -1.0416e-03,\n",
       "          5.1432e-04, -4.8644e-04, -1.1087e-03, -4.2717e-04, -1.6047e-04,\n",
       "          1.0668e-03,  6.2022e-04, -7.2998e-04,  5.2533e-05, -7.0137e-04,\n",
       "          7.6529e-05,  3.9422e-04, -3.7672e-04, -1.5258e-03, -9.5454e-04,\n",
       "         -1.6603e-04,  1.0066e-04, -6.7393e-04,  5.1202e-04, -3.6541e-04,\n",
       "          1.0055e-03,  1.2547e-03, -6.1500e-04,  6.2897e-04, -1.2510e-04,\n",
       "         -1.6528e-04, -3.3754e-04, -1.5333e-04, -1.5244e-04, -4.7832e-04,\n",
       "         -1.6998e-04,  1.0089e-03, -1.1636e-06, -9.0903e-04, -3.6681e-04,\n",
       "          9.0982e-04, -1.3382e-04,  6.4636e-05,  2.4537e-04, -6.4064e-04,\n",
       "          2.1280e-04, -8.3309e-04, -1.2755e-03,  1.5150e-04, -1.3939e-04,\n",
       "          1.5243e-03, -1.9171e-03,  1.2643e-04, -9.6938e-04, -1.6114e-04,\n",
       "          3.8143e-04, -1.8748e-03,  9.5399e-05, -2.6614e-04, -1.4337e-04,\n",
       "         -9.2896e-05,  5.9097e-04,  6.6117e-04,  1.6326e-03,  2.1265e-04,\n",
       "         -3.0114e-04,  6.1347e-04, -3.7957e-04, -3.4589e-04,  6.7724e-04,\n",
       "          3.3814e-04, -1.4609e-03, -2.2049e-03,  5.5038e-06,  4.6795e-04,\n",
       "          1.1483e-03, -8.1720e-04, -1.1727e-04,  7.2167e-04, -6.7244e-05,\n",
       "          9.9075e-04, -3.3671e-04, -1.7607e-03,  5.2160e-04,  1.7278e-04,\n",
       "         -3.5763e-04, -8.2190e-04,  7.4950e-04,  5.1791e-04, -4.1582e-04,\n",
       "          5.3585e-04, -3.4772e-04,  1.7818e-03, -1.7912e-03,  6.4504e-04,\n",
       "         -4.4585e-04, -9.9812e-04, -1.3150e-04, -1.0656e-03, -2.1290e-04,\n",
       "         -1.4338e-03,  1.0230e-03, -8.6562e-04,  1.1053e-03,  9.5361e-04,\n",
       "         -9.9644e-04,  1.1059e-03, -7.9882e-04,  2.3603e-04,  4.7932e-05,\n",
       "         -1.2076e-03, -2.4408e-04,  7.3018e-05,  2.1585e-04, -4.5027e-04,\n",
       "         -2.2340e-04, -2.8532e-04,  8.7791e-04, -1.0026e-03, -2.1607e-04,\n",
       "         -6.0459e-04, -1.2777e-03,  1.7015e-04,  4.3244e-04, -4.6378e-04,\n",
       "          1.0579e-03,  1.4600e-04,  2.8669e-04, -8.1613e-04,  2.8949e-04,\n",
       "          1.1244e-04,  6.9714e-04,  1.6726e-04,  5.7441e-04, -5.0338e-04,\n",
       "          1.5250e-03, -6.9851e-05,  1.0738e-03,  6.1064e-04, -9.6976e-05,\n",
       "          6.0606e-04, -1.8134e-04]),\n",
       " 'transformer.resblocks.1.attn.in_proj_weight': tensor([[ 0.0343, -0.0361,  0.0056,  ...,  0.0081,  0.0311, -0.0538],\n",
       "         [ 0.0598, -0.0050,  0.0264,  ...,  0.0198, -0.0013,  0.0044],\n",
       "         [ 0.0621, -0.0384, -0.0414,  ..., -0.0209,  0.0109, -0.1039],\n",
       "         ...,\n",
       "         [-0.0405,  0.1010, -0.0057,  ..., -0.0620, -0.0283, -0.0215],\n",
       "         [ 0.0045, -0.0111, -0.0103,  ..., -0.0077, -0.0472,  0.0444],\n",
       "         [ 0.0693,  0.0695,  0.0275,  ..., -0.0405,  0.0203,  0.0274]]),\n",
       " 'transformer.resblocks.1.attn.in_proj_bias': tensor([ 6.5028e-05, -4.4116e-03,  4.5259e-03,  ...,  7.8695e-04,\n",
       "          2.6951e-04,  7.3296e-04]),\n",
       " 'transformer.resblocks.1.attn.out_proj.weight': tensor([[ 7.5292e-03,  6.2797e-03, -1.4232e-02,  ...,  1.0955e-02,\n",
       "           1.2070e-02, -2.6318e-03],\n",
       "         [ 3.9137e-03,  1.2300e-02,  8.6482e-03,  ..., -8.3133e-03,\n",
       "           9.7519e-03,  5.4085e-03],\n",
       "         [-4.2132e-03, -1.2602e-02, -2.7509e-03,  ...,  1.3563e-04,\n",
       "          -3.2271e-03, -1.2637e-02],\n",
       "         ...,\n",
       "         [-7.2892e-03,  7.1092e-03, -5.0384e-03,  ..., -9.0821e-03,\n",
       "           1.1353e-02,  4.8089e-03],\n",
       "         [ 7.9822e-03,  7.7361e-03,  1.0798e-02,  ...,  7.9541e-03,\n",
       "           2.3363e-05, -8.0400e-03],\n",
       "         [ 1.3715e-03, -1.1132e-03, -1.7193e-03,  ..., -1.8136e-03,\n",
       "           5.3000e-03,  1.8606e-03]]),\n",
       " 'transformer.resblocks.1.attn.out_proj.bias': tensor([ 4.2207e-04,  5.9482e-04, -1.3804e-04, -2.4173e-05,  1.0069e-04,\n",
       "          7.5809e-04,  1.7478e-04,  8.9945e-04, -9.7875e-04, -4.0496e-04,\n",
       "          1.0269e-03, -9.2491e-04,  1.0614e-03,  1.3702e-04, -6.9678e-04,\n",
       "         -8.4398e-04, -1.1482e-03,  9.3758e-05,  6.8151e-04,  6.4762e-04,\n",
       "          1.6128e-04, -4.0688e-04, -4.1826e-04, -9.6739e-04,  6.7642e-04,\n",
       "          2.0117e-04, -3.3474e-05,  4.6531e-04,  1.6544e-03, -9.8876e-04,\n",
       "          6.5115e-04, -6.2753e-04, -6.6538e-04, -4.1957e-04, -5.9487e-04,\n",
       "          1.0107e-03, -9.0763e-04, -5.1338e-04,  2.5297e-04, -1.3693e-03,\n",
       "         -2.4599e-04, -6.2417e-04, -7.5137e-04,  1.7110e-03, -2.0287e-03,\n",
       "          1.0965e-03,  7.2397e-04,  1.3392e-04,  1.9986e-04, -1.5059e-03,\n",
       "         -2.7897e-04, -1.1740e-03, -1.8420e-03, -9.4526e-04,  1.3321e-03,\n",
       "          7.0618e-04,  1.2298e-03,  1.8279e-03,  6.9629e-04,  5.6759e-04,\n",
       "          3.7784e-04, -2.8927e-04, -5.2789e-04,  4.3498e-04,  2.2239e-04,\n",
       "         -1.3636e-04, -8.1286e-04, -6.7444e-04, -9.2958e-05,  8.0763e-04,\n",
       "         -4.8793e-04,  1.4640e-03, -7.9184e-04, -6.7715e-05,  7.9212e-04,\n",
       "         -1.8859e-04, -3.4793e-04,  6.0356e-04, -1.5799e-04, -3.3628e-04,\n",
       "         -1.2349e-04,  8.4303e-04,  4.3496e-04,  5.7033e-04,  3.5908e-04,\n",
       "         -1.1258e-03, -3.7184e-04, -1.2667e-03, -7.4906e-04, -9.7727e-04,\n",
       "          1.3550e-04, -4.8218e-04, -1.1098e-04, -7.7249e-04,  1.0671e-03,\n",
       "          1.4325e-04, -1.3003e-04,  1.1918e-04,  9.0555e-04, -9.7398e-05,\n",
       "          6.4523e-04, -1.6114e-04,  1.2668e-03,  6.5963e-04,  5.7633e-04,\n",
       "         -3.9466e-05,  1.2536e-03, -2.6724e-05, -1.5549e-04,  4.4624e-04,\n",
       "         -5.8809e-04, -1.0292e-03, -6.8816e-04, -3.9276e-04,  1.0089e-03,\n",
       "         -1.5794e-03,  3.2095e-04,  1.3261e-04, -5.0586e-04, -1.3979e-03,\n",
       "         -3.6436e-05,  6.5892e-04, -2.3206e-04,  3.8227e-04,  5.1119e-04,\n",
       "         -5.2662e-04,  5.8077e-05,  1.5363e-04,  5.5897e-04,  1.5778e-04,\n",
       "          1.2870e-03, -3.1923e-04, -1.9642e-04, -1.9429e-03,  2.8019e-04,\n",
       "          2.3585e-04, -1.2425e-03, -5.3101e-04, -6.9356e-04, -2.5621e-04,\n",
       "         -7.8006e-04, -6.4217e-05,  1.9343e-04,  5.5124e-04,  6.9079e-05,\n",
       "         -5.5420e-04,  7.5108e-04, -6.5991e-05,  1.3400e-03, -2.4312e-05,\n",
       "          5.7033e-04, -2.1149e-04, -1.9603e-04,  1.3623e-03,  4.4836e-04,\n",
       "          1.1972e-03,  1.3751e-04,  2.5535e-04,  1.2410e-03,  8.4908e-04,\n",
       "         -8.1091e-04,  7.2184e-04,  8.4753e-04, -5.0800e-04,  1.0352e-04,\n",
       "          4.8247e-05,  8.1394e-05,  2.0316e-04, -6.4953e-04,  3.7543e-04,\n",
       "         -3.0553e-04,  3.1837e-04,  2.6164e-04, -3.7246e-05, -2.3254e-04,\n",
       "         -1.1952e-03,  2.2944e-04, -2.9401e-04, -8.1035e-05,  3.8096e-04,\n",
       "         -7.2322e-04,  1.4235e-04, -5.0597e-05,  6.0897e-04, -7.3677e-04,\n",
       "         -4.8604e-05,  6.1957e-04, -5.3849e-05, -2.2309e-03, -2.0926e-04,\n",
       "          1.5279e-03,  7.6426e-04, -3.5743e-04, -1.3062e-03, -1.7903e-05,\n",
       "         -9.9922e-04,  4.9754e-04,  8.1403e-05,  6.8694e-04,  1.3580e-04,\n",
       "          6.7742e-04, -1.2586e-03, -3.6853e-04,  1.2987e-03, -6.6745e-04,\n",
       "          5.5158e-04, -4.3895e-04,  2.3167e-04,  5.8675e-04,  2.6429e-04,\n",
       "          2.7038e-04, -1.7901e-03,  4.8022e-05,  7.1756e-04,  2.0249e-04,\n",
       "         -1.5908e-03,  8.9479e-04,  9.2792e-04,  9.1996e-04,  8.4281e-04,\n",
       "          6.1710e-04,  1.1558e-03,  2.0288e-05, -7.1253e-04, -1.2792e-03,\n",
       "          1.4605e-03, -8.4250e-04,  3.4713e-04, -5.1910e-05,  2.3476e-04,\n",
       "         -2.2314e-04, -1.0003e-03, -1.2892e-04, -6.1734e-04, -3.2950e-04,\n",
       "          4.0165e-04,  1.1637e-04,  3.1058e-06,  4.5949e-04,  1.2583e-03,\n",
       "         -1.3025e-03,  7.2172e-05,  6.5306e-04,  1.5785e-03, -4.4538e-04,\n",
       "          4.0680e-04,  9.3226e-04, -1.9534e-04,  4.1834e-04,  5.7744e-04,\n",
       "         -5.6714e-04, -8.4446e-05, -2.6932e-04,  1.8279e-04, -1.3329e-04,\n",
       "          1.0872e-03, -1.0100e-03,  7.3224e-04,  6.1025e-04, -1.1460e-03,\n",
       "          3.2876e-04, -6.5620e-04,  1.0706e-03, -7.7553e-04, -2.7093e-04,\n",
       "          8.4196e-04, -1.3258e-03,  7.1273e-04,  1.3507e-04, -1.3592e-03,\n",
       "         -5.6831e-05,  8.2173e-04, -7.8866e-04,  1.5439e-03, -1.4099e-03,\n",
       "         -4.3447e-04, -2.0427e-04,  6.7479e-04,  3.2638e-04, -8.5399e-04,\n",
       "          7.4594e-05,  9.0323e-04, -1.5098e-05,  5.2700e-04, -4.7706e-04,\n",
       "          1.0632e-03,  9.1166e-04,  2.0917e-03,  3.5908e-04,  3.7218e-04,\n",
       "          6.9631e-04, -4.5619e-04, -1.8139e-04,  9.5960e-04,  3.8350e-04,\n",
       "          8.6880e-05, -1.8742e-05, -4.1829e-04,  1.0841e-03,  2.6442e-04,\n",
       "          9.1673e-04,  7.9619e-04,  2.1840e-04,  1.3015e-03,  7.0227e-04,\n",
       "          7.0186e-04, -2.0687e-04, -7.4703e-04, -3.2702e-04, -7.2957e-04,\n",
       "          1.3421e-03,  6.7977e-05,  2.7181e-04, -4.4912e-06, -4.4922e-04,\n",
       "         -6.5312e-04,  2.3912e-04,  6.1900e-04, -8.8884e-04,  2.8374e-04,\n",
       "         -3.0761e-04, -1.5532e-03,  8.2068e-04, -7.3194e-04,  4.6527e-04,\n",
       "          4.1667e-04, -1.7607e-04, -1.1090e-03,  1.2000e-04, -3.3708e-04,\n",
       "         -1.3504e-03, -2.7047e-04,  6.5228e-05,  7.1382e-04,  4.1380e-04,\n",
       "         -8.7489e-04, -9.0296e-04, -1.2223e-03, -8.4464e-04,  8.9385e-04,\n",
       "         -8.8267e-04,  3.5387e-05, -3.7731e-04, -2.1966e-04, -4.9647e-04,\n",
       "         -4.1372e-04, -5.6462e-04, -4.0466e-04, -7.0373e-04, -3.5127e-05,\n",
       "         -2.3559e-04,  3.2125e-04, -1.0309e-03,  7.3264e-04,  6.7204e-04,\n",
       "         -9.7392e-04, -1.4915e-03,  3.2139e-04,  2.5929e-04,  9.7745e-05,\n",
       "         -1.6555e-04, -3.8292e-04, -2.1123e-04,  1.2693e-03, -2.6279e-04,\n",
       "          4.6657e-04, -6.4830e-05, -1.4545e-04, -1.9174e-03, -4.2756e-04,\n",
       "         -3.9727e-04, -7.3876e-04,  5.5474e-05, -1.0293e-04, -3.4245e-04,\n",
       "         -5.1097e-04,  1.3053e-04,  2.1279e-04, -9.2556e-04, -5.7658e-04,\n",
       "         -1.0098e-03, -1.6727e-03,  5.9875e-05, -6.7845e-04, -7.2156e-04,\n",
       "         -7.9899e-04,  8.8845e-04,  4.6655e-04, -7.9180e-05, -1.1921e-03,\n",
       "          2.6180e-04, -1.0050e-03,  1.0159e-04, -2.0174e-04,  2.4170e-04,\n",
       "         -1.8664e-05,  5.4935e-04, -2.0788e-04, -8.9495e-05,  1.1413e-03,\n",
       "          5.7335e-04,  2.9020e-05, -1.0915e-03,  1.2753e-03,  3.7467e-04,\n",
       "         -5.7608e-04, -4.7631e-04,  6.3283e-04,  7.3182e-04, -5.8975e-04,\n",
       "         -6.7256e-04, -6.5000e-04,  4.6836e-04,  2.1565e-04, -1.1884e-04,\n",
       "         -4.5613e-05, -1.0522e-03,  1.4065e-03, -3.4673e-04,  9.1401e-04,\n",
       "         -5.3963e-04,  3.2495e-04, -1.8496e-04, -3.5440e-04, -5.3652e-04,\n",
       "          1.1731e-03,  4.6869e-04,  5.2978e-05, -3.8036e-04, -9.0752e-04,\n",
       "         -5.3709e-04,  1.3828e-03, -6.2401e-04, -2.5071e-04, -1.0468e-03,\n",
       "         -4.1277e-04,  1.2892e-03, -1.0668e-03, -5.0647e-04, -1.8107e-04,\n",
       "         -1.2714e-04, -4.8547e-04,  6.4900e-04, -2.5868e-04, -9.7693e-04,\n",
       "          2.4540e-06, -1.0958e-03,  6.1819e-05,  6.8470e-04,  1.0745e-03,\n",
       "         -1.2694e-03,  4.4143e-04, -3.0956e-04,  3.6485e-04, -5.6330e-04,\n",
       "         -7.1612e-04, -7.0977e-04,  1.5660e-04, -1.5002e-03,  4.4592e-04,\n",
       "         -2.8502e-04, -1.1168e-03,  1.8970e-05,  2.7814e-04,  1.5484e-03,\n",
       "          3.4298e-04,  7.4382e-04, -1.2277e-03,  9.5325e-04,  3.6420e-04,\n",
       "         -8.0097e-04, -9.8498e-05,  1.1936e-03,  6.1123e-04,  2.0385e-04,\n",
       "          7.3572e-04, -7.4903e-04, -1.7093e-04, -1.3822e-03,  2.7139e-04,\n",
       "         -4.5825e-04,  3.4927e-05,  2.3176e-04,  1.0734e-03,  1.2674e-03,\n",
       "         -3.7664e-04,  1.9435e-03, -8.8220e-04,  1.0310e-03, -7.7865e-05,\n",
       "          1.2554e-03,  1.1072e-03,  1.1751e-04, -1.6405e-04, -8.1201e-04,\n",
       "         -9.0235e-04,  9.2547e-05, -5.7771e-04, -3.1717e-04,  2.5763e-04,\n",
       "          6.7480e-05,  8.1625e-04,  1.1566e-03, -4.4597e-04, -9.5578e-04,\n",
       "         -8.4381e-04,  4.6470e-04,  2.7711e-05,  1.5125e-03,  2.2702e-05,\n",
       "          4.1230e-04,  5.3662e-04]),\n",
       " 'transformer.resblocks.1.ln_1.weight': tensor([0.9967, 1.0000, 0.9997, 1.0014, 0.9985, 0.9996, 1.0016, 0.9990, 0.9998,\n",
       "         1.0000, 1.0002, 1.0017, 1.0005, 0.9994, 0.9999, 0.9989, 1.0002, 1.0002,\n",
       "         0.9991, 0.9988, 1.0009, 0.9999, 0.9993, 1.0016, 0.9989, 1.0013, 1.0007,\n",
       "         1.0019, 1.0029, 0.9985, 1.0009, 1.0010, 0.9983, 1.0010, 1.0017, 1.0006,\n",
       "         1.0007, 0.9998, 0.9972, 0.9990, 0.9989, 1.0002, 1.0012, 1.0002, 1.0012,\n",
       "         1.0001, 1.0024, 1.0006, 1.0029, 1.0016, 1.0015, 0.9977, 1.0016, 0.9992,\n",
       "         1.0009, 1.0019, 0.9990, 1.0013, 0.9998, 0.9999, 1.0001, 1.0011, 0.9978,\n",
       "         1.0002, 1.0008, 0.9992, 1.0004, 1.0024, 1.0006, 1.0002, 0.9977, 0.9997,\n",
       "         1.0003, 0.9985, 0.9989, 1.0013, 1.0021, 0.9997, 1.0011, 0.9999, 0.9990,\n",
       "         0.9995, 0.9992, 1.0035, 1.0020, 0.9979, 1.0016, 1.0020, 0.9961, 1.0026,\n",
       "         1.0001, 1.0015, 0.9979, 0.9993, 0.9996, 1.0000, 1.0009, 1.0006, 0.9993,\n",
       "         1.0014, 1.0000, 1.0001, 1.0004, 1.0020, 0.9998, 1.0012, 0.9997, 0.9985,\n",
       "         0.9993, 1.0016, 1.0003, 1.0016, 1.0006, 1.0009, 0.9990, 1.0014, 0.9996,\n",
       "         1.0011, 1.0008, 1.0022, 1.0016, 0.9995, 0.9976, 0.9995, 0.9991, 1.0005,\n",
       "         1.0002, 1.0004, 0.9968, 1.0006, 0.9976, 1.0017, 1.0016, 1.0025, 1.0016,\n",
       "         0.9997, 0.9988, 1.0013, 0.9996, 1.0006, 0.9969, 0.9995, 1.0002, 1.0015,\n",
       "         0.9989, 1.0010, 0.9989, 1.0001, 1.0018, 0.9992, 0.9999, 1.0009, 1.0011,\n",
       "         1.0012, 0.9998, 1.0014, 0.9992, 1.0011, 0.9982, 1.0045, 1.0003, 1.0002,\n",
       "         1.0048, 0.9972, 1.0015, 1.0034, 0.9996, 0.9999, 0.9986, 1.0022, 0.9995,\n",
       "         1.0001, 1.0023, 1.0004, 0.9993, 1.0014, 1.0006, 0.9997, 0.9998, 1.0011,\n",
       "         0.9985, 0.9987, 0.9991, 0.9996, 1.0006, 0.9998, 1.0005, 0.9995, 1.0001,\n",
       "         0.9979, 1.0059, 0.9991, 0.9994, 0.9986, 1.0007, 1.0028, 1.0007, 1.0004,\n",
       "         0.9994, 1.0014, 1.0006, 0.9991, 1.0000, 1.0016, 1.0013, 0.9996, 1.0005,\n",
       "         1.0000, 1.0023, 1.0020, 0.9985, 1.0009, 1.0022, 1.0026, 0.9980, 1.0003,\n",
       "         0.9991, 0.9993, 1.0002, 0.9991, 1.0006, 1.0001, 0.9987, 1.0003, 1.0000,\n",
       "         0.9995, 1.0007, 0.9991, 1.0011, 1.0026, 0.9998, 1.0007, 1.0018, 0.9936,\n",
       "         0.9992, 1.0031, 1.0018, 0.9982, 1.0009, 1.0018, 0.9971, 1.0009, 1.0014,\n",
       "         0.9981, 1.0009, 1.0011, 0.9988, 0.9993, 0.9986, 0.9987, 1.0024, 1.0007,\n",
       "         0.9999, 1.0003, 0.9973, 1.0022, 1.0038, 1.0003, 0.9988, 0.9991, 1.0010,\n",
       "         0.9989, 1.0004, 1.0021, 1.0005, 1.0003, 0.9998, 0.9973, 0.9988, 1.0003,\n",
       "         1.0016, 1.0030, 0.9983, 1.0009, 1.0010, 0.9988, 1.0005, 1.0006, 0.9994,\n",
       "         1.0012, 1.0013, 1.0007, 0.9985, 1.0009, 0.9982, 1.0007, 1.0019, 1.0006,\n",
       "         0.9988, 0.9990, 0.9989, 0.9993, 1.0009, 1.0026, 1.0014, 1.0001, 1.0015,\n",
       "         0.9986, 1.0009, 1.0025, 0.9988, 0.9949, 0.9989, 0.9999, 0.9996, 0.9984,\n",
       "         1.0002, 0.9997, 1.0000, 1.0032, 0.9993, 1.0008, 0.9979, 0.9992, 1.0014,\n",
       "         0.9985, 1.0018, 1.0009, 1.0003, 1.0000, 0.9973, 1.0001, 0.9983, 1.0005,\n",
       "         1.0040, 0.9994, 0.9993, 0.9991, 0.9987, 1.0021, 0.9992, 0.9993, 1.0010,\n",
       "         1.0032, 1.0019, 0.9954, 1.0003, 0.9996, 0.9997, 1.0009, 1.0010, 0.9993,\n",
       "         1.0004, 1.0022, 0.9981, 1.0003, 0.9970, 0.9983, 1.0006, 0.9978, 1.0010,\n",
       "         0.9986, 0.9980, 1.0003, 1.0017, 1.0012, 1.0019, 0.9984, 0.9985, 0.9976,\n",
       "         1.0012, 1.0001, 0.9996, 1.0035, 1.0004, 1.0003, 1.0017, 1.0011, 1.0013,\n",
       "         0.9999, 1.0005, 0.9967, 1.0000, 1.0026, 1.0025, 0.9997, 0.9987, 1.0006,\n",
       "         1.0010, 1.0019, 1.0012, 0.9997, 0.9987, 1.0004, 1.0031, 0.9997, 0.9997,\n",
       "         1.0019, 1.0010, 1.0001, 1.0009, 1.0023, 0.9978, 1.0001, 1.0011, 0.9996,\n",
       "         1.0007, 1.0026, 0.9970, 1.0012, 0.9972, 1.0009, 1.0000, 1.0033, 1.0006,\n",
       "         1.0025, 1.0002, 0.9973, 0.9998, 1.0001, 1.0011, 1.0026, 1.0011, 1.0003,\n",
       "         0.9997, 1.0007, 1.0006, 1.0018, 1.0004, 0.9994, 1.0003, 0.9998, 1.0015,\n",
       "         1.0012, 0.9964, 1.0016, 1.0011, 1.0002, 1.0012, 1.0002, 0.9995, 1.0027,\n",
       "         1.0002, 1.0016, 0.9995, 0.9990, 1.0018, 0.9990, 1.0008, 1.0009, 1.0012,\n",
       "         0.9986, 1.0001, 0.9994, 0.9993, 1.0011, 0.9991, 0.9990, 1.0001, 0.9989,\n",
       "         0.9983, 1.0000, 1.0001, 1.0022, 1.0015, 0.9993, 0.9996, 1.0028, 0.9999,\n",
       "         0.9997, 1.0015, 0.9993, 0.9994, 1.0007, 0.9991, 1.0011, 1.0013, 0.9995,\n",
       "         0.9976, 1.0012, 1.0044, 1.0025, 0.9989, 0.9992, 1.0000, 0.9999, 1.0028,\n",
       "         0.9993, 0.9977, 1.0018, 0.9992, 1.0004, 0.9997, 0.9975, 1.0003, 1.0005,\n",
       "         1.0003, 1.0031, 0.9999, 1.0013, 1.0000, 0.9996, 1.0002, 0.9993, 0.9996,\n",
       "         0.9988, 0.9937, 0.9996, 0.9997, 1.0004, 1.0010, 1.0007, 0.9999, 0.9998,\n",
       "         1.0004, 1.0011, 0.9994, 1.0010, 0.9968, 1.0013, 0.9993, 0.9984]),\n",
       " 'transformer.resblocks.1.ln_1.bias': tensor([-1.1046e-03, -3.1054e-03, -6.6932e-04,  4.5530e-04, -1.1702e-04,\n",
       "         -2.0908e-03, -7.7018e-04, -5.0672e-04,  2.0244e-03,  7.8007e-04,\n",
       "         -3.6512e-04, -4.8029e-04, -3.0810e-03, -1.1465e-04, -5.1986e-04,\n",
       "          1.8198e-03,  1.5890e-03,  2.2555e-04, -1.2895e-04,  5.3938e-04,\n",
       "         -1.1830e-03, -5.7388e-05,  5.1957e-04, -3.7690e-04, -1.2176e-03,\n",
       "         -2.8478e-04,  3.2485e-04, -1.1839e-03,  5.4946e-04,  1.0431e-03,\n",
       "         -1.3429e-04,  1.2417e-03,  2.1433e-03,  1.5648e-03, -1.3753e-03,\n",
       "         -5.2844e-04,  1.2163e-03,  5.1388e-04, -5.9141e-04,  1.5163e-03,\n",
       "          9.3875e-04,  8.2068e-04,  6.4773e-04, -1.1880e-03,  9.7963e-04,\n",
       "         -1.4660e-03, -4.6368e-04,  6.4859e-04, -4.1412e-04,  1.2170e-03,\n",
       "         -3.4726e-04,  5.7276e-04,  5.3979e-04,  7.8049e-04,  1.7615e-04,\n",
       "         -3.9187e-04, -2.0501e-03, -1.3484e-03, -1.0493e-03, -1.1369e-03,\n",
       "         -1.6068e-03, -9.4048e-04,  9.4362e-04, -1.2171e-03, -7.5002e-04,\n",
       "         -1.3821e-03,  2.6425e-04, -8.7452e-04, -3.7438e-06, -1.6616e-03,\n",
       "         -2.5639e-04,  9.9614e-04, -5.1156e-04, -7.4260e-05,  2.3105e-04,\n",
       "          4.5966e-04, -2.8476e-04, -8.1361e-04,  1.6494e-03,  7.9005e-04,\n",
       "          8.3056e-04, -1.6519e-04,  7.4502e-04,  2.4768e-04,  7.9063e-04,\n",
       "          3.6859e-04,  2.6522e-03,  1.3504e-03,  7.1521e-04,  1.8554e-03,\n",
       "         -9.7081e-04, -2.7821e-04,  6.3400e-04,  3.4775e-04, -1.6589e-03,\n",
       "         -2.2290e-04, -1.1160e-03, -6.4676e-04, -2.4373e-03, -6.9192e-04,\n",
       "         -4.0484e-04, -4.4606e-04, -9.9687e-04, -1.5560e-04,  5.9203e-04,\n",
       "         -2.2738e-03, -1.2503e-03, -2.0714e-04,  7.8123e-04,  3.8348e-04,\n",
       "          1.3394e-04,  1.4068e-03,  7.1620e-04,  1.1818e-03, -2.0303e-03,\n",
       "          2.7344e-04,  2.5474e-04,  8.6790e-04, -1.0103e-03,  2.8260e-03,\n",
       "          2.5937e-03,  1.9817e-04, -4.0565e-04, -5.0743e-04, -2.7203e-03,\n",
       "          3.5816e-05,  9.8082e-04,  6.2154e-05, -2.6693e-04, -6.4006e-04,\n",
       "         -1.2398e-03,  4.0824e-04, -3.1707e-04,  6.5160e-04, -1.8630e-03,\n",
       "         -2.4076e-03,  1.4212e-03, -1.5048e-03,  5.2906e-04,  9.7016e-04,\n",
       "         -1.2702e-04, -1.7248e-07,  7.9880e-04,  8.2787e-04,  2.7429e-04,\n",
       "          2.4488e-04,  7.8006e-04, -4.4065e-04, -5.7814e-04,  6.7771e-05,\n",
       "         -1.6224e-03,  1.2827e-03, -1.1840e-03,  8.2220e-04, -8.3254e-04,\n",
       "         -3.2138e-04,  9.2355e-04,  2.6418e-04,  1.3454e-04, -1.4224e-03,\n",
       "         -5.9561e-04, -4.5271e-04, -4.2212e-04,  2.7519e-04, -1.3478e-03,\n",
       "         -9.7182e-05, -9.1131e-05, -5.4034e-04,  7.8112e-04,  6.3276e-06,\n",
       "          3.2584e-04, -9.2576e-04,  9.0612e-04, -7.3965e-04,  1.1144e-03,\n",
       "          1.8505e-03,  3.1877e-04, -6.0852e-04,  4.4617e-04, -1.2671e-03,\n",
       "          1.0286e-03,  1.8037e-04,  1.1537e-03,  6.2178e-04,  1.1204e-03,\n",
       "          1.3291e-03, -8.5909e-05, -7.0596e-04,  1.3475e-03, -2.9936e-04,\n",
       "         -1.7502e-03, -2.8569e-04, -1.3905e-03,  1.4310e-03, -1.8005e-04,\n",
       "          1.0229e-03, -7.6908e-04, -6.9239e-04, -1.3111e-03,  1.0798e-04,\n",
       "          1.4656e-03,  4.6243e-04,  9.7533e-04, -6.6932e-04, -3.4913e-04,\n",
       "         -3.7348e-04, -6.2349e-04, -6.0680e-04, -6.9110e-04, -1.0667e-03,\n",
       "          1.3844e-03,  1.2398e-03,  2.4380e-05, -1.3949e-03, -1.3027e-04,\n",
       "          1.3500e-03, -2.6697e-04, -3.0181e-04, -1.1084e-03,  8.7702e-05,\n",
       "         -5.1227e-05, -8.7122e-04, -3.9561e-04,  1.0321e-03, -1.7078e-05,\n",
       "         -4.1396e-04,  8.0909e-04,  4.2218e-04,  5.2829e-04, -1.0767e-03,\n",
       "         -3.1343e-04,  3.0145e-03,  6.6299e-04, -1.2023e-04,  9.4666e-04,\n",
       "          1.2106e-03,  1.6888e-03,  1.2782e-03, -4.7497e-04, -1.9197e-03,\n",
       "          9.1823e-04,  5.8141e-04,  1.9240e-04,  2.0661e-04,  2.1830e-03,\n",
       "         -4.4051e-04, -7.2977e-04,  1.9252e-03, -7.9451e-04, -5.8106e-04,\n",
       "          1.3277e-03,  1.7498e-04, -4.5632e-04,  4.6932e-05, -1.4923e-03,\n",
       "          6.3882e-05,  2.5686e-03, -1.5367e-03,  2.8462e-04,  1.4223e-03,\n",
       "          1.5309e-04,  1.1930e-04, -2.1085e-04,  1.0138e-03, -2.1614e-03,\n",
       "         -2.7899e-04,  1.6737e-03, -1.5012e-03,  4.5826e-05,  8.4009e-04,\n",
       "          1.2021e-03, -1.2082e-03,  3.9659e-04, -6.9616e-04,  2.2211e-04,\n",
       "         -2.6008e-03, -1.7633e-03, -1.6673e-03, -1.0106e-03,  1.0561e-03,\n",
       "          6.0682e-04,  9.6671e-04, -3.5161e-04, -3.0367e-04,  2.0035e-03,\n",
       "          6.1572e-05, -1.1929e-04, -1.1398e-03, -1.5061e-03, -7.4692e-04,\n",
       "         -2.4590e-05,  1.0710e-03,  8.9456e-05,  1.8068e-03,  2.0307e-04,\n",
       "         -8.8116e-04, -6.0159e-04,  5.8491e-04, -1.8746e-03,  1.2827e-03,\n",
       "         -5.0117e-04, -9.9861e-04, -7.0812e-04, -1.6646e-04, -1.7091e-03,\n",
       "          3.0853e-04, -1.0811e-03,  9.2810e-04, -9.2750e-04,  1.4872e-03,\n",
       "         -2.6892e-03,  7.1342e-04, -1.5711e-03,  6.3471e-04,  1.5700e-03,\n",
       "          2.0587e-03, -2.1495e-04,  1.3122e-03,  7.6859e-04,  1.3097e-03,\n",
       "          4.0069e-04,  1.3089e-03, -4.1580e-04, -9.2977e-04, -5.3965e-04,\n",
       "          1.8715e-04,  1.0572e-03,  3.5791e-04, -2.5673e-04, -7.2434e-04,\n",
       "          2.1634e-03, -1.1418e-03, -1.4714e-04, -6.4496e-04, -7.3982e-04,\n",
       "          1.1302e-03,  8.9157e-04,  1.5177e-03,  1.3970e-03, -7.1988e-04,\n",
       "          6.8486e-04,  5.8826e-04,  8.7287e-05, -1.8633e-05,  1.2269e-04,\n",
       "          1.1819e-03,  2.0959e-04,  1.3513e-03,  1.0171e-03,  8.5133e-04,\n",
       "          7.9019e-04, -1.8992e-03,  6.9188e-05, -2.0090e-04, -1.4947e-05,\n",
       "          8.2296e-04,  1.7394e-03, -3.9870e-04, -1.0389e-03, -4.6902e-04,\n",
       "          1.0807e-04,  7.1329e-04,  2.0870e-04, -9.9298e-04,  9.6324e-04,\n",
       "          2.5057e-05, -1.5838e-03, -2.0620e-03,  1.1021e-03, -1.1191e-03,\n",
       "          5.8174e-05, -7.2935e-05, -3.1161e-04,  2.1455e-03, -1.0849e-03,\n",
       "          9.3084e-04,  5.1715e-04, -1.4936e-03,  1.1985e-03,  9.1074e-04,\n",
       "          1.3503e-03,  2.6016e-03,  1.3006e-03,  1.1834e-03,  1.3247e-03,\n",
       "          2.9021e-04, -9.3421e-05, -2.0477e-03,  7.0757e-04,  1.9188e-03,\n",
       "         -7.4537e-04,  2.7545e-03,  2.0402e-04,  1.8916e-03, -1.4743e-04,\n",
       "          1.1998e-04, -6.0622e-04, -2.9626e-04,  9.7106e-04, -1.2750e-03,\n",
       "         -8.1409e-04, -4.2229e-04,  4.1747e-04, -3.8903e-03, -6.7353e-04,\n",
       "         -6.1084e-05,  4.4722e-04,  1.5943e-04, -6.0266e-04,  7.7707e-04,\n",
       "         -4.7805e-06,  1.7994e-03,  4.2888e-05,  5.9057e-04, -1.9053e-04,\n",
       "         -1.3595e-03,  1.0693e-03, -2.5960e-05,  9.2632e-04,  3.4613e-05,\n",
       "          4.1768e-04,  1.8472e-04,  1.1580e-04, -1.0089e-03,  1.7734e-03,\n",
       "         -1.2855e-03, -1.2566e-03,  1.1434e-03, -2.9101e-04, -3.0624e-04,\n",
       "         -6.6592e-04, -6.2216e-04,  8.8469e-04,  8.8327e-05, -6.9409e-05,\n",
       "         -1.6552e-04, -7.3188e-04,  4.2729e-04, -1.0077e-03,  7.0889e-04,\n",
       "          5.2830e-04,  1.4502e-03,  6.2777e-04, -2.9698e-04,  6.1219e-04,\n",
       "          4.9949e-04, -2.1342e-04,  2.6530e-03, -1.7964e-03, -1.5057e-03,\n",
       "         -1.1545e-03,  8.4841e-04,  1.2475e-03,  8.4561e-04, -1.0021e-04,\n",
       "         -2.1901e-04,  2.2175e-03, -7.0654e-04,  4.0203e-04, -2.2336e-04,\n",
       "          1.2369e-03, -6.6010e-04,  4.5980e-04, -9.2627e-05, -1.9773e-03,\n",
       "         -1.6814e-04, -5.0174e-04,  2.1548e-03, -5.1006e-04,  1.7838e-04,\n",
       "         -4.4361e-04, -1.4091e-03, -9.7264e-04, -6.4662e-04,  8.1110e-04,\n",
       "         -5.1362e-04,  1.9722e-03,  4.3724e-04, -5.4441e-04, -1.5291e-04,\n",
       "          7.8483e-05,  1.0694e-03, -4.7156e-04, -6.5073e-04, -1.0428e-03,\n",
       "          8.3004e-04, -1.1166e-03,  3.3179e-04,  1.0235e-03,  8.4541e-04,\n",
       "         -2.9949e-03, -4.8472e-04,  1.0419e-03, -9.0853e-04,  7.7575e-04,\n",
       "          1.1226e-03,  1.0638e-03,  7.0678e-04,  1.3690e-03,  4.5288e-05,\n",
       "          8.6883e-04, -1.9236e-04, -2.0240e-03,  7.8285e-04,  2.5963e-04,\n",
       "          1.4235e-03, -8.2377e-04, -1.2681e-03, -1.4955e-03,  6.9451e-05,\n",
       "         -2.0471e-03,  3.0484e-04]),\n",
       " 'transformer.resblocks.1.mlp.c_fc.weight': tensor([[ 0.0288,  0.0415, -0.0304,  ..., -0.0044, -0.0503, -0.0120],\n",
       "         [-0.0554, -0.0274, -0.0496,  ...,  0.0449,  0.0816,  0.0139],\n",
       "         [ 0.0336, -0.0324, -0.0257,  ...,  0.0171, -0.0412, -0.0335],\n",
       "         ...,\n",
       "         [-0.0750,  0.0535,  0.0232,  ..., -0.0054, -0.0533, -0.0473],\n",
       "         [-0.0116,  0.0088,  0.0338,  ..., -0.0057, -0.0123,  0.0154],\n",
       "         [-0.0250, -0.0119,  0.0244,  ...,  0.0419, -0.0143,  0.0039]]),\n",
       " 'transformer.resblocks.1.mlp.c_fc.bias': tensor([-0.0230,  0.0081, -0.0118,  ...,  0.0264, -0.0137,  0.0386]),\n",
       " 'transformer.resblocks.1.mlp.c_proj.weight': tensor([[ 9.8953e-04,  5.9639e-05,  7.3232e-03,  ...,  4.4759e-05,\n",
       "           3.5091e-03,  8.1685e-03],\n",
       "         [-4.3447e-03, -8.7134e-03, -1.1988e-02,  ...,  3.7066e-03,\n",
       "          -3.8676e-03,  9.9153e-03],\n",
       "         [ 2.6333e-03,  3.2195e-03,  9.1827e-03,  ...,  2.1357e-03,\n",
       "          -5.8614e-03,  8.9763e-03],\n",
       "         ...,\n",
       "         [ 1.2765e-02, -1.3748e-02,  8.5348e-04,  ..., -1.7411e-02,\n",
       "           2.0333e-02,  9.5142e-04],\n",
       "         [ 5.9059e-03, -1.8698e-02, -6.7441e-03,  ...,  7.2327e-03,\n",
       "           8.7717e-03, -4.5567e-03],\n",
       "         [-5.2666e-03,  3.9976e-03,  2.1196e-03,  ...,  1.4854e-02,\n",
       "           2.0703e-02, -5.3053e-03]]),\n",
       " 'transformer.resblocks.1.mlp.c_proj.bias': tensor([-1.2552e-02,  1.5178e-03,  4.7129e-03, -1.3544e-02,  1.4004e-02,\n",
       "          1.1505e-02, -3.9763e-03, -4.7296e-03, -2.1417e-02,  1.1577e-02,\n",
       "          1.5547e-02, -5.3428e-03, -1.9047e-02, -1.1564e-05,  1.2991e-02,\n",
       "          8.8063e-03, -1.1849e-02,  1.5420e-02, -1.8830e-02,  4.3607e-03,\n",
       "          6.0272e-03, -1.4898e-02, -1.5738e-02, -1.5821e-02, -5.7050e-03,\n",
       "         -9.6674e-03, -8.6855e-03,  1.3227e-03, -1.8688e-02, -2.0295e-03,\n",
       "          8.8019e-03,  9.6632e-03,  2.1485e-02, -1.7951e-02, -9.4753e-03,\n",
       "         -2.0934e-02,  1.2740e-03, -6.5856e-03,  1.7475e-02,  1.8781e-02,\n",
       "         -2.2495e-02, -3.0251e-03, -1.2490e-02,  8.0612e-03,  8.3916e-03,\n",
       "         -1.1011e-02,  1.5231e-03, -1.7024e-02, -2.1721e-03, -1.2556e-02,\n",
       "         -2.1806e-03,  9.0133e-03, -1.8912e-02, -1.4266e-02,  1.1383e-02,\n",
       "         -2.3261e-04,  7.8287e-04, -5.9371e-03, -1.8131e-02,  1.0878e-02,\n",
       "          1.2276e-02, -1.3105e-02, -1.0729e-02,  1.9080e-03,  4.8878e-03,\n",
       "          1.0750e-02, -1.8828e-02,  8.0655e-03,  1.0157e-02, -1.8427e-02,\n",
       "          3.8381e-03, -1.1786e-02, -1.2972e-02,  1.8793e-02, -2.4444e-03,\n",
       "          1.3157e-02,  7.1091e-04, -1.3650e-02,  2.2730e-02,  5.8246e-03,\n",
       "         -8.0756e-03, -2.2136e-03, -1.7172e-02,  1.1444e-02, -1.4251e-02,\n",
       "         -1.2396e-02, -3.5397e-03,  2.5896e-03,  3.8783e-03, -1.8975e-02,\n",
       "          1.4311e-02, -1.8547e-02, -7.7287e-03,  1.2525e-02,  3.5587e-03,\n",
       "         -5.3268e-03,  5.3298e-03, -1.6876e-02,  1.4006e-02, -1.6565e-02,\n",
       "          1.0538e-02,  1.3293e-02,  1.2022e-02,  1.3728e-02,  2.0026e-02,\n",
       "         -4.0816e-03, -1.6818e-02,  3.8781e-03, -1.1276e-02,  1.0549e-02,\n",
       "         -3.7817e-03,  7.3295e-03, -9.5695e-03, -6.9872e-03,  2.0771e-02,\n",
       "         -2.2632e-03,  3.3699e-03,  2.1698e-03,  1.4791e-02,  1.9799e-02,\n",
       "          1.9343e-02,  5.8462e-03, -1.0707e-02,  1.7260e-02,  1.8500e-02,\n",
       "          1.0030e-02,  1.1501e-02, -1.1773e-02, -6.2760e-03,  6.6463e-03,\n",
       "         -1.8152e-02,  1.0965e-02, -1.7286e-02, -1.5873e-02, -2.1405e-02,\n",
       "          5.0413e-04, -4.2615e-03,  8.9802e-04,  1.4593e-02,  1.0389e-02,\n",
       "         -3.3529e-03, -1.4475e-02,  9.2309e-03, -3.1365e-03,  6.3959e-03,\n",
       "         -9.5858e-03,  2.9811e-03,  2.0824e-02,  2.2679e-02, -1.1187e-02,\n",
       "         -2.0116e-02,  1.5652e-02,  9.8173e-03, -3.4304e-03,  7.8777e-03,\n",
       "         -6.4079e-03,  1.8038e-03,  1.7989e-02, -2.2355e-03,  1.4973e-02,\n",
       "          2.0088e-02,  1.0047e-02,  1.2108e-02, -1.4833e-02,  1.3458e-02,\n",
       "         -3.8972e-03,  1.0305e-02,  1.8020e-03, -1.5366e-02, -2.1792e-02,\n",
       "         -3.4767e-03, -9.7686e-04, -1.9731e-02,  1.7126e-02, -1.8181e-03,\n",
       "          1.4809e-02,  6.9518e-03,  1.8649e-02, -4.4772e-03,  1.3954e-02,\n",
       "         -5.5424e-04,  8.5287e-03, -9.8983e-03, -1.7911e-02, -2.1258e-02,\n",
       "          1.4199e-02, -6.0674e-03,  1.3409e-02, -1.1570e-02,  2.1307e-03,\n",
       "          3.3063e-03, -9.4288e-03, -1.6165e-03, -9.2380e-03, -5.4340e-03,\n",
       "         -8.8310e-03, -6.9176e-03, -1.9024e-02,  1.8324e-02,  3.2193e-03,\n",
       "          7.5730e-03,  1.9773e-02,  4.2848e-03, -1.5599e-02,  4.9172e-03,\n",
       "         -1.2600e-02, -8.9543e-03, -6.6536e-03, -6.5685e-03, -2.3068e-04,\n",
       "         -1.6960e-02,  1.8786e-02,  4.4473e-03, -7.9454e-03, -1.9205e-02,\n",
       "         -6.4650e-03, -4.6060e-03,  8.6991e-03, -8.2429e-03,  1.6875e-02,\n",
       "          1.4933e-02,  1.8862e-02,  7.3228e-03, -1.6638e-02,  2.6916e-03,\n",
       "         -6.0206e-03, -5.2247e-03,  1.3736e-03,  6.0272e-03, -7.9295e-03,\n",
       "         -1.4232e-02,  1.9796e-02,  6.0285e-03,  1.8486e-02,  9.7183e-03,\n",
       "         -1.9438e-02,  1.7219e-02,  4.0359e-03,  2.0426e-02,  1.0735e-02,\n",
       "          7.0179e-03, -7.2895e-03,  9.1735e-03, -1.9936e-02,  1.7245e-02,\n",
       "          1.2122e-02, -1.6198e-02,  1.4168e-02, -4.2986e-03, -1.4570e-02,\n",
       "          2.0438e-02,  1.2803e-02, -1.3804e-02, -1.5284e-02,  1.6333e-02,\n",
       "         -5.5530e-03,  1.2190e-02,  6.5724e-03,  1.0041e-03, -6.6043e-03,\n",
       "          2.6850e-03,  2.0771e-02, -1.7656e-02, -2.3717e-02,  1.0856e-02,\n",
       "         -9.7858e-03, -9.8193e-03, -1.6320e-02,  1.3040e-02,  6.5482e-03,\n",
       "         -1.6223e-02, -3.6275e-03,  2.0853e-04,  1.6793e-02,  1.3024e-03,\n",
       "          2.8403e-03, -6.3195e-03, -1.5445e-02, -1.8172e-02,  6.0922e-03,\n",
       "          2.0878e-02,  1.9200e-02, -1.9502e-02, -1.9266e-02,  7.6684e-04,\n",
       "          1.8109e-02, -1.8796e-02,  9.6665e-03, -1.5045e-02,  2.0296e-02,\n",
       "         -1.4725e-02, -6.0194e-03,  1.8594e-02,  1.0848e-02, -1.6777e-02,\n",
       "         -1.7512e-02,  1.4560e-03, -9.9024e-03, -9.2415e-03,  4.3573e-04,\n",
       "          5.3215e-03,  3.2930e-03,  2.0380e-02,  1.6871e-02, -1.2925e-02,\n",
       "          3.2790e-03,  1.5025e-02,  7.5235e-03, -3.2110e-03, -4.1795e-03,\n",
       "         -3.8863e-03, -1.8779e-02, -1.4355e-02,  5.8031e-03,  1.4101e-02,\n",
       "          1.4760e-02, -1.5045e-03,  6.7505e-03,  8.7784e-03, -4.2731e-03,\n",
       "          2.0939e-02, -1.9507e-02, -1.3038e-02, -1.2368e-02,  2.0300e-02,\n",
       "          9.3385e-03, -2.7551e-03,  1.0820e-02,  1.1191e-02, -2.2257e-03,\n",
       "         -1.5723e-02, -7.5772e-03,  1.5513e-02,  2.1418e-02,  1.4977e-02,\n",
       "         -8.5432e-03,  1.6973e-02, -5.3634e-03,  1.2726e-03,  1.6480e-02,\n",
       "          1.1066e-02, -4.1585e-03, -4.8507e-03, -9.7299e-03, -1.3292e-02,\n",
       "          1.9633e-02, -6.7923e-03, -1.1923e-02, -2.0964e-02,  9.8989e-03,\n",
       "          1.0553e-02, -2.0967e-02, -1.0707e-02, -1.3329e-02, -1.5607e-02,\n",
       "          1.9256e-03, -2.0054e-02,  1.9554e-02,  2.7420e-03, -9.1376e-03,\n",
       "          1.9369e-02,  2.0149e-02, -4.5931e-03, -1.1046e-02,  1.0219e-02,\n",
       "         -8.2711e-03, -8.1176e-04,  1.5270e-03, -6.7804e-03,  1.7041e-03,\n",
       "          1.7638e-02, -2.2655e-03,  1.7384e-02, -4.4960e-03,  1.4367e-02,\n",
       "         -1.0011e-02,  3.5169e-03,  9.3199e-03, -6.2088e-03, -1.1552e-02,\n",
       "          1.2052e-02, -3.1756e-03,  2.7337e-03, -1.0527e-02, -2.1189e-02,\n",
       "          1.7900e-02, -1.2960e-02, -7.1024e-04, -9.1810e-03, -9.6799e-03,\n",
       "         -1.0655e-02, -2.2082e-02,  1.4847e-02, -1.0552e-02, -2.9725e-03,\n",
       "         -1.6496e-02, -7.7401e-03, -2.0055e-02,  5.7734e-03,  1.0101e-03,\n",
       "          1.2736e-02, -1.1875e-02, -1.4376e-02, -1.7306e-02,  3.5090e-03,\n",
       "         -2.0516e-02, -1.7520e-02, -1.2008e-02,  8.2425e-03,  8.4726e-03,\n",
       "          2.8114e-04,  1.9298e-02, -6.9560e-03, -1.4020e-02,  2.0484e-02,\n",
       "         -2.0304e-03, -8.7688e-03, -6.5122e-03,  3.4936e-03,  1.5862e-03,\n",
       "          1.1313e-02,  9.9250e-03,  1.1659e-02, -1.1217e-02, -2.0840e-02,\n",
       "         -1.1784e-03,  1.3519e-02, -8.4638e-03, -1.4876e-02, -1.0111e-02,\n",
       "          5.3046e-03,  8.6559e-03,  7.6758e-03,  2.1061e-02,  1.8709e-02,\n",
       "          1.9401e-02, -2.1122e-02, -6.4294e-03, -1.0811e-02, -2.0815e-02,\n",
       "          4.0658e-03,  4.1013e-03,  1.9165e-02, -8.5052e-03, -1.3964e-02,\n",
       "         -6.8839e-03,  7.4628e-03, -9.9590e-03, -8.8546e-03,  1.9545e-03,\n",
       "          1.2598e-02,  1.8478e-02,  6.9762e-03,  1.8813e-03,  2.0747e-02,\n",
       "         -2.1401e-02,  5.3541e-03, -8.6192e-03,  1.8894e-02,  1.8465e-03,\n",
       "          9.7852e-03,  1.7565e-02, -2.8449e-03,  1.6523e-02, -1.9129e-02,\n",
       "         -1.1034e-02,  2.0254e-02, -9.3505e-03,  4.8876e-03, -1.5838e-02,\n",
       "         -1.6048e-04,  1.4641e-02, -1.4339e-02,  1.4102e-02, -1.5941e-02,\n",
       "          2.0720e-04, -1.9605e-02, -1.8442e-02, -1.8927e-02, -8.9952e-03,\n",
       "          4.9509e-04, -1.0405e-02, -8.8046e-03,  1.7796e-02,  7.1814e-03,\n",
       "          6.5564e-03,  1.3150e-02,  9.2907e-03, -3.5211e-03, -1.8482e-02,\n",
       "         -1.8094e-02,  3.5503e-03, -1.5616e-02,  4.7652e-03,  8.8828e-03,\n",
       "          1.9963e-03, -7.2376e-03, -1.3153e-02,  9.0163e-03, -1.1443e-02,\n",
       "         -4.6033e-03, -1.5321e-04, -4.3359e-03,  2.0287e-02, -7.7023e-03,\n",
       "         -1.5479e-02, -1.7830e-02, -2.9146e-03, -6.0447e-03, -7.3886e-05,\n",
       "         -7.9130e-03, -1.1641e-02]),\n",
       " 'transformer.resblocks.1.ln_2.weight': tensor([1.0023, 1.0012, 1.0015, 1.0025, 1.0028, 0.9997, 1.0014, 1.0015, 1.0028,\n",
       "         1.0030, 0.9994, 1.0026, 1.0029, 1.0005, 1.0055, 1.0018, 0.9988, 1.0035,\n",
       "         1.0010, 1.0025, 0.9986, 1.0021, 0.9994, 1.0023, 1.0008, 1.0016, 1.0017,\n",
       "         1.0023, 1.0025, 1.0005, 0.9992, 0.9996, 1.0041, 1.0018, 0.9986, 1.0004,\n",
       "         1.0036, 1.0033, 1.0025, 1.0023, 1.0024, 1.0002, 1.0033, 1.0003, 1.0010,\n",
       "         1.0017, 1.0014, 1.0007, 1.0033, 1.0009, 1.0018, 1.0032, 1.0017, 0.9980,\n",
       "         1.0015, 1.0035, 1.0030, 1.0020, 1.0009, 1.0056, 1.0032, 1.0031, 0.9998,\n",
       "         1.0025, 0.9998, 1.0019, 1.0019, 1.0013, 1.0006, 1.0035, 0.9998, 1.0037,\n",
       "         1.0029, 1.0010, 1.0025, 1.0028, 1.0023, 1.0030, 1.0002, 1.0017, 1.0022,\n",
       "         1.0018, 1.0018, 1.0018, 1.0044, 0.9985, 1.0005, 1.0039, 0.9999, 1.0007,\n",
       "         1.0008, 1.0019, 1.0022, 1.0004, 1.0030, 1.0029, 1.0032, 0.9989, 1.0028,\n",
       "         1.0014, 1.0005, 1.0041, 1.0005, 1.0030, 1.0036, 1.0019, 0.9997, 0.9976,\n",
       "         1.0019, 1.0032, 1.0026, 1.0008, 1.0038, 1.0026, 1.0016, 1.0018, 1.0018,\n",
       "         1.0035, 1.0021, 1.0034, 1.0008, 1.0004, 0.9973, 1.0006, 1.0018, 1.0002,\n",
       "         0.9996, 1.0016, 1.0013, 0.9994, 1.0031, 1.0013, 1.0018, 1.0036, 1.0019,\n",
       "         1.0003, 1.0014, 1.0010, 0.9985, 1.0007, 1.0021, 1.0047, 0.9983, 0.9996,\n",
       "         1.0020, 1.0017, 1.0012, 1.0027, 1.0051, 1.0029, 1.0020, 1.0014, 1.0042,\n",
       "         0.9986, 1.0027, 1.0025, 0.9993, 0.9995, 1.0004, 1.0026, 1.0038, 0.9998,\n",
       "         0.9998, 0.9993, 1.0024, 1.0003, 1.0009, 1.0021, 1.0022, 1.0028, 1.0015,\n",
       "         1.0033, 1.0029, 1.0020, 1.0007, 1.0015, 1.0025, 1.0001, 1.0032, 1.0026,\n",
       "         1.0025, 1.0029, 1.0012, 1.0015, 1.0025, 1.0048, 1.0023, 1.0037, 1.0008,\n",
       "         1.0033, 1.0028, 1.0019, 1.0018, 1.0033, 1.0033, 0.9997, 1.0011, 1.0037,\n",
       "         1.0030, 1.0020, 1.0023, 1.0023, 0.9983, 1.0051, 1.0010, 1.0043, 1.0015,\n",
       "         1.0008, 0.9992, 1.0055, 1.0028, 1.0012, 1.0007, 1.0013, 1.0018, 1.0022,\n",
       "         1.0024, 1.0021, 1.0049, 1.0025, 1.0032, 1.0006, 1.0026, 1.0031, 1.0018,\n",
       "         1.0016, 0.9993, 1.0016, 1.0043, 1.0021, 1.0026, 0.9999, 1.0007, 1.0029,\n",
       "         1.0033, 1.0002, 1.0024, 0.9992, 1.0042, 0.9974, 1.0019, 1.0029, 1.0023,\n",
       "         1.0022, 0.9977, 1.0033, 1.0023, 1.0013, 1.0033, 1.0020, 1.0021, 1.0013,\n",
       "         1.0022, 1.0002, 0.9969, 1.0028, 1.0001, 1.0022, 1.0023, 1.0032, 1.0009,\n",
       "         1.0039, 1.0035, 1.0014, 1.0028, 1.0026, 1.0044, 1.0002, 1.0023, 1.0027,\n",
       "         1.0018, 1.0014, 1.0029, 1.0006, 1.0010, 1.0031, 1.0018, 0.9993, 1.0045,\n",
       "         1.0036, 1.0004, 1.0026, 1.0030, 1.0012, 1.0029, 1.0042, 1.0017, 1.0010,\n",
       "         1.0040, 0.9994, 1.0023, 0.9989, 1.0015, 1.0024, 1.0026, 1.0012, 1.0016,\n",
       "         1.0051, 1.0029, 1.0014, 1.0012, 0.9993, 1.0036, 1.0044, 1.0019, 1.0026,\n",
       "         1.0012, 1.0038, 1.0007, 1.0029, 1.0006, 1.0004, 1.0030, 0.9983, 1.0028,\n",
       "         1.0034, 1.0023, 1.0025, 1.0033, 1.0034, 1.0019, 1.0039, 1.0025, 1.0023,\n",
       "         1.0038, 1.0004, 1.0021, 0.9991, 1.0005, 1.0034, 0.9999, 1.0033, 0.9997,\n",
       "         0.9990, 1.0032, 1.0015, 1.0047, 0.9974, 1.0047, 1.0048, 1.0058, 1.0013,\n",
       "         0.9995, 1.0014, 1.0035, 1.0001, 1.0013, 1.0032, 1.0025, 1.0044, 1.0038,\n",
       "         1.0008, 1.0019, 1.0018, 1.0001, 1.0019, 1.0002, 1.0016, 1.0021, 1.0031,\n",
       "         1.0031, 1.0009, 1.0009, 0.9983, 1.0031, 1.0059, 1.0008, 1.0059, 1.0042,\n",
       "         1.0006, 1.0019, 1.0016, 1.0032, 1.0002, 1.0035, 1.0018, 1.0019, 1.0013,\n",
       "         1.0012, 1.0039, 1.0037, 1.0003, 1.0010, 1.0004, 1.0028, 1.0042, 1.0022,\n",
       "         1.0037, 1.0035, 1.0010, 1.0001, 1.0003, 0.9987, 0.9994, 0.9962, 1.0019,\n",
       "         1.0025, 1.0016, 1.0017, 1.0026, 1.0008, 1.0005, 1.0003, 1.0012, 0.9997,\n",
       "         1.0002, 1.0022, 0.9989, 1.0005, 1.0000, 1.0040, 1.0026, 1.0018, 1.0046,\n",
       "         1.0032, 1.0006, 1.0018, 1.0035, 1.0012, 0.9994, 1.0018, 1.0016, 0.9989,\n",
       "         1.0037, 1.0025, 1.0006, 1.0024, 1.0018, 1.0023, 1.0004, 1.0020, 1.0021,\n",
       "         1.0023, 1.0008, 1.0040, 1.0021, 1.0028, 1.0039, 1.0028, 1.0030, 1.0027,\n",
       "         1.0026, 1.0032, 1.0007, 1.0041, 1.0002, 1.0014, 1.0041, 1.0022, 1.0020,\n",
       "         0.9990, 1.0022, 1.0017, 1.0020, 1.0010, 1.0013, 1.0014, 0.9991, 1.0020,\n",
       "         1.0016, 1.0037, 1.0013, 1.0015, 1.0016, 1.0018, 1.0030, 0.9986, 1.0012,\n",
       "         1.0020, 1.0001, 0.9988, 1.0040, 1.0022, 1.0047, 1.0013, 1.0014, 1.0037,\n",
       "         1.0045, 1.0017, 1.0014, 1.0034, 1.0040, 1.0049, 1.0038, 1.0035, 1.0037,\n",
       "         0.9999, 1.0000, 1.0045, 1.0018, 1.0007, 1.0012, 1.0005, 1.0037, 1.0040,\n",
       "         1.0003, 1.0036, 1.0033, 1.0014, 1.0049, 0.9982, 1.0013, 1.0010, 0.9995,\n",
       "         0.9997, 0.9998, 1.0009, 1.0025, 1.0035, 1.0045, 1.0043, 1.0007]),\n",
       " 'transformer.resblocks.1.ln_2.bias': tensor([-1.7531e-04,  1.1325e-03,  8.5695e-05, -4.9151e-05, -1.9804e-04,\n",
       "          9.4031e-04,  7.3279e-04,  1.3407e-03,  7.8689e-04, -8.5933e-04,\n",
       "          6.8870e-05, -3.5730e-04,  4.5939e-04, -3.1089e-04,  7.7989e-04,\n",
       "         -8.0758e-04,  1.0036e-03, -5.9625e-05,  7.9493e-04,  1.2474e-03,\n",
       "         -1.8400e-04,  5.5826e-04, -1.9515e-03, -9.1973e-04, -7.4602e-05,\n",
       "         -2.1512e-04, -8.6247e-05,  6.8410e-04,  1.0594e-03, -8.9426e-04,\n",
       "         -1.2118e-03,  2.1502e-04, -1.9932e-03, -4.5484e-05, -4.6130e-06,\n",
       "          1.2479e-03, -5.9894e-04,  3.3832e-04,  1.0601e-03, -6.8471e-05,\n",
       "          1.9417e-04,  2.2735e-03, -5.0904e-04,  1.5236e-03, -1.5296e-03,\n",
       "          6.9803e-04,  6.4963e-04, -5.9010e-05,  1.7989e-04, -2.8333e-03,\n",
       "         -4.2725e-04, -1.0492e-03, -2.1238e-03, -2.9892e-04,  1.0041e-03,\n",
       "         -3.8980e-04,  2.9573e-05,  5.4246e-04,  2.8305e-04,  1.4142e-03,\n",
       "          4.5401e-04,  9.5024e-04, -1.1101e-03,  1.6300e-04, -3.2568e-05,\n",
       "          1.0209e-03, -5.1718e-04, -1.0093e-03, -1.7314e-03, -3.0794e-04,\n",
       "          4.9329e-04,  2.2417e-03,  3.8224e-04, -9.2650e-04,  1.8855e-03,\n",
       "         -7.1570e-04,  2.6282e-04, -3.8030e-04, -6.1093e-04, -9.3620e-04,\n",
       "         -1.7670e-04, -2.7358e-04, -2.0462e-03, -3.4203e-04, -3.0499e-04,\n",
       "         -1.3133e-03, -5.8583e-04, -9.5613e-04, -5.3008e-04, -1.2988e-03,\n",
       "         -1.6157e-04, -4.7129e-05, -5.3731e-04, -6.2493e-04,  6.7010e-04,\n",
       "         -6.6985e-05, -8.6038e-04, -3.4460e-04, -5.2382e-04, -5.8637e-04,\n",
       "          1.9360e-05, -4.2330e-04,  1.9784e-03,  5.9714e-05,  1.0100e-03,\n",
       "         -2.6553e-05,  2.3213e-03,  7.2481e-04, -1.4372e-03, -9.5178e-04,\n",
       "         -8.8428e-04, -1.9324e-03, -2.1890e-03, -1.3751e-03,  8.3426e-04,\n",
       "         -1.3851e-03, -1.1388e-06, -7.7323e-04, -1.0272e-03, -2.0599e-03,\n",
       "         -5.3378e-04, -1.3899e-03,  9.6581e-05, -1.0406e-03, -9.3266e-04,\n",
       "         -4.4846e-05,  1.1228e-04, -1.5734e-04,  1.8069e-03, -1.5400e-03,\n",
       "          5.9841e-04, -1.6487e-03,  1.4709e-04, -1.3561e-03,  1.5764e-03,\n",
       "          3.3601e-04, -4.8169e-04, -3.2868e-04, -8.6204e-04, -2.7726e-04,\n",
       "         -2.3913e-03, -1.5025e-03,  1.0094e-03, -1.8545e-03,  4.5471e-04,\n",
       "         -1.5764e-03,  1.9878e-03,  5.7371e-04,  1.3585e-03, -4.7827e-04,\n",
       "          7.1912e-04, -2.0861e-03,  8.2077e-04,  8.3292e-04,  8.5135e-04,\n",
       "         -6.6636e-04, -5.6541e-04,  1.4785e-03,  7.3206e-04,  6.0834e-04,\n",
       "         -1.4826e-03, -7.8589e-04, -2.9711e-04,  2.6691e-04,  1.3190e-03,\n",
       "          1.0852e-03,  2.4316e-04,  2.0735e-05, -6.8169e-04,  2.7380e-04,\n",
       "         -1.2401e-03, -1.8042e-04, -1.8289e-03, -9.9907e-04, -3.3674e-04,\n",
       "         -1.8691e-03, -6.6203e-04,  1.0401e-03, -1.0595e-03,  9.5027e-04,\n",
       "         -6.9710e-04, -4.2813e-04,  8.1192e-04,  2.1216e-03, -2.2067e-03,\n",
       "          5.3782e-04, -9.8133e-05,  2.7927e-04, -2.4901e-03,  8.2238e-04,\n",
       "          1.5682e-03,  6.9675e-04,  6.3293e-05, -6.3719e-04, -7.7844e-04,\n",
       "         -7.8678e-05,  8.7726e-04,  1.5670e-05, -6.7790e-05,  1.4307e-03,\n",
       "          1.8813e-03, -1.6860e-03,  9.3880e-04,  1.8345e-03, -4.6774e-04,\n",
       "          1.3847e-04, -7.9204e-04,  1.4519e-03,  1.1647e-03,  8.6700e-04,\n",
       "         -9.2026e-04, -1.5102e-03, -1.3366e-03,  2.4530e-03,  1.9541e-04,\n",
       "         -6.2207e-04,  5.9372e-04, -1.0828e-03,  8.5876e-04,  1.3112e-03,\n",
       "          6.2531e-04,  4.7379e-05, -2.4044e-04, -7.5248e-04, -2.1958e-03,\n",
       "          1.4836e-03,  1.8066e-03,  1.2544e-03, -5.2126e-04,  4.3538e-04,\n",
       "         -1.5960e-03, -9.2726e-04, -1.0406e-03, -5.3068e-04, -4.7262e-04,\n",
       "          6.6950e-04, -1.2520e-04,  1.3417e-03, -9.1301e-04,  2.1527e-03,\n",
       "         -1.7645e-03,  5.4476e-04, -2.9223e-05,  4.8417e-04, -9.4653e-05,\n",
       "         -6.1643e-04,  1.0906e-03, -9.5584e-04, -2.9231e-04,  6.8181e-04,\n",
       "         -2.3615e-03,  1.4820e-04, -6.1740e-04, -3.9502e-04,  1.2951e-03,\n",
       "          3.9347e-03,  4.2468e-04,  4.6001e-04,  1.9090e-03,  1.5321e-04,\n",
       "          1.7961e-03, -2.4236e-04,  3.4199e-04,  1.4368e-03,  4.4133e-04,\n",
       "          1.3114e-03, -1.9766e-03, -7.0770e-04,  5.5797e-04, -1.3865e-03,\n",
       "          2.1378e-04,  1.4856e-03, -5.1598e-04,  1.1652e-03, -9.3211e-04,\n",
       "         -1.4044e-04, -3.1224e-05,  1.3215e-03, -1.0012e-03, -3.9408e-04,\n",
       "         -1.1850e-03,  4.8692e-04,  3.7978e-04,  9.7262e-04, -1.5960e-03,\n",
       "          1.6402e-03,  1.2064e-03, -8.4783e-05,  2.8199e-04,  1.9107e-03,\n",
       "          8.7505e-04, -1.7858e-03,  5.5013e-04,  2.6085e-04, -5.9738e-04,\n",
       "          4.9359e-04, -1.6162e-03, -1.8104e-03,  6.4926e-05,  7.2889e-04,\n",
       "          2.5399e-03,  2.3860e-03,  3.1295e-04,  1.2445e-03,  7.0804e-04,\n",
       "          1.7478e-03,  6.2246e-04, -6.3554e-06,  1.2558e-03, -5.3389e-04,\n",
       "          1.3527e-03, -1.0884e-03, -1.3425e-03,  1.1917e-03, -1.3737e-03,\n",
       "         -1.0408e-03,  3.8343e-04,  1.3584e-03, -6.5066e-04, -2.9019e-05,\n",
       "         -4.7903e-04, -1.1613e-03, -2.0707e-04,  2.2595e-04,  2.2027e-03,\n",
       "          7.6569e-04, -8.2208e-04, -1.9935e-04, -1.4450e-04, -2.4240e-04,\n",
       "         -2.3028e-03,  1.0964e-03, -1.2918e-04,  1.4632e-03,  4.1980e-04,\n",
       "         -3.6541e-04, -1.1103e-03, -1.4534e-03, -2.0827e-03,  6.1462e-05,\n",
       "         -2.0315e-03, -3.6778e-04, -7.1758e-04, -7.0943e-05, -8.7207e-04,\n",
       "         -6.5221e-04, -1.0429e-05, -6.3917e-04, -1.4582e-04, -6.3740e-04,\n",
       "         -1.5393e-03, -2.6705e-04,  7.8633e-04,  1.0681e-03,  9.8650e-04,\n",
       "         -3.8191e-04, -1.7546e-03,  1.2735e-03, -7.1221e-04,  5.6516e-04,\n",
       "         -7.3274e-04,  2.5629e-04, -9.1867e-04,  9.5285e-05, -7.0983e-04,\n",
       "         -8.9380e-04, -7.4184e-04, -1.0367e-03, -2.7533e-03,  8.3803e-04,\n",
       "          9.6746e-04, -2.3838e-03, -4.7251e-04,  1.1507e-03, -7.8580e-04,\n",
       "         -2.8550e-04, -6.0086e-04,  1.0170e-03, -5.4699e-04, -1.2185e-04,\n",
       "         -1.2701e-03,  1.8962e-04, -2.0373e-05, -5.3270e-04, -8.9792e-04,\n",
       "         -1.0434e-03, -1.0353e-04,  5.4145e-04, -8.4580e-04, -4.2091e-04,\n",
       "         -1.9959e-04, -1.2182e-03, -9.5107e-04,  1.6642e-03,  1.4110e-03,\n",
       "         -1.3678e-03,  6.2536e-04,  5.0817e-04, -7.3478e-04, -1.0014e-03,\n",
       "          1.6463e-04, -1.2005e-04, -1.7800e-03,  1.8123e-03, -1.6538e-04,\n",
       "         -1.0378e-03, -9.3386e-04,  5.4809e-04,  5.4639e-04, -9.4138e-04,\n",
       "         -1.8992e-03, -1.5581e-03, -5.2702e-04, -7.7125e-04,  2.9691e-04,\n",
       "         -2.0104e-05, -3.2568e-03,  7.0274e-04,  6.8795e-05, -1.2523e-03,\n",
       "         -1.2092e-03,  8.5016e-04, -1.7662e-03,  6.4849e-04,  1.2488e-03,\n",
       "          1.8173e-03, -9.3038e-04,  1.0152e-03, -5.6534e-04, -9.8509e-04,\n",
       "          5.5720e-04,  9.0832e-05,  6.2413e-04, -3.5534e-04, -5.1995e-04,\n",
       "         -7.3027e-05,  2.9697e-03,  3.6348e-04, -9.6433e-05, -1.3858e-03,\n",
       "         -1.0298e-03, -2.1538e-03, -8.8415e-04,  2.2685e-04, -3.8461e-04,\n",
       "          8.3534e-04, -3.7923e-04,  6.5326e-04, -6.9336e-04, -2.2569e-04,\n",
       "         -1.3832e-03, -6.8627e-04,  1.1389e-03, -4.4038e-04, -3.3680e-04,\n",
       "          4.4599e-04,  1.3368e-03,  1.3626e-03, -1.3768e-03, -7.2915e-05,\n",
       "         -1.3246e-05, -1.1867e-03,  2.1866e-03,  1.8890e-04,  1.8691e-03,\n",
       "          8.9357e-04, -8.4880e-04, -5.5885e-04, -1.0045e-03,  6.5294e-04,\n",
       "         -3.0596e-04,  4.2054e-04,  1.0185e-03, -1.5039e-03,  1.5547e-03,\n",
       "          2.2426e-03,  8.0436e-04,  8.4806e-04, -1.3863e-03, -9.5023e-05,\n",
       "          1.6360e-03,  7.7671e-04,  9.6099e-04,  2.8871e-03,  1.5773e-03,\n",
       "         -1.3453e-03,  1.9321e-03, -1.4288e-04,  1.3516e-03,  1.0959e-03,\n",
       "         -2.6969e-05,  2.0077e-03,  1.3502e-03,  3.1677e-04, -1.3286e-04,\n",
       "         -1.0545e-03,  1.0882e-03,  8.9698e-04, -2.1083e-04,  1.7430e-03,\n",
       "          5.0498e-05,  2.0618e-03,  2.3635e-03, -1.7848e-03, -1.7961e-03,\n",
       "          3.7661e-04,  1.0689e-03, -6.1276e-04,  2.4085e-03, -6.6575e-04,\n",
       "          1.8265e-04,  9.9780e-04]),\n",
       " 'transformer.resblocks.2.attn.in_proj_weight': tensor([[-0.0007, -0.0477, -0.0097,  ...,  0.0352,  0.1438, -0.0686],\n",
       "         [-0.0065,  0.0114,  0.0644,  ...,  0.0504,  0.0361, -0.0444],\n",
       "         [ 0.0259,  0.0060,  0.0076,  ...,  0.0322,  0.0029,  0.0257],\n",
       "         ...,\n",
       "         [-0.0206, -0.0454, -0.0010,  ..., -0.1015, -0.0443, -0.0271],\n",
       "         [ 0.0135,  0.0225,  0.0503,  ...,  0.0383, -0.0516,  0.0661],\n",
       "         [-0.0427, -0.0005,  0.0876,  ..., -0.1104,  0.0324,  0.0689]]),\n",
       " 'transformer.resblocks.2.attn.in_proj_bias': tensor([ 1.3126e-03,  9.2339e-04,  2.2762e-05,  ..., -5.1400e-04,\n",
       "          7.3806e-05, -3.3662e-04]),\n",
       " 'transformer.resblocks.2.attn.out_proj.weight': tensor([[ 0.0004, -0.0007,  0.0074,  ..., -0.0078, -0.0100, -0.0011],\n",
       "         [-0.0084,  0.0010, -0.0126,  ...,  0.0052, -0.0084, -0.0007],\n",
       "         [-0.0017, -0.0163,  0.0077,  ..., -0.0095, -0.0064,  0.0080],\n",
       "         ...,\n",
       "         [ 0.0053, -0.0154,  0.0037,  ..., -0.0040, -0.0200,  0.0065],\n",
       "         [-0.0140,  0.0058,  0.0051,  ..., -0.0073,  0.0108, -0.0085],\n",
       "         [-0.0044, -0.0054,  0.0134,  ...,  0.0011, -0.0104, -0.0049]]),\n",
       " 'transformer.resblocks.2.attn.out_proj.bias': tensor([ 1.0788e-03, -7.0583e-05,  3.2976e-04,  5.5309e-05,  1.8382e-03,\n",
       "         -6.0728e-04,  2.2773e-04, -2.1526e-04, -4.8404e-04, -3.0273e-04,\n",
       "          5.1810e-04, -8.3277e-04,  1.0824e-04, -4.5048e-04, -2.3310e-04,\n",
       "          9.2724e-04,  4.1237e-04,  4.7027e-04, -1.8701e-04,  2.0805e-04,\n",
       "          2.7908e-04, -3.2525e-04,  1.2453e-03,  6.9027e-04,  6.4053e-04,\n",
       "         -2.1366e-04, -1.1909e-03, -7.1214e-04,  6.3259e-04, -1.2775e-03,\n",
       "          1.5964e-03,  2.4853e-04, -2.0982e-04, -1.3061e-03, -7.4067e-04,\n",
       "          5.6419e-04, -3.4099e-04,  7.0211e-04,  6.7352e-04, -1.3441e-03,\n",
       "         -4.7957e-04, -9.3063e-04, -5.5904e-04,  4.2345e-04, -1.3661e-03,\n",
       "          2.5806e-04, -4.6396e-04,  5.5423e-04,  6.6091e-06, -9.1748e-04,\n",
       "         -4.0955e-04, -2.3621e-04, -5.5982e-04, -2.2214e-04,  1.5596e-03,\n",
       "          6.4546e-04,  1.7707e-04,  8.9775e-04,  1.1972e-04,  2.4123e-04,\n",
       "         -2.3887e-04, -1.1946e-03, -4.2026e-04,  1.7873e-04, -3.0884e-04,\n",
       "         -3.0755e-05,  2.5013e-04, -5.9692e-04, -1.0264e-04,  2.4260e-04,\n",
       "         -7.2011e-04,  1.9058e-05, -2.9902e-04,  9.6719e-05, -1.2158e-03,\n",
       "         -8.0117e-05, -7.8910e-04,  3.9300e-04,  1.9539e-04, -2.3336e-04,\n",
       "         -5.0787e-04,  1.3985e-03,  1.6600e-03,  1.0836e-03,  3.0494e-06,\n",
       "          2.4455e-04, -4.3761e-04, -1.7686e-03,  6.4135e-04,  2.2536e-04,\n",
       "          6.9802e-04, -1.0627e-04,  8.9352e-05, -1.5041e-03,  4.5531e-04,\n",
       "         -5.7663e-04,  2.2210e-04, -2.9919e-04, -4.4846e-04,  4.2794e-04,\n",
       "          9.4707e-04, -3.4071e-04,  6.9957e-04, -5.7645e-04,  3.6512e-04,\n",
       "         -2.1415e-04,  5.4139e-04,  4.7210e-04,  9.5310e-05,  5.1223e-04,\n",
       "         -6.4295e-04,  3.8983e-04, -5.1591e-04, -3.2967e-04, -3.4366e-04,\n",
       "         -9.5598e-04,  6.0265e-04,  1.0413e-04, -2.3267e-04, -1.2103e-03,\n",
       "         -5.4584e-04,  2.0285e-04, -6.1499e-04,  6.0621e-04,  9.8089e-04,\n",
       "          2.8191e-04, -2.9952e-04, -4.5847e-05,  6.0392e-04,  1.0396e-04,\n",
       "          4.3177e-04,  6.7270e-04, -7.2846e-04, -9.5107e-04, -3.5862e-04,\n",
       "          3.3390e-05, -8.4610e-04, -6.0319e-05,  2.7791e-04,  1.5646e-04,\n",
       "         -3.9709e-04,  7.4023e-05,  3.4979e-04,  1.9138e-03, -1.3593e-03,\n",
       "          4.7592e-04, -4.7401e-04,  7.7366e-05,  3.0934e-05,  8.1610e-04,\n",
       "          1.4155e-03,  1.9201e-03,  2.3244e-04,  1.3913e-03, -7.5672e-04,\n",
       "          1.4882e-03, -5.1688e-04, -8.8179e-05,  6.6690e-04,  2.9419e-04,\n",
       "          4.8436e-05,  7.1962e-04,  2.6451e-04,  5.0492e-05, -1.0588e-03,\n",
       "         -5.4774e-05, -5.2343e-05,  1.0464e-04, -2.7150e-04, -4.1247e-04,\n",
       "          2.0257e-04, -3.7790e-04,  6.5057e-04, -9.6552e-04, -7.5995e-05,\n",
       "         -1.0471e-03, -6.1579e-04, -6.4189e-04, -4.3193e-04, -3.0548e-04,\n",
       "         -1.6856e-04,  1.4702e-04, -3.9176e-04,  1.9407e-04,  2.8180e-06,\n",
       "         -1.0685e-03,  1.9275e-04,  7.0346e-04, -1.6894e-04, -8.9843e-04,\n",
       "          5.3915e-04,  7.0277e-04, -8.0556e-04, -4.4410e-04,  1.8077e-04,\n",
       "         -5.3031e-04,  5.1932e-04,  1.9905e-04,  7.8995e-04,  9.8020e-05,\n",
       "          3.6028e-04,  1.7358e-05, -9.3899e-05,  1.4030e-03, -4.1572e-04,\n",
       "          1.5576e-03,  1.0847e-04, -1.8568e-04,  3.6438e-04,  1.7861e-04,\n",
       "          1.8369e-03, -1.2600e-03,  2.7639e-04,  9.2959e-04, -4.7694e-04,\n",
       "         -4.9693e-04,  1.0255e-03,  7.3612e-04, -6.3738e-04, -3.2830e-04,\n",
       "         -1.7013e-03, -8.8892e-04,  6.6115e-04,  1.5598e-03,  1.0114e-04,\n",
       "          3.3394e-04, -9.2118e-04, -1.4201e-03, -3.6957e-04,  6.3859e-04,\n",
       "          1.3185e-03,  5.3853e-04, -2.0258e-04, -4.5248e-04,  1.0791e-03,\n",
       "          5.2073e-04,  6.1999e-04, -3.4162e-04,  1.5875e-03,  5.9377e-04,\n",
       "          1.4183e-04, -8.8916e-04,  5.1975e-04,  1.2660e-04, -9.2949e-04,\n",
       "          2.2943e-04, -7.3619e-04,  6.3960e-04, -7.2609e-04,  1.6218e-04,\n",
       "          5.3813e-04, -4.1680e-04, -3.9633e-04, -1.5390e-04, -1.1866e-03,\n",
       "          4.0020e-04, -1.2720e-03,  7.8017e-04,  1.6455e-04, -1.2908e-03,\n",
       "          1.2463e-04, -1.5802e-05,  8.9100e-04, -1.1256e-03, -3.0600e-05,\n",
       "          1.2639e-04,  1.4033e-04,  1.4958e-03,  2.4708e-04, -3.1140e-04,\n",
       "         -3.8112e-04, -1.4629e-04, -6.0581e-04,  3.6817e-04, -5.5054e-04,\n",
       "          3.9778e-05, -1.2278e-04, -2.1709e-04,  1.0626e-03, -6.0697e-04,\n",
       "          6.7491e-04,  1.5049e-03,  2.9085e-05, -1.4303e-03,  8.0451e-05,\n",
       "          6.7944e-04,  1.4171e-03,  5.6739e-04,  1.0099e-04, -1.4261e-03,\n",
       "          4.8575e-04, -8.0646e-04, -2.4492e-04,  1.0417e-03,  9.4370e-04,\n",
       "         -3.8685e-05, -6.2090e-04,  8.4426e-04,  9.4159e-04, -8.0211e-04,\n",
       "         -1.0534e-04, -1.0362e-05,  1.1322e-03, -2.6193e-04, -7.9493e-04,\n",
       "         -1.8730e-04, -3.1713e-04, -7.8827e-05, -7.0631e-04, -7.6681e-04,\n",
       "         -2.4357e-05,  4.7177e-04,  4.0871e-04, -3.6200e-05, -7.3406e-04,\n",
       "         -1.0590e-04, -2.0656e-04,  6.1089e-04, -3.9895e-05,  4.1355e-04,\n",
       "          8.4839e-04, -2.1535e-03,  1.8341e-04,  1.3777e-04, -6.8699e-04,\n",
       "         -9.9359e-06, -1.4356e-04, -9.9310e-04, -6.1075e-04, -4.3136e-04,\n",
       "         -1.7524e-03, -5.9916e-04, -4.4398e-05, -8.3048e-05,  1.0870e-03,\n",
       "         -3.2931e-04, -8.0827e-04, -7.9985e-05, -2.0494e-04,  9.9773e-04,\n",
       "         -6.4604e-04,  8.5203e-04, -5.3238e-04,  3.9169e-04, -6.6036e-04,\n",
       "          4.2416e-04, -4.1965e-04, -1.8193e-04, -1.5361e-03,  2.8673e-04,\n",
       "         -3.0903e-04, -6.8094e-04, -1.1882e-03, -1.1868e-03,  7.7630e-04,\n",
       "         -2.0596e-03,  1.3206e-04,  1.0915e-03, -1.5177e-04, -5.3107e-04,\n",
       "         -1.2611e-04, -2.4533e-04,  1.2457e-03,  1.1482e-03, -6.4767e-04,\n",
       "          7.4927e-04, -3.6312e-04, -2.5870e-05,  1.5398e-03,  4.6609e-04,\n",
       "         -4.3740e-04, -6.5781e-05, -9.1245e-04, -4.0970e-04, -2.1774e-04,\n",
       "          9.3327e-04,  2.4129e-04,  1.5777e-04, -8.7031e-04, -8.4288e-04,\n",
       "          2.8838e-04, -1.2539e-03, -5.2089e-04, -3.4935e-04,  3.5709e-04,\n",
       "          3.5417e-04,  9.5692e-04,  9.8879e-04,  4.3006e-04, -1.0650e-04,\n",
       "         -7.0912e-04,  7.0298e-05,  5.8975e-04,  5.7852e-04, -5.7818e-04,\n",
       "          2.7570e-04, -1.2301e-04,  1.5596e-04, -6.6864e-05,  4.8282e-04,\n",
       "         -3.0710e-04, -1.0350e-03, -6.6080e-04,  1.3351e-04,  1.3865e-03,\n",
       "          4.1617e-04,  4.5625e-04, -7.8825e-04,  4.9521e-04,  2.5923e-05,\n",
       "          4.0810e-04, -8.1699e-04,  4.2258e-04,  4.8234e-04,  8.6421e-04,\n",
       "         -1.3123e-03,  7.7346e-04, -3.6454e-04, -5.8584e-04,  1.0098e-03,\n",
       "         -1.1580e-04, -5.4598e-04,  1.4967e-03, -2.0328e-04, -7.3160e-04,\n",
       "          2.3529e-06, -4.3107e-05, -4.1007e-04,  7.0130e-04, -1.7926e-04,\n",
       "         -1.1366e-03,  2.5910e-03,  5.5488e-04,  4.9055e-04, -6.2487e-04,\n",
       "         -1.6388e-03,  2.5095e-04, -9.9801e-04, -5.1282e-04,  4.7347e-04,\n",
       "         -9.4108e-04,  8.2245e-04,  3.7064e-04,  5.6703e-04, -1.0893e-03,\n",
       "         -7.5438e-04, -9.6756e-04, -1.1849e-03,  1.2655e-04,  1.9076e-03,\n",
       "          1.2258e-03,  1.2572e-03, -2.5650e-04,  8.5367e-04,  1.1700e-04,\n",
       "          1.6480e-04,  1.0927e-04, -3.8082e-04, -2.7805e-04, -1.7614e-04,\n",
       "         -1.9891e-04, -2.3209e-04, -5.8167e-04,  1.0508e-03,  8.4568e-04,\n",
       "         -2.1456e-04,  8.7826e-04, -5.1724e-04, -3.8735e-04, -2.5477e-04,\n",
       "         -1.3751e-03, -5.0870e-04,  6.1244e-05,  6.8089e-04, -7.9311e-04,\n",
       "          5.1918e-04, -1.2979e-03, -1.2205e-04, -1.3645e-04,  7.5354e-04,\n",
       "         -1.0016e-03,  1.2364e-04,  4.1173e-04,  9.0910e-04, -2.2905e-04,\n",
       "          5.5227e-04, -6.8065e-04,  2.2228e-04,  6.9378e-04, -3.5166e-04,\n",
       "          1.8163e-03,  2.0868e-04, -7.3755e-04,  2.4698e-04, -8.1239e-04,\n",
       "         -7.2992e-04, -6.6769e-06, -8.2464e-06,  7.7072e-04, -2.9898e-04,\n",
       "         -6.9937e-05,  8.6123e-05, -1.2876e-04,  2.9448e-04, -3.1574e-04,\n",
       "         -6.7214e-04, -3.5098e-04, -3.2520e-04,  1.6787e-04,  5.2162e-04,\n",
       "          8.8198e-04, -4.5967e-04]),\n",
       " 'transformer.resblocks.2.ln_1.weight': tensor([1.0004, 0.9991, 0.9978, 0.9982, 1.0014, 1.0031, 0.9967, 1.0000, 0.9982,\n",
       "         0.9991, 0.9963, 1.0004, 0.9974, 1.0000, 0.9984, 1.0007, 0.9971, 1.0001,\n",
       "         1.0000, 1.0011, 1.0020, 0.9995, 1.0003, 1.0030, 0.9996, 0.9976, 0.9998,\n",
       "         0.9995, 0.9986, 0.9963, 0.9984, 1.0001, 0.9995, 0.9987, 0.9995, 0.9991,\n",
       "         1.0021, 0.9981, 1.0002, 1.0002, 1.0020, 0.9998, 1.0011, 0.9973, 0.9977,\n",
       "         1.0003, 0.9995, 0.9978, 1.0004, 0.9988, 1.0016, 1.0000, 0.9988, 0.9990,\n",
       "         0.9972, 0.9954, 1.0003, 0.9969, 0.9992, 0.9991, 0.9986, 0.9996, 0.9987,\n",
       "         0.9982, 0.9990, 0.9989, 0.9984, 1.0006, 0.9996, 1.0007, 0.9986, 0.9998,\n",
       "         0.9969, 0.9963, 0.9977, 1.0002, 0.9982, 0.9982, 0.9985, 1.0024, 0.9995,\n",
       "         0.9991, 0.9993, 0.9989, 0.9980, 0.9999, 1.0009, 0.9985, 0.9980, 0.9977,\n",
       "         0.9997, 0.9972, 1.0002, 0.9952, 0.9985, 0.9991, 0.9988, 0.9974, 0.9987,\n",
       "         0.9989, 0.9965, 0.9990, 0.9997, 0.9994, 0.9979, 0.9981, 0.9972, 0.9960,\n",
       "         0.9997, 0.9991, 0.9988, 1.0005, 1.0013, 0.9995, 0.9988, 0.9971, 0.9998,\n",
       "         0.9982, 1.0012, 1.0007, 1.0009, 0.9977, 0.9991, 0.9981, 0.9999, 1.0000,\n",
       "         1.0015, 0.9974, 1.0001, 1.0003, 0.9987, 0.9991, 1.0017, 0.9998, 0.9987,\n",
       "         0.9974, 1.0019, 0.9990, 1.0002, 1.0019, 0.9984, 1.0028, 0.9996, 0.9968,\n",
       "         0.9969, 0.9992, 0.9975, 0.9977, 1.0007, 1.0015, 0.9997, 1.0028, 1.0003,\n",
       "         0.9983, 0.9998, 0.9995, 0.9991, 1.0000, 0.9993, 0.9986, 0.9975, 0.9991,\n",
       "         0.9982, 0.9973, 1.0001, 0.9992, 0.9981, 0.9983, 0.9991, 1.0000, 1.0016,\n",
       "         0.9969, 0.9988, 0.9987, 0.9952, 0.9981, 0.9998, 0.9984, 1.0005, 0.9993,\n",
       "         1.0002, 0.9960, 1.0021, 0.9974, 1.0012, 0.9990, 0.9973, 0.9994, 0.9995,\n",
       "         0.9985, 0.9987, 1.0000, 0.9975, 0.9999, 0.9982, 1.0003, 0.9976, 0.9986,\n",
       "         1.0003, 0.9982, 0.9983, 1.0005, 0.9972, 0.9981, 1.0039, 0.9968, 0.9982,\n",
       "         0.9992, 0.9980, 1.0013, 1.0000, 1.0003, 0.9994, 1.0001, 0.9979, 0.9990,\n",
       "         0.9989, 0.9994, 1.0000, 0.9987, 0.9999, 1.0001, 0.9999, 1.0002, 0.9985,\n",
       "         1.0012, 0.9963, 0.9975, 0.9978, 0.9997, 0.9967, 0.9955, 1.0000, 0.9987,\n",
       "         0.9977, 0.9971, 1.0008, 0.9952, 0.9981, 1.0002, 1.0022, 1.0021, 0.9961,\n",
       "         1.0002, 0.9997, 1.0013, 0.9980, 0.9970, 1.0000, 1.0020, 0.9979, 0.9990,\n",
       "         0.9996, 0.9996, 0.9987, 0.9979, 0.9997, 1.0012, 0.9975, 0.9989, 0.9989,\n",
       "         0.9980, 0.9972, 0.9992, 0.9981, 1.0020, 1.0011, 0.9988, 0.9948, 0.9977,\n",
       "         0.9964, 0.9995, 1.0010, 0.9979, 0.9983, 0.9960, 0.9962, 1.0008, 0.9984,\n",
       "         0.9988, 1.0002, 1.0000, 0.9984, 0.9989, 0.9976, 0.9963, 0.9958, 1.0018,\n",
       "         1.0001, 0.9983, 0.9989, 1.0006, 0.9983, 1.0001, 1.0015, 0.9999, 0.9979,\n",
       "         0.9987, 0.9992, 0.9983, 1.0015, 1.0020, 1.0020, 1.0005, 1.0003, 0.9987,\n",
       "         0.9952, 0.9988, 0.9979, 0.9988, 0.9968, 0.9986, 0.9968, 1.0009, 0.9994,\n",
       "         1.0000, 1.0000, 0.9980, 0.9998, 0.9985, 0.9996, 0.9987, 0.9970, 0.9968,\n",
       "         1.0026, 0.9972, 1.0002, 0.9990, 0.9991, 0.9996, 0.9991, 0.9990, 0.9997,\n",
       "         1.0007, 0.9988, 0.9980, 0.9985, 0.9961, 0.9985, 1.0005, 1.0003, 0.9992,\n",
       "         0.9982, 1.0001, 0.9983, 1.0036, 0.9974, 0.9977, 1.0008, 0.9980, 0.9998,\n",
       "         1.0016, 1.0004, 1.0004, 0.9982, 0.9977, 0.9973, 1.0007, 0.9978, 0.9989,\n",
       "         0.9997, 0.9994, 0.9989, 0.9990, 0.9980, 1.0016, 0.9995, 0.9996, 0.9989,\n",
       "         1.0021, 0.9985, 1.0001, 0.9986, 0.9993, 0.9996, 0.9970, 0.9976, 0.9996,\n",
       "         0.9998, 0.9987, 0.9990, 0.9994, 0.9981, 0.9995, 1.0017, 0.9993, 1.0011,\n",
       "         0.9993, 0.9974, 0.9990, 0.9981, 0.9958, 0.9995, 0.9982, 1.0009, 0.9958,\n",
       "         0.9994, 0.9995, 1.0005, 1.0005, 0.9974, 0.9969, 1.0001, 1.0002, 0.9969,\n",
       "         0.9978, 1.0001, 1.0011, 1.0019, 1.0004, 0.9978, 1.0001, 0.9978, 1.0004,\n",
       "         1.0021, 0.9998, 1.0000, 0.9997, 0.9997, 0.9983, 0.9992, 0.9972, 0.9983,\n",
       "         1.0033, 1.0004, 0.9985, 0.9974, 0.9989, 1.0011, 0.9953, 0.9996, 0.9973,\n",
       "         0.9990, 0.9994, 0.9967, 1.0007, 0.9979, 1.0011, 0.9991, 1.0045, 0.9989,\n",
       "         0.9989, 0.9998, 0.9966, 0.9985, 0.9981, 0.9982, 0.9964, 0.9972, 0.9988,\n",
       "         1.0004, 0.9977, 0.9972, 0.9976, 1.0000, 1.0015, 1.0007, 0.9960, 0.9992,\n",
       "         0.9984, 0.9984, 1.0004, 0.9999, 0.9993, 0.9974, 1.0036, 0.9998, 1.0001,\n",
       "         0.9986, 0.9976, 0.9985, 0.9980, 0.9970, 0.9991, 0.9994, 1.0010, 0.9985,\n",
       "         0.9977, 0.9974, 0.9989, 0.9997, 1.0001, 0.9991, 0.9997, 0.9989, 0.9980,\n",
       "         0.9976, 0.9977, 1.0017, 0.9981, 0.9995, 0.9949, 1.0018, 0.9984, 0.9986,\n",
       "         0.9992, 1.0011, 1.0005, 1.0006, 0.9997, 0.9992, 1.0018, 0.9990, 0.9994,\n",
       "         1.0002, 0.9971, 0.9973, 0.9981, 0.9992, 0.9990, 0.9987, 0.9984]),\n",
       " 'transformer.resblocks.2.ln_1.bias': tensor([-4.4546e-04,  8.8843e-04, -4.1047e-04,  7.2467e-06, -8.9200e-04,\n",
       "         -1.3068e-03, -6.1239e-04, -4.0535e-04,  9.7893e-04,  8.9550e-04,\n",
       "          1.2836e-03,  1.3907e-04,  2.6542e-03,  1.6187e-04, -4.4291e-04,\n",
       "         -1.1143e-03, -2.2917e-04, -1.7242e-03, -6.6833e-05, -1.1338e-03,\n",
       "          7.2650e-04,  3.7882e-04, -7.4557e-04, -1.5618e-04, -1.3321e-03,\n",
       "         -7.9174e-04,  6.0709e-04,  1.0031e-03,  3.6306e-05,  7.4209e-04,\n",
       "          1.7536e-03,  2.4772e-03, -7.0597e-04,  2.3468e-03,  1.4676e-04,\n",
       "          1.3034e-03, -1.9029e-03, -1.8199e-03, -8.4511e-04, -1.4899e-03,\n",
       "         -1.5577e-03, -4.1813e-04,  3.7513e-05,  1.5307e-04, -9.2085e-05,\n",
       "         -1.0511e-03, -4.9811e-04, -5.4862e-04, -5.7123e-04,  7.7654e-04,\n",
       "          2.7024e-04,  2.1958e-04,  3.5945e-04,  9.6344e-05, -1.3560e-03,\n",
       "         -1.9566e-05,  1.6102e-03, -1.3056e-03,  2.6139e-04, -1.8275e-03,\n",
       "          1.1387e-03, -4.1119e-04,  5.0858e-04,  4.1987e-04,  9.8437e-04,\n",
       "         -2.4829e-03, -1.0152e-03,  9.5354e-04, -3.3328e-04,  8.7416e-04,\n",
       "         -8.7421e-04,  2.1722e-03, -3.9623e-04,  1.3034e-03,  6.2277e-04,\n",
       "          1.2159e-03,  1.2921e-03,  9.8481e-04, -2.7996e-04, -7.1582e-04,\n",
       "         -1.9334e-03, -1.8301e-04, -1.0663e-04, -9.3536e-04, -2.1396e-03,\n",
       "         -2.0761e-06, -2.1454e-04,  1.9664e-03, -2.0798e-04, -3.9662e-04,\n",
       "          9.2449e-04,  1.1670e-03,  9.3299e-04,  9.9641e-04, -1.1698e-03,\n",
       "         -9.1854e-04,  4.7713e-04,  9.6114e-04, -8.9039e-04,  6.0679e-04,\n",
       "         -6.4519e-05, -1.7151e-03,  5.4813e-04, -3.3829e-04, -2.2382e-05,\n",
       "         -6.0529e-04,  1.7633e-03,  1.3646e-03,  4.5924e-05,  8.9560e-04,\n",
       "         -1.7415e-03,  1.4536e-03,  1.6059e-03, -5.1923e-04, -1.1207e-03,\n",
       "          8.1851e-04,  1.8413e-04,  2.2221e-04, -3.3718e-03,  1.8738e-03,\n",
       "          1.0960e-03,  1.5343e-03,  1.0596e-04,  3.3406e-05, -1.3515e-03,\n",
       "          1.2108e-03, -1.3200e-03, -1.6234e-03, -2.2934e-04, -3.1028e-04,\n",
       "         -7.7774e-04,  1.1608e-03,  8.3825e-04, -6.9018e-04, -3.9324e-04,\n",
       "         -3.1901e-04,  1.9514e-03,  7.3752e-04,  1.7079e-03,  7.5587e-04,\n",
       "         -1.3583e-03,  1.2732e-03, -4.9160e-04, -2.0733e-03, -1.5459e-04,\n",
       "         -1.3045e-03,  8.9162e-04,  9.3904e-05,  8.3076e-04,  2.8708e-03,\n",
       "         -1.7893e-03, -1.4453e-03, -2.3089e-03, -7.4743e-04, -1.6937e-03,\n",
       "          6.2819e-04,  1.2114e-03, -1.1239e-03, -4.3046e-04,  1.5366e-03,\n",
       "         -1.1924e-03, -3.3064e-04,  5.1816e-04, -8.5037e-04, -1.6528e-03,\n",
       "          4.9459e-04, -1.3531e-03, -2.6030e-04,  4.1690e-04,  2.2740e-04,\n",
       "         -7.0553e-04, -3.4369e-04, -5.4524e-04,  6.7023e-04,  7.4304e-04,\n",
       "          1.1398e-03,  2.5175e-03, -1.4857e-03,  3.3549e-04, -5.6069e-04,\n",
       "         -1.7609e-04,  1.0665e-03,  1.3541e-03, -2.0515e-03,  1.7404e-03,\n",
       "          1.0866e-03,  2.4746e-03, -2.5226e-04, -3.2820e-03,  5.0599e-04,\n",
       "         -1.2273e-03,  1.4449e-03, -2.4038e-04, -5.0655e-04, -6.2203e-04,\n",
       "         -4.4052e-04, -4.9622e-04, -7.9033e-04, -4.9222e-04, -1.3202e-03,\n",
       "          2.2881e-04, -4.6825e-04,  1.5993e-04, -1.8070e-03, -1.1941e-03,\n",
       "         -1.5269e-03,  4.2193e-05, -2.1027e-05, -3.8297e-04, -2.6729e-03,\n",
       "         -6.5022e-04,  4.8625e-04, -2.2257e-04, -2.3433e-03, -2.9355e-04,\n",
       "          7.6358e-05, -8.6654e-04, -4.0577e-04, -4.1698e-04, -3.1332e-04,\n",
       "          2.0004e-03,  1.8206e-03, -3.7466e-04,  8.6883e-04, -1.9567e-03,\n",
       "          4.5798e-04,  1.3840e-04, -5.7103e-05,  1.6430e-04,  5.3611e-04,\n",
       "          2.2972e-04, -1.9354e-03, -3.1615e-05,  1.1869e-03, -1.3409e-03,\n",
       "          2.8250e-03,  1.5549e-05, -1.0998e-03, -1.1555e-05, -1.0116e-03,\n",
       "          3.2253e-04, -8.6895e-04,  1.7572e-03, -1.2006e-04,  1.2308e-03,\n",
       "         -1.0552e-03,  1.1761e-03,  7.2239e-04,  3.0958e-03, -1.1152e-03,\n",
       "          1.5802e-05,  5.5150e-04,  1.0869e-03,  1.2856e-03, -3.9723e-04,\n",
       "          2.9019e-04, -6.2024e-04, -8.6106e-04, -2.7091e-03,  1.1470e-03,\n",
       "          4.4069e-04, -1.1085e-03,  5.9756e-04, -1.9856e-03,  2.5649e-04,\n",
       "         -1.0478e-03,  5.4950e-05, -5.8322e-04, -9.1375e-04,  9.5585e-04,\n",
       "          1.7710e-03, -1.0876e-03,  1.9927e-04,  6.3013e-04, -5.0361e-04,\n",
       "          1.9279e-04, -8.8037e-04, -3.7732e-04, -1.3201e-03, -1.3113e-03,\n",
       "         -8.9387e-04,  6.6140e-04, -8.2554e-04, -4.4002e-04,  1.9433e-03,\n",
       "         -6.3226e-04, -4.5870e-04, -4.4024e-04, -2.5860e-04,  2.5323e-03,\n",
       "          8.2420e-04,  1.7760e-03, -2.7166e-04, -3.4893e-04,  1.3207e-04,\n",
       "         -6.9796e-05,  2.4471e-04,  2.0193e-03,  3.9278e-04, -9.5148e-04,\n",
       "         -7.9983e-05,  1.2764e-03,  2.5291e-04, -4.1086e-04,  1.0854e-03,\n",
       "         -9.0034e-05, -1.6609e-03, -2.8514e-04,  3.3949e-04,  6.2351e-05,\n",
       "          3.2613e-03, -1.5729e-03, -1.8290e-03,  9.3268e-04, -1.4300e-03,\n",
       "         -1.9674e-03, -1.3958e-03,  2.0416e-04, -7.6786e-04,  8.0886e-04,\n",
       "         -1.0515e-03,  1.2177e-03, -2.2295e-04, -2.8953e-03,  4.7717e-04,\n",
       "          1.2154e-03,  9.3861e-04, -4.7353e-04,  8.9754e-05, -7.5558e-06,\n",
       "          1.0443e-03,  1.4789e-03,  1.0364e-03, -4.9387e-05, -6.6088e-04,\n",
       "         -8.7420e-04,  1.4396e-03, -2.0869e-03, -4.3213e-04, -4.6639e-04,\n",
       "          4.4111e-04, -3.6654e-04, -8.9418e-04,  1.0059e-03,  1.5750e-03,\n",
       "         -1.8889e-04, -3.3461e-04, -3.2549e-05, -6.0426e-04, -8.8984e-04,\n",
       "         -2.8571e-04, -1.3165e-03,  6.0368e-04, -9.0067e-04, -1.0202e-03,\n",
       "          8.4934e-04,  5.1094e-05, -1.3577e-05,  2.5093e-04,  6.9483e-04,\n",
       "          1.7383e-03,  2.8034e-03,  1.7691e-04, -5.8064e-04, -4.3861e-04,\n",
       "         -6.3327e-04, -5.9792e-04, -1.6169e-04,  3.8607e-04,  1.2399e-05,\n",
       "          1.0813e-03,  2.3725e-04, -8.7465e-05, -9.7921e-05, -4.0188e-04,\n",
       "          1.3986e-03,  3.5458e-04,  9.9887e-05,  2.7956e-03,  1.6032e-03,\n",
       "          1.4489e-03, -3.1322e-04, -2.5792e-03,  2.4761e-04,  1.1484e-03,\n",
       "          5.3593e-04, -1.2585e-03, -7.5779e-04, -1.5434e-03, -1.1433e-03,\n",
       "         -5.3709e-04, -5.0934e-04, -5.3021e-04,  8.5810e-05, -3.1659e-04,\n",
       "         -7.7686e-04,  5.1303e-04, -2.7524e-04, -8.7304e-04,  1.8904e-03,\n",
       "          6.9433e-04,  1.6577e-03, -2.7056e-03, -1.5481e-03, -1.2301e-03,\n",
       "         -1.2571e-03, -2.6307e-03,  1.3578e-03,  5.0100e-05, -1.1657e-03,\n",
       "         -7.6775e-04, -1.4533e-03, -9.9939e-04,  3.9229e-04, -5.2965e-04,\n",
       "         -1.0414e-03, -1.8961e-03,  1.4649e-03,  6.7364e-04,  1.1151e-03,\n",
       "         -7.8906e-04,  1.0227e-03, -1.4181e-03, -1.6619e-04,  2.8335e-06,\n",
       "         -6.4676e-04,  1.8603e-03, -5.2451e-05,  2.3614e-03,  5.7129e-04,\n",
       "          1.0840e-04, -7.9552e-04, -1.6576e-03, -1.4790e-03,  3.0347e-04,\n",
       "          1.4614e-03,  9.3177e-04, -3.9483e-04,  1.0347e-03,  8.6136e-04,\n",
       "          5.9201e-04,  5.0072e-04,  3.4097e-04,  4.4190e-04, -5.3450e-04,\n",
       "         -7.9252e-04, -7.1761e-04,  2.4509e-03,  8.9944e-04, -1.6533e-03,\n",
       "         -1.0112e-03, -1.1529e-03,  4.0781e-04,  3.9858e-04,  1.1631e-03,\n",
       "         -2.1696e-03, -2.1243e-03, -1.1696e-03, -8.7088e-04, -2.0510e-03,\n",
       "         -5.9170e-04,  9.7465e-04,  5.9890e-04,  6.9382e-04,  1.2656e-03,\n",
       "          1.1558e-03,  2.4040e-05, -1.3017e-03,  2.4537e-04, -2.5250e-03,\n",
       "          8.0209e-04,  3.9157e-03,  1.7916e-03, -1.0685e-03, -2.8837e-04,\n",
       "         -1.6408e-03,  4.7082e-04, -1.8125e-03, -1.1593e-03,  1.0252e-03,\n",
       "         -1.1276e-03, -1.8781e-03,  3.2961e-05, -5.7781e-04,  5.0489e-04,\n",
       "          5.9771e-04,  1.9168e-03, -7.9666e-04, -1.8934e-03, -3.0352e-03,\n",
       "          9.0288e-04,  7.6004e-04,  2.3590e-03,  1.0824e-03, -1.6476e-03,\n",
       "         -6.3389e-04,  4.7478e-06,  9.7434e-04, -1.1579e-03, -1.0707e-03,\n",
       "         -1.6228e-04,  6.0308e-04, -1.2332e-03,  1.2992e-03, -3.9006e-04,\n",
       "         -1.3635e-03,  2.8378e-04,  1.8723e-03, -8.0753e-04,  5.0518e-05,\n",
       "         -1.3136e-03,  5.3384e-04]),\n",
       " 'transformer.resblocks.2.mlp.c_fc.weight': tensor([[-0.0184,  0.0025, -0.0093,  ..., -0.0033,  0.0117, -0.0083],\n",
       "         [-0.0119, -0.0238,  0.0075,  ...,  0.0202, -0.0287,  0.0171],\n",
       "         [ 0.0161,  0.0217,  0.0151,  ..., -0.0035, -0.0074, -0.0144],\n",
       "         ...,\n",
       "         [-0.0707, -0.0287,  0.0065,  ...,  0.0163,  0.0445, -0.0531],\n",
       "         [-0.0027,  0.0056, -0.0641,  ...,  0.0288,  0.0100,  0.0623],\n",
       "         [ 0.0897,  0.0072, -0.0267,  ..., -0.0459,  0.0264, -0.0346]]),\n",
       " 'transformer.resblocks.2.mlp.c_fc.bias': tensor([-0.0443, -0.0097,  0.0345,  ..., -0.0305, -0.0269,  0.0379]),\n",
       " 'transformer.resblocks.2.mlp.c_proj.weight': tensor([[-0.0005,  0.0064,  0.0142,  ...,  0.0188, -0.0069, -0.0109],\n",
       "         [-0.0145,  0.0039, -0.0093,  ..., -0.0056, -0.0110,  0.0004],\n",
       "         [ 0.0017,  0.0026,  0.0132,  ...,  0.0007, -0.0021, -0.0010],\n",
       "         ...,\n",
       "         [-0.0176,  0.0069, -0.0146,  ..., -0.0106,  0.0040, -0.0057],\n",
       "         [-0.0076,  0.0014,  0.0013,  ..., -0.0064, -0.0194,  0.0022],\n",
       "         [ 0.0012, -0.0116, -0.0108,  ...,  0.0043, -0.0086, -0.0009]]),\n",
       " 'transformer.resblocks.2.mlp.c_proj.bias': tensor([-1.1358e-02,  1.6665e-02, -9.8716e-03, -3.3284e-03, -7.9953e-03,\n",
       "         -8.8469e-03,  1.0394e-02, -1.4970e-02, -3.2713e-05,  7.7888e-03,\n",
       "         -2.0841e-02,  1.0335e-02,  1.7064e-02,  1.7698e-02,  1.0023e-02,\n",
       "          1.7507e-02, -5.7339e-03, -3.3850e-03,  7.8721e-03,  1.1352e-02,\n",
       "         -1.3457e-02, -4.9709e-03,  1.0983e-02, -1.5058e-02, -2.4549e-03,\n",
       "         -4.8717e-03,  4.0386e-03, -1.8235e-02, -1.7343e-02,  4.0737e-03,\n",
       "         -1.2172e-03, -2.2402e-03, -8.5369e-03, -1.7702e-02,  1.2314e-02,\n",
       "         -1.8754e-02, -1.1743e-02,  1.5475e-02, -1.4310e-02, -1.1048e-03,\n",
       "         -5.0330e-03, -4.2174e-03,  1.0398e-02, -4.7339e-03, -4.7212e-03,\n",
       "          3.7604e-04, -3.5973e-03, -5.3992e-03, -8.8087e-03, -8.0324e-04,\n",
       "         -7.0642e-03, -9.3759e-04,  1.1978e-02,  1.5125e-02,  1.6144e-02,\n",
       "          1.9529e-02,  1.0500e-02,  3.8431e-04, -4.1249e-03,  9.6950e-03,\n",
       "         -3.0037e-03,  1.6204e-02,  1.1624e-02,  9.5642e-03, -1.7405e-03,\n",
       "         -3.1775e-03, -7.1509e-03, -1.0279e-02, -6.9883e-03,  8.2077e-03,\n",
       "         -2.0755e-02, -4.1031e-04,  1.9247e-02,  1.8352e-03,  1.3280e-02,\n",
       "          1.9351e-04, -1.9460e-02,  1.4148e-02, -1.5767e-02, -2.1631e-02,\n",
       "          1.1708e-02, -4.6858e-03,  1.6424e-02, -5.5765e-03, -7.1241e-04,\n",
       "          5.2721e-03, -1.0672e-02, -1.3879e-02, -2.1268e-02, -1.9410e-02,\n",
       "         -1.1485e-02, -1.5675e-02,  1.1498e-02, -1.1412e-02, -1.7453e-02,\n",
       "          1.7243e-02,  3.6769e-03, -2.0523e-02,  2.1181e-02,  1.1290e-02,\n",
       "          7.9846e-03, -5.2080e-03, -4.6519e-03, -1.2635e-03,  7.3146e-03,\n",
       "         -1.0997e-02,  1.8071e-02, -1.3564e-02,  1.4146e-02,  3.8101e-03,\n",
       "         -9.8616e-03,  9.1994e-03, -8.4997e-03,  2.1784e-02, -9.4519e-03,\n",
       "         -1.2689e-02, -5.2905e-03, -1.6947e-02, -1.4504e-02,  8.9473e-03,\n",
       "         -7.2387e-03, -8.2120e-03, -1.2232e-02, -2.0607e-02, -2.1910e-03,\n",
       "          1.3008e-02,  1.0269e-02, -1.9651e-02,  1.6709e-02, -1.8992e-02,\n",
       "          9.4309e-03,  1.3942e-02,  1.8639e-02,  1.4576e-02, -4.7102e-04,\n",
       "         -9.5657e-03,  6.2176e-03,  1.6900e-02, -1.0441e-02,  1.5837e-02,\n",
       "         -1.1029e-03, -1.0196e-02,  5.6936e-03, -1.8939e-02, -1.9352e-02,\n",
       "          1.8685e-02, -1.2535e-02,  1.5237e-03, -2.0125e-02, -6.7442e-03,\n",
       "         -1.4241e-02,  1.5870e-02, -6.0302e-03, -1.3794e-02, -4.3689e-03,\n",
       "         -1.6035e-02,  1.5747e-03, -1.1690e-02, -1.4908e-02,  6.6484e-03,\n",
       "         -1.7606e-02, -7.1918e-03, -1.7160e-02, -8.1126e-03, -5.2905e-03,\n",
       "          2.0023e-02,  2.5999e-03,  1.7084e-02,  5.3595e-03,  9.3157e-03,\n",
       "         -5.2936e-03, -1.2346e-02, -9.2797e-03, -1.6128e-02, -1.4866e-02,\n",
       "          2.1763e-03, -1.2338e-02,  1.6545e-02, -2.1438e-02, -1.7131e-02,\n",
       "         -2.2705e-02, -3.5126e-03, -1.9162e-02, -1.9080e-03,  5.9786e-03,\n",
       "         -1.2120e-02, -1.9374e-02, -6.9554e-03, -1.8953e-02,  9.6957e-03,\n",
       "         -1.6281e-02,  1.1890e-02, -2.2407e-03,  1.4673e-02, -2.7592e-04,\n",
       "         -1.1958e-02, -2.0825e-03,  1.3089e-02, -2.0084e-02, -7.1828e-03,\n",
       "         -1.4101e-02, -1.9796e-03, -1.4368e-02,  1.3972e-02,  1.9137e-02,\n",
       "          5.1512e-03,  1.5218e-02,  1.7198e-02, -1.1232e-02, -2.0947e-02,\n",
       "         -8.3671e-03, -5.1537e-03, -3.7251e-03,  2.1775e-02,  6.8582e-03,\n",
       "          8.9409e-03,  2.2042e-02,  2.0083e-02, -5.2449e-03, -3.5118e-03,\n",
       "         -1.2641e-02,  1.6523e-02, -1.3691e-02, -1.3327e-02, -2.0639e-02,\n",
       "          1.3046e-02,  2.9010e-03, -5.9822e-04, -1.1983e-02, -9.4485e-03,\n",
       "          2.1736e-02, -1.2472e-02,  1.3686e-02,  1.6145e-02, -1.3766e-02,\n",
       "         -7.3519e-03, -1.6260e-02, -8.9438e-03, -2.0378e-02,  2.1754e-02,\n",
       "         -3.0694e-03,  9.0052e-03,  1.2026e-02, -1.7939e-02, -6.0594e-04,\n",
       "          1.5539e-02,  2.0124e-02,  1.8901e-02,  1.7361e-02,  1.8182e-02,\n",
       "          4.8452e-04,  1.2052e-02,  1.2909e-02, -8.9946e-04, -1.4988e-02,\n",
       "          1.0262e-02,  2.0550e-04,  1.0781e-02,  3.3126e-03, -1.8020e-02,\n",
       "         -2.1534e-02, -1.3788e-02,  1.0778e-02, -1.8575e-02, -1.3458e-03,\n",
       "          1.5679e-02, -3.0968e-03,  4.9653e-04, -1.2143e-02, -7.6490e-03,\n",
       "          7.9093e-04, -1.6596e-02,  1.7121e-02,  1.4869e-02,  1.5057e-02,\n",
       "          1.8272e-02, -3.4053e-03,  6.4328e-03,  4.3557e-03,  1.1960e-02,\n",
       "          2.8147e-03,  6.6270e-03, -1.2905e-02,  1.6300e-02, -2.0053e-02,\n",
       "          3.5751e-03,  1.7020e-02, -1.8774e-03,  2.1545e-02, -1.8825e-02,\n",
       "         -1.6106e-02, -1.4759e-02, -1.5563e-02,  2.2336e-02, -1.3144e-02,\n",
       "          1.7416e-03, -8.6378e-03,  1.3188e-02, -4.5008e-03, -1.3372e-02,\n",
       "         -1.7961e-02, -1.3025e-02, -1.8146e-02,  3.4078e-03,  8.9923e-03,\n",
       "         -1.0706e-02,  2.3066e-03, -1.0892e-03,  1.5363e-02, -8.5036e-03,\n",
       "          2.2004e-02, -1.6681e-02, -1.1885e-02,  1.4349e-02, -5.0074e-03,\n",
       "         -1.6082e-02,  1.8667e-02, -4.0502e-03, -1.2102e-02,  1.9299e-02,\n",
       "         -1.6409e-02,  1.0850e-02,  1.5311e-02,  1.7330e-02, -1.6980e-02,\n",
       "         -1.1759e-02, -1.4861e-02,  2.1662e-02, -8.7827e-03,  1.9150e-02,\n",
       "         -1.2108e-02, -2.1370e-02,  6.1153e-03,  1.3930e-02, -1.3670e-02,\n",
       "          1.1802e-02, -1.9444e-02,  6.1098e-03, -4.6323e-03,  3.7608e-03,\n",
       "         -3.8465e-03, -9.1787e-03,  2.0497e-02, -2.0917e-02,  1.0092e-02,\n",
       "         -1.2018e-03,  1.1661e-02, -1.7251e-02, -2.0667e-02, -1.5093e-03,\n",
       "         -1.0754e-02, -5.2275e-03,  6.4626e-03, -1.5276e-02,  9.4500e-03,\n",
       "          6.0789e-03, -1.4474e-02,  1.8884e-02, -1.2774e-02,  1.7839e-02,\n",
       "          5.6105e-03, -1.2791e-02, -7.4086e-03, -1.4415e-03,  1.9464e-02,\n",
       "          2.1726e-02, -1.5070e-02,  7.8849e-03, -1.5026e-02,  1.7511e-02,\n",
       "         -1.1754e-02, -1.9662e-02,  7.9313e-03,  1.9456e-02, -1.0469e-02,\n",
       "         -1.6985e-02, -9.0128e-03,  2.2394e-03, -1.8231e-02, -1.2322e-02,\n",
       "          2.0788e-02, -7.6048e-03,  6.2258e-03,  1.1665e-02, -1.6942e-02,\n",
       "          5.9329e-04, -1.0751e-02,  1.5584e-02, -1.8749e-02, -9.5851e-03,\n",
       "         -1.2636e-02, -7.0228e-03,  2.1575e-02,  1.4070e-03,  1.8672e-02,\n",
       "         -1.7308e-02, -1.7013e-02, -1.8334e-02,  5.3994e-03,  1.8766e-02,\n",
       "          3.9621e-03, -2.0104e-02, -1.7730e-02,  1.0281e-02, -6.1198e-03,\n",
       "          1.1802e-02, -4.1950e-03, -1.5977e-03,  1.7160e-02, -7.3725e-03,\n",
       "         -1.9198e-02, -3.3684e-03,  1.5646e-02,  3.3937e-04,  1.5110e-02,\n",
       "          8.9554e-04, -1.2173e-02,  2.1333e-02,  9.8638e-03,  2.0609e-02,\n",
       "         -1.5700e-02,  1.9667e-02, -2.1471e-02,  1.1301e-02, -2.0013e-02,\n",
       "          1.7330e-03, -1.9439e-02, -1.4968e-02, -2.6490e-03,  1.8445e-02,\n",
       "          1.2353e-02,  6.3921e-03,  9.9192e-03,  8.2863e-03, -2.0878e-02,\n",
       "         -2.2753e-02, -2.4875e-03,  1.7436e-02,  9.6849e-03,  1.3678e-02,\n",
       "          1.3411e-02,  1.2884e-02,  1.8005e-02,  8.7254e-03,  1.4011e-02,\n",
       "          7.3733e-03, -1.3896e-02,  1.2485e-02,  1.4408e-02, -1.9013e-02,\n",
       "          1.0726e-03, -1.2986e-02,  1.2524e-02, -1.6764e-02, -1.9910e-02,\n",
       "          5.7418e-03, -5.2724e-03, -4.0596e-04, -1.7184e-02,  1.5067e-02,\n",
       "          9.3770e-03, -9.4317e-03,  1.3349e-02, -1.4187e-02, -6.3902e-04,\n",
       "         -2.4382e-03, -1.3622e-02,  8.5342e-03, -2.2565e-02,  1.9866e-02,\n",
       "         -4.6369e-03,  5.3922e-03, -1.6225e-02,  1.2661e-02,  5.2068e-03,\n",
       "         -5.7568e-03,  9.3254e-03,  8.3909e-03, -8.2053e-03,  7.6527e-03,\n",
       "         -1.7501e-02,  1.7117e-02, -1.7327e-02,  1.6929e-02, -1.7197e-03,\n",
       "         -1.0311e-02, -1.3931e-03, -3.2504e-03,  1.3912e-02, -2.2089e-02,\n",
       "         -2.1703e-02,  2.1129e-02,  2.0952e-02,  2.1682e-02,  4.8102e-03,\n",
       "         -4.7879e-03,  2.0135e-02, -2.6767e-04,  1.2846e-02, -4.0949e-03,\n",
       "         -1.4024e-02, -6.7281e-03, -1.1861e-02, -2.0639e-02, -5.0866e-03,\n",
       "         -8.0060e-03,  1.6898e-02, -1.6653e-02,  5.2644e-03,  1.0062e-02,\n",
       "         -1.9718e-02,  1.5639e-02]),\n",
       " 'transformer.resblocks.2.ln_2.weight': tensor([1.0017, 1.0015, 1.0036, 1.0019, 1.0027, 0.9997, 1.0015, 1.0020, 1.0010,\n",
       "         1.0033, 0.9993, 1.0018, 1.0027, 1.0013, 1.0009, 1.0036, 1.0044, 1.0025,\n",
       "         1.0014, 1.0010, 0.9983, 0.9994, 1.0029, 1.0030, 1.0002, 1.0008, 0.9996,\n",
       "         1.0016, 1.0032, 1.0035, 1.0009, 1.0001, 1.0007, 1.0048, 0.9985, 1.0026,\n",
       "         0.9994, 1.0045, 1.0015, 1.0023, 1.0022, 1.0005, 1.0017, 0.9990, 1.0028,\n",
       "         1.0019, 1.0021, 1.0023, 1.0024, 1.0020, 1.0029, 1.0040, 1.0019, 1.0011,\n",
       "         0.9986, 1.0005, 1.0009, 1.0024, 1.0013, 1.0025, 1.0026, 1.0019, 1.0001,\n",
       "         1.0002, 1.0048, 1.0032, 1.0013, 1.0018, 1.0018, 1.0031, 1.0025, 1.0017,\n",
       "         1.0022, 1.0033, 1.0019, 0.9998, 1.0003, 1.0029, 1.0041, 1.0003, 1.0001,\n",
       "         1.0011, 1.0024, 1.0022, 1.0029, 1.0018, 1.0005, 0.9989, 1.0013, 1.0000,\n",
       "         1.0021, 1.0017, 1.0032, 1.0017, 1.0025, 1.0032, 1.0036, 0.9995, 1.0036,\n",
       "         0.9997, 1.0021, 1.0007, 1.0017, 1.0014, 1.0019, 1.0039, 1.0018, 1.0049,\n",
       "         1.0030, 1.0021, 1.0021, 1.0029, 1.0025, 1.0030, 1.0029, 1.0015, 1.0043,\n",
       "         0.9981, 1.0012, 0.9992, 0.9989, 1.0042, 0.9997, 1.0006, 1.0013, 1.0033,\n",
       "         1.0012, 0.9977, 1.0021, 1.0026, 1.0001, 1.0038, 1.0014, 1.0024, 1.0017,\n",
       "         1.0034, 0.9992, 1.0021, 1.0035, 1.0010, 1.0044, 1.0008, 1.0009, 1.0010,\n",
       "         1.0023, 1.0004, 1.0003, 0.9974, 1.0022, 1.0015, 1.0030, 1.0041, 1.0023,\n",
       "         1.0032, 1.0013, 1.0050, 0.9996, 1.0040, 1.0012, 1.0022, 1.0031, 1.0026,\n",
       "         1.0023, 1.0008, 1.0036, 1.0016, 1.0029, 1.0010, 1.0006, 1.0031, 1.0007,\n",
       "         1.0057, 1.0024, 1.0040, 1.0028, 0.9995, 1.0018, 1.0020, 1.0012, 1.0037,\n",
       "         1.0013, 1.0033, 1.0036, 1.0007, 1.0010, 1.0015, 1.0006, 1.0022, 1.0009,\n",
       "         1.0018, 1.0016, 0.9992, 1.0028, 1.0033, 1.0032, 1.0036, 1.0009, 1.0018,\n",
       "         1.0040, 1.0024, 1.0005, 0.9991, 1.0027, 1.0032, 1.0004, 1.0043, 0.9989,\n",
       "         1.0015, 1.0035, 1.0016, 1.0035, 1.0023, 1.0018, 0.9995, 1.0014, 1.0044,\n",
       "         0.9985, 1.0015, 0.9974, 0.9981, 1.0034, 0.9997, 1.0011, 0.9995, 1.0029,\n",
       "         1.0032, 1.0010, 1.0003, 1.0029, 0.9982, 1.0002, 0.9993, 0.9999, 1.0015,\n",
       "         1.0017, 0.9999, 1.0007, 1.0044, 1.0020, 1.0001, 1.0009, 0.9994, 1.0032,\n",
       "         1.0028, 1.0008, 1.0049, 1.0017, 1.0033, 1.0002, 0.9987, 1.0009, 1.0032,\n",
       "         0.9977, 1.0007, 1.0032, 0.9999, 1.0039, 1.0003, 1.0023, 1.0006, 1.0037,\n",
       "         1.0016, 1.0018, 1.0014, 1.0048, 1.0007, 1.0022, 1.0056, 1.0021, 1.0026,\n",
       "         1.0030, 1.0054, 1.0030, 1.0015, 1.0013, 1.0002, 1.0019, 1.0016, 1.0022,\n",
       "         1.0012, 1.0016, 1.0006, 1.0021, 1.0025, 1.0030, 1.0023, 1.0005, 1.0020,\n",
       "         1.0037, 1.0021, 1.0012, 1.0005, 0.9993, 1.0001, 1.0015, 1.0023, 1.0006,\n",
       "         1.0029, 1.0024, 0.9986, 1.0017, 1.0033, 1.0029, 1.0015, 1.0006, 1.0020,\n",
       "         1.0045, 1.0010, 1.0015, 1.0027, 1.0024, 1.0009, 1.0020, 1.0035, 1.0008,\n",
       "         1.0020, 1.0031, 1.0011, 1.0029, 0.9990, 1.0008, 1.0035, 1.0013, 1.0020,\n",
       "         1.0032, 1.0017, 1.0031, 1.0018, 1.0021, 1.0029, 0.9990, 1.0012, 1.0010,\n",
       "         1.0026, 1.0041, 1.0046, 1.0029, 1.0006, 0.9994, 1.0002, 1.0001, 1.0004,\n",
       "         1.0013, 1.0018, 1.0021, 1.0036, 1.0001, 0.9997, 1.0029, 1.0019, 1.0040,\n",
       "         1.0024, 0.9978, 1.0029, 1.0008, 1.0022, 1.0019, 0.9980, 0.9999, 1.0014,\n",
       "         1.0006, 1.0014, 1.0007, 1.0005, 1.0020, 1.0038, 1.0033, 1.0037, 0.9993,\n",
       "         1.0017, 1.0010, 0.9977, 1.0011, 1.0026, 1.0037, 1.0037, 1.0005, 1.0010,\n",
       "         1.0028, 1.0023, 1.0040, 1.0014, 0.9994, 1.0012, 1.0020, 0.9990, 1.0049,\n",
       "         1.0025, 1.0051, 1.0004, 1.0016, 1.0025, 1.0011, 1.0023, 1.0021, 1.0000,\n",
       "         1.0026, 1.0006, 0.9995, 1.0051, 1.0031, 1.0012, 1.0013, 1.0013, 1.0036,\n",
       "         1.0044, 0.9997, 1.0005, 1.0012, 1.0001, 1.0015, 1.0033, 1.0030, 1.0031,\n",
       "         1.0013, 0.9995, 0.9996, 1.0028, 1.0021, 1.0017, 1.0029, 1.0016, 1.0008,\n",
       "         1.0018, 1.0009, 1.0018, 1.0047, 1.0024, 0.9991, 1.0023, 1.0030, 1.0004,\n",
       "         1.0013, 1.0008, 1.0044, 1.0012, 1.0006, 1.0024, 1.0037, 1.0007, 0.9992,\n",
       "         1.0004, 1.0011, 0.9998, 1.0027, 1.0019, 1.0005, 1.0028, 0.9998, 1.0025,\n",
       "         0.9989, 1.0032, 1.0028, 1.0039, 1.0001, 1.0039, 1.0006, 0.9976, 1.0006,\n",
       "         1.0016, 0.9991, 1.0011, 1.0024, 1.0030, 1.0007, 1.0024, 0.9982, 0.9984,\n",
       "         1.0009, 1.0004, 1.0002, 1.0013, 1.0024, 1.0017, 1.0023, 1.0012, 1.0019,\n",
       "         1.0010, 1.0036, 1.0035, 1.0015, 1.0025, 1.0031, 1.0055, 1.0001, 1.0012,\n",
       "         0.9990, 1.0029, 1.0059, 1.0015, 1.0021, 1.0007, 1.0015, 1.0019, 1.0030,\n",
       "         0.9990, 1.0013, 1.0022, 1.0021, 1.0022, 0.9993, 1.0025, 1.0003, 1.0018,\n",
       "         1.0005, 1.0016, 1.0032, 1.0009, 1.0037, 1.0037, 1.0007, 1.0026]),\n",
       " 'transformer.resblocks.2.ln_2.bias': tensor([ 9.6036e-04, -5.5686e-04,  5.1491e-04, -2.3627e-03,  1.5546e-03,\n",
       "         -1.0721e-03, -7.9342e-04, -9.5401e-04,  1.7343e-03, -1.0483e-03,\n",
       "          3.6719e-03, -8.5162e-04,  4.6825e-04,  6.8710e-04,  1.2538e-03,\n",
       "         -1.0737e-03,  1.9568e-03,  2.1247e-04, -8.7972e-04, -6.9864e-04,\n",
       "         -1.0814e-03, -1.9967e-03,  4.5694e-04,  1.3153e-03,  2.5881e-03,\n",
       "         -3.1177e-04, -1.4079e-03,  2.2728e-03,  1.6887e-03,  1.7027e-03,\n",
       "         -2.7296e-04, -7.0640e-04,  3.1903e-04, -1.1301e-03, -4.3833e-04,\n",
       "         -1.1563e-05, -1.3032e-03,  1.2124e-03,  5.7198e-04, -6.6166e-04,\n",
       "         -2.7743e-03,  2.2294e-04,  1.6027e-03,  1.1529e-03, -1.1566e-04,\n",
       "          1.4296e-03,  4.3116e-04,  5.0315e-04, -2.0903e-03,  8.3096e-05,\n",
       "          9.2212e-04, -2.0101e-04, -2.1918e-03,  1.2798e-03,  4.7629e-03,\n",
       "          1.0251e-03,  9.2637e-04,  3.1349e-03, -5.8664e-04,  1.8150e-03,\n",
       "          7.4137e-04, -1.3332e-03,  8.8133e-04, -7.0228e-04,  2.0083e-04,\n",
       "          1.4939e-04,  1.2702e-03, -1.9764e-03, -1.1765e-03,  9.8065e-04,\n",
       "          4.5193e-05, -2.6606e-03, -4.9362e-04, -1.5944e-03,  2.5023e-03,\n",
       "         -1.7781e-03,  1.1934e-03, -8.9991e-05,  1.3139e-03, -1.7968e-03,\n",
       "         -9.6552e-04,  3.7701e-03,  2.7256e-03, -2.7840e-04, -1.5532e-03,\n",
       "          7.5135e-04, -2.3470e-03, -1.0811e-03,  1.5311e-03,  1.7424e-03,\n",
       "          2.1970e-03, -1.5876e-03,  2.8857e-04, -8.5501e-04, -1.2918e-03,\n",
       "          1.8935e-03, -4.8391e-04, -9.6690e-05, -9.2241e-04, -7.5884e-04,\n",
       "          2.8214e-04,  4.9544e-04, -3.7123e-04, -1.8933e-03,  2.1390e-03,\n",
       "          5.2118e-04,  1.7296e-03,  2.2571e-03,  3.2415e-04,  1.2744e-03,\n",
       "         -9.2447e-04,  1.0312e-03, -1.9136e-03,  3.1393e-04, -7.0964e-04,\n",
       "          1.0598e-03,  2.6238e-03,  7.0327e-04, -1.0784e-03, -1.7698e-03,\n",
       "         -1.3468e-03,  8.3846e-04, -1.5271e-03, -1.8079e-04,  2.8792e-03,\n",
       "          1.8043e-03,  2.6025e-04,  2.2267e-03,  2.4429e-03,  1.8193e-03,\n",
       "         -3.7372e-06,  1.9908e-04,  5.2142e-04, -2.0503e-03,  2.5437e-03,\n",
       "         -7.1719e-04, -1.9009e-03, -1.3668e-04,  7.7856e-04,  3.8749e-04,\n",
       "          3.9504e-04, -2.5191e-03, -2.0639e-04,  3.6486e-04, -3.1881e-04,\n",
       "          1.1559e-03, -3.9358e-04, -8.1601e-05,  1.2746e-04, -1.8014e-04,\n",
       "          2.5576e-03,  2.3151e-03, -1.7276e-04,  9.4496e-05,  3.7654e-04,\n",
       "          9.2347e-04, -1.9163e-03,  2.8564e-03, -1.2782e-03,  3.3517e-04,\n",
       "          2.3834e-03,  1.4023e-03, -5.1615e-05,  4.0447e-04, -7.3634e-04,\n",
       "         -1.9770e-04, -3.4948e-04,  5.8188e-04, -2.3576e-03,  4.6603e-04,\n",
       "          7.9387e-04, -2.5870e-03,  2.7028e-04, -1.7998e-03, -2.5819e-03,\n",
       "         -7.7725e-04,  7.8235e-04, -1.9233e-03, -6.6900e-04, -9.0536e-04,\n",
       "          2.2278e-04,  8.1287e-04,  9.5900e-04,  3.5805e-03,  4.5147e-05,\n",
       "         -2.9736e-03, -2.5015e-04,  1.1322e-03,  1.6711e-04,  3.9709e-04,\n",
       "         -1.2462e-03, -1.3678e-03, -1.5381e-03, -2.4091e-03,  1.4203e-03,\n",
       "         -4.8219e-04, -4.9122e-04, -1.4540e-03,  2.9819e-05, -1.3505e-03,\n",
       "          7.4738e-04,  3.9912e-03,  1.9542e-04,  2.0235e-03, -4.1164e-04,\n",
       "          1.0931e-03, -1.2867e-03, -1.5176e-03,  9.8335e-04,  3.0250e-03,\n",
       "          4.7075e-04, -2.0575e-03,  1.7414e-03,  9.6168e-05,  2.3070e-04,\n",
       "          4.7563e-04,  1.1301e-03,  1.2266e-04, -4.7097e-04,  1.1789e-03,\n",
       "         -2.1233e-03, -1.3371e-03,  3.3766e-04, -5.3108e-04, -1.5088e-03,\n",
       "          7.8605e-04,  1.2916e-03, -1.9587e-03, -1.3695e-04,  1.7508e-03,\n",
       "          4.2078e-03, -6.0515e-04, -2.1308e-03,  2.1383e-03,  7.4593e-05,\n",
       "          9.1606e-04, -1.3195e-03,  9.2175e-04,  1.8367e-03,  2.2186e-03,\n",
       "          4.8414e-04, -1.0804e-03,  1.6001e-04,  3.6883e-04, -1.5874e-03,\n",
       "         -1.1511e-03, -5.6079e-04,  3.0869e-03, -5.5352e-04,  9.6025e-04,\n",
       "          6.7800e-05,  1.1954e-03, -3.5784e-04,  6.8722e-04, -2.5602e-03,\n",
       "          8.2967e-04, -1.6470e-03,  7.7879e-04,  6.3187e-04, -3.1758e-04,\n",
       "          1.4541e-03, -1.2118e-03,  3.6547e-04, -9.7687e-04,  2.7097e-03,\n",
       "         -4.4467e-04, -1.7786e-03,  1.1590e-03,  1.6243e-05,  1.2969e-03,\n",
       "         -9.3034e-04, -3.6795e-04, -4.5187e-04, -5.5089e-04, -7.7353e-04,\n",
       "          1.0512e-03,  4.6909e-04,  1.7174e-03,  9.8037e-04,  1.9934e-03,\n",
       "          2.4868e-03, -2.7199e-03,  1.3936e-03, -2.5735e-03,  2.5853e-03,\n",
       "          7.5870e-04,  2.4877e-03,  1.5328e-03, -2.9404e-03,  7.7285e-04,\n",
       "          5.5241e-04, -8.8980e-04,  1.2134e-03,  2.5291e-03,  6.0783e-04,\n",
       "          7.1562e-04, -1.9041e-03,  2.1805e-03, -1.2733e-05,  4.0275e-04,\n",
       "          2.0434e-03,  1.0208e-03,  3.3948e-04,  5.3178e-04, -5.5920e-04,\n",
       "          6.2152e-04, -8.4364e-05,  2.2554e-04,  8.1837e-05, -7.1517e-04,\n",
       "          3.7523e-04,  1.6851e-03,  6.5777e-04,  8.4397e-04, -1.7491e-03,\n",
       "         -6.9004e-05, -1.2867e-03,  2.3481e-04, -2.3063e-04,  2.3344e-04,\n",
       "          2.1264e-04, -2.1577e-03,  1.6304e-04,  2.5500e-03,  2.5870e-03,\n",
       "         -3.2757e-04, -5.3066e-04, -2.2426e-03,  4.0972e-04,  1.1742e-03,\n",
       "         -8.8545e-04, -2.7238e-03, -2.1051e-03,  4.1600e-04,  2.5871e-03,\n",
       "         -8.1362e-04, -7.0827e-04,  5.3345e-04,  2.6090e-04,  9.2796e-04,\n",
       "          4.7837e-04, -1.0978e-03, -1.4581e-03,  9.0868e-04,  2.6720e-06,\n",
       "          9.2143e-04,  7.8322e-04, -3.2395e-03, -2.2707e-03, -1.6847e-04,\n",
       "          1.4857e-03,  1.2316e-03, -1.0708e-03, -2.9717e-04,  7.6947e-04,\n",
       "         -2.9018e-03, -1.9575e-03,  1.4303e-03, -2.4194e-03,  5.9708e-05,\n",
       "         -1.8624e-04, -1.3808e-04, -1.5041e-03,  2.8829e-03, -2.5314e-04,\n",
       "          8.8265e-04, -2.6715e-03, -2.0868e-04,  2.2024e-03,  8.1313e-04,\n",
       "         -3.8304e-04, -2.2214e-03, -1.4958e-03, -6.4811e-04, -1.6366e-03,\n",
       "         -1.0401e-03,  1.3264e-03, -7.8210e-04, -5.2994e-05,  2.8493e-03,\n",
       "          1.2239e-03,  6.6392e-05, -6.7804e-04,  1.6265e-03,  2.0180e-03,\n",
       "          1.7708e-03,  1.7087e-03,  2.3881e-03, -2.3148e-03,  1.2658e-05,\n",
       "         -2.0781e-03, -1.3105e-04,  3.5067e-04, -1.0786e-03,  1.0297e-03,\n",
       "         -1.6367e-05,  1.7033e-03, -9.6938e-04, -1.0537e-03, -1.0197e-03,\n",
       "         -6.6334e-05, -2.4906e-03, -6.7434e-04,  5.6243e-04,  3.3497e-03,\n",
       "          7.9156e-04,  1.0765e-03, -1.3600e-04, -5.1377e-04, -8.4126e-04,\n",
       "          1.0880e-03, -2.1945e-03, -1.4300e-03, -2.1416e-03, -1.3192e-04,\n",
       "          7.6611e-04, -7.1508e-04,  2.5955e-04, -1.0631e-03, -3.9108e-04,\n",
       "          3.6419e-03, -1.3243e-03,  1.9678e-03, -2.5880e-04,  1.2530e-03,\n",
       "          9.2748e-04,  9.8708e-04,  1.6815e-03,  1.7524e-03,  2.6324e-03,\n",
       "          4.6496e-04,  2.6325e-03,  9.2720e-04, -1.1844e-03,  1.9958e-03,\n",
       "         -2.7506e-03, -1.5969e-03,  1.1272e-03, -2.6340e-03, -1.5733e-04,\n",
       "         -2.0889e-03,  1.7880e-03, -9.6916e-04,  1.9068e-04, -1.2977e-04,\n",
       "          1.9977e-03,  1.8005e-03, -2.2072e-03, -1.2077e-03,  3.6453e-03,\n",
       "          1.7621e-03,  1.7448e-03, -6.4222e-04, -1.5561e-03, -7.2467e-04,\n",
       "          2.4648e-03,  1.2332e-04, -4.4748e-04, -8.4531e-05,  6.7919e-05,\n",
       "          6.3510e-04, -5.2366e-04, -1.9447e-03,  8.6605e-04,  4.5338e-04,\n",
       "          2.2409e-04,  1.5899e-04, -7.5691e-04, -5.3770e-04,  3.3785e-03,\n",
       "         -9.7621e-04,  1.1709e-03,  1.0616e-03,  8.3273e-04, -3.0788e-04,\n",
       "          1.7569e-03,  1.7982e-04, -1.3010e-03,  3.0246e-04,  2.1820e-03,\n",
       "          1.4556e-04, -1.1976e-03,  4.6039e-04,  2.1461e-03, -3.9154e-04,\n",
       "         -1.3102e-04, -1.4051e-04,  3.5775e-03,  9.8829e-04, -1.3237e-03,\n",
       "          1.6038e-03,  1.3733e-04,  8.8791e-04,  6.1196e-04, -7.4076e-04,\n",
       "         -2.0579e-03, -1.4424e-03,  3.2579e-04, -6.3689e-04,  1.1592e-03,\n",
       "          7.7510e-04,  9.0274e-04,  8.0486e-04,  1.0745e-03,  1.4727e-03,\n",
       "         -1.9081e-03,  2.1803e-04,  8.7186e-04, -8.3855e-04, -1.0766e-03,\n",
       "          2.1053e-03, -6.4145e-04]),\n",
       " 'transformer.resblocks.3.attn.in_proj_weight': tensor([[-1.9890e-02,  7.9917e-04, -7.8904e-03,  ...,  1.3954e-02,\n",
       "           3.7170e-02, -3.1203e-02],\n",
       "         [ 1.1332e-01, -1.7356e-03,  2.6082e-02,  ..., -1.0695e-03,\n",
       "          -8.6503e-03, -9.9977e-05],\n",
       "         [-7.5998e-02, -3.1627e-03, -2.6613e-02,  ..., -2.7176e-02,\n",
       "          -6.9952e-03, -7.2870e-02],\n",
       "         ...,\n",
       "         [ 1.9832e-02, -2.1329e-02,  2.3266e-02,  ...,  4.5910e-02,\n",
       "           2.7750e-02,  1.2606e-02],\n",
       "         [-6.0293e-02,  2.6826e-02, -5.0036e-02,  ...,  4.1613e-02,\n",
       "          -5.9368e-02,  7.4325e-02],\n",
       "         [ 2.5464e-02, -1.3515e-02,  1.9036e-02,  ..., -7.1347e-03,\n",
       "           4.7304e-02, -3.7281e-03]]),\n",
       " 'transformer.resblocks.3.attn.in_proj_bias': tensor([-0.0055,  0.0003,  0.0025,  ...,  0.0008,  0.0004, -0.0018]),\n",
       " 'transformer.resblocks.3.attn.out_proj.weight': tensor([[ 1.2138e-03, -8.6110e-03, -5.4841e-03,  ...,  1.7725e-02,\n",
       "           1.6979e-02,  1.3019e-02],\n",
       "         [-6.9294e-03, -6.9068e-03, -2.0699e-02,  ..., -1.7289e-02,\n",
       "           6.1705e-05,  1.0792e-03],\n",
       "         [-8.9462e-03,  1.5370e-02,  1.3009e-02,  ...,  1.0167e-02,\n",
       "          -5.7642e-03,  6.6167e-03],\n",
       "         ...,\n",
       "         [ 2.6358e-03, -7.2706e-03,  1.7091e-02,  ...,  3.2101e-03,\n",
       "          -6.9854e-03, -6.6105e-03],\n",
       "         [-5.8642e-03,  6.0897e-03,  2.4875e-03,  ..., -9.1316e-03,\n",
       "          -1.0531e-02,  9.6797e-03],\n",
       "         [-2.0460e-03,  1.6571e-02, -6.0884e-03,  ..., -1.4865e-02,\n",
       "          -2.0749e-02, -1.0724e-02]]),\n",
       " 'transformer.resblocks.3.attn.out_proj.bias': tensor([ 7.8802e-04, -1.0959e-03, -4.2048e-04,  1.4813e-04,  6.0576e-04,\n",
       "         -5.3310e-04,  5.4590e-04,  9.0415e-04, -1.4848e-03,  1.1802e-04,\n",
       "         -8.0188e-04,  1.0137e-04, -8.4586e-04, -5.1828e-04, -1.2619e-03,\n",
       "          1.4398e-03, -6.9278e-04,  9.8153e-04,  2.3402e-04, -2.4516e-07,\n",
       "          8.6155e-04,  1.2917e-04,  3.5875e-04, -1.9857e-04, -1.2062e-03,\n",
       "         -6.9480e-04, -5.5211e-04, -6.0590e-04, -5.9502e-04, -1.3321e-03,\n",
       "          1.0205e-03,  3.3011e-04,  9.5217e-05, -1.0696e-03, -4.3874e-04,\n",
       "          1.4174e-03, -3.8197e-04,  4.3887e-04,  4.3105e-04, -1.0742e-03,\n",
       "          6.3918e-04, -2.8960e-04,  1.5397e-06,  8.2915e-04, -9.4773e-04,\n",
       "         -2.5488e-05, -8.3063e-04, -3.2671e-04,  4.0846e-04, -2.6176e-04,\n",
       "         -4.4150e-04, -1.8788e-04, -4.3656e-04, -4.8919e-04,  2.5166e-04,\n",
       "          3.9613e-04,  2.1661e-04, -8.4374e-04,  4.2055e-04, -1.0261e-03,\n",
       "         -1.9355e-04, -5.8355e-04,  1.2540e-04,  8.2405e-04, -3.0776e-04,\n",
       "         -8.7003e-04, -8.2438e-06,  1.3459e-03, -1.7467e-05,  8.9075e-04,\n",
       "         -7.6448e-04,  9.5889e-04,  1.7319e-04, -1.0553e-04, -1.3001e-03,\n",
       "          1.0777e-03,  2.4758e-04, -1.0296e-04, -1.5317e-04,  4.1763e-04,\n",
       "         -2.7092e-04, -2.1349e-04,  1.0770e-04,  1.0083e-03,  6.3577e-04,\n",
       "         -1.2707e-03, -4.3986e-04, -2.7567e-04, -1.5546e-04, -3.9017e-04,\n",
       "          6.0790e-05,  6.9696e-04,  3.6312e-04, -1.0002e-03, -8.4289e-04,\n",
       "         -1.3749e-03, -5.6882e-05, -1.7781e-04, -6.1451e-04,  6.5963e-04,\n",
       "          6.1513e-04, -7.5487e-04,  4.5378e-04,  1.4616e-04,  5.3374e-05,\n",
       "         -5.8909e-04,  4.8549e-05,  7.4830e-04,  1.4821e-03, -1.0714e-03,\n",
       "          6.9931e-04, -1.3285e-04,  1.1482e-03,  7.3617e-04, -9.6476e-04,\n",
       "         -1.4727e-03,  9.4538e-04, -2.8883e-04,  3.0555e-04, -1.3034e-03,\n",
       "          2.6022e-04, -6.8250e-04,  8.3749e-04, -5.0996e-04, -4.1121e-04,\n",
       "          3.3096e-04, -7.9333e-04, -6.2958e-04, -4.4812e-04, -3.8888e-04,\n",
       "          3.7361e-04,  6.1388e-04, -6.4968e-04, -5.1006e-04, -1.5977e-03,\n",
       "          5.9844e-04, -2.9833e-07,  3.8871e-04, -9.0284e-04, -1.5753e-04,\n",
       "         -3.4038e-04,  1.1600e-03,  9.5851e-04,  1.8910e-03,  8.0115e-04,\n",
       "          3.1665e-04, -2.6407e-04,  3.5776e-04,  5.4981e-04,  8.2429e-04,\n",
       "          1.0509e-03,  1.5045e-03,  7.0476e-04,  1.2283e-03,  5.7058e-05,\n",
       "          1.4599e-03,  2.7068e-04, -2.7271e-04,  8.9926e-04,  4.9774e-05,\n",
       "         -1.0894e-03,  5.3381e-05,  3.4414e-04, -2.8551e-05, -3.2911e-04,\n",
       "          3.6158e-04,  3.0408e-04, -1.9796e-04,  2.0800e-05, -4.2019e-04,\n",
       "          6.4472e-05, -3.1430e-04, -4.5353e-04, -1.5587e-04,  1.2014e-03,\n",
       "         -6.6050e-04, -2.3075e-05,  1.1487e-03, -1.8113e-04, -1.0363e-04,\n",
       "         -8.4471e-04, -7.0410e-04, -4.3648e-04, -1.3517e-03,  1.2999e-03,\n",
       "         -1.5741e-04, -6.4317e-05,  1.5858e-04,  1.2971e-04, -5.3209e-04,\n",
       "          7.6090e-04,  1.2330e-03,  2.1183e-04,  8.8640e-04,  3.6444e-04,\n",
       "          9.7396e-04,  1.2760e-03,  9.1501e-04,  1.2344e-03,  1.3314e-03,\n",
       "          1.2384e-05, -1.3890e-03, -8.3275e-05,  1.7686e-03,  5.3634e-04,\n",
       "          1.0630e-03, -7.9051e-04,  3.1074e-04,  1.7452e-05,  1.3248e-04,\n",
       "          6.4428e-04,  6.0721e-04, -5.1045e-04,  7.8799e-04, -1.3709e-03,\n",
       "          2.1905e-05,  9.2415e-04, -1.2904e-03, -7.2438e-04, -3.1965e-04,\n",
       "         -3.6496e-04, -1.0768e-03,  6.5583e-04,  1.2268e-03,  7.7026e-04,\n",
       "          4.9016e-04, -1.2054e-03, -1.7441e-04, -2.2835e-04,  4.7594e-04,\n",
       "         -6.4864e-04,  7.6710e-04,  3.9312e-04, -1.5368e-04,  7.8510e-04,\n",
       "         -5.4224e-05,  7.9357e-04, -1.3349e-03,  3.6998e-04, -2.6502e-04,\n",
       "          5.2317e-04,  4.7961e-04,  4.1234e-04,  8.1329e-04, -5.9499e-04,\n",
       "         -3.5871e-04,  7.7752e-04, -8.5179e-04, -3.2383e-04, -8.1965e-04,\n",
       "          8.2194e-04, -3.9534e-04,  1.2912e-04,  3.2043e-05, -9.0936e-04,\n",
       "          4.3199e-05, -1.2230e-04, -4.5630e-04, -5.9366e-04, -8.3725e-04,\n",
       "          6.0340e-04, -9.8721e-05,  7.4366e-04, -5.0440e-04, -5.9440e-04,\n",
       "          1.0165e-03,  4.4093e-04,  8.7921e-04, -3.2764e-04, -1.0831e-03,\n",
       "          1.5001e-04, -1.4628e-04, -7.8241e-04,  3.5347e-04,  1.1361e-03,\n",
       "         -6.3424e-04, -2.3917e-04, -4.2841e-04,  1.4266e-03, -1.7132e-03,\n",
       "         -2.0740e-04,  2.2647e-03, -7.2318e-04, -8.4101e-04, -1.3343e-03,\n",
       "          4.9310e-04,  7.7356e-04,  5.7242e-04,  1.5091e-03, -5.4748e-04,\n",
       "          3.4816e-04, -2.9374e-04, -1.1992e-03,  6.9843e-04, -4.3892e-04,\n",
       "          1.1672e-03, -4.0814e-04,  4.8874e-04, -9.5569e-04, -1.0131e-03,\n",
       "         -6.8439e-04,  1.3148e-04,  7.5449e-04, -6.2787e-04, -6.2198e-05,\n",
       "         -5.2232e-04, -6.5035e-05,  4.2281e-04, -1.2790e-03, -1.2987e-03,\n",
       "         -8.5242e-05,  8.0088e-04, -1.0703e-03, -9.2119e-04,  4.1333e-04,\n",
       "          2.6160e-04,  1.7740e-03,  1.0124e-03,  1.0885e-04,  9.2491e-04,\n",
       "          1.2716e-03, -7.0258e-04, -4.4747e-04, -1.0666e-04, -1.5210e-03,\n",
       "         -9.3008e-05,  7.0121e-04,  1.4860e-04, -1.1332e-03, -1.7892e-03,\n",
       "         -1.2459e-03,  5.0255e-04,  9.4984e-04,  2.1389e-04,  9.9234e-04,\n",
       "          6.6279e-05, -9.5051e-04, -3.1415e-04,  8.7414e-04,  1.5654e-03,\n",
       "         -9.2401e-04,  1.8031e-03,  4.4282e-04, -5.9303e-04, -1.5326e-03,\n",
       "          5.7764e-04, -3.7566e-04,  8.1183e-04, -2.9159e-04,  1.2006e-03,\n",
       "         -5.3101e-04, -1.6982e-03, -1.1380e-03, -9.8595e-04, -8.1732e-05,\n",
       "         -8.9956e-04,  1.9590e-03,  8.7024e-04,  1.1465e-03, -1.0403e-03,\n",
       "          5.4723e-04, -4.5247e-04,  1.4954e-03,  1.5580e-03, -6.1752e-04,\n",
       "          1.3165e-03,  7.3467e-04, -9.9260e-04,  1.7953e-03,  1.0783e-03,\n",
       "         -3.5182e-04,  9.3690e-04, -2.8399e-04, -3.0742e-04,  1.6531e-04,\n",
       "          2.2229e-03, -7.9666e-04,  1.2598e-03,  2.9490e-04, -2.3668e-04,\n",
       "          5.2772e-04, -2.4313e-04, -1.3510e-03, -6.8196e-04, -1.8706e-04,\n",
       "         -5.1962e-04,  3.9069e-04,  8.8417e-04,  8.5028e-04,  4.9849e-04,\n",
       "          1.8812e-04, -6.0347e-05,  1.2807e-04,  1.4597e-03, -1.5506e-05,\n",
       "          3.7962e-04, -3.9221e-04,  2.8316e-04,  2.6325e-04,  1.1370e-03,\n",
       "         -2.5141e-04, -6.4522e-04, -7.0129e-04,  4.6410e-04, -6.0807e-05,\n",
       "          3.5090e-04,  1.0198e-04, -4.3816e-04,  6.2294e-04,  1.9892e-04,\n",
       "         -6.2974e-04, -3.8650e-04,  6.2637e-04,  2.1897e-04,  3.8680e-04,\n",
       "         -1.3793e-03,  2.3594e-04,  7.5515e-05,  6.8007e-04,  6.5702e-04,\n",
       "         -1.6208e-03,  5.1776e-05,  1.6230e-04, -3.3797e-05, -1.0200e-03,\n",
       "          3.2066e-05, -8.5971e-04,  8.3477e-05,  2.7172e-04, -6.7379e-04,\n",
       "         -1.1178e-03,  2.0455e-03,  3.4878e-05,  7.3972e-04, -1.2518e-03,\n",
       "         -4.7098e-04,  5.4639e-04, -6.1397e-04,  8.2873e-04,  2.4208e-04,\n",
       "         -5.2396e-04,  3.4206e-04,  4.8837e-04,  1.5775e-04, -1.2023e-03,\n",
       "         -9.9014e-04, -9.8919e-04,  8.0634e-04,  9.5122e-04, -1.8510e-04,\n",
       "         -6.6679e-05,  7.6840e-05, -7.0688e-04,  2.0569e-03,  8.8566e-04,\n",
       "         -2.0983e-04, -1.3996e-04,  6.3445e-04,  2.8372e-04, -5.5682e-04,\n",
       "         -2.5763e-04, -5.1818e-04,  2.6674e-04,  6.9409e-04,  2.2938e-04,\n",
       "         -4.9832e-04, -4.8772e-04, -4.2784e-04, -5.1990e-04, -2.2592e-03,\n",
       "         -9.4148e-04, -4.5198e-04, -2.9995e-04,  1.6443e-04, -6.9892e-04,\n",
       "          7.2194e-04, -1.4327e-03, -4.9201e-04,  1.3065e-04, -6.5380e-04,\n",
       "         -8.9832e-04, -2.5321e-04,  3.8051e-04,  1.3319e-04, -9.0289e-05,\n",
       "          6.4482e-04, -1.2881e-03, -1.4111e-03,  3.8473e-04,  9.0052e-05,\n",
       "          7.3723e-04, -6.9619e-04, -4.8719e-04, -8.1045e-06,  2.8656e-04,\n",
       "          1.9117e-04,  1.8277e-03, -7.4678e-04, -3.6882e-04, -1.1045e-03,\n",
       "         -7.9170e-04, -4.4762e-04,  1.4845e-05,  4.6272e-04, -9.0748e-04,\n",
       "         -1.3037e-04, -3.4248e-04, -4.4875e-04,  2.2371e-04,  4.8383e-04,\n",
       "         -3.1138e-04,  1.9673e-04]),\n",
       " 'transformer.resblocks.3.ln_1.weight': tensor([1.0009, 0.9997, 0.9979, 0.9977, 0.9980, 0.9988, 0.9997, 1.0002, 0.9987,\n",
       "         1.0013, 0.9989, 0.9987, 0.9993, 0.9967, 0.9982, 0.9991, 0.9961, 1.0003,\n",
       "         1.0005, 1.0015, 0.9991, 0.9973, 0.9993, 0.9975, 0.9998, 0.9987, 0.9993,\n",
       "         0.9987, 0.9991, 1.0011, 0.9988, 0.9963, 0.9986, 1.0006, 0.9977, 0.9983,\n",
       "         1.0012, 0.9965, 0.9944, 1.0017, 1.0017, 0.9981, 0.9968, 1.0018, 0.9980,\n",
       "         0.9984, 1.0010, 0.9990, 0.9984, 0.9983, 1.0005, 0.9994, 1.0020, 0.9986,\n",
       "         0.9996, 1.0008, 1.0015, 0.9979, 1.0026, 0.9988, 0.9988, 1.0006, 1.0014,\n",
       "         1.0006, 1.0003, 0.9999, 0.9991, 0.9984, 0.9986, 0.9987, 1.0009, 1.0004,\n",
       "         0.9971, 0.9990, 0.9988, 0.9973, 0.9981, 0.9988, 0.9989, 1.0019, 0.9985,\n",
       "         0.9986, 0.9954, 0.9998, 0.9976, 0.9977, 1.0003, 0.9990, 0.9986, 0.9975,\n",
       "         0.9981, 0.9987, 0.9991, 0.9982, 1.0012, 0.9990, 0.9995, 1.0009, 1.0006,\n",
       "         0.9981, 0.9989, 1.0006, 0.9992, 0.9998, 1.0005, 0.9975, 0.9997, 1.0010,\n",
       "         0.9992, 0.9972, 0.9997, 0.9989, 0.9998, 0.9996, 0.9992, 1.0008, 0.9982,\n",
       "         0.9988, 1.0006, 0.9994, 0.9978, 1.0000, 0.9989, 0.9981, 0.9972, 0.9982,\n",
       "         1.0010, 0.9970, 0.9998, 0.9992, 0.9960, 0.9979, 0.9997, 0.9987, 1.0004,\n",
       "         0.9984, 1.0016, 0.9999, 0.9993, 0.9991, 0.9990, 0.9996, 1.0010, 0.9971,\n",
       "         0.9991, 1.0016, 1.0005, 0.9972, 0.9987, 1.0003, 1.0018, 0.9975, 0.9988,\n",
       "         0.9985, 1.0020, 1.0005, 1.0014, 0.9972, 1.0003, 1.0023, 0.9977, 0.9986,\n",
       "         0.9999, 0.9985, 1.0017, 0.9987, 0.9993, 0.9977, 0.9988, 0.9999, 1.0016,\n",
       "         1.0007, 1.0018, 0.9964, 0.9990, 0.9992, 0.9980, 1.0019, 0.9990, 0.9981,\n",
       "         0.9973, 0.9978, 0.9980, 1.0012, 0.9974, 0.9955, 0.9987, 0.9974, 1.0037,\n",
       "         0.9993, 0.9980, 0.9986, 0.9981, 0.9982, 1.0011, 0.9999, 0.9979, 1.0000,\n",
       "         1.0005, 0.9998, 1.0017, 1.0032, 0.9992, 1.0002, 1.0001, 0.9986, 0.9997,\n",
       "         0.9993, 0.9997, 1.0011, 0.9984, 0.9991, 1.0000, 1.0019, 0.9952, 1.0012,\n",
       "         0.9999, 0.9988, 0.9992, 0.9991, 0.9998, 1.0015, 0.9968, 0.9994, 0.9991,\n",
       "         1.0007, 0.9995, 0.9983, 0.9992, 0.9990, 0.9978, 0.9999, 0.9986, 0.9985,\n",
       "         0.9973, 0.9966, 1.0007, 1.0012, 0.9989, 0.9972, 1.0010, 0.9977, 0.9991,\n",
       "         1.0007, 1.0014, 1.0014, 1.0000, 0.9983, 0.9996, 0.9987, 1.0006, 0.9999,\n",
       "         0.9975, 0.9996, 1.0015, 0.9980, 1.0002, 0.9947, 0.9987, 1.0001, 0.9990,\n",
       "         0.9998, 0.9950, 0.9998, 0.9993, 1.0001, 0.9974, 0.9987, 1.0005, 0.9989,\n",
       "         0.9988, 1.0006, 1.0019, 0.9993, 0.9983, 0.9998, 0.9982, 0.9970, 0.9994,\n",
       "         0.9983, 1.0018, 0.9982, 0.9990, 0.9980, 0.9996, 1.0005, 0.9978, 0.9979,\n",
       "         0.9979, 0.9997, 0.9974, 1.0003, 0.9989, 0.9993, 0.9996, 1.0003, 1.0009,\n",
       "         1.0004, 1.0022, 0.9995, 0.9990, 1.0001, 1.0019, 1.0011, 1.0022, 1.0012,\n",
       "         0.9981, 0.9975, 0.9996, 1.0014, 0.9994, 0.9961, 0.9991, 0.9977, 0.9953,\n",
       "         1.0028, 1.0002, 0.9990, 0.9958, 0.9983, 1.0000, 0.9991, 0.9972, 0.9988,\n",
       "         1.0009, 0.9964, 1.0013, 0.9963, 0.9990, 1.0020, 0.9958, 0.9992, 0.9951,\n",
       "         1.0002, 1.0011, 0.9998, 0.9989, 0.9988, 1.0006, 0.9974, 0.9974, 0.9971,\n",
       "         0.9987, 0.9960, 0.9990, 1.0035, 0.9992, 1.0004, 1.0005, 0.9977, 0.9971,\n",
       "         1.0012, 1.0024, 0.9966, 1.0004, 0.9990, 1.0029, 1.0000, 0.9977, 1.0011,\n",
       "         0.9992, 0.9991, 0.9985, 0.9994, 1.0026, 0.9989, 0.9982, 0.9953, 0.9963,\n",
       "         1.0016, 0.9983, 1.0014, 0.9992, 1.0016, 1.0003, 0.9999, 1.0004, 0.9994,\n",
       "         0.9982, 0.9994, 0.9987, 0.9999, 0.9996, 0.9974, 1.0021, 1.0006, 0.9982,\n",
       "         0.9985, 0.9981, 1.0005, 0.9992, 0.9984, 0.9998, 0.9971, 0.9990, 1.0007,\n",
       "         0.9977, 0.9990, 0.9991, 0.9963, 1.0004, 1.0018, 1.0004, 0.9975, 1.0005,\n",
       "         1.0000, 0.9961, 0.9986, 0.9964, 0.9955, 0.9967, 1.0000, 0.9976, 0.9984,\n",
       "         1.0000, 0.9999, 0.9970, 0.9981, 0.9996, 0.9962, 0.9997, 0.9967, 1.0029,\n",
       "         1.0003, 0.9987, 0.9999, 0.9988, 1.0021, 0.9986, 0.9978, 0.9959, 0.9976,\n",
       "         1.0019, 0.9992, 1.0001, 0.9987, 0.9980, 0.9992, 0.9980, 0.9991, 0.9984,\n",
       "         1.0013, 0.9992, 1.0006, 0.9984, 1.0035, 0.9990, 0.9995, 0.9980, 0.9993,\n",
       "         0.9987, 0.9978, 1.0000, 1.0004, 0.9997, 0.9967, 0.9995, 0.9979, 0.9989,\n",
       "         1.0011, 0.9997, 0.9981, 1.0017, 0.9966, 0.9983, 0.9961, 0.9991, 0.9985,\n",
       "         1.0010, 0.9989, 0.9979, 0.9991, 0.9994, 0.9985, 0.9985, 0.9998, 1.0017,\n",
       "         0.9989, 0.9984, 0.9994, 0.9992, 1.0014, 1.0012, 0.9974, 1.0004, 0.9966,\n",
       "         1.0011, 0.9976, 1.0017, 0.9992, 0.9992, 0.9997, 1.0021, 1.0002, 1.0006,\n",
       "         0.9987, 0.9967, 0.9994, 1.0004, 1.0000, 1.0002, 0.9981, 0.9976, 1.0004,\n",
       "         1.0001, 1.0011, 1.0013, 1.0009, 0.9980, 1.0006, 1.0003, 1.0000]),\n",
       " 'transformer.resblocks.3.ln_1.bias': tensor([-2.4861e-03,  2.1519e-03,  9.7229e-04, -8.2207e-04,  1.9397e-03,\n",
       "         -5.7950e-04,  1.7623e-03,  7.8550e-04,  1.5230e-03, -2.3798e-04,\n",
       "          2.6095e-04, -1.2852e-03, -9.9785e-04,  2.0135e-04,  9.7510e-04,\n",
       "          6.5948e-04,  3.6901e-03, -2.6710e-03,  1.0632e-03,  1.1073e-04,\n",
       "          2.3453e-04,  2.0244e-03, -1.5344e-03,  8.7341e-04,  9.0135e-04,\n",
       "          1.1691e-03,  8.4032e-04,  5.3118e-05,  1.1185e-03,  8.7272e-04,\n",
       "          1.4120e-03,  2.1370e-03, -3.5364e-04,  1.0282e-03, -1.2979e-03,\n",
       "         -3.0660e-04,  3.0068e-04,  2.3198e-04, -5.8566e-04, -1.8514e-03,\n",
       "         -3.1058e-04, -1.8899e-03, -3.3776e-03,  1.0938e-03,  1.3189e-04,\n",
       "          6.7407e-05, -5.0654e-04,  2.8079e-03,  4.3597e-04, -8.6556e-04,\n",
       "         -1.4609e-03, -8.2352e-04, -1.4101e-03,  2.0268e-03,  1.2993e-04,\n",
       "          7.4745e-04, -7.6546e-04, -8.0416e-04,  1.5982e-03, -8.4997e-04,\n",
       "          8.6487e-04, -1.4640e-03, -2.3641e-04, -6.8628e-04,  1.7921e-03,\n",
       "         -1.7826e-04, -7.3164e-04, -1.9462e-03, -8.8355e-04, -4.7341e-04,\n",
       "         -2.2678e-03,  1.0454e-03,  4.7874e-04,  1.7989e-03, -7.2877e-04,\n",
       "          9.1259e-04, -3.0678e-03,  3.9214e-04,  6.4268e-04,  9.3604e-04,\n",
       "         -4.7752e-04, -8.3416e-05, -2.8326e-04,  6.7544e-04, -3.7350e-03,\n",
       "          3.0877e-03, -5.7715e-05, -9.7318e-06,  1.8807e-03,  1.9440e-03,\n",
       "          1.0586e-03,  6.3622e-05, -2.7303e-03, -1.0423e-03, -1.7722e-03,\n",
       "          2.2969e-04,  1.7374e-03,  6.8741e-05, -1.8007e-03, -1.1366e-03,\n",
       "         -2.4168e-04, -7.2631e-04,  1.7491e-03, -3.2704e-04, -4.7461e-04,\n",
       "         -6.3503e-05,  8.0527e-06,  6.5138e-04, -3.5380e-03, -2.5722e-03,\n",
       "         -3.0947e-03, -3.1301e-04, -2.3492e-03, -1.2072e-03, -2.8192e-04,\n",
       "          1.9629e-04, -4.9416e-04,  9.5982e-04, -7.1303e-04,  1.9661e-03,\n",
       "         -9.6088e-04, -3.5951e-04, -9.9225e-04,  2.2256e-03, -7.1311e-04,\n",
       "          5.8773e-04,  9.9984e-04,  6.3036e-04, -3.9246e-04, -2.8844e-03,\n",
       "          8.4830e-04, -4.7269e-04, -1.5437e-03,  2.4670e-03,  2.5073e-03,\n",
       "         -9.4972e-04,  3.4181e-03,  2.0117e-03,  1.4560e-03,  8.2751e-04,\n",
       "         -7.5708e-04, -2.3647e-03, -2.1695e-03, -2.6505e-04, -2.7102e-03,\n",
       "         -1.6129e-03,  8.3203e-04, -2.5683e-03, -2.6866e-03,  3.7137e-04,\n",
       "         -5.5472e-04, -3.3747e-03, -1.5703e-03, -9.3306e-04,  2.9553e-04,\n",
       "         -2.3470e-03, -5.1577e-04, -1.4862e-03, -7.9288e-06, -1.8964e-03,\n",
       "          2.6141e-04, -9.8872e-04, -9.7121e-04,  7.2890e-04,  3.8181e-04,\n",
       "         -1.0526e-04, -2.0979e-05, -1.5735e-04, -4.7011e-04,  6.2466e-04,\n",
       "          6.1811e-04, -1.4126e-03,  2.7676e-03, -1.3061e-03,  8.2033e-04,\n",
       "         -3.0907e-04, -2.4613e-03, -2.5244e-05,  1.9685e-03, -1.3228e-03,\n",
       "          8.0125e-04, -9.9313e-04, -6.0108e-04, -4.6919e-04, -2.6027e-03,\n",
       "          9.3714e-04,  6.6765e-05, -1.7160e-03, -7.4901e-04,  1.4437e-03,\n",
       "         -3.5534e-03,  2.1199e-03,  8.4368e-04, -1.2631e-03, -2.5512e-03,\n",
       "          2.3911e-04, -4.2770e-04, -6.0184e-04, -3.2896e-04, -1.6497e-03,\n",
       "         -2.7171e-04,  8.3445e-04,  3.3881e-07, -3.4866e-05, -1.8079e-03,\n",
       "         -2.2399e-03,  1.3657e-03,  5.3544e-04,  1.2633e-03,  1.9445e-04,\n",
       "          2.1160e-03, -1.2828e-03, -7.4616e-04,  7.9448e-04, -2.1268e-04,\n",
       "         -1.1219e-03,  1.3731e-04,  3.1699e-04, -3.4861e-04, -6.3299e-05,\n",
       "          1.8001e-03, -1.3548e-03, -4.3483e-06, -8.1971e-05, -2.9227e-04,\n",
       "          7.7679e-04,  1.0852e-03, -8.5305e-04,  1.4293e-03, -1.0087e-03,\n",
       "          3.1116e-03,  3.7148e-04, -1.3865e-03,  8.4609e-04,  9.6548e-04,\n",
       "          8.8937e-04,  1.1158e-03,  1.0259e-03,  2.1917e-03,  5.7237e-04,\n",
       "         -1.4249e-03, -1.5993e-03,  1.3254e-03, -9.0606e-04, -1.0346e-03,\n",
       "         -1.6165e-04, -5.2510e-04, -9.7268e-04, -5.8891e-04,  2.2560e-04,\n",
       "          2.1825e-04, -1.0245e-03, -5.2008e-04, -4.9444e-04, -1.9286e-03,\n",
       "          2.5580e-04, -6.4158e-04,  1.8161e-03, -1.3493e-03,  1.5419e-03,\n",
       "         -5.7512e-05, -2.2385e-03, -1.1651e-03,  7.5593e-04,  3.4512e-03,\n",
       "          9.0335e-04, -4.9301e-04, -7.1101e-04, -1.1111e-03,  1.5330e-03,\n",
       "          1.6582e-03, -9.1689e-04,  9.2732e-04,  2.4467e-03, -7.5715e-04,\n",
       "          6.4746e-04, -2.4591e-04,  4.3311e-04, -2.5962e-03,  1.5322e-03,\n",
       "          9.7632e-05,  2.5124e-04, -1.5479e-03, -1.5255e-03,  1.3828e-03,\n",
       "          8.1151e-04,  3.4778e-04, -7.0227e-04, -1.7366e-03, -1.2809e-03,\n",
       "          1.5503e-03, -1.5131e-03,  3.8606e-05, -1.3071e-04,  1.2714e-03,\n",
       "         -6.3026e-04,  2.6723e-03,  1.6381e-03,  2.1374e-03,  5.2557e-04,\n",
       "          5.1346e-04, -3.0362e-04,  5.5191e-04, -9.7745e-04,  8.5926e-04,\n",
       "         -5.2288e-04,  6.5873e-04, -2.0439e-04,  1.9774e-04,  2.4136e-03,\n",
       "          1.7973e-03, -2.7468e-03, -1.0975e-03, -3.6401e-04, -3.0035e-03,\n",
       "         -9.5805e-04, -2.6023e-03, -7.4680e-04, -2.1841e-04, -1.9792e-04,\n",
       "         -3.4181e-04, -1.7563e-04, -1.6091e-03, -3.1568e-03,  4.1580e-04,\n",
       "          6.9771e-04, -5.1127e-04, -5.2842e-04, -5.5596e-04,  2.8105e-03,\n",
       "         -1.0495e-03,  2.0213e-03,  2.6398e-03, -2.9614e-04,  3.3223e-04,\n",
       "         -8.6724e-04,  1.2733e-04, -1.1350e-03, -5.1567e-04, -3.4727e-03,\n",
       "          5.0180e-04,  9.7363e-04, -1.7177e-03,  1.9143e-03,  1.6385e-03,\n",
       "          1.5545e-03,  6.0507e-04, -5.8635e-05,  1.0609e-03, -2.8843e-03,\n",
       "         -2.8438e-04, -2.2922e-04,  5.6299e-04, -6.5997e-04,  2.5060e-03,\n",
       "         -1.3296e-04,  1.0116e-03, -1.8706e-03, -9.8979e-04,  1.6247e-03,\n",
       "          1.2741e-03,  1.6384e-03, -7.0952e-04, -3.6662e-03,  1.4126e-05,\n",
       "         -1.7497e-03, -2.9646e-05, -9.7027e-04, -1.3352e-04, -5.3253e-04,\n",
       "         -1.3089e-04, -1.6869e-03,  1.5269e-03,  1.9191e-03,  7.8044e-04,\n",
       "          1.6729e-03,  3.1086e-03,  7.0140e-04,  2.0133e-04, -7.7823e-04,\n",
       "         -8.3517e-04, -7.5951e-04, -1.0518e-03, -2.1049e-03, -3.9793e-05,\n",
       "          9.4935e-04, -7.0567e-04, -7.2115e-04, -1.7236e-03,  3.0323e-04,\n",
       "          3.5925e-05, -6.9075e-04, -7.6012e-04,  1.2679e-03, -4.1781e-03,\n",
       "          2.9346e-04, -5.5198e-05, -1.6073e-03, -4.1123e-04,  1.3823e-03,\n",
       "         -1.7144e-03,  1.3097e-03, -1.0224e-03, -5.0773e-04, -6.2890e-04,\n",
       "         -5.4880e-05, -2.4537e-03,  1.0118e-04, -2.7251e-03, -2.2817e-03,\n",
       "          1.5745e-03,  7.1879e-04, -2.2625e-03, -6.9641e-04,  1.3881e-03,\n",
       "         -1.3256e-03,  7.9782e-05,  8.9470e-04, -1.7018e-04,  1.6953e-03,\n",
       "         -1.7410e-03, -4.3192e-04, -5.7634e-04,  1.6050e-03,  5.4330e-04,\n",
       "         -3.5487e-03,  4.5887e-04, -3.8822e-04,  1.5548e-03, -1.5873e-03,\n",
       "         -9.2297e-04,  3.9693e-04,  3.4501e-03, -8.8302e-04,  1.3541e-03,\n",
       "         -2.2177e-03,  5.6708e-04, -1.4917e-03, -2.2527e-03,  7.4489e-04,\n",
       "          1.2083e-03,  2.6950e-04, -2.0043e-03,  1.9499e-03, -4.5519e-04,\n",
       "          3.6194e-04, -1.5669e-03, -7.1123e-04,  7.7738e-04,  5.0382e-04,\n",
       "          1.5472e-03, -7.0284e-04, -8.5162e-05,  3.4250e-04, -1.2897e-03,\n",
       "         -1.6889e-03,  2.2665e-03, -7.4750e-04, -9.7723e-04,  7.9580e-04,\n",
       "          9.5360e-04,  7.0800e-04,  1.3720e-03,  1.3730e-03,  6.0985e-04,\n",
       "         -9.5625e-05,  4.5984e-03, -1.5460e-05, -1.2404e-03, -5.5184e-04,\n",
       "          1.3034e-03, -1.0825e-03,  2.8848e-04, -6.9726e-04,  6.9275e-04,\n",
       "          2.6404e-04,  2.3116e-03,  2.1744e-03,  2.0381e-03, -2.1517e-03,\n",
       "          9.5159e-04, -7.5530e-04, -1.3608e-03,  5.8465e-04,  7.1314e-04,\n",
       "         -9.3071e-04,  4.7246e-04,  2.8343e-03, -1.9146e-04, -7.3592e-04,\n",
       "         -1.0438e-03,  2.9188e-03,  7.5097e-04,  8.6870e-04, -2.7054e-03,\n",
       "         -6.5860e-05, -2.4945e-03,  2.4329e-03,  2.0404e-03,  3.6067e-04,\n",
       "          1.0442e-03,  1.2999e-03, -1.4762e-03, -6.0371e-05, -3.8432e-04,\n",
       "          1.8487e-03,  1.8270e-04, -1.8652e-03, -1.8160e-03,  6.6437e-04,\n",
       "         -5.6282e-04,  2.0484e-03]),\n",
       " 'transformer.resblocks.3.mlp.c_fc.weight': tensor([[-0.0029, -0.0201,  0.0074,  ..., -0.0017, -0.0191, -0.0387],\n",
       "         [ 0.0168, -0.0186, -0.0351,  ..., -0.0518,  0.0473,  0.0177],\n",
       "         [-0.0536,  0.0384, -0.0092,  ...,  0.0449, -0.0375, -0.0436],\n",
       "         ...,\n",
       "         [-0.0122, -0.0258,  0.0102,  ..., -0.0071,  0.0067,  0.0308],\n",
       "         [ 0.0405,  0.0140,  0.0296,  ...,  0.0139, -0.0145, -0.0472],\n",
       "         [-0.0043,  0.0148, -0.0282,  ...,  0.0034,  0.0312, -0.0106]]),\n",
       " 'transformer.resblocks.3.mlp.c_fc.bias': tensor([-0.0245,  0.0081,  0.0188,  ..., -0.0158, -0.0482,  0.0396]),\n",
       " 'transformer.resblocks.3.mlp.c_proj.weight': tensor([[-0.0040,  0.0078,  0.0106,  ..., -0.0152, -0.0005, -0.0208],\n",
       "         [ 0.0069,  0.0055, -0.0093,  ...,  0.0003, -0.0137, -0.0117],\n",
       "         [ 0.0061, -0.0025, -0.0059,  ...,  0.0016,  0.0035,  0.0116],\n",
       "         ...,\n",
       "         [ 0.0058, -0.0042, -0.0008,  ..., -0.0010, -0.0098, -0.0207],\n",
       "         [-0.0052,  0.0136,  0.0071,  ...,  0.0049,  0.0078,  0.0006],\n",
       "         [-0.0057, -0.0127,  0.0025,  ...,  0.0042,  0.0056,  0.0101]]),\n",
       " 'transformer.resblocks.3.mlp.c_proj.bias': tensor([-5.8409e-03,  3.8791e-03,  1.7918e-02, -1.9382e-02, -6.0233e-03,\n",
       "          1.7848e-02, -1.3533e-02, -4.0706e-05,  5.8826e-03,  1.6242e-02,\n",
       "          2.6704e-04, -1.7231e-02, -1.9241e-02, -8.7804e-03, -1.4382e-02,\n",
       "          7.9628e-03,  2.6188e-03, -9.4492e-03, -2.6861e-03,  1.7336e-02,\n",
       "         -1.0771e-03, -1.3445e-02, -7.2540e-03, -1.0637e-02,  3.5719e-03,\n",
       "          2.0973e-02,  1.5206e-02, -5.3253e-03,  2.1087e-02, -2.2301e-02,\n",
       "          2.8852e-03,  1.0672e-02,  2.0391e-02, -1.1791e-02, -4.4628e-03,\n",
       "          2.9414e-03,  1.0190e-02,  2.1073e-02,  1.9499e-02, -1.4440e-03,\n",
       "         -1.1566e-02, -1.2269e-02, -1.4125e-02,  1.8563e-02,  1.0595e-02,\n",
       "          2.1040e-02, -9.4561e-03, -5.1367e-03, -1.0913e-02, -1.9641e-02,\n",
       "         -4.6314e-03,  6.3427e-03,  1.5225e-02, -1.4631e-02, -1.8856e-02,\n",
       "          1.1665e-02,  2.1885e-02,  1.5867e-02, -3.5648e-03, -4.7866e-03,\n",
       "         -4.7547e-04,  1.0947e-03, -1.1883e-03, -1.6625e-02, -1.7168e-02,\n",
       "         -1.8458e-03, -7.5488e-03, -1.4457e-02, -1.6612e-02,  5.2938e-03,\n",
       "         -1.5321e-03, -4.2207e-03,  1.8070e-02,  1.7735e-02, -2.3199e-02,\n",
       "          5.2922e-03, -1.7038e-02, -1.6787e-02, -7.0685e-03,  1.8485e-02,\n",
       "          1.1666e-04, -2.2117e-02,  1.3264e-02, -3.3670e-03, -1.2113e-02,\n",
       "         -1.5223e-02,  1.7217e-03,  1.6496e-02,  1.8062e-02,  2.6923e-03,\n",
       "         -3.0115e-03,  1.7005e-02,  9.4466e-04, -1.0927e-02, -8.1845e-04,\n",
       "         -2.0901e-02, -1.8794e-02, -1.9230e-02, -1.2496e-02,  2.0741e-02,\n",
       "         -7.7921e-03,  6.0850e-03, -2.0869e-02, -2.2869e-02,  1.9516e-02,\n",
       "         -1.2480e-02,  1.8440e-02, -9.3801e-03, -1.6639e-02, -2.1401e-02,\n",
       "          1.9155e-02,  4.1256e-03, -3.8467e-05,  2.1707e-02, -1.8806e-02,\n",
       "         -1.1920e-02, -8.0906e-03, -3.4068e-03,  1.7143e-02,  1.2732e-02,\n",
       "         -9.9457e-03, -2.0299e-02, -3.9889e-03, -7.5014e-04,  1.0458e-02,\n",
       "          1.3778e-03,  1.7049e-02, -9.8862e-03, -4.1749e-03,  9.0777e-03,\n",
       "          1.6572e-02,  1.7816e-02,  3.0632e-03, -9.7177e-03,  1.9848e-02,\n",
       "         -1.1933e-02,  5.5496e-04,  1.8768e-02,  1.1540e-02, -7.4095e-03,\n",
       "          4.2723e-04,  1.6612e-02, -1.7719e-02, -9.7982e-03, -2.3223e-03,\n",
       "          1.9009e-02,  1.4129e-03, -1.7086e-02, -4.4262e-03, -9.2563e-03,\n",
       "          1.9929e-02, -1.1113e-04,  1.2885e-02,  1.4541e-02, -4.1334e-03,\n",
       "          1.2403e-02,  1.3956e-02,  4.0008e-03, -2.1985e-02,  4.0564e-04,\n",
       "          5.7316e-03, -5.7665e-03,  9.8198e-03, -6.6959e-04, -1.2441e-02,\n",
       "          2.0795e-02, -1.3711e-02,  2.0339e-02, -9.6687e-03, -1.5100e-03,\n",
       "         -1.2402e-02,  1.4365e-02,  4.2621e-03,  1.8102e-02, -6.6819e-03,\n",
       "          4.1195e-03, -3.3865e-03,  1.6948e-02,  8.8938e-03,  1.5476e-02,\n",
       "          9.5680e-03, -1.3189e-02, -3.3402e-03,  1.3307e-02, -5.7769e-03,\n",
       "          5.7748e-03,  6.1341e-03,  2.1389e-02,  1.5376e-02,  1.0627e-02,\n",
       "         -1.6123e-02, -4.2584e-03,  4.8605e-03,  5.8339e-03, -2.0020e-02,\n",
       "         -1.9905e-02,  1.2909e-02,  6.0877e-03, -9.3208e-03,  7.9707e-03,\n",
       "         -7.4660e-03, -1.0683e-02,  2.0656e-02,  1.4786e-02,  7.7393e-03,\n",
       "          2.2091e-02,  3.3865e-03,  1.2695e-02,  8.9610e-03, -2.1932e-02,\n",
       "          1.3369e-02, -1.1635e-02,  4.6547e-03, -7.0882e-03,  1.2598e-02,\n",
       "          4.6004e-03, -9.2549e-03,  5.6966e-03,  9.5057e-03, -1.2244e-02,\n",
       "          8.8674e-03,  1.5985e-02,  2.1815e-02,  7.8997e-03, -1.9818e-02,\n",
       "          4.8254e-03, -9.5736e-03,  2.1237e-02,  1.6990e-02,  8.6688e-03,\n",
       "         -1.3262e-02, -4.2839e-04, -1.1357e-02, -1.3570e-02,  1.4499e-02,\n",
       "         -1.5667e-02, -8.6371e-03, -1.9075e-02,  1.0610e-02,  5.6299e-03,\n",
       "          1.5768e-02, -7.8117e-03, -8.6240e-03, -3.3488e-03,  4.6609e-03,\n",
       "         -5.4107e-05, -1.1197e-03,  1.5267e-02,  1.6394e-02, -3.1962e-03,\n",
       "         -3.5832e-03, -1.4557e-02,  1.8056e-02,  9.0503e-03, -9.2193e-03,\n",
       "          4.7540e-04,  7.7932e-04,  1.6167e-02, -7.2688e-03,  4.9218e-03,\n",
       "          1.9990e-02, -2.2309e-03, -1.1245e-02, -1.7299e-04, -1.4347e-02,\n",
       "         -1.5271e-02,  2.1138e-03,  1.2351e-02,  2.1645e-04,  8.2656e-03,\n",
       "         -4.3321e-03,  1.5287e-02,  1.5030e-02, -4.4991e-04, -4.7924e-03,\n",
       "         -9.4809e-03, -4.7777e-03,  2.9117e-03,  1.7179e-02, -4.4496e-03,\n",
       "          2.1005e-02, -6.0120e-03, -4.8798e-03, -1.2183e-02,  1.2918e-02,\n",
       "          2.3030e-02, -6.4155e-03, -2.7703e-03, -9.6577e-03, -2.1263e-02,\n",
       "          2.0255e-02, -3.7575e-03, -1.6812e-03, -1.8652e-02,  2.0889e-02,\n",
       "          1.5805e-02,  1.7577e-02, -5.0995e-03, -1.9697e-02,  6.3566e-03,\n",
       "         -2.8048e-03, -4.4728e-03,  7.7371e-03,  5.2879e-04, -1.4063e-02,\n",
       "          5.2804e-03,  2.1222e-02,  1.2136e-02,  1.5728e-02, -3.7742e-03,\n",
       "         -2.1144e-02, -1.3548e-02, -1.8529e-02,  1.1347e-02,  1.4645e-02,\n",
       "          6.5521e-04,  1.4251e-02,  5.4167e-03, -1.8900e-02,  1.7385e-02,\n",
       "          7.2031e-03, -1.8578e-02, -1.7972e-03,  6.0413e-03, -2.0530e-02,\n",
       "         -8.3594e-03,  8.2011e-03,  2.1842e-02, -2.0160e-02,  1.6352e-02,\n",
       "          6.5400e-03, -8.6125e-03, -1.7067e-02, -2.6084e-03,  2.1046e-02,\n",
       "         -2.0112e-02,  1.1413e-02, -1.2670e-02, -9.4481e-04,  8.5833e-03,\n",
       "          1.0805e-03,  4.1324e-03,  4.2686e-03, -1.0304e-02, -1.2167e-02,\n",
       "          1.9717e-02,  6.3432e-03, -1.7240e-02, -1.7574e-02,  9.9338e-03,\n",
       "         -8.5583e-03,  1.3129e-02, -4.0948e-03, -1.1248e-02, -1.5229e-02,\n",
       "         -1.8666e-02,  1.4488e-02,  1.8461e-02, -2.3071e-03,  6.5715e-03,\n",
       "         -1.5402e-02, -3.6749e-03, -6.4246e-03, -1.2099e-02,  1.4376e-03,\n",
       "         -1.6310e-03, -1.9049e-02,  1.6218e-02, -7.2614e-03,  2.0800e-03,\n",
       "          1.2371e-02,  1.1689e-02, -8.4292e-03, -1.6004e-02, -4.3028e-03,\n",
       "          1.6115e-02,  1.3068e-02,  1.2593e-02, -1.4827e-03,  2.2742e-02,\n",
       "         -1.5195e-02,  2.2040e-02,  1.7178e-02,  1.9612e-02, -1.8362e-02,\n",
       "          1.3937e-02, -1.0465e-03, -8.4362e-03,  2.1359e-02, -3.6130e-03,\n",
       "          1.2827e-02, -2.4098e-04, -1.2985e-02,  3.4403e-04, -1.6733e-02,\n",
       "         -7.9467e-04,  1.5374e-02, -1.9767e-02,  2.1378e-02,  1.4380e-02,\n",
       "          1.5017e-02,  1.5502e-02,  1.4229e-02,  2.1800e-03, -4.4236e-03,\n",
       "          2.0604e-02,  2.1985e-02, -1.6457e-02, -5.6937e-03, -1.4203e-02,\n",
       "          7.8176e-03,  7.3258e-03,  6.5030e-03, -1.6167e-02,  7.9791e-03,\n",
       "         -1.4603e-02,  9.5296e-03, -5.4324e-03, -1.2740e-02,  1.6540e-02,\n",
       "         -9.6136e-03,  9.8199e-03,  9.0375e-03, -4.7168e-03,  1.4224e-02,\n",
       "          1.9338e-02, -1.3866e-03,  1.9867e-02, -1.7772e-02, -3.7069e-03,\n",
       "          7.1615e-03,  1.3877e-02,  4.0938e-03,  3.6228e-03, -2.2117e-02,\n",
       "         -3.5575e-03,  1.9530e-02,  4.9798e-03, -1.5519e-02, -1.0984e-02,\n",
       "         -2.2130e-02,  6.3060e-03,  1.0475e-02,  1.5753e-02, -1.2076e-02,\n",
       "          1.2497e-02, -2.5638e-03,  1.7128e-02,  4.3722e-04,  2.2321e-02,\n",
       "          1.5876e-02,  4.7526e-03,  9.6241e-03,  1.2639e-02,  1.5768e-02,\n",
       "          7.2068e-03, -6.7314e-03,  1.3799e-02,  6.7809e-03,  7.8075e-03,\n",
       "          1.3050e-02,  1.9341e-02, -3.4571e-04,  3.9084e-03,  3.8042e-03,\n",
       "          2.1186e-02, -1.6529e-02,  2.1026e-02,  1.7258e-02, -1.1417e-02,\n",
       "          6.4663e-03,  7.3309e-03, -1.8820e-02,  1.5246e-02, -1.7889e-02,\n",
       "          9.3579e-04,  3.7743e-03,  1.2566e-02, -1.2411e-02, -2.2130e-02,\n",
       "          2.0366e-02,  1.2853e-02,  2.1420e-02,  9.4818e-03, -2.2074e-02,\n",
       "         -9.5855e-03,  2.0504e-02, -6.6835e-03, -7.3368e-03, -1.3472e-02,\n",
       "         -6.5845e-03, -8.4792e-03,  9.0328e-03, -1.7121e-02, -1.4922e-02,\n",
       "          6.8720e-03,  1.5402e-02, -6.9500e-03, -7.9388e-03, -5.2289e-03,\n",
       "          1.0884e-02,  7.5447e-03, -7.7954e-04,  1.4411e-02, -1.3847e-02,\n",
       "          7.2909e-03, -1.7730e-02,  7.7982e-03,  3.4077e-03,  2.1977e-02,\n",
       "          4.0875e-03, -2.7331e-03]),\n",
       " 'transformer.resblocks.3.ln_2.weight': tensor([0.9985, 1.0006, 0.9968, 1.0017, 1.0012, 1.0000, 1.0016, 1.0014, 1.0040,\n",
       "         1.0000, 1.0027, 1.0029, 1.0014, 1.0030, 1.0010, 1.0029, 1.0023, 1.0052,\n",
       "         1.0036, 1.0021, 1.0013, 1.0003, 1.0026, 0.9981, 1.0038, 1.0022, 1.0030,\n",
       "         0.9989, 0.9993, 1.0000, 1.0001, 1.0023, 1.0005, 0.9995, 1.0036, 1.0009,\n",
       "         1.0000, 1.0021, 1.0017, 0.9980, 1.0039, 1.0030, 0.9979, 1.0008, 1.0020,\n",
       "         1.0017, 1.0010, 0.9998, 0.9974, 0.9990, 1.0026, 0.9996, 1.0002, 1.0013,\n",
       "         1.0027, 1.0020, 1.0016, 1.0029, 1.0021, 1.0024, 1.0008, 1.0026, 1.0010,\n",
       "         1.0018, 1.0010, 1.0002, 0.9999, 0.9996, 1.0023, 1.0031, 1.0010, 1.0030,\n",
       "         1.0009, 0.9995, 1.0013, 1.0055, 1.0010, 1.0037, 1.0027, 0.9996, 1.0026,\n",
       "         1.0019, 1.0018, 1.0001, 1.0024, 1.0022, 1.0020, 1.0024, 1.0014, 1.0031,\n",
       "         1.0024, 1.0022, 0.9998, 1.0000, 1.0007, 1.0027, 1.0036, 0.9997, 1.0019,\n",
       "         1.0012, 1.0022, 1.0018, 1.0019, 1.0000, 0.9994, 0.9996, 1.0024, 1.0019,\n",
       "         1.0006, 1.0020, 1.0041, 1.0006, 1.0012, 0.9982, 1.0029, 1.0024, 1.0015,\n",
       "         1.0014, 1.0029, 0.9998, 1.0008, 0.9998, 1.0019, 0.9996, 1.0015, 1.0020,\n",
       "         0.9990, 1.0044, 0.9997, 1.0002, 1.0019, 1.0003, 1.0028, 1.0021, 1.0022,\n",
       "         1.0028, 1.0011, 1.0006, 0.9979, 1.0004, 1.0023, 1.0015, 1.0007, 1.0028,\n",
       "         1.0025, 1.0033, 1.0034, 1.0015, 1.0015, 1.0022, 1.0008, 1.0015, 1.0021,\n",
       "         1.0056, 1.0025, 1.0030, 1.0025, 1.0032, 1.0014, 1.0034, 1.0015, 1.0015,\n",
       "         1.0004, 1.0040, 0.9991, 1.0022, 1.0029, 1.0013, 1.0012, 0.9992, 1.0040,\n",
       "         1.0015, 1.0036, 1.0008, 1.0052, 1.0023, 0.9989, 1.0008, 1.0016, 1.0034,\n",
       "         1.0019, 1.0021, 1.0027, 1.0025, 0.9989, 1.0037, 1.0014, 1.0012, 1.0002,\n",
       "         0.9968, 1.0040, 1.0025, 1.0011, 0.9977, 1.0028, 1.0022, 1.0037, 0.9990,\n",
       "         1.0034, 1.0041, 1.0041, 0.9999, 1.0016, 1.0044, 0.9985, 1.0061, 0.9998,\n",
       "         1.0026, 0.9991, 1.0021, 0.9998, 0.9999, 1.0001, 1.0014, 0.9977, 1.0018,\n",
       "         1.0000, 1.0031, 1.0000, 1.0005, 1.0015, 1.0036, 1.0021, 1.0010, 1.0013,\n",
       "         1.0067, 1.0035, 1.0036, 1.0041, 1.0017, 1.0013, 1.0007, 1.0029, 1.0014,\n",
       "         1.0024, 1.0020, 1.0030, 1.0021, 1.0017, 1.0021, 0.9993, 1.0029, 1.0007,\n",
       "         0.9998, 1.0015, 1.0042, 1.0031, 1.0013, 1.0029, 1.0006, 1.0007, 1.0024,\n",
       "         1.0000, 1.0003, 0.9998, 1.0022, 1.0013, 1.0008, 1.0021, 1.0023, 1.0015,\n",
       "         1.0048, 1.0016, 1.0008, 1.0027, 1.0032, 0.9995, 1.0038, 0.9998, 1.0005,\n",
       "         1.0037, 1.0022, 1.0028, 1.0012, 0.9992, 1.0010, 1.0002, 1.0017, 1.0004,\n",
       "         1.0004, 1.0007, 1.0018, 1.0051, 1.0030, 1.0030, 0.9983, 1.0013, 0.9976,\n",
       "         1.0023, 1.0027, 1.0036, 1.0028, 1.0009, 1.0017, 1.0008, 0.9991, 0.9988,\n",
       "         1.0038, 1.0019, 1.0020, 1.0028, 0.9992, 1.0037, 1.0015, 1.0007, 1.0028,\n",
       "         1.0014, 1.0010, 1.0011, 1.0001, 1.0039, 0.9998, 0.9989, 1.0025, 1.0026,\n",
       "         1.0018, 0.9997, 1.0008, 1.0001, 1.0033, 1.0025, 1.0030, 1.0028, 1.0048,\n",
       "         1.0051, 1.0014, 0.9975, 1.0032, 1.0023, 1.0011, 1.0017, 0.9997, 1.0017,\n",
       "         1.0044, 1.0023, 1.0032, 1.0004, 0.9976, 0.9994, 0.9974, 1.0033, 1.0026,\n",
       "         1.0032, 1.0035, 1.0006, 1.0037, 0.9991, 1.0021, 1.0026, 0.9987, 1.0058,\n",
       "         1.0001, 1.0025, 1.0017, 1.0024, 0.9992, 1.0007, 1.0031, 0.9987, 1.0024,\n",
       "         1.0016, 1.0025, 1.0011, 1.0008, 1.0020, 1.0028, 1.0040, 1.0049, 1.0029,\n",
       "         0.9993, 1.0005, 1.0005, 1.0017, 0.9992, 0.9997, 1.0031, 1.0032, 1.0001,\n",
       "         0.9996, 1.0042, 1.0052, 1.0006, 1.0034, 1.0031, 1.0015, 0.9978, 1.0019,\n",
       "         1.0014, 1.0031, 1.0002, 0.9994, 1.0017, 0.9998, 1.0020, 0.9998, 1.0039,\n",
       "         1.0007, 1.0036, 1.0004, 1.0024, 1.0004, 1.0036, 1.0007, 1.0017, 1.0012,\n",
       "         1.0028, 1.0025, 1.0034, 1.0034, 1.0027, 1.0003, 1.0018, 1.0019, 1.0022,\n",
       "         1.0028, 1.0030, 1.0016, 1.0015, 1.0036, 1.0025, 1.0037, 1.0024, 1.0023,\n",
       "         1.0017, 1.0032, 0.9997, 1.0043, 1.0026, 1.0012, 1.0023, 0.9998, 1.0020,\n",
       "         1.0010, 1.0012, 1.0021, 1.0043, 1.0030, 1.0021, 1.0029, 1.0015, 0.9996,\n",
       "         1.0011, 1.0008, 1.0017, 1.0051, 1.0024, 0.9990, 0.9994, 1.0008, 1.0003,\n",
       "         1.0022, 1.0020, 0.9999, 1.0004, 1.0014, 1.0025, 1.0005, 1.0028, 1.0016,\n",
       "         1.0006, 1.0011, 1.0009, 1.0003, 1.0016, 1.0039, 1.0017, 1.0019, 0.9987,\n",
       "         1.0012, 1.0021, 1.0041, 1.0029, 1.0020, 1.0013, 0.9998, 1.0020, 1.0035,\n",
       "         1.0001, 1.0024, 1.0025, 1.0037, 1.0020, 1.0022, 1.0036, 1.0018, 1.0028,\n",
       "         1.0004, 1.0017, 1.0020, 1.0027, 1.0017, 0.9998, 1.0016, 1.0027, 1.0010,\n",
       "         1.0013, 1.0057, 1.0020, 1.0021, 1.0013, 0.9995, 1.0007, 1.0032, 0.9997,\n",
       "         0.9992, 1.0006, 1.0015, 0.9995, 1.0010, 1.0036, 1.0013, 1.0017]),\n",
       " 'transformer.resblocks.3.ln_2.bias': tensor([ 7.0525e-04, -1.0080e-03, -1.1541e-04,  9.8632e-04, -8.4418e-04,\n",
       "          2.5932e-03,  4.6204e-05, -1.2504e-03, -2.0696e-03, -6.6637e-06,\n",
       "          4.6071e-04,  1.6400e-03, -2.4061e-03,  7.3408e-04,  1.1761e-03,\n",
       "          2.9553e-03, -1.0573e-03,  2.5455e-04, -1.2550e-03,  4.4080e-04,\n",
       "          1.1156e-03,  3.2369e-03, -1.7572e-03, -8.9840e-04, -1.1053e-03,\n",
       "          8.7524e-04,  1.8608e-03,  4.4265e-05, -1.5523e-03, -3.0897e-03,\n",
       "          2.4167e-03, -1.1698e-03, -1.5992e-03,  4.0401e-03, -5.2688e-04,\n",
       "          2.1936e-03, -5.5257e-04,  9.8818e-04, -1.4932e-04, -1.1727e-03,\n",
       "         -1.6793e-03, -4.0486e-03, -2.5383e-03,  3.1958e-03,  5.8040e-04,\n",
       "         -2.9995e-03,  1.4835e-03,  4.1808e-04,  1.9329e-03, -2.4676e-03,\n",
       "         -3.0448e-05,  2.2216e-03, -1.2538e-04, -1.0762e-03, -2.2929e-03,\n",
       "         -9.9071e-04, -9.7847e-04,  1.4167e-03, -9.4269e-04, -1.3619e-03,\n",
       "         -1.0026e-04, -8.7794e-04,  1.2830e-03,  2.7048e-03,  1.2464e-03,\n",
       "         -5.5926e-04,  3.1330e-06,  1.6418e-04, -3.9449e-04,  8.1995e-05,\n",
       "          4.5408e-04,  1.3482e-03,  2.7104e-03, -4.1295e-04, -3.6592e-04,\n",
       "         -1.8879e-03,  3.0383e-03,  2.2332e-04,  2.5196e-03,  1.9705e-04,\n",
       "          3.3726e-04,  4.2562e-04,  1.7332e-03, -7.9571e-04, -1.7246e-03,\n",
       "         -1.1639e-03, -1.1358e-04, -1.7947e-03,  1.5683e-03,  1.4057e-03,\n",
       "         -1.3758e-03,  3.4464e-04, -3.2796e-04, -9.9155e-04, -1.3726e-03,\n",
       "         -1.1227e-03, -4.3490e-03,  1.3230e-03,  1.3892e-03,  3.3990e-04,\n",
       "          2.3484e-03, -4.1518e-04,  2.4759e-04,  4.4348e-03,  1.1392e-03,\n",
       "          1.1116e-04,  1.4229e-03,  3.0855e-03,  2.1583e-03, -2.9449e-03,\n",
       "          1.6063e-03, -1.3810e-03, -2.4750e-03, -6.5625e-04,  3.1825e-04,\n",
       "          8.3004e-04,  2.3915e-03, -3.5755e-04,  4.2590e-03, -3.0329e-03,\n",
       "         -9.7484e-04,  4.8311e-04, -7.6144e-04, -3.5786e-04, -1.2319e-03,\n",
       "          5.2428e-04, -1.3708e-05, -1.2941e-04, -3.7417e-03, -3.3635e-04,\n",
       "          1.2842e-03,  8.5784e-04,  1.1643e-03,  2.3335e-03, -2.4968e-03,\n",
       "          1.7850e-03,  1.4181e-03, -2.0127e-04, -5.7117e-04, -2.2644e-03,\n",
       "          1.0186e-03, -3.8014e-04, -1.5719e-03,  1.2038e-03,  1.1261e-03,\n",
       "         -5.0198e-04, -1.6160e-03,  9.3859e-04, -1.7272e-03,  2.7996e-03,\n",
       "          3.1320e-04, -1.9720e-03, -2.3019e-03,  1.2831e-03,  1.1825e-03,\n",
       "          3.7408e-03, -1.0147e-03,  1.2203e-03,  5.1837e-03, -3.2189e-04,\n",
       "         -7.3332e-04, -1.1921e-03,  5.1855e-04, -5.8448e-04,  2.3716e-03,\n",
       "          5.0288e-04, -5.1955e-04, -9.1668e-04,  1.1187e-03,  2.7363e-03,\n",
       "          2.3246e-03, -2.6166e-03, -2.9054e-04, -7.9319e-04, -1.1660e-03,\n",
       "         -3.3886e-03,  1.4235e-03,  1.7899e-03,  2.0892e-04, -3.0694e-04,\n",
       "         -9.2871e-05, -2.2787e-03,  1.2819e-03,  9.2320e-04, -2.2683e-03,\n",
       "         -4.0170e-05,  5.3582e-05,  7.7139e-05, -3.5484e-04,  5.2989e-03,\n",
       "          6.3844e-04,  1.3836e-03, -8.0682e-04, -1.4723e-03,  1.6756e-03,\n",
       "          2.4333e-03,  1.8856e-03, -6.7253e-04,  3.2843e-04,  4.8485e-04,\n",
       "          4.9786e-04,  1.4691e-03, -2.5092e-03, -6.7086e-04,  1.8036e-03,\n",
       "          1.9549e-03, -5.2367e-04, -6.2671e-04, -2.7448e-03,  1.4985e-03,\n",
       "          5.3587e-04, -9.8311e-04,  1.0077e-03,  5.1487e-04,  1.2407e-03,\n",
       "          6.9351e-04,  3.5713e-04, -1.5763e-03, -1.3304e-03, -6.3982e-04,\n",
       "         -1.9448e-04,  1.6173e-04,  6.3908e-04, -1.6319e-03, -5.0166e-04,\n",
       "          8.9092e-04, -2.1586e-03, -9.5202e-05, -1.7039e-03, -1.8413e-03,\n",
       "          1.9611e-03,  8.5019e-04,  8.2440e-04,  6.4437e-04, -1.0031e-04,\n",
       "          1.0303e-03, -2.3609e-04,  4.6430e-04,  1.1966e-03,  1.3260e-03,\n",
       "          3.7952e-03,  5.6845e-04, -6.6301e-04,  3.1701e-03, -5.6359e-04,\n",
       "         -1.9626e-03,  4.0367e-04, -1.9359e-03, -1.5724e-04, -2.3125e-03,\n",
       "          9.3039e-04,  8.1273e-04,  1.3213e-03,  1.8451e-03,  1.6286e-03,\n",
       "         -5.4422e-04, -1.2035e-03,  5.8880e-05,  3.0687e-04,  1.4931e-04,\n",
       "          1.4582e-03, -5.8100e-05,  1.2549e-03,  1.7984e-03,  6.8165e-04,\n",
       "          1.4016e-03,  1.4800e-03,  7.6559e-04, -2.4218e-03, -8.5604e-04,\n",
       "         -2.3288e-03,  8.2200e-04, -4.3826e-04,  1.2490e-03,  1.2797e-03,\n",
       "          1.1323e-03, -2.9438e-03, -2.1743e-03,  8.0637e-04, -6.5364e-04,\n",
       "         -4.7579e-04,  1.1417e-03, -8.2485e-04, -6.5200e-04,  3.2630e-04,\n",
       "         -1.3597e-04,  1.6423e-03, -7.0570e-04,  3.8786e-03,  2.6954e-03,\n",
       "          7.5291e-04,  4.9423e-04, -1.5876e-03, -1.4201e-03, -2.8533e-03,\n",
       "         -8.4906e-04,  5.8540e-04, -1.0386e-04, -3.0205e-04, -5.3505e-06,\n",
       "         -9.6618e-04, -1.2307e-04,  1.4493e-03,  2.4579e-03, -6.4822e-04,\n",
       "         -1.4247e-04, -9.2518e-04,  1.7299e-03, -1.8297e-03,  9.3454e-04,\n",
       "         -7.6952e-05, -7.7843e-04, -7.7669e-04,  5.0798e-04,  3.8001e-04,\n",
       "         -1.9347e-03, -1.4060e-04,  1.1558e-03,  7.8181e-04,  1.0834e-03,\n",
       "          1.6092e-03, -1.4279e-03,  5.2805e-04, -7.4512e-04, -9.3622e-04,\n",
       "         -9.5262e-04,  1.0144e-03,  1.2509e-03,  1.7358e-03, -4.3775e-03,\n",
       "         -3.3861e-03, -1.5937e-03,  1.8479e-03,  1.8326e-03,  2.7920e-03,\n",
       "          1.2567e-03, -6.7769e-04,  1.1531e-03,  2.5096e-03,  2.3194e-03,\n",
       "         -3.0320e-04,  5.6412e-04,  3.7122e-04, -8.9617e-04, -6.4568e-04,\n",
       "          3.1424e-03,  1.6724e-03, -1.7363e-03, -2.1291e-03,  1.6665e-03,\n",
       "         -6.3102e-04,  1.3857e-03, -1.8079e-03, -1.2942e-03,  6.8173e-04,\n",
       "         -4.4354e-03,  8.4687e-04, -5.5934e-04,  2.6238e-03,  6.5150e-06,\n",
       "          9.0495e-04, -1.3522e-03,  6.6624e-04,  2.8273e-03, -2.1609e-03,\n",
       "          2.0450e-03, -1.6219e-04, -5.6482e-05,  1.8004e-04,  1.2604e-03,\n",
       "         -1.2699e-03,  4.3629e-04, -1.7586e-03, -5.8706e-04,  5.9260e-04,\n",
       "          4.4866e-03, -4.9300e-04,  1.6396e-03, -1.1944e-04, -1.1865e-03,\n",
       "          6.7767e-04, -1.9480e-03, -6.9381e-04,  1.9810e-03, -2.1550e-03,\n",
       "         -1.2176e-03, -2.8014e-04,  2.8765e-03, -2.1567e-03,  7.9658e-04,\n",
       "          1.2436e-04,  4.3909e-04, -1.0960e-04,  2.0798e-03, -1.6639e-03,\n",
       "         -1.7165e-03, -1.7913e-03, -1.4124e-04, -1.5513e-03, -1.1971e-05,\n",
       "         -1.8078e-03,  9.8482e-04,  4.8929e-04,  3.6623e-04, -7.1866e-04,\n",
       "         -4.4376e-04, -9.6299e-06,  8.8934e-04, -5.0454e-04, -5.1960e-04,\n",
       "          4.7536e-04,  2.5970e-03, -1.2312e-03, -1.5680e-03, -4.0195e-04,\n",
       "         -5.3123e-04, -2.4619e-03, -1.2870e-03,  1.2545e-03,  2.6398e-03,\n",
       "         -1.4334e-03, -3.1230e-05,  2.5067e-03,  1.0541e-03, -4.8688e-04,\n",
       "          5.9309e-04,  1.3299e-03,  2.6641e-03, -1.0093e-03,  7.8157e-04,\n",
       "         -1.6016e-03, -7.3387e-04, -1.5766e-03,  9.1472e-04,  1.7305e-03,\n",
       "         -1.3138e-03, -9.5588e-04,  4.1251e-03, -1.6452e-04,  2.6640e-04,\n",
       "         -1.2746e-03, -1.6412e-03,  5.4027e-04,  1.3143e-03, -1.3241e-03,\n",
       "         -5.0485e-04,  3.5349e-03,  1.9661e-03,  1.7487e-03, -1.7396e-03,\n",
       "          5.3648e-04, -1.6078e-03,  8.5314e-04,  1.4915e-03,  2.4734e-04,\n",
       "          2.0026e-03,  1.1642e-03, -2.5665e-03, -3.4679e-03,  5.9986e-04,\n",
       "         -1.0356e-03, -4.9422e-04, -2.7800e-03, -2.1390e-03,  1.2765e-03,\n",
       "         -2.0047e-03, -4.7550e-03, -3.9517e-03,  7.3443e-04, -1.0245e-03,\n",
       "         -8.1882e-04, -3.4139e-04,  2.1744e-03,  3.4185e-04, -5.6270e-04,\n",
       "          3.3053e-03, -2.6026e-03, -1.9418e-03,  6.0738e-04, -2.6428e-03,\n",
       "          3.5495e-04, -1.6947e-03,  1.4718e-03, -9.7265e-04,  1.6731e-04,\n",
       "          2.8896e-03,  3.6771e-05, -2.1400e-03, -1.9045e-03, -1.5388e-03,\n",
       "         -8.7546e-04, -8.4942e-04, -1.4032e-03,  3.1346e-03, -1.6123e-04,\n",
       "          1.0896e-03,  6.2741e-04, -1.8359e-03,  1.2386e-03,  1.9437e-03,\n",
       "          6.2709e-04, -1.5770e-04,  1.0291e-03, -2.1568e-03, -5.8530e-04,\n",
       "         -1.7397e-03,  2.8070e-03,  5.1234e-04,  2.1194e-03,  1.9292e-03,\n",
       "          2.1491e-03,  2.5555e-03]),\n",
       " 'transformer.resblocks.4.attn.in_proj_weight': tensor([[-2.9977e-05,  9.0779e-02,  2.0533e-02,  ..., -6.6499e-02,\n",
       "          -1.4057e-02, -5.0733e-02],\n",
       "         [ 6.5693e-02,  1.3732e-02,  1.0102e-02,  ...,  6.4059e-02,\n",
       "          -1.2841e-02,  6.6509e-02],\n",
       "         [-8.3357e-02, -5.1397e-02, -3.6674e-03,  ...,  4.3974e-02,\n",
       "          -2.0159e-02, -4.2068e-02],\n",
       "         ...,\n",
       "         [-1.6104e-02, -1.8409e-02,  4.2974e-02,  ...,  3.0739e-03,\n",
       "          -5.4371e-02, -1.9126e-02],\n",
       "         [-9.4987e-02, -1.5964e-02, -4.3564e-03,  ..., -1.2993e-03,\n",
       "           1.7492e-02, -7.7231e-02],\n",
       "         [ 1.5379e-02, -7.9369e-02, -6.1884e-02,  ..., -5.2390e-02,\n",
       "          -1.0102e-01,  4.8598e-02]]),\n",
       " 'transformer.resblocks.4.attn.in_proj_bias': tensor([ 0.0005,  0.0044, -0.0019,  ..., -0.0012, -0.0009,  0.0011]),\n",
       " 'transformer.resblocks.4.attn.out_proj.weight': tensor([[ 6.8837e-03,  7.1463e-03,  5.9215e-03,  ..., -1.0785e-02,\n",
       "          -1.1825e-02, -4.3921e-03],\n",
       "         [ 5.5591e-03,  1.0299e-02,  7.0278e-03,  ...,  1.6966e-02,\n",
       "           3.4595e-03, -1.0380e-02],\n",
       "         [-2.2847e-03, -3.1872e-03, -8.4193e-03,  ...,  4.8170e-03,\n",
       "          -5.6731e-05, -5.5826e-03],\n",
       "         ...,\n",
       "         [ 2.7595e-03,  2.1953e-02, -4.1520e-03,  ...,  9.8359e-03,\n",
       "           2.3345e-03, -5.3715e-04],\n",
       "         [ 1.2648e-03, -5.3049e-03,  2.1420e-03,  ..., -1.4171e-02,\n",
       "          -5.0970e-04, -5.5682e-03],\n",
       "         [ 1.5010e-02,  1.2983e-02,  1.7471e-02,  ..., -1.0500e-03,\n",
       "           6.8602e-04,  1.6628e-02]]),\n",
       " 'transformer.resblocks.4.attn.out_proj.bias': tensor([ 1.0701e-04, -1.4125e-03, -6.0178e-04, -4.1122e-04,  3.7985e-05,\n",
       "         -1.1110e-03,  8.1144e-04,  1.8608e-03, -1.0009e-03,  4.7030e-04,\n",
       "         -1.8490e-03, -3.4507e-04, -4.8290e-04, -5.3953e-04, -1.4953e-03,\n",
       "          5.8117e-04, -6.0534e-04,  6.3953e-04,  6.6298e-05, -1.0151e-03,\n",
       "         -5.3026e-04, -7.7708e-04,  1.6273e-04, -1.0218e-03, -1.0352e-03,\n",
       "         -4.3104e-05, -5.6953e-04, -8.0118e-04, -7.6660e-04, -5.9475e-04,\n",
       "          2.1743e-04,  6.5686e-04,  3.7513e-04, -2.2833e-03, -3.8660e-04,\n",
       "          1.3470e-03, -1.8474e-05, -3.3186e-04, -4.3260e-04, -1.5652e-03,\n",
       "          9.5142e-04,  1.2551e-03,  1.5657e-03,  1.4330e-04, -1.4279e-03,\n",
       "          7.2975e-04, -1.1984e-03, -1.2413e-04, -2.9560e-04,  9.2757e-04,\n",
       "         -3.4314e-04, -1.2835e-03, -7.1318e-05,  2.7566e-04,  1.6939e-03,\n",
       "          7.4321e-04, -3.6005e-04, -1.2264e-03,  2.4130e-04, -7.1545e-04,\n",
       "         -8.4239e-04,  3.6269e-04, -3.8638e-05,  4.3317e-04, -1.4206e-03,\n",
       "         -6.3904e-04,  1.4028e-03,  9.7105e-04,  6.4532e-04,  1.2155e-03,\n",
       "         -1.0937e-03,  2.9588e-04, -2.9245e-04, -6.8377e-04, -6.7291e-04,\n",
       "          1.4650e-03, -1.5478e-03, -6.9594e-04, -1.4376e-03,  1.3641e-03,\n",
       "         -2.6384e-04, -4.6395e-04,  7.4352e-05,  1.2742e-03,  1.0599e-03,\n",
       "         -5.3038e-04, -7.4895e-04,  3.4249e-04, -8.3838e-04, -9.0396e-04,\n",
       "          3.9288e-04,  2.3467e-05,  3.6978e-04,  2.0310e-04, -1.5027e-03,\n",
       "         -1.5034e-03,  9.1725e-04, -7.2161e-04, -3.0770e-04,  3.4146e-04,\n",
       "          3.4687e-04, -1.0827e-03,  3.2893e-04, -6.0999e-04, -4.6403e-04,\n",
       "         -8.9069e-04, -1.1501e-03,  1.8961e-04,  9.9525e-04,  1.2464e-04,\n",
       "          1.2947e-03,  9.8721e-04,  1.3642e-03,  8.3541e-04, -6.7469e-04,\n",
       "         -1.6954e-03,  1.1075e-03,  6.6005e-04, -9.8002e-04, -9.1491e-04,\n",
       "          3.0115e-04, -2.9888e-04,  1.0928e-03, -7.1634e-04, -1.1327e-04,\n",
       "          3.5564e-04, -9.0694e-04, -1.0264e-03, -1.0630e-04, -9.0509e-05,\n",
       "          3.1803e-04,  1.0184e-03, -5.8198e-04, -1.4324e-03, -7.7054e-04,\n",
       "          3.7145e-04, -1.7299e-04,  3.8363e-04, -4.4430e-04,  6.4239e-04,\n",
       "         -8.9105e-05,  1.5195e-03,  9.3479e-04,  1.6942e-03,  3.6633e-04,\n",
       "          8.5314e-04, -2.1279e-04, -1.1701e-04,  1.2975e-03,  3.7744e-04,\n",
       "          1.4615e-03,  2.4738e-03,  2.4512e-03,  5.1166e-04, -1.7935e-04,\n",
       "          1.2459e-04,  8.8614e-04, -8.1821e-04, -1.5944e-04,  1.1707e-04,\n",
       "         -5.7445e-04, -2.8283e-04,  9.7671e-04,  1.8011e-04, -1.0316e-03,\n",
       "          3.7412e-04,  1.1260e-03, -4.1771e-04,  5.5235e-05, -6.4569e-04,\n",
       "          1.7528e-04,  2.1489e-04, -4.9988e-04, -1.3754e-04,  1.7018e-03,\n",
       "          2.4914e-04, -8.3500e-04,  3.1355e-04,  5.4053e-04, -1.3121e-04,\n",
       "         -1.9012e-03, -2.9046e-04,  4.5861e-04, -1.8125e-03,  2.5732e-03,\n",
       "         -2.4651e-04,  9.4212e-04, -3.1066e-05,  7.1588e-04, -2.4422e-03,\n",
       "          4.9219e-04,  5.8476e-04,  1.0852e-04,  1.8307e-03, -3.1157e-04,\n",
       "         -5.9013e-04,  1.2991e-03,  1.6765e-03,  1.2609e-03,  2.0623e-03,\n",
       "         -2.8558e-04, -1.3755e-03,  1.1442e-03,  2.5031e-03,  2.7520e-04,\n",
       "         -3.9048e-04, -8.4643e-04, -2.3969e-04,  8.9636e-04,  3.4556e-04,\n",
       "          4.9573e-04,  3.0262e-04, -8.1957e-04,  8.2145e-04, -1.8370e-03,\n",
       "         -5.8695e-04,  2.0307e-04, -1.5220e-03, -7.4053e-04,  1.2559e-04,\n",
       "         -8.4829e-04, -3.8886e-04,  6.6536e-04,  6.2357e-04,  6.5968e-04,\n",
       "          2.3933e-04, -1.4737e-04, -7.2008e-04,  1.1976e-03,  1.2134e-03,\n",
       "         -8.9313e-04,  7.6214e-04,  7.4967e-04, -5.7017e-05,  1.0260e-03,\n",
       "          2.0679e-04,  1.9043e-03, -1.6586e-03,  1.4093e-05,  6.9502e-05,\n",
       "         -3.4508e-04,  2.6038e-04,  1.1158e-03,  6.0891e-04, -1.1053e-03,\n",
       "         -6.2534e-06,  6.8595e-04, -3.4166e-04, -6.4796e-04, -1.0835e-03,\n",
       "          6.3204e-04, -1.0910e-03, -3.9547e-05, -2.5664e-04, -1.3161e-03,\n",
       "          4.9012e-04,  5.4888e-04, -3.4844e-04, -7.7800e-04, -1.2931e-03,\n",
       "          9.3830e-04, -6.0804e-04,  1.2003e-03, -1.3325e-03, -7.5898e-04,\n",
       "          1.5683e-03,  4.5581e-04,  6.0514e-04, -1.3223e-04, -9.5583e-04,\n",
       "          1.0503e-03, -5.9797e-04, -1.0422e-03, -5.9236e-05,  7.7909e-05,\n",
       "         -1.1057e-03,  1.7302e-03, -7.8286e-05,  2.2204e-03, -6.8496e-04,\n",
       "         -3.4709e-04,  2.1739e-03, -2.3103e-04, -9.8244e-04, -1.6738e-03,\n",
       "          1.3106e-03,  7.3115e-04,  9.2536e-04,  8.6948e-04, -7.8405e-04,\n",
       "          1.8621e-04,  4.1791e-04, -1.3595e-03,  1.5951e-03, -6.7007e-05,\n",
       "          1.8047e-03, -6.9384e-04,  7.1180e-04, -1.0028e-03, -1.7915e-03,\n",
       "         -1.3605e-03,  1.0733e-03,  6.1034e-04, -1.8844e-03,  1.6856e-04,\n",
       "         -1.0064e-03, -1.0026e-04,  5.5007e-04, -1.5130e-03, -1.8145e-03,\n",
       "          3.7696e-04,  1.7236e-03, -7.8965e-04, -1.4897e-03, -1.7525e-04,\n",
       "          1.7113e-03,  1.7536e-03,  8.2100e-04,  2.1555e-04,  8.2075e-04,\n",
       "          1.5046e-03,  5.5333e-05, -7.5568e-04,  5.6040e-04, -5.8821e-04,\n",
       "          6.6879e-04,  1.3207e-03,  2.4066e-04, -1.7405e-03, -1.7049e-03,\n",
       "          1.4008e-04,  1.6504e-03,  4.9109e-04,  3.9070e-04,  1.9642e-04,\n",
       "         -1.8040e-04, -6.5225e-04, -2.7623e-04, -8.8013e-06,  1.1426e-03,\n",
       "         -1.5825e-03,  1.2946e-03,  1.0419e-03, -2.9336e-04, -1.8035e-03,\n",
       "         -2.3512e-04, -1.4635e-03,  8.5209e-04, -7.5036e-05,  1.1072e-03,\n",
       "         -1.3028e-04, -2.7114e-03, -9.3493e-04, -4.3212e-04, -5.0287e-04,\n",
       "          3.1841e-04,  2.9169e-03,  1.5966e-03,  9.0227e-04, -1.5018e-03,\n",
       "          1.8029e-04, -2.6649e-04,  1.0183e-03,  1.7345e-03, -6.7304e-05,\n",
       "          1.1091e-03,  9.5785e-04, -1.5629e-03,  1.7279e-03,  4.2978e-04,\n",
       "         -1.9577e-04,  5.3422e-04,  1.1794e-04, -6.0950e-04,  6.8378e-05,\n",
       "          1.9415e-03, -1.4599e-03,  7.4045e-04, -2.8292e-04,  3.2430e-04,\n",
       "          5.9249e-04,  7.9143e-04, -1.1558e-03, -5.5710e-04, -3.7360e-04,\n",
       "         -3.8222e-04,  7.7641e-04, -2.5964e-05,  1.3482e-03,  6.3476e-04,\n",
       "         -4.5741e-04,  2.2213e-04,  1.8357e-04,  1.3541e-03,  1.5526e-03,\n",
       "          1.1864e-03,  2.1776e-04,  6.3512e-04,  1.6157e-04,  1.3279e-03,\n",
       "         -3.5990e-05, -1.0617e-03, -1.6650e-03,  5.4507e-04, -5.0478e-04,\n",
       "          4.2737e-04,  4.0553e-04, -3.3695e-04,  1.2967e-03,  7.3535e-04,\n",
       "         -7.3789e-04, -1.4518e-03,  1.3569e-04,  1.2256e-03,  3.1746e-05,\n",
       "         -1.3513e-03,  1.2968e-03,  6.9805e-04,  1.0264e-03,  8.2835e-04,\n",
       "         -1.0936e-03, -7.6976e-05, -2.7072e-04, -7.8551e-04, -1.1133e-03,\n",
       "         -5.5599e-04, -1.4949e-03, -5.3496e-04,  1.0272e-03, -2.9023e-04,\n",
       "         -1.1913e-03,  1.5372e-03,  1.0182e-03, -1.3887e-03, -1.4237e-03,\n",
       "         -2.0293e-04,  1.5332e-04, -1.7627e-03,  1.1534e-03, -7.6026e-06,\n",
       "         -5.5274e-04,  4.9941e-04,  7.4484e-06, -6.7139e-04, -1.3053e-03,\n",
       "         -6.8040e-04, -1.4089e-03,  8.4157e-04,  4.2063e-04,  1.2869e-03,\n",
       "         -2.4687e-05,  3.3124e-05, -1.6400e-03,  1.8237e-03,  1.0680e-03,\n",
       "         -6.2285e-04,  7.6649e-05,  1.5015e-03,  1.5117e-03, -6.5988e-04,\n",
       "          2.7560e-04, -9.9407e-04,  1.2956e-03,  1.8070e-03,  4.2741e-04,\n",
       "          4.1715e-05,  2.2991e-04,  7.4719e-04,  5.8048e-05, -1.3835e-03,\n",
       "         -7.4816e-04,  4.6841e-06, -9.4058e-04,  8.0342e-05, -5.1723e-04,\n",
       "         -9.1443e-05, -1.7171e-03, -7.0354e-05,  5.0293e-05, -5.7355e-05,\n",
       "         -1.0845e-03, -8.9608e-04,  5.5330e-04,  3.3742e-04, -5.8636e-04,\n",
       "         -1.6502e-06, -1.5699e-03, -1.3348e-03,  1.7918e-03,  4.8490e-04,\n",
       "          6.3214e-04, -4.4346e-04,  1.1869e-04, -1.6104e-03, -4.1017e-04,\n",
       "         -4.9083e-04,  1.1880e-03, -2.0930e-04, -1.1670e-03, -2.4671e-03,\n",
       "         -9.2137e-04, -4.3471e-04, -3.0803e-04,  1.3472e-03, -9.0419e-04,\n",
       "          1.0015e-03, -4.1529e-04, -6.5192e-04,  5.1219e-04, -1.9191e-05,\n",
       "         -9.1677e-04, -1.6852e-04]),\n",
       " 'transformer.resblocks.4.ln_1.weight': tensor([0.9969, 0.9998, 0.9996, 0.9993, 1.0011, 1.0007, 0.9980, 1.0001, 0.9991,\n",
       "         0.9998, 0.9992, 0.9997, 1.0031, 1.0012, 1.0006, 0.9963, 0.9990, 0.9996,\n",
       "         1.0019, 0.9998, 0.9999, 1.0001, 1.0009, 0.9992, 0.9992, 0.9995, 0.9984,\n",
       "         0.9973, 1.0025, 1.0007, 1.0017, 0.9983, 0.9997, 0.9992, 0.9987, 0.9964,\n",
       "         0.9990, 0.9989, 0.9990, 0.9991, 1.0028, 1.0009, 0.9980, 0.9984, 0.9957,\n",
       "         0.9989, 1.0022, 0.9994, 1.0005, 1.0004, 0.9993, 0.9989, 0.9983, 0.9993,\n",
       "         0.9995, 1.0000, 0.9990, 0.9986, 0.9990, 0.9994, 0.9979, 0.9994, 0.9986,\n",
       "         1.0012, 1.0009, 0.9978, 0.9982, 1.0012, 0.9987, 0.9994, 0.9991, 0.9979,\n",
       "         0.9999, 0.9984, 1.0029, 1.0006, 1.0008, 0.9976, 0.9995, 1.0006, 0.9991,\n",
       "         1.0016, 0.9996, 1.0000, 0.9962, 1.0005, 0.9995, 1.0010, 0.9980, 0.9995,\n",
       "         0.9999, 0.9978, 1.0000, 0.9985, 0.9993, 0.9982, 0.9998, 0.9994, 0.9988,\n",
       "         1.0012, 1.0022, 1.0011, 1.0014, 1.0018, 0.9984, 0.9991, 0.9973, 0.9991,\n",
       "         0.9976, 1.0005, 0.9986, 0.9974, 1.0006, 1.0010, 1.0009, 0.9977, 1.0002,\n",
       "         1.0009, 1.0011, 0.9990, 0.9994, 1.0010, 0.9982, 0.9959, 0.9970, 0.9990,\n",
       "         0.9989, 0.9987, 1.0004, 1.0007, 1.0005, 0.9996, 1.0010, 1.0000, 0.9990,\n",
       "         0.9987, 0.9991, 0.9993, 0.9991, 0.9988, 1.0008, 0.9952, 0.9977, 1.0018,\n",
       "         1.0013, 0.9984, 0.9994, 1.0000, 0.9980, 0.9992, 0.9972, 0.9983, 0.9993,\n",
       "         0.9989, 0.9984, 1.0007, 1.0009, 0.9999, 0.9974, 0.9986, 0.9983, 0.9997,\n",
       "         1.0009, 1.0005, 0.9983, 1.0015, 0.9969, 0.9989, 1.0009, 0.9983, 0.9986,\n",
       "         1.0021, 1.0009, 0.9971, 1.0011, 1.0006, 1.0002, 1.0011, 1.0001, 0.9999,\n",
       "         1.0007, 1.0024, 0.9999, 0.9976, 1.0002, 0.9985, 0.9982, 0.9998, 1.0019,\n",
       "         1.0000, 1.0018, 0.9977, 0.9992, 1.0001, 0.9994, 0.9995, 1.0004, 0.9968,\n",
       "         0.9983, 0.9969, 0.9988, 0.9996, 1.0006, 1.0021, 0.9990, 1.0005, 1.0040,\n",
       "         1.0016, 1.0034, 1.0014, 1.0047, 1.0012, 1.0020, 0.9983, 0.9979, 0.9994,\n",
       "         1.0021, 1.0004, 0.9978, 0.9995, 0.9948, 0.9995, 1.0006, 1.0006, 0.9980,\n",
       "         1.0018, 0.9994, 0.9991, 1.0016, 0.9991, 0.9960, 0.9969, 0.9991, 1.0002,\n",
       "         0.9999, 0.9985, 0.9978, 1.0020, 0.9992, 0.9972, 0.9997, 1.0004, 1.0019,\n",
       "         0.9992, 1.0002, 1.0006, 1.0018, 0.9980, 0.9981, 1.0007, 1.0009, 0.9972,\n",
       "         0.9981, 0.9985, 0.9995, 1.0000, 0.9987, 0.9988, 0.9998, 0.9980, 0.9995,\n",
       "         0.9987, 0.9991, 1.0035, 1.0000, 0.9994, 1.0004, 1.0007, 0.9978, 1.0005,\n",
       "         1.0003, 0.9985, 1.0003, 0.9979, 0.9991, 0.9985, 1.0004, 1.0017, 0.9979,\n",
       "         1.0001, 0.9986, 0.9994, 0.9979, 1.0042, 0.9982, 1.0013, 0.9986, 1.0018,\n",
       "         0.9975, 0.9990, 0.9978, 1.0005, 1.0001, 0.9992, 1.0021, 1.0023, 0.9975,\n",
       "         0.9995, 0.9988, 1.0000, 0.9996, 0.9995, 1.0014, 0.9977, 1.0018, 1.0005,\n",
       "         0.9993, 0.9997, 0.9967, 0.9979, 1.0000, 0.9991, 1.0030, 1.0048, 0.9992,\n",
       "         1.0001, 0.9990, 1.0012, 1.0001, 1.0034, 1.0004, 1.0002, 1.0003, 0.9981,\n",
       "         1.0015, 0.9995, 0.9992, 0.9988, 1.0007, 1.0004, 1.0006, 0.9986, 1.0000,\n",
       "         0.9985, 0.9975, 1.0019, 0.9983, 0.9965, 0.9957, 1.0009, 0.9987, 0.9994,\n",
       "         0.9973, 1.0010, 0.9977, 0.9999, 0.9998, 0.9982, 0.9991, 1.0007, 1.0009,\n",
       "         1.0002, 0.9983, 0.9971, 0.9988, 0.9995, 0.9995, 0.9968, 0.9996, 0.9990,\n",
       "         0.9991, 0.9990, 0.9965, 0.9985, 1.0023, 0.9987, 0.9981, 1.0019, 1.0013,\n",
       "         0.9964, 0.9984, 0.9982, 1.0006, 1.0014, 1.0004, 1.0006, 1.0001, 0.9985,\n",
       "         0.9983, 0.9979, 0.9977, 1.0020, 0.9977, 0.9995, 0.9977, 0.9994, 0.9987,\n",
       "         0.9997, 1.0013, 0.9991, 0.9979, 0.9996, 0.9983, 1.0019, 0.9970, 0.9987,\n",
       "         0.9973, 1.0020, 1.0007, 0.9965, 0.9992, 0.9961, 0.9985, 0.9995, 0.9990,\n",
       "         0.9985, 1.0005, 0.9994, 1.0001, 1.0004, 1.0018, 1.0023, 1.0003, 0.9988,\n",
       "         1.0035, 0.9994, 0.9966, 1.0013, 0.9986, 0.9997, 0.9994, 0.9987, 0.9989,\n",
       "         0.9982, 0.9959, 1.0015, 0.9977, 1.0010, 0.9980, 0.9953, 0.9972, 0.9977,\n",
       "         0.9996, 1.0002, 0.9994, 1.0005, 0.9992, 1.0001, 0.9996, 0.9962, 0.9985,\n",
       "         0.9982, 0.9997, 0.9987, 0.9990, 1.0008, 0.9989, 0.9990, 1.0010, 0.9984,\n",
       "         0.9972, 0.9971, 0.9992, 0.9997, 0.9981, 1.0012, 1.0012, 0.9974, 0.9953,\n",
       "         1.0008, 1.0023, 0.9963, 0.9960, 0.9982, 1.0009, 1.0022, 1.0013, 0.9999,\n",
       "         1.0010, 1.0000, 1.0000, 1.0004, 0.9985, 0.9992, 0.9997, 1.0004, 0.9998,\n",
       "         1.0012, 0.9997, 1.0000, 1.0038, 0.9999, 0.9988, 0.9975, 0.9982, 0.9987,\n",
       "         0.9997, 1.0000, 1.0002, 0.9985, 0.9974, 0.9943, 1.0023, 1.0006, 1.0031,\n",
       "         1.0002, 0.9979, 1.0006, 1.0002, 1.0009, 0.9983, 1.0018, 0.9993, 0.9987,\n",
       "         0.9978, 0.9993, 0.9990, 1.0027, 1.0016, 0.9998, 1.0001, 0.9963]),\n",
       " 'transformer.resblocks.4.ln_1.bias': tensor([ 4.1849e-04,  1.0509e-03, -1.4732e-04,  1.1957e-04,  7.6760e-04,\n",
       "         -3.3341e-04, -8.5496e-04, -1.5629e-03,  3.7133e-04, -1.2822e-03,\n",
       "          2.6551e-03,  1.3447e-03, -1.0399e-03,  7.4318e-04, -5.0577e-04,\n",
       "         -3.5178e-04,  2.4371e-03, -2.0920e-03,  2.2028e-03,  2.4433e-03,\n",
       "          1.5457e-03, -1.1358e-03,  9.4679e-04,  1.0696e-03, -2.5419e-03,\n",
       "         -1.7335e-03,  2.1583e-05,  4.9748e-04,  1.0059e-03, -1.2936e-03,\n",
       "          1.3214e-03,  7.8757e-04,  3.8062e-04,  2.5926e-03, -3.1287e-04,\n",
       "         -1.9903e-03,  4.5117e-04,  1.9366e-03,  1.8816e-03,  6.1170e-06,\n",
       "          7.3780e-05,  2.1852e-03, -1.4948e-03, -6.9078e-04,  2.3297e-03,\n",
       "          1.8041e-03, -5.4216e-04,  1.3346e-03,  4.5043e-04, -8.5813e-05,\n",
       "         -3.5842e-04,  8.0744e-04, -8.7472e-04, -4.0754e-04, -8.0763e-04,\n",
       "          1.7486e-03,  8.1464e-04, -7.0614e-04,  9.0701e-04, -5.2821e-04,\n",
       "          1.3756e-03, -3.1329e-03, -3.1831e-03,  3.5873e-05,  6.1745e-04,\n",
       "         -3.2886e-03, -2.0882e-03, -2.4227e-03, -2.7260e-03,  5.5955e-04,\n",
       "          3.7154e-03,  1.4358e-03,  6.0469e-04,  1.9883e-03, -2.7435e-04,\n",
       "          9.1241e-04,  2.8477e-03,  8.8956e-05, -1.8600e-03, -1.6500e-03,\n",
       "         -3.1226e-03,  1.9945e-03, -1.6997e-03,  7.6192e-04,  1.1762e-03,\n",
       "          5.8465e-04,  1.1481e-03,  1.1192e-03,  2.2681e-03, -1.2946e-03,\n",
       "          8.8673e-04,  1.7150e-03,  1.4883e-04, -1.1398e-03,  5.4878e-03,\n",
       "          3.6923e-03,  2.6286e-03, -5.3648e-05, -1.8743e-03,  3.8491e-04,\n",
       "         -1.2171e-03,  8.3581e-04, -2.1576e-04, -2.6385e-03, -6.0328e-04,\n",
       "         -1.7370e-03,  3.0697e-03,  1.6909e-03,  2.0443e-03, -7.7465e-04,\n",
       "         -2.8187e-03, -1.3204e-03,  2.4133e-03,  7.0642e-04, -1.0772e-03,\n",
       "         -1.1934e-03, -3.4674e-03, -1.7463e-03,  2.1479e-03,  2.4390e-03,\n",
       "          6.7380e-04, -2.8559e-03,  1.2898e-04,  1.1734e-03, -3.4859e-03,\n",
       "          2.0544e-03,  1.0530e-03, -8.2288e-04,  1.5530e-03, -2.2774e-03,\n",
       "         -2.3470e-03, -7.2087e-07, -1.7089e-03, -7.9023e-04,  1.9752e-03,\n",
       "         -1.5312e-04,  3.3027e-03,  4.8974e-04,  3.2047e-05, -1.6260e-03,\n",
       "         -1.3154e-03, -1.1477e-03,  1.7337e-03,  2.1191e-03,  1.2189e-03,\n",
       "          1.1661e-03,  1.8615e-03,  3.0115e-03, -9.0482e-05,  6.8998e-04,\n",
       "          1.2521e-03, -2.1232e-03, -2.3637e-03,  2.2410e-03,  2.2588e-03,\n",
       "          1.0693e-03,  1.9900e-04,  2.2167e-03, -2.3800e-03,  8.0758e-05,\n",
       "         -2.0150e-03,  1.8197e-03, -6.4330e-06, -3.9622e-04,  3.8659e-04,\n",
       "         -6.3609e-05, -3.1763e-03,  1.4943e-03, -2.3172e-04, -2.9062e-03,\n",
       "         -2.3522e-03, -2.8469e-03,  1.0950e-03, -1.4528e-03, -3.4476e-04,\n",
       "          3.2363e-03,  7.0278e-05, -2.7954e-04, -1.9240e-03, -3.3448e-04,\n",
       "          1.3149e-04,  2.6120e-04, -1.9041e-03,  6.0852e-04, -9.9429e-04,\n",
       "          1.8268e-04, -1.6492e-03, -4.9134e-04, -2.4864e-03,  9.1324e-04,\n",
       "          2.5441e-04, -1.5387e-03,  2.3500e-03, -3.0935e-03,  3.2126e-03,\n",
       "          5.7879e-03,  8.0986e-04, -2.5072e-03, -1.5324e-03, -1.5051e-03,\n",
       "          5.0842e-04, -5.4421e-04, -7.1663e-04,  2.3816e-04, -3.6042e-03,\n",
       "          2.3628e-03,  1.4981e-03,  8.8549e-04,  8.9252e-04, -1.4333e-03,\n",
       "         -9.3212e-04,  1.2219e-03,  6.3094e-04,  9.9029e-04, -3.4545e-03,\n",
       "          1.6282e-03,  1.7618e-03,  1.3484e-03, -6.7706e-04, -1.6863e-03,\n",
       "          3.6116e-03,  5.8109e-04,  6.5930e-04,  4.2664e-04, -2.6154e-03,\n",
       "         -1.7953e-03, -3.1158e-04, -1.4602e-04, -1.9735e-03, -1.6418e-03,\n",
       "         -1.5781e-03,  1.6468e-03, -2.6315e-03, -8.4738e-04, -1.2557e-04,\n",
       "          1.3933e-03, -1.1702e-03,  6.8713e-04,  6.3104e-04, -8.0031e-05,\n",
       "         -8.6134e-04,  4.3033e-05, -7.3537e-04, -2.5610e-04,  1.9312e-03,\n",
       "         -7.7175e-04, -1.1082e-03, -1.3439e-03,  3.7209e-04,  1.1522e-03,\n",
       "          1.0437e-03,  2.3567e-03,  7.9867e-04, -1.7167e-03, -9.7261e-04,\n",
       "          1.3528e-03, -2.0154e-03, -1.5545e-03,  9.6530e-04,  5.0143e-04,\n",
       "         -9.8880e-04,  1.3264e-04, -4.1515e-03, -4.7253e-04,  5.9511e-04,\n",
       "         -4.1989e-03, -1.7053e-03,  5.1654e-04,  1.7985e-03, -1.3590e-03,\n",
       "          7.4869e-05,  1.1366e-03,  2.0326e-03,  2.1838e-03,  2.8016e-03,\n",
       "         -1.8092e-05, -2.5734e-03,  6.4250e-04, -3.4023e-03, -2.8520e-03,\n",
       "         -1.0782e-03, -1.9039e-04,  3.1225e-04, -9.9231e-04, -1.8892e-04,\n",
       "         -2.0052e-03, -7.5088e-04,  5.3136e-06, -1.3314e-03, -1.4225e-03,\n",
       "          8.7766e-04, -2.9934e-03, -2.4672e-03, -1.8834e-04,  6.5098e-04,\n",
       "         -3.0957e-03,  2.3207e-03, -1.1156e-04,  5.0045e-04,  3.2501e-03,\n",
       "          2.9561e-03, -2.8238e-03,  1.3772e-03,  5.7995e-04,  6.4291e-04,\n",
       "         -6.3371e-04,  5.7259e-04, -7.6770e-04,  1.7053e-03,  2.5764e-03,\n",
       "          2.0296e-03, -1.6200e-03, -1.7055e-03,  7.0267e-04,  6.7013e-04,\n",
       "         -3.4647e-04, -5.2061e-04,  1.0174e-03,  3.6463e-04, -1.3768e-03,\n",
       "         -2.3798e-03, -2.7041e-03, -1.2150e-03, -2.4624e-03, -1.9644e-03,\n",
       "         -8.2414e-04, -2.3854e-03, -3.8093e-05,  9.2514e-04,  1.7768e-03,\n",
       "          8.3885e-04, -2.8949e-03, -3.9499e-04, -1.8069e-03,  4.8566e-04,\n",
       "         -8.5843e-04, -9.3588e-04,  3.9050e-04, -7.0268e-04,  9.9952e-05,\n",
       "          4.1162e-04, -6.9960e-04, -4.5198e-03,  2.7188e-05,  5.7534e-04,\n",
       "          2.2866e-03,  2.2083e-04, -5.1752e-04,  3.3869e-03, -2.2545e-03,\n",
       "         -1.4023e-03,  6.5602e-04,  1.0919e-03,  1.3725e-03,  1.7441e-03,\n",
       "          2.1464e-03, -2.1173e-03, -4.1424e-05, -1.5475e-03, -6.3466e-04,\n",
       "         -6.6401e-05, -1.0346e-03,  8.4563e-04, -1.0491e-03,  2.0775e-03,\n",
       "         -1.5716e-04, -1.6706e-03, -1.1294e-04,  1.8149e-03,  1.2543e-03,\n",
       "          4.6740e-04, -1.0847e-03, -5.7981e-05,  2.9412e-03, -9.3028e-04,\n",
       "          1.1916e-03,  2.2182e-03,  1.6622e-03,  2.0564e-03,  1.4570e-03,\n",
       "          9.8767e-04, -1.7323e-03, -2.9296e-03, -7.3355e-04,  4.8907e-04,\n",
       "          1.3769e-03,  5.9446e-04,  1.6562e-03, -7.9774e-04, -7.8354e-04,\n",
       "         -1.0896e-03, -2.6517e-03,  2.9490e-04,  1.3145e-05, -3.8839e-03,\n",
       "          2.1383e-04, -1.5267e-03, -1.0282e-03,  1.5463e-03, -4.0539e-04,\n",
       "         -2.1387e-04, -3.6015e-04,  1.2729e-03, -1.1076e-03,  8.6566e-04,\n",
       "          1.5914e-03,  1.5351e-03, -1.0270e-03, -2.5873e-03, -3.5648e-04,\n",
       "          2.4360e-03,  5.8927e-04,  6.3506e-04, -2.7210e-03,  2.0444e-03,\n",
       "          6.0329e-04,  1.6314e-03, -4.3436e-04, -8.5923e-04, -3.1784e-03,\n",
       "         -1.8557e-03, -1.2202e-03, -1.0999e-04,  9.4894e-04,  1.7216e-03,\n",
       "         -4.6757e-04, -8.1416e-04,  1.7373e-03, -1.0275e-03, -2.6963e-03,\n",
       "         -3.4588e-04,  3.6756e-03,  1.3312e-03,  2.6060e-03,  1.0873e-03,\n",
       "         -3.6316e-03,  1.2893e-03, -1.8492e-03,  1.3540e-03, -1.2636e-03,\n",
       "          3.4736e-04,  1.4198e-03,  1.4115e-03,  3.4938e-03,  8.9338e-04,\n",
       "         -4.3026e-04, -8.4588e-04, -1.9187e-03, -1.5718e-03, -1.1751e-03,\n",
       "         -2.5973e-04,  2.1149e-03,  4.1390e-03,  2.7156e-04, -2.5866e-03,\n",
       "         -6.5614e-04, -3.3063e-04, -1.5647e-03, -1.3460e-03, -1.2627e-03,\n",
       "         -8.8377e-04,  3.1786e-03,  5.6534e-05, -6.5750e-04, -2.3463e-03,\n",
       "         -1.0535e-03,  2.0128e-04,  6.0220e-04, -2.0749e-03, -1.9648e-03,\n",
       "          1.7430e-03, -9.7403e-04,  1.5949e-04, -1.4321e-03, -4.7561e-04,\n",
       "          9.0337e-05,  2.2209e-03, -1.4816e-03,  3.3545e-04,  5.2927e-04,\n",
       "         -1.8015e-03,  1.0192e-03,  1.2735e-03,  2.7105e-03, -2.5752e-04,\n",
       "         -1.4091e-03,  3.0124e-03,  2.6379e-03, -4.4681e-03,  2.8863e-05,\n",
       "          2.3690e-03,  4.6207e-04,  1.4325e-03,  8.4582e-04, -1.1199e-04,\n",
       "          1.5826e-03,  1.3403e-03,  1.1966e-03,  7.0007e-04, -1.6249e-04,\n",
       "          6.1346e-04,  2.0875e-03,  1.0125e-03, -6.2817e-04,  1.8400e-03,\n",
       "         -1.0221e-03,  1.6247e-06,  6.4815e-04, -4.8043e-03, -1.2310e-03,\n",
       "          1.1592e-04,  1.2344e-03]),\n",
       " 'transformer.resblocks.4.mlp.c_fc.weight': tensor([[ 0.0098, -0.0158, -0.0473,  ...,  0.0316,  0.0359,  0.0312],\n",
       "         [ 0.0457,  0.0248, -0.0404,  ..., -0.0123,  0.0059,  0.0237],\n",
       "         [ 0.0136,  0.0594,  0.0232,  ...,  0.0216, -0.0270, -0.0031],\n",
       "         ...,\n",
       "         [ 0.0139, -0.0186,  0.0100,  ...,  0.0138, -0.0098, -0.0107],\n",
       "         [ 0.0410, -0.0325,  0.0260,  ...,  0.0078,  0.0456, -0.0050],\n",
       "         [ 0.0004, -0.0274, -0.0589,  ...,  0.0116, -0.0147, -0.0339]]),\n",
       " 'transformer.resblocks.4.mlp.c_fc.bias': tensor([ 0.0214,  0.0250, -0.0275,  ..., -0.0196, -0.0365,  0.0021]),\n",
       " 'transformer.resblocks.4.mlp.c_proj.weight': tensor([[ 1.7848e-02,  1.8178e-03, -3.9683e-03,  ..., -1.1278e-02,\n",
       "          -1.0144e-02, -1.2181e-02],\n",
       "         [ 1.3135e-02,  9.0031e-03, -1.4309e-02,  ...,  4.5222e-03,\n",
       "           1.0200e-02,  3.4806e-04],\n",
       "         [ 6.8479e-03,  9.9185e-03, -1.4209e-02,  ..., -1.4172e-03,\n",
       "          -9.2559e-03,  9.7664e-05],\n",
       "         ...,\n",
       "         [-8.8473e-03,  8.5056e-03,  6.4733e-03,  ..., -1.0218e-02,\n",
       "           1.7082e-03, -1.6149e-03],\n",
       "         [-8.0250e-03, -2.6735e-02, -1.9225e-02,  ...,  1.5969e-03,\n",
       "          -9.0673e-03, -5.5906e-03],\n",
       "         [ 1.6868e-02, -6.1775e-03,  1.4351e-03,  ..., -2.2733e-02,\n",
       "           3.4934e-03, -4.8325e-04]]),\n",
       " 'transformer.resblocks.4.mlp.c_proj.bias': tensor([-1.7909e-02,  2.4840e-03, -2.0411e-03,  2.0588e-02,  8.5909e-03,\n",
       "         -3.7049e-03,  1.4476e-02,  1.1570e-02, -7.3911e-03,  1.7529e-02,\n",
       "          7.3678e-03, -1.7562e-02,  3.9079e-03,  1.2906e-02,  2.2539e-03,\n",
       "          1.8867e-02, -8.5530e-03, -8.8994e-03, -1.1533e-02,  5.1083e-03,\n",
       "          1.9021e-02, -1.0732e-02,  1.9139e-02, -1.5860e-02,  1.6200e-02,\n",
       "         -3.9300e-03,  1.2956e-02, -2.0637e-02,  1.4934e-03,  1.9675e-02,\n",
       "         -1.9959e-02, -2.0750e-03, -1.8862e-03, -4.0714e-03, -1.1582e-03,\n",
       "         -2.9624e-03, -1.5703e-02,  2.1365e-02,  1.0590e-02, -9.7307e-03,\n",
       "         -7.3795e-03,  1.2613e-02,  1.3637e-02, -1.5487e-02, -9.3243e-03,\n",
       "          4.8002e-03, -1.6130e-02,  1.5471e-02, -1.8056e-02, -4.4131e-03,\n",
       "         -8.2554e-03,  3.3696e-03,  1.1178e-02, -9.1563e-03, -1.0578e-02,\n",
       "         -1.0479e-02,  1.3372e-03,  1.4931e-02, -4.5365e-03, -2.2033e-03,\n",
       "          4.8243e-03, -1.5251e-02,  3.0196e-03,  2.1775e-02,  1.9607e-02,\n",
       "         -1.4163e-02, -1.1078e-02,  2.1219e-02, -1.8459e-03, -1.4662e-02,\n",
       "          6.7663e-03, -1.9159e-03,  1.7092e-02, -1.4342e-02,  6.0850e-03,\n",
       "         -3.9195e-03, -7.1628e-03,  8.3907e-03, -1.8078e-02,  2.0479e-02,\n",
       "         -3.4257e-04, -1.4944e-02, -9.5011e-03,  1.0608e-02, -1.1692e-02,\n",
       "         -8.9496e-03,  8.6009e-03, -4.3031e-03,  2.0980e-02, -2.1383e-02,\n",
       "         -6.4169e-03, -1.8388e-02,  3.1247e-03, -1.3285e-02,  9.6722e-03,\n",
       "         -1.6716e-02,  2.0365e-02, -1.5442e-02,  1.5770e-02,  1.2188e-02,\n",
       "          2.1016e-02,  4.3801e-03,  2.2898e-03, -4.2811e-03, -8.6304e-03,\n",
       "         -2.3283e-02,  1.1749e-02, -2.0810e-02, -8.7287e-03, -1.7272e-02,\n",
       "         -5.6503e-03,  1.3952e-02, -7.7234e-03, -1.2124e-02, -1.6900e-02,\n",
       "         -7.8636e-03, -1.6818e-02, -1.2063e-02,  1.2977e-02, -1.1456e-04,\n",
       "         -8.6078e-03,  1.2398e-03, -2.1072e-02,  8.4610e-03, -1.7692e-02,\n",
       "         -2.2651e-03, -2.1919e-02,  1.7262e-03, -2.1429e-02,  1.8056e-02,\n",
       "         -1.5801e-02,  1.0721e-02,  1.6768e-02, -1.6782e-02,  2.6420e-03,\n",
       "         -5.2210e-03, -1.5383e-02,  2.0046e-02,  9.5137e-03,  2.0754e-03,\n",
       "          9.1011e-03,  1.0046e-02,  1.8924e-02,  1.8762e-02,  6.5064e-03,\n",
       "         -1.3169e-04, -2.1146e-03,  1.1203e-02, -6.2605e-05,  2.0968e-02,\n",
       "         -2.0255e-02,  5.5300e-03, -7.5995e-03,  1.5940e-02,  6.7842e-03,\n",
       "         -6.6552e-04,  1.5538e-02, -2.2866e-03,  1.6831e-02, -2.0332e-03,\n",
       "          2.3675e-03,  2.1217e-02,  1.2584e-02, -2.3780e-03,  1.3933e-02,\n",
       "         -1.3862e-02,  2.2449e-03, -4.2871e-03, -1.1867e-03, -1.6094e-02,\n",
       "         -2.3671e-03,  1.1689e-02, -4.3814e-03, -3.9323e-03, -5.5611e-03,\n",
       "         -5.1477e-03,  1.2622e-02,  1.5625e-02,  1.2050e-02, -1.8203e-02,\n",
       "         -1.7444e-02, -8.3838e-04,  1.4958e-02,  9.6947e-03,  3.8032e-03,\n",
       "          1.7126e-02, -1.4713e-02, -2.1340e-02,  1.9803e-04,  9.8708e-03,\n",
       "         -1.7425e-02, -5.5917e-03, -1.6784e-02,  6.8245e-04,  8.4315e-03,\n",
       "          4.6044e-03, -1.9213e-03,  9.8178e-03, -8.9074e-03,  8.7406e-03,\n",
       "          9.9955e-03,  1.8825e-02, -1.9990e-02,  1.5256e-02,  4.5012e-03,\n",
       "         -6.7368e-03,  8.5042e-03, -1.9857e-02, -1.5724e-03,  1.3032e-02,\n",
       "          1.2922e-02, -5.7244e-03,  3.4204e-03, -7.8922e-03, -1.9895e-02,\n",
       "          1.2395e-02, -1.9119e-02,  1.2520e-02, -3.1093e-03,  8.1754e-03,\n",
       "          7.2273e-03,  1.9418e-02, -6.2420e-03, -2.0700e-02, -1.0495e-02,\n",
       "         -4.4124e-03, -1.5318e-03,  3.9075e-03, -1.1912e-02, -9.6354e-03,\n",
       "          2.1355e-03, -3.1552e-03,  5.9906e-03, -2.3140e-04, -3.0216e-03,\n",
       "         -2.3365e-03,  3.4913e-03, -3.4128e-03, -1.2568e-02,  2.0428e-03,\n",
       "         -2.2380e-02,  1.5864e-02,  1.5391e-02,  1.6333e-02, -3.6199e-03,\n",
       "         -1.0770e-02, -8.1670e-03, -2.1623e-02, -6.7519e-03, -1.7227e-02,\n",
       "         -1.2668e-02, -8.1624e-03, -5.0435e-03, -1.3111e-02, -8.7724e-03,\n",
       "          1.1100e-02,  2.1642e-02,  1.6833e-02, -1.6995e-02, -1.1710e-02,\n",
       "         -1.6910e-02, -1.6266e-02, -6.2657e-03,  1.5799e-02, -2.1295e-02,\n",
       "          1.5787e-02, -5.1488e-03, -1.1019e-03,  1.2995e-02, -1.6715e-02,\n",
       "          1.3524e-03,  7.1194e-03,  1.1790e-02,  3.3154e-04, -2.0337e-02,\n",
       "         -1.5884e-02, -1.2360e-02, -2.2144e-02,  5.2928e-03,  1.4690e-02,\n",
       "          5.4308e-03, -1.9400e-02, -5.1857e-03, -5.4966e-03, -7.5451e-03,\n",
       "         -1.5080e-02,  1.9087e-02,  2.3313e-02,  1.0950e-02, -5.5712e-03,\n",
       "          1.3839e-02,  5.4958e-03, -4.3688e-03,  1.4372e-02,  6.3626e-03,\n",
       "         -4.0752e-03, -4.8537e-03,  2.2866e-02, -1.0204e-02,  1.2485e-02,\n",
       "         -4.0269e-03, -1.0427e-02,  1.7103e-03, -1.5547e-02,  1.5095e-03,\n",
       "         -1.9355e-02,  7.9806e-03, -8.4107e-03,  1.7059e-02,  1.9804e-02,\n",
       "         -1.2499e-02,  1.1544e-02, -1.9366e-02,  5.4062e-03,  9.1896e-03,\n",
       "          4.0501e-03,  1.0166e-02,  1.9359e-02,  1.4074e-02, -1.9084e-02,\n",
       "          1.1732e-03, -4.4383e-03, -2.3205e-03,  2.1365e-02, -2.0588e-02,\n",
       "          1.0184e-03,  3.0336e-03,  2.1238e-02, -1.4383e-02,  1.7428e-02,\n",
       "         -1.7405e-02,  1.6283e-02,  7.4546e-03,  6.2035e-04, -1.7655e-02,\n",
       "         -4.2239e-03,  4.8836e-03,  9.5376e-03, -6.5338e-03, -3.2162e-03,\n",
       "          1.6573e-02, -8.5808e-03,  1.5157e-02,  2.0617e-02, -1.7760e-02,\n",
       "          2.9624e-03, -1.6229e-02, -1.7644e-02, -4.8620e-03, -1.5405e-02,\n",
       "         -8.3136e-04, -1.1488e-02, -6.9235e-03, -2.0421e-02,  1.9149e-02,\n",
       "         -1.2461e-02,  1.2861e-02, -5.5166e-03, -1.5748e-02, -9.2394e-03,\n",
       "          1.7216e-02,  2.0652e-02,  8.7181e-03,  1.8410e-02,  1.4903e-02,\n",
       "         -6.0552e-03, -1.9558e-02,  1.0531e-02, -1.0511e-02, -1.5236e-02,\n",
       "         -1.8059e-02,  6.8614e-03, -1.8788e-02,  8.7747e-03, -2.2315e-02,\n",
       "          2.2389e-02,  1.2387e-02,  1.4982e-02,  2.0678e-02,  4.6773e-03,\n",
       "          1.8279e-02, -9.6889e-03, -1.4728e-02,  9.2289e-04,  8.3116e-03,\n",
       "          1.2314e-02, -9.2880e-03, -2.1755e-02, -1.8588e-04, -7.9500e-04,\n",
       "          1.9261e-02, -1.5577e-02,  3.5806e-03, -1.0081e-02,  2.4030e-02,\n",
       "         -1.1296e-02,  1.1992e-02,  2.1238e-02, -2.0989e-02, -1.6407e-02,\n",
       "          1.7248e-02, -1.2007e-02, -1.6665e-02, -1.9818e-02,  7.1233e-03,\n",
       "         -5.7725e-03, -1.6730e-02,  9.9224e-03, -1.6966e-02,  1.4748e-02,\n",
       "         -2.0838e-02, -1.4922e-02, -2.1185e-02,  9.4171e-03, -6.3514e-03,\n",
       "          1.0314e-02, -9.1079e-03,  1.3172e-02, -1.9430e-02,  1.2753e-02,\n",
       "          1.9439e-02,  5.5291e-03, -1.4059e-02, -4.8144e-03, -2.2140e-02,\n",
       "         -1.6990e-02, -3.0454e-04,  1.6201e-02,  5.4114e-03, -1.7567e-02,\n",
       "          1.8614e-02,  5.6380e-03,  2.1686e-03, -1.4463e-02,  6.8768e-03,\n",
       "          1.5598e-02,  1.0158e-02, -2.0198e-02, -5.9783e-04, -8.4075e-04,\n",
       "         -3.7572e-03, -1.9773e-04, -1.5582e-02, -1.9307e-02,  7.4101e-03,\n",
       "         -5.8894e-03,  1.8758e-03, -1.6984e-02, -1.7395e-02,  8.5604e-03,\n",
       "          2.0785e-02, -1.1940e-03, -5.0576e-03,  1.8386e-02,  4.4971e-03,\n",
       "          1.9678e-02, -1.2155e-02,  3.5040e-04, -1.3874e-03,  1.6774e-02,\n",
       "          2.2330e-03,  6.0075e-03, -8.9683e-03, -2.3986e-03, -4.0814e-03,\n",
       "          1.2219e-02, -7.0167e-03, -1.2937e-02, -7.2016e-03, -7.8731e-05,\n",
       "         -8.8205e-03,  1.8901e-02, -1.1294e-03,  7.5605e-03, -8.8429e-04,\n",
       "          1.1263e-02,  1.3741e-02, -6.9180e-03, -1.0386e-04, -6.1742e-03,\n",
       "          1.1350e-02, -4.4559e-03, -7.6649e-04, -1.6159e-03,  3.1629e-03,\n",
       "         -1.1662e-02, -2.2772e-04,  1.1777e-02, -1.1954e-02,  1.2468e-02,\n",
       "         -1.4164e-02,  6.5356e-03,  1.7415e-02, -1.9133e-02, -4.1817e-03,\n",
       "          5.9168e-03, -5.8977e-03,  2.3112e-03, -1.6699e-02, -8.5986e-03,\n",
       "         -1.3469e-02, -1.6627e-02, -1.0367e-06,  8.1138e-03, -1.6088e-02,\n",
       "         -3.0085e-03, -2.0884e-02,  1.9377e-02, -1.6835e-03,  1.7692e-02,\n",
       "         -2.0013e-02,  7.7919e-03]),\n",
       " 'transformer.resblocks.4.ln_2.weight': tensor([0.9985, 1.0007, 1.0007, 1.0023, 1.0003, 1.0016, 1.0015, 1.0020, 1.0008,\n",
       "         1.0014, 1.0020, 0.9992, 0.9992, 1.0037, 0.9982, 1.0023, 1.0038, 1.0068,\n",
       "         1.0013, 1.0016, 1.0046, 1.0034, 1.0003, 1.0011, 1.0013, 1.0035, 1.0006,\n",
       "         1.0002, 0.9989, 1.0009, 1.0016, 1.0030, 1.0004, 1.0027, 1.0008, 1.0042,\n",
       "         0.9993, 1.0025, 1.0007, 1.0040, 1.0018, 0.9996, 1.0028, 0.9991, 1.0012,\n",
       "         1.0007, 1.0021, 1.0039, 0.9993, 1.0010, 1.0012, 1.0002, 1.0022, 1.0002,\n",
       "         1.0037, 1.0028, 1.0015, 1.0018, 1.0010, 1.0025, 1.0014, 1.0015, 0.9999,\n",
       "         1.0019, 1.0028, 1.0008, 0.9992, 1.0035, 1.0015, 1.0037, 1.0022, 1.0005,\n",
       "         1.0006, 1.0004, 1.0024, 1.0039, 1.0020, 1.0018, 1.0007, 1.0013, 1.0001,\n",
       "         1.0066, 1.0027, 0.9974, 1.0026, 1.0015, 1.0029, 1.0000, 1.0020, 1.0024,\n",
       "         1.0034, 1.0014, 1.0022, 1.0028, 1.0011, 1.0003, 0.9984, 1.0016, 1.0032,\n",
       "         1.0025, 1.0025, 1.0003, 0.9963, 0.9996, 1.0018, 1.0014, 1.0029, 1.0024,\n",
       "         0.9997, 1.0026, 1.0029, 0.9970, 1.0003, 1.0013, 1.0042, 1.0000, 1.0006,\n",
       "         1.0011, 1.0024, 1.0032, 0.9997, 1.0012, 1.0010, 1.0035, 1.0031, 1.0022,\n",
       "         1.0022, 1.0023, 1.0057, 1.0027, 1.0037, 1.0016, 1.0006, 1.0045, 1.0029,\n",
       "         1.0001, 1.0002, 1.0004, 1.0034, 1.0022, 1.0008, 1.0013, 1.0035, 1.0014,\n",
       "         0.9981, 1.0024, 1.0031, 1.0012, 1.0019, 1.0012, 0.9989, 1.0021, 1.0000,\n",
       "         1.0029, 1.0043, 1.0025, 0.9978, 1.0021, 1.0007, 1.0032, 1.0022, 0.9988,\n",
       "         0.9997, 1.0056, 0.9992, 1.0014, 1.0012, 1.0015, 1.0019, 1.0023, 1.0031,\n",
       "         1.0005, 1.0041, 1.0023, 1.0017, 1.0006, 1.0010, 1.0012, 1.0018, 0.9993,\n",
       "         1.0044, 1.0028, 1.0013, 0.9994, 0.9999, 1.0007, 1.0024, 1.0019, 1.0005,\n",
       "         1.0049, 1.0059, 1.0012, 1.0032, 1.0023, 1.0024, 1.0011, 1.0027, 1.0020,\n",
       "         1.0020, 1.0007, 1.0029, 1.0004, 0.9998, 1.0011, 1.0016, 1.0024, 1.0023,\n",
       "         1.0016, 1.0033, 1.0036, 0.9980, 1.0012, 1.0019, 1.0014, 1.0013, 1.0033,\n",
       "         1.0014, 1.0031, 1.0021, 1.0008, 0.9992, 1.0034, 1.0027, 1.0018, 0.9995,\n",
       "         1.0037, 1.0023, 1.0006, 1.0005, 1.0051, 1.0027, 1.0016, 1.0007, 1.0016,\n",
       "         1.0015, 0.9982, 1.0001, 1.0031, 1.0030, 1.0014, 1.0027, 1.0007, 1.0022,\n",
       "         0.9997, 1.0032, 1.0052, 1.0041, 1.0015, 1.0016, 0.9996, 1.0013, 1.0003,\n",
       "         0.9998, 1.0026, 1.0022, 0.9990, 1.0021, 0.9980, 1.0030, 1.0001, 1.0062,\n",
       "         1.0021, 0.9999, 1.0023, 1.0001, 1.0038, 0.9994, 1.0005, 1.0014, 1.0019,\n",
       "         1.0031, 1.0006, 0.9992, 1.0022, 0.9988, 0.9991, 1.0014, 1.0027, 0.9999,\n",
       "         1.0019, 1.0042, 1.0017, 0.9990, 1.0049, 1.0026, 1.0024, 1.0029, 1.0017,\n",
       "         1.0062, 1.0008, 1.0038, 0.9997, 1.0035, 0.9996, 1.0003, 1.0012, 1.0012,\n",
       "         1.0034, 1.0020, 1.0029, 0.9987, 1.0033, 1.0046, 1.0000, 1.0021, 1.0029,\n",
       "         0.9997, 1.0039, 1.0017, 1.0012, 0.9991, 1.0017, 1.0023, 1.0048, 1.0027,\n",
       "         1.0019, 1.0002, 1.0042, 1.0059, 0.9994, 0.9992, 1.0022, 1.0029, 1.0023,\n",
       "         1.0011, 1.0022, 0.9988, 1.0029, 1.0015, 1.0019, 1.0019, 1.0017, 1.0036,\n",
       "         1.0025, 1.0002, 1.0019, 1.0022, 0.9993, 1.0042, 1.0017, 0.9991, 1.0001,\n",
       "         1.0001, 1.0035, 1.0025, 1.0024, 1.0001, 1.0005, 1.0001, 1.0014, 0.9975,\n",
       "         1.0021, 1.0031, 1.0042, 1.0025, 0.9988, 1.0030, 0.9988, 1.0007, 1.0003,\n",
       "         1.0042, 1.0003, 1.0029, 1.0000, 1.0015, 1.0009, 1.0041, 1.0029, 1.0024,\n",
       "         1.0035, 1.0017, 1.0000, 1.0036, 1.0012, 1.0022, 1.0039, 1.0025, 0.9991,\n",
       "         1.0029, 1.0025, 1.0038, 1.0000, 1.0030, 1.0002, 1.0014, 1.0016, 1.0014,\n",
       "         1.0012, 1.0032, 0.9985, 1.0050, 1.0019, 1.0037, 1.0012, 1.0009, 1.0004,\n",
       "         1.0006, 1.0004, 1.0003, 1.0014, 0.9991, 1.0019, 1.0010, 1.0014, 1.0003,\n",
       "         1.0021, 1.0014, 1.0002, 1.0028, 0.9996, 1.0026, 1.0022, 1.0055, 1.0027,\n",
       "         0.9995, 1.0004, 0.9985, 1.0030, 1.0008, 1.0014, 1.0008, 0.9988, 0.9976,\n",
       "         1.0005, 1.0035, 1.0037, 1.0038, 1.0014, 1.0003, 1.0023, 1.0014, 1.0024,\n",
       "         1.0029, 1.0026, 1.0014, 1.0020, 1.0010, 1.0010, 1.0019, 1.0044, 1.0025,\n",
       "         1.0031, 1.0010, 1.0006, 1.0024, 1.0027, 0.9985, 1.0015, 1.0009, 1.0023,\n",
       "         1.0026, 0.9997, 1.0022, 0.9971, 1.0011, 1.0031, 1.0032, 0.9998, 1.0036,\n",
       "         0.9986, 1.0018, 1.0002, 1.0049, 1.0028, 1.0018, 1.0021, 1.0022, 0.9995,\n",
       "         1.0039, 1.0010, 1.0009, 1.0022, 1.0018, 1.0035, 1.0011, 1.0024, 1.0038,\n",
       "         1.0025, 0.9987, 1.0017, 1.0021, 1.0028, 1.0021, 1.0018, 0.9982, 1.0006,\n",
       "         0.9983, 1.0022, 1.0028, 1.0015, 1.0005, 1.0023, 1.0000, 1.0032, 1.0032,\n",
       "         1.0025, 0.9991, 1.0013, 0.9998, 1.0011, 1.0016, 1.0021, 1.0012, 1.0022,\n",
       "         0.9976, 1.0012, 1.0014, 1.0001, 0.9999, 1.0033, 0.9977, 1.0014]),\n",
       " 'transformer.resblocks.4.ln_2.bias': tensor([-2.1088e-03,  5.1827e-04, -1.8954e-03, -1.2989e-03,  8.7875e-04,\n",
       "          2.1135e-03,  2.4986e-03, -2.2701e-03,  8.5565e-04, -1.0638e-03,\n",
       "         -8.3637e-04,  5.1872e-03,  2.2578e-03,  9.6840e-04,  7.9140e-03,\n",
       "         -2.7123e-04, -2.5433e-03, -2.5430e-03, -2.7849e-03, -1.6321e-03,\n",
       "          1.0599e-03, -4.9188e-04, -1.3598e-03,  1.5115e-04, -8.0332e-04,\n",
       "          2.3903e-04,  2.1017e-03,  6.2974e-04, -2.9173e-03,  2.8236e-03,\n",
       "         -2.5455e-03,  1.2261e-04, -1.7107e-03, -1.4538e-03,  8.2482e-04,\n",
       "          2.0037e-03,  3.5553e-04,  1.3883e-03, -8.5187e-04, -7.5818e-04,\n",
       "         -8.7849e-04,  2.2282e-03, -1.6716e-03, -1.9746e-03,  2.0472e-03,\n",
       "         -4.8989e-04, -1.7514e-03, -1.9362e-03,  2.0366e-03,  1.2387e-03,\n",
       "          1.5505e-03, -3.0553e-03,  1.3551e-03, -2.0304e-03, -6.4996e-04,\n",
       "         -8.7840e-04, -1.2885e-03, -2.8886e-03,  1.2770e-03, -3.0091e-03,\n",
       "          3.0140e-03,  8.0800e-04, -4.4728e-04, -1.3946e-03, -2.7351e-03,\n",
       "         -2.3380e-03,  1.0766e-03,  3.6543e-04,  3.2641e-03,  4.5218e-04,\n",
       "         -2.1410e-04,  1.1246e-03,  2.5895e-03, -1.2001e-03,  3.3010e-04,\n",
       "          8.3150e-04, -1.7842e-03, -7.3127e-04,  1.2781e-03, -3.8038e-05,\n",
       "         -3.3709e-03,  2.8044e-03,  2.0046e-03, -1.8109e-03, -2.4205e-03,\n",
       "         -4.9079e-04, -6.3869e-04, -4.7296e-04, -2.9496e-03, -4.7613e-04,\n",
       "         -4.1795e-04,  1.9861e-03, -1.4015e-03,  2.1416e-03, -6.7258e-04,\n",
       "          9.4509e-04,  9.3764e-04, -5.7610e-04, -5.4958e-04,  1.6547e-03,\n",
       "          1.0727e-03,  4.7197e-04, -2.0625e-03,  9.3439e-04,  2.8795e-03,\n",
       "          7.1990e-04,  7.8253e-04,  1.8702e-04,  3.4639e-03, -6.7790e-04,\n",
       "         -1.2995e-03, -3.5152e-04,  3.5830e-04, -3.8730e-03,  1.9227e-03,\n",
       "          1.6902e-03, -2.6782e-03, -2.9078e-03,  1.5801e-03, -1.1982e-03,\n",
       "          1.3989e-03,  9.8971e-04,  2.0395e-03, -7.8901e-04,  3.9985e-04,\n",
       "          8.8097e-04, -3.2174e-04,  4.8522e-04,  1.8501e-03,  2.6628e-04,\n",
       "          4.4464e-04, -1.0066e-03, -1.1742e-03, -2.0848e-03,  1.5449e-03,\n",
       "          2.6463e-04,  1.0503e-03, -7.7245e-04, -2.2413e-03,  2.1588e-03,\n",
       "         -2.3192e-04, -3.2811e-03,  3.2279e-03, -1.3912e-04,  1.6573e-03,\n",
       "         -1.3037e-03,  1.8386e-03, -1.6521e-04, -2.2423e-03, -3.0243e-03,\n",
       "          9.6323e-04, -1.2760e-03,  4.5957e-04, -3.5818e-03,  5.0797e-04,\n",
       "         -2.1363e-03, -7.5253e-04, -1.9384e-04, -1.0939e-03, -1.3526e-04,\n",
       "          3.9305e-03, -8.4958e-04, -2.4578e-03,  2.2382e-03, -2.1390e-03,\n",
       "         -2.3063e-03,  4.5510e-04, -1.9188e-03, -7.6466e-05,  6.2097e-05,\n",
       "         -1.1829e-03, -5.9502e-04,  9.4719e-05, -2.6720e-03, -1.7073e-04,\n",
       "         -1.0680e-03,  1.2574e-03,  9.1701e-04,  1.1067e-03,  1.3669e-03,\n",
       "         -1.2438e-03, -2.1197e-03,  2.2677e-03, -1.0684e-03, -2.1273e-03,\n",
       "         -2.0746e-04,  3.5032e-04, -3.3955e-04,  3.9713e-04, -1.7434e-03,\n",
       "         -8.9186e-04, -2.8095e-04, -7.1345e-04, -1.9496e-03,  8.9987e-04,\n",
       "         -8.2929e-04,  1.3869e-03,  1.4913e-03, -2.9854e-03, -1.8827e-03,\n",
       "         -4.2007e-04,  1.0111e-03, -2.9458e-04, -1.9856e-04, -4.9218e-04,\n",
       "         -2.0833e-03, -6.3693e-04, -2.2221e-05,  1.5139e-03,  8.5988e-05,\n",
       "         -1.4685e-03,  1.7787e-04,  2.0616e-04, -4.3660e-04, -2.0543e-03,\n",
       "          9.4790e-04, -2.7430e-03, -1.8840e-03,  3.9888e-04,  1.8763e-03,\n",
       "          4.9218e-04,  6.9247e-04, -2.5977e-03, -1.1935e-03, -1.2762e-03,\n",
       "         -2.3299e-04, -1.7351e-04,  1.2514e-03, -8.8074e-04,  1.3658e-04,\n",
       "         -4.7461e-04, -1.9638e-03, -1.8793e-04,  1.5157e-03, -1.5710e-03,\n",
       "          1.7671e-03, -3.2469e-03,  2.2188e-03, -1.1386e-03, -1.7832e-03,\n",
       "          2.1852e-03, -2.8328e-03, -4.0903e-05, -2.6735e-03,  2.1676e-04,\n",
       "         -3.3501e-04,  1.4523e-03, -1.9514e-03, -2.3426e-03, -2.0651e-03,\n",
       "          3.2624e-04,  2.6000e-03, -1.2695e-03,  2.0810e-03, -1.2666e-03,\n",
       "          2.7101e-03,  2.0086e-03, -1.6433e-04, -1.5487e-03,  1.7573e-03,\n",
       "          1.7607e-03, -4.6813e-03,  3.8892e-03, -1.6995e-03,  4.6997e-03,\n",
       "         -1.5374e-04, -3.4739e-04, -1.8181e-03, -2.1511e-03, -3.1901e-03,\n",
       "          1.8731e-03,  7.7908e-04,  7.4910e-04, -4.1517e-04,  5.9257e-04,\n",
       "         -5.9586e-04, -1.4925e-03,  1.1484e-03, -4.2100e-04, -2.5055e-03,\n",
       "         -3.0203e-03,  5.0110e-05,  8.2434e-04, -5.6738e-04, -7.9046e-05,\n",
       "          1.5093e-03,  9.3218e-04,  1.7733e-03,  1.3518e-03, -3.9232e-03,\n",
       "          1.5402e-03, -3.7190e-03, -1.6753e-03,  6.4656e-05, -2.9425e-03,\n",
       "          2.3962e-03,  3.5885e-03, -6.5142e-04,  3.3376e-04, -4.8302e-04,\n",
       "          3.2426e-03,  2.0071e-03, -2.1806e-03,  2.6661e-03,  2.9287e-03,\n",
       "          3.0910e-03, -9.0298e-05,  2.1611e-03, -2.6124e-03,  8.7867e-04,\n",
       "         -2.3682e-03, -2.9448e-04, -1.8533e-03, -1.2977e-03, -4.2867e-03,\n",
       "          9.1487e-04, -2.8141e-04,  7.3042e-04, -2.3196e-03,  2.7931e-04,\n",
       "          1.2142e-03,  1.4802e-03, -2.1717e-03,  8.0736e-04,  1.0384e-03,\n",
       "          1.0393e-03, -2.8262e-03,  1.6610e-03,  8.6371e-04,  1.6485e-03,\n",
       "          1.0074e-03,  2.2756e-04,  2.4006e-05,  4.2303e-03, -9.1750e-04,\n",
       "         -1.1800e-03, -2.5561e-04, -1.9546e-03,  1.1788e-04, -1.9242e-03,\n",
       "          5.2425e-04,  7.0829e-05,  3.8065e-04,  8.4156e-04, -1.4611e-04,\n",
       "         -1.1116e-03, -9.0162e-05,  2.1068e-03,  3.4277e-04,  3.2250e-04,\n",
       "          3.0418e-03, -6.2832e-04,  1.8962e-04, -1.0053e-03, -9.2209e-04,\n",
       "          1.7212e-03,  2.3032e-03, -2.8877e-03,  1.6617e-03,  9.3663e-04,\n",
       "         -1.1336e-03, -1.0458e-03, -1.5417e-03, -2.5013e-03, -4.0129e-03,\n",
       "         -2.3153e-03,  4.4455e-03,  3.5322e-03, -6.9285e-04,  1.5920e-03,\n",
       "          2.4185e-03, -3.8840e-03, -2.5044e-03, -9.2316e-04,  2.3093e-03,\n",
       "         -5.9129e-05, -3.7003e-03, -2.1981e-03,  3.1403e-03,  5.5915e-03,\n",
       "          1.6021e-03, -2.3513e-04,  1.7396e-03, -3.8845e-05,  1.1832e-03,\n",
       "          2.3112e-03, -3.4715e-03, -1.1596e-04,  6.2497e-04, -1.6268e-04,\n",
       "          1.2914e-03, -6.3224e-04,  3.0945e-03,  1.4075e-03, -2.0288e-03,\n",
       "          3.8964e-04,  1.6901e-03, -2.1932e-03,  8.6370e-04,  2.1946e-03,\n",
       "          1.3130e-03, -1.9549e-04, -1.0557e-03,  3.2578e-03, -3.6526e-03,\n",
       "         -2.2915e-03, -1.8573e-03, -2.9129e-03,  1.3664e-03, -2.0430e-03,\n",
       "         -1.7965e-03,  2.3540e-03, -1.4358e-03, -1.2618e-03, -1.7517e-04,\n",
       "          2.3229e-05, -5.5001e-04, -7.4347e-04,  3.7008e-03,  3.0971e-05,\n",
       "         -4.8517e-04, -3.2253e-03, -3.4674e-03, -2.0021e-04,  1.8940e-03,\n",
       "         -2.0344e-03, -6.6340e-04,  1.3047e-03, -3.4404e-03, -1.6246e-03,\n",
       "          1.3067e-03,  2.1887e-03,  3.1229e-04, -1.3011e-03,  1.4274e-03,\n",
       "         -1.5872e-03, -2.6413e-04,  1.7307e-03, -7.9879e-04, -2.4891e-04,\n",
       "         -8.2877e-04,  1.7539e-03, -9.6112e-05,  1.2027e-03,  2.3305e-03,\n",
       "          3.2075e-03, -1.2695e-04,  1.4387e-03,  2.6088e-03,  1.8193e-03,\n",
       "         -6.8730e-06, -1.4576e-03, -1.0897e-03, -2.5944e-03,  1.6546e-03,\n",
       "          4.4230e-04, -9.1329e-04, -1.4173e-03, -8.8838e-05, -1.0435e-03,\n",
       "          9.1580e-04, -1.1012e-03,  2.4917e-03, -2.3412e-03, -1.8668e-03,\n",
       "          3.0792e-03, -8.3794e-04, -2.2140e-03, -9.1496e-04,  2.4296e-04,\n",
       "          9.4497e-04,  1.9585e-04,  2.2019e-03,  1.9935e-03,  1.0126e-03,\n",
       "         -1.1694e-03,  1.0340e-04,  1.7243e-03,  6.7992e-04,  2.2612e-03,\n",
       "          1.8998e-03, -1.0094e-03,  1.7034e-03,  2.3678e-03,  3.2144e-03,\n",
       "         -1.3459e-03, -1.0936e-03,  7.4782e-04,  2.1632e-03, -5.5496e-05,\n",
       "          8.7180e-04, -2.6625e-04, -2.3372e-05, -1.2671e-03, -1.9109e-04,\n",
       "         -1.8764e-03, -2.4892e-03, -2.5613e-03, -3.3258e-03, -2.2604e-03,\n",
       "         -5.0678e-04, -1.4384e-03,  1.1977e-03,  2.0950e-04,  3.9847e-03,\n",
       "          1.9780e-03, -7.2917e-04, -9.1127e-04, -2.7850e-03,  1.4063e-03,\n",
       "          2.0117e-03, -5.5650e-04]),\n",
       " 'transformer.resblocks.5.attn.in_proj_weight': tensor([[-0.0261, -0.0406,  0.0477,  ..., -0.0594,  0.0750, -0.0129],\n",
       "         [-0.0490,  0.0029, -0.0467,  ...,  0.0518,  0.0082,  0.0391],\n",
       "         [-0.0878, -0.0116,  0.0859,  ..., -0.0594, -0.0120, -0.0313],\n",
       "         ...,\n",
       "         [ 0.0500,  0.0117,  0.0238,  ..., -0.0217,  0.0155, -0.0081],\n",
       "         [-0.0107,  0.0081,  0.0114,  ..., -0.0249,  0.0657,  0.0080],\n",
       "         [-0.0026,  0.0033,  0.0186,  ..., -0.0055, -0.0728, -0.0795]]),\n",
       " 'transformer.resblocks.5.attn.in_proj_bias': tensor([ 0.0047, -0.0007,  0.0021,  ..., -0.0015,  0.0018,  0.0047]),\n",
       " 'transformer.resblocks.5.attn.out_proj.weight': tensor([[-0.0048, -0.0083,  0.0123,  ..., -0.0025, -0.0113, -0.0048],\n",
       "         [ 0.0036,  0.0153, -0.0032,  ...,  0.0077,  0.0074,  0.0025],\n",
       "         [ 0.0038, -0.0023, -0.0144,  ...,  0.0041,  0.0142, -0.0133],\n",
       "         ...,\n",
       "         [ 0.0002,  0.0050,  0.0059,  ..., -0.0105, -0.0093, -0.0117],\n",
       "         [-0.0106, -0.0004,  0.0153,  ...,  0.0111,  0.0113,  0.0011],\n",
       "         [ 0.0102,  0.0101,  0.0071,  ..., -0.0087, -0.0061,  0.0079]]),\n",
       " 'transformer.resblocks.5.attn.out_proj.bias': tensor([ 2.4651e-04, -2.3750e-03, -5.6595e-04, -3.1697e-04, -5.3946e-04,\n",
       "         -1.8096e-03,  4.0529e-04,  2.5911e-03, -2.0566e-03,  7.9013e-04,\n",
       "         -3.1772e-03, -1.5002e-03, -9.5942e-04, -9.8812e-04, -2.8524e-03,\n",
       "          9.0309e-04, -2.4133e-04,  1.2364e-03,  3.8457e-04, -2.0816e-04,\n",
       "         -8.2343e-04, -5.9293e-04,  4.3962e-04, -1.4312e-03, -1.0292e-03,\n",
       "          2.5007e-04, -1.1599e-03, -1.0204e-03, -4.7630e-04, -8.4493e-04,\n",
       "          8.9666e-04,  2.8398e-04,  1.6901e-03, -2.1723e-03, -3.8391e-04,\n",
       "          8.7182e-04,  3.0931e-04,  2.3319e-04, -2.3188e-04, -1.6780e-03,\n",
       "          1.1268e-03,  1.3317e-03,  1.7519e-03,  5.9086e-04, -1.7445e-03,\n",
       "          1.1746e-03, -1.3770e-03, -3.7659e-04, -9.9192e-04,  1.4303e-03,\n",
       "         -3.9001e-04, -1.3223e-03, -8.1430e-04,  1.2428e-03,  2.6928e-03,\n",
       "          6.1791e-04, -3.4893e-04, -9.3965e-04,  4.3712e-04, -5.9840e-04,\n",
       "         -1.5983e-03,  2.0665e-04, -2.9484e-04,  5.2301e-04, -3.6183e-04,\n",
       "         -2.8944e-04,  7.4178e-04,  1.6628e-03, -1.4775e-04,  1.1770e-03,\n",
       "         -6.9389e-04,  1.7061e-04, -1.3068e-03, -9.0906e-04, -7.5223e-04,\n",
       "          1.6424e-03, -2.0745e-03, -8.1652e-04, -1.6874e-03,  1.8055e-03,\n",
       "          5.1341e-04, -1.5432e-03, -3.7307e-04,  1.9306e-03,  1.7199e-03,\n",
       "         -7.6875e-05, -1.2027e-03,  6.1943e-04,  3.4578e-04, -1.1300e-03,\n",
       "          7.0467e-05, -8.7743e-04,  7.4229e-04, -6.6882e-04, -1.8819e-03,\n",
       "         -2.3405e-03,  6.2954e-04, -9.7345e-04, -7.2062e-04,  3.7674e-04,\n",
       "         -1.6602e-05, -1.2380e-03,  3.6161e-04, -1.2744e-03, -1.3781e-03,\n",
       "         -1.3296e-03, -1.7799e-03, -1.9234e-04,  7.1272e-04, -4.0261e-06,\n",
       "          1.8176e-03,  7.5297e-04,  1.0864e-03,  1.7538e-03, -1.0110e-03,\n",
       "         -2.6629e-03,  1.3793e-03,  1.3355e-03, -1.4327e-03, -1.2510e-03,\n",
       "          7.2530e-04, -5.2262e-04,  1.0301e-03, -9.0193e-04, -5.6368e-04,\n",
       "          3.8877e-04, -1.1761e-03, -1.9807e-03, -6.4689e-04,  6.2611e-04,\n",
       "          6.4663e-04,  1.7245e-03,  4.2464e-04, -1.3477e-03, -6.5665e-04,\n",
       "          5.4853e-05, -6.4736e-04,  1.1954e-03, -5.3474e-04,  8.5093e-04,\n",
       "          2.0021e-04,  1.7871e-03,  6.6146e-04,  2.6644e-03,  1.5571e-04,\n",
       "          1.2781e-03, -1.0166e-03,  1.4038e-04,  1.3553e-03,  2.1822e-04,\n",
       "          1.3223e-03,  3.2826e-03,  3.3183e-03,  8.9792e-04,  1.3039e-04,\n",
       "          8.6365e-04,  1.2918e-03, -6.7295e-05,  2.1041e-04, -4.7352e-04,\n",
       "         -1.3837e-03, -3.7449e-04,  1.8123e-03, -6.3464e-05, -1.3676e-03,\n",
       "          7.5011e-04,  1.2794e-03, -6.9025e-04,  1.7024e-04, -1.3375e-03,\n",
       "          2.6954e-04,  1.0639e-04, -9.3571e-04,  5.5649e-04,  1.9333e-03,\n",
       "          6.0563e-04, -1.3607e-03,  3.5695e-04,  5.1089e-04, -8.6592e-04,\n",
       "         -1.7493e-03,  5.5781e-04,  8.8041e-05, -2.0245e-03,  3.7855e-03,\n",
       "         -9.6210e-05,  1.0473e-03, -1.2196e-04,  9.7499e-04, -2.0703e-03,\n",
       "          6.3148e-04,  8.0724e-04,  1.0864e-04,  2.4925e-03, -4.3071e-04,\n",
       "         -3.7173e-04,  1.4170e-03,  1.4335e-03,  2.2784e-03,  2.8395e-03,\n",
       "          2.1914e-04, -1.8709e-03,  1.2232e-03,  2.6019e-03,  3.2063e-05,\n",
       "         -1.8706e-04, -8.9179e-04, -1.6814e-04,  8.1102e-04, -2.1620e-05,\n",
       "          1.6168e-03,  1.7812e-04, -5.2722e-04,  1.6215e-03, -1.4151e-03,\n",
       "         -6.3882e-04,  3.9313e-04, -1.8054e-03, -9.7490e-04,  6.9305e-05,\n",
       "         -1.2773e-03, -5.7119e-04,  1.3029e-03,  5.8674e-04,  1.2678e-03,\n",
       "          2.6280e-04, -1.0303e-04, -8.2360e-04,  8.4827e-04,  1.2751e-03,\n",
       "         -1.0737e-03,  1.1674e-03,  9.0670e-04, -6.3208e-04,  1.2390e-03,\n",
       "         -3.1535e-04,  2.1728e-03, -2.3470e-03,  1.3397e-04,  8.1596e-04,\n",
       "         -1.1525e-03,  1.6678e-03,  1.1636e-03,  1.5389e-03, -1.5125e-03,\n",
       "         -1.7473e-04,  2.6575e-04, -1.8458e-04,  1.3876e-05, -1.1471e-03,\n",
       "          6.8375e-04, -1.6715e-03,  1.1859e-04, -5.6556e-04, -1.0623e-03,\n",
       "         -8.4278e-04, -2.1532e-04, -6.9865e-04, -4.1084e-04, -2.7469e-03,\n",
       "          5.5550e-04,  4.2218e-04,  1.2083e-04, -1.6871e-03, -1.3745e-03,\n",
       "          2.2848e-03,  1.1376e-03,  7.4042e-04,  5.9895e-04, -1.1722e-03,\n",
       "          7.0944e-04, -5.0399e-04, -1.0909e-03,  2.4486e-04,  5.2412e-04,\n",
       "         -1.0836e-03,  2.6408e-03, -7.2116e-04,  2.6400e-03, -5.8979e-05,\n",
       "          5.7883e-04,  2.3839e-03, -5.7722e-04, -1.1334e-03, -1.9856e-03,\n",
       "          1.2438e-03,  3.3371e-04,  6.5197e-04,  1.0563e-03,  8.0094e-05,\n",
       "         -2.4664e-04,  1.2443e-03, -8.5997e-04,  2.8914e-03,  3.5185e-04,\n",
       "          1.2943e-03, -1.0555e-03,  8.1741e-04, -1.0089e-03, -1.3800e-03,\n",
       "         -1.9559e-03,  6.0413e-04,  1.2915e-03, -2.5264e-03, -5.2050e-04,\n",
       "         -1.5875e-03,  1.2201e-04, -2.9421e-04, -1.1615e-03, -2.0298e-03,\n",
       "          6.7110e-04,  1.7155e-03, -7.5403e-04, -2.0997e-03,  9.4139e-04,\n",
       "          1.5002e-03,  2.2004e-03,  9.4524e-04,  7.5117e-04,  5.5432e-04,\n",
       "          1.4066e-03, -3.6637e-04, -8.4413e-04,  1.0061e-03, -1.7700e-03,\n",
       "          5.2916e-04,  2.1432e-03,  2.6542e-04, -2.0079e-03, -2.4511e-03,\n",
       "         -2.4895e-04,  1.9058e-03,  6.7480e-04, -4.3627e-04,  2.0936e-04,\n",
       "          1.5874e-04, -7.1614e-04,  3.0722e-04,  3.7926e-04,  1.6402e-03,\n",
       "         -2.3767e-03,  1.9838e-03,  9.5849e-04, -6.3784e-04, -1.6513e-03,\n",
       "         -2.9168e-04, -1.6092e-03,  5.0454e-04, -1.0016e-04,  1.1059e-03,\n",
       "         -9.6288e-04, -2.7723e-03, -1.1225e-03, -6.1160e-04,  1.2553e-05,\n",
       "          1.8852e-05,  1.9386e-03,  2.8104e-03,  4.3820e-04, -2.1538e-03,\n",
       "          5.5348e-04, -4.3036e-05,  1.7450e-03,  2.1917e-03,  7.5802e-04,\n",
       "          2.1399e-03, -1.7301e-04, -3.0129e-03,  1.9685e-03,  1.8417e-04,\n",
       "         -9.0916e-04,  1.2800e-03,  9.7013e-04, -8.9258e-04, -2.6781e-04,\n",
       "          1.8849e-03, -8.7174e-04,  1.0099e-03, -1.3076e-03, -1.4136e-03,\n",
       "          9.2604e-04,  9.8995e-04, -1.9136e-03, -1.0068e-03, -1.2434e-03,\n",
       "         -7.4420e-04,  1.6711e-03, -1.8896e-04,  1.9978e-03,  1.0280e-03,\n",
       "         -5.5758e-04,  3.2687e-04, -8.1493e-04,  1.0564e-03,  2.5707e-03,\n",
       "          1.3770e-03, -3.1227e-04,  1.7365e-03, -1.4470e-04,  4.3056e-04,\n",
       "         -6.5382e-04, -1.0170e-03, -1.4909e-03, -3.6675e-04,  6.6724e-06,\n",
       "          1.3480e-03,  9.4683e-04,  9.7055e-04,  1.1786e-03,  1.5207e-03,\n",
       "          9.2897e-06, -1.9248e-03,  2.0043e-04,  1.8537e-03,  1.6011e-04,\n",
       "         -1.3712e-03,  1.2919e-03,  1.3901e-03,  2.1510e-04,  1.5648e-03,\n",
       "         -9.4865e-04,  2.0148e-04,  5.0877e-04, -7.3688e-04, -1.4563e-03,\n",
       "         -1.1389e-04, -1.1253e-03, -6.0859e-04,  2.0010e-03, -1.5391e-04,\n",
       "         -1.6675e-03,  6.2360e-04,  8.1935e-04, -8.1064e-04, -2.4830e-03,\n",
       "          2.0546e-04,  8.7943e-04, -2.2289e-03,  1.5005e-03, -4.8104e-04,\n",
       "         -6.8149e-04,  7.3658e-05,  1.4483e-04, -5.4926e-04, -1.6325e-03,\n",
       "         -2.0638e-03, -8.9028e-04,  5.2887e-04, -1.9482e-04,  7.9934e-04,\n",
       "          1.0198e-04,  8.7143e-04, -1.3476e-03,  2.8699e-03,  1.1753e-03,\n",
       "         -1.1059e-03,  1.2352e-03,  2.8318e-03,  1.6883e-03,  1.1725e-04,\n",
       "          6.0854e-04, -1.0516e-03,  1.0083e-03,  2.3357e-03,  1.0400e-03,\n",
       "          1.0910e-04,  4.3492e-04,  1.3312e-03,  3.4502e-04, -2.2847e-03,\n",
       "         -1.2615e-03,  7.2298e-05, -2.2157e-03, -6.4723e-04, -1.0571e-03,\n",
       "         -4.3726e-05, -2.0390e-03, -9.1978e-04, -5.8687e-04, -6.1852e-04,\n",
       "         -1.5884e-03, -1.0945e-03,  5.1015e-04,  4.5515e-04, -1.3140e-03,\n",
       "          6.1119e-05, -1.3072e-03, -1.1973e-03,  1.7632e-03,  7.8301e-04,\n",
       "          6.0709e-04, -1.7508e-04,  2.4433e-04, -1.8912e-03, -9.4491e-04,\n",
       "          2.1554e-04,  1.6215e-03,  3.1428e-04, -3.7835e-04, -2.5270e-03,\n",
       "         -5.1109e-04, -1.3628e-05, -6.8120e-04,  1.2651e-03, -1.6933e-03,\n",
       "          5.7672e-04, -1.1035e-03, -9.9962e-04,  1.4817e-03, -3.1600e-04,\n",
       "         -1.4488e-03,  4.3532e-06]),\n",
       " 'transformer.resblocks.5.ln_1.weight': tensor([0.9992, 0.9967, 1.0000, 0.9999, 1.0015, 0.9970, 1.0013, 0.9978, 0.9992,\n",
       "         1.0000, 0.9981, 1.0033, 0.9990, 1.0005, 0.9980, 1.0002, 0.9980, 0.9955,\n",
       "         1.0005, 1.0014, 0.9976, 0.9985, 0.9988, 0.9998, 1.0008, 1.0007, 0.9954,\n",
       "         0.9975, 1.0023, 0.9998, 0.9970, 1.0004, 0.9977, 0.9994, 0.9969, 0.9988,\n",
       "         1.0002, 1.0007, 1.0034, 1.0008, 1.0020, 1.0021, 1.0009, 0.9990, 0.9996,\n",
       "         1.0000, 0.9980, 0.9963, 0.9996, 0.9975, 1.0018, 0.9989, 0.9976, 1.0012,\n",
       "         0.9999, 1.0000, 0.9993, 1.0019, 0.9993, 0.9996, 0.9978, 0.9955, 1.0006,\n",
       "         0.9967, 1.0028, 0.9998, 1.0016, 1.0026, 0.9989, 1.0034, 1.0020, 1.0009,\n",
       "         0.9971, 1.0000, 0.9984, 0.9988, 0.9969, 0.9994, 1.0020, 0.9990, 0.9988,\n",
       "         1.0033, 0.9975, 1.0011, 1.0008, 1.0005, 1.0026, 0.9990, 0.9977, 0.9996,\n",
       "         0.9984, 0.9973, 0.9974, 1.0004, 1.0023, 0.9969, 0.9987, 1.0016, 0.9986,\n",
       "         1.0007, 1.0004, 0.9994, 1.0017, 1.0024, 0.9992, 0.9984, 0.9999, 0.9995,\n",
       "         0.9975, 1.0008, 1.0004, 0.9976, 1.0013, 0.9960, 0.9982, 0.9990, 1.0006,\n",
       "         1.0013, 1.0047, 0.9983, 0.9974, 0.9994, 0.9998, 1.0007, 0.9981, 0.9999,\n",
       "         1.0002, 1.0010, 0.9983, 1.0003, 1.0004, 1.0008, 1.0008, 1.0016, 1.0024,\n",
       "         1.0027, 0.9999, 0.9982, 1.0007, 0.9977, 0.9994, 1.0010, 1.0029, 0.9990,\n",
       "         0.9981, 1.0024, 0.9993, 1.0004, 0.9967, 0.9997, 0.9975, 0.9983, 0.9987,\n",
       "         1.0005, 1.0007, 1.0023, 1.0003, 1.0009, 0.9977, 1.0014, 1.0008, 0.9985,\n",
       "         0.9976, 0.9999, 0.9981, 1.0018, 1.0021, 0.9983, 0.9980, 1.0003, 0.9965,\n",
       "         0.9976, 0.9994, 0.9995, 0.9996, 0.9986, 1.0028, 0.9978, 1.0014, 0.9986,\n",
       "         1.0025, 0.9972, 1.0027, 0.9983, 0.9966, 0.9974, 0.9992, 0.9998, 1.0024,\n",
       "         0.9997, 0.9985, 1.0025, 0.9992, 0.9984, 0.9996, 0.9978, 0.9973, 0.9988,\n",
       "         0.9978, 0.9987, 1.0001, 1.0022, 1.0028, 0.9974, 0.9989, 0.9984, 1.0018,\n",
       "         1.0039, 0.9980, 0.9980, 0.9976, 1.0011, 1.0023, 0.9982, 1.0006, 0.9996,\n",
       "         0.9972, 0.9973, 0.9976, 0.9972, 1.0012, 0.9996, 0.9971, 0.9995, 0.9990,\n",
       "         1.0049, 0.9990, 1.0001, 1.0008, 0.9989, 0.9983, 0.9995, 0.9991, 1.0006,\n",
       "         1.0013, 1.0029, 0.9986, 0.9998, 1.0030, 1.0002, 1.0008, 0.9973, 0.9961,\n",
       "         1.0004, 0.9988, 1.0012, 1.0023, 0.9990, 0.9982, 0.9982, 0.9988, 0.9992,\n",
       "         1.0025, 1.0013, 1.0002, 0.9982, 0.9991, 0.9986, 0.9994, 1.0007, 0.9981,\n",
       "         1.0026, 0.9995, 1.0027, 0.9992, 0.9970, 0.9999, 0.9999, 0.9960, 1.0013,\n",
       "         0.9984, 0.9979, 1.0000, 0.9977, 0.9970, 0.9999, 0.9983, 0.9974, 1.0007,\n",
       "         0.9999, 0.9967, 0.9991, 0.9966, 1.0005, 0.9970, 1.0007, 1.0007, 1.0004,\n",
       "         1.0013, 0.9955, 0.9997, 1.0010, 0.9997, 0.9962, 1.0017, 1.0007, 0.9989,\n",
       "         1.0006, 1.0005, 1.0020, 0.9992, 0.9981, 1.0014, 1.0004, 0.9988, 1.0015,\n",
       "         0.9996, 0.9968, 0.9968, 0.9975, 0.9979, 1.0002, 1.0010, 1.0000, 0.9988,\n",
       "         0.9985, 0.9985, 0.9989, 1.0009, 0.9968, 0.9998, 0.9976, 0.9965, 0.9985,\n",
       "         1.0005, 0.9985, 1.0023, 1.0051, 0.9994, 0.9990, 0.9993, 0.9975, 1.0005,\n",
       "         1.0008, 0.9996, 0.9994, 1.0020, 0.9990, 0.9984, 0.9975, 1.0009, 1.0010,\n",
       "         0.9978, 0.9968, 1.0023, 1.0002, 0.9987, 0.9989, 0.9998, 1.0016, 1.0016,\n",
       "         0.9976, 1.0002, 0.9970, 0.9986, 0.9996, 1.0010, 0.9989, 1.0010, 0.9995,\n",
       "         1.0026, 0.9989, 0.9982, 0.9973, 0.9984, 1.0008, 0.9996, 0.9989, 0.9956,\n",
       "         0.9972, 0.9982, 1.0005, 0.9969, 0.9978, 0.9995, 1.0017, 1.0001, 1.0003,\n",
       "         0.9979, 0.9992, 0.9961, 1.0017, 0.9992, 1.0001, 0.9980, 0.9994, 0.9976,\n",
       "         1.0008, 0.9969, 0.9986, 1.0003, 1.0002, 1.0010, 0.9968, 0.9971, 0.9977,\n",
       "         1.0009, 0.9989, 0.9988, 1.0013, 1.0029, 0.9968, 1.0009, 0.9982, 1.0003,\n",
       "         1.0012, 0.9987, 0.9963, 1.0013, 0.9991, 0.9979, 1.0007, 1.0019, 1.0009,\n",
       "         1.0006, 0.9994, 0.9988, 0.9944, 1.0003, 0.9990, 0.9983, 1.0013, 0.9974,\n",
       "         0.9994, 0.9996, 0.9958, 0.9991, 1.0028, 1.0023, 1.0001, 1.0012, 0.9972,\n",
       "         1.0027, 0.9981, 1.0012, 0.9965, 0.9989, 0.9991, 0.9994, 1.0007, 1.0000,\n",
       "         1.0006, 1.0011, 0.9983, 1.0018, 0.9984, 0.9987, 0.9967, 0.9994, 0.9990,\n",
       "         1.0009, 0.9996, 0.9998, 1.0018, 0.9973, 0.9972, 0.9994, 0.9960, 0.9998,\n",
       "         0.9976, 0.9980, 0.9968, 1.0027, 1.0014, 0.9983, 0.9974, 0.9993, 0.9976,\n",
       "         1.0019, 0.9972, 0.9969, 1.0000, 0.9996, 0.9990, 0.9980, 0.9970, 1.0004,\n",
       "         0.9984, 1.0031, 1.0017, 0.9995, 1.0035, 0.9985, 1.0006, 0.9979, 0.9996,\n",
       "         0.9970, 0.9996, 0.9995, 0.9976, 0.9961, 0.9986, 0.9978, 0.9978, 1.0004,\n",
       "         1.0018, 1.0016, 0.9980, 0.9998, 1.0013, 1.0008, 0.9983, 1.0012, 1.0028,\n",
       "         1.0022, 0.9984, 0.9985, 1.0010, 0.9984, 1.0046, 0.9994, 0.9980]),\n",
       " 'transformer.resblocks.5.ln_1.bias': tensor([-7.4996e-04,  1.7314e-03,  1.7885e-03, -2.8553e-04,  2.4047e-03,\n",
       "         -2.3522e-04,  2.0961e-03,  6.7383e-04,  3.8108e-05,  1.0449e-03,\n",
       "          3.0284e-03, -1.1532e-03, -1.4138e-03,  2.1201e-03,  3.8107e-04,\n",
       "         -2.3671e-03, -7.8928e-04, -1.9340e-03,  2.9564e-04,  2.7611e-04,\n",
       "          1.8795e-03,  3.2937e-04,  1.0796e-03,  1.0800e-03,  1.0556e-03,\n",
       "          2.4023e-04,  1.2916e-03,  2.2174e-03,  7.5555e-04, -1.0555e-03,\n",
       "         -1.3453e-03,  3.8066e-03, -1.6335e-03, -4.0885e-04, -1.5286e-03,\n",
       "          2.7670e-03, -2.5141e-03, -4.7331e-03, -1.1538e-03, -3.4697e-03,\n",
       "          2.4305e-04, -5.4737e-04,  1.6712e-03,  4.3807e-04,  5.2914e-04,\n",
       "         -1.6545e-03,  2.3256e-03,  5.0881e-03,  1.7956e-03, -1.4192e-03,\n",
       "         -1.9066e-03,  5.0051e-04,  6.0221e-04, -1.4223e-03, -2.8022e-03,\n",
       "          1.1614e-03,  2.1212e-03, -7.2069e-04, -9.9218e-04, -1.6469e-03,\n",
       "         -3.8471e-04, -2.8915e-04,  1.9594e-03,  5.1341e-04, -8.5428e-04,\n",
       "         -7.0082e-04,  1.6002e-03, -1.0368e-03,  2.1414e-04,  2.5417e-05,\n",
       "         -5.0661e-04, -4.6287e-04, -1.7813e-04,  1.3706e-03, -2.8944e-03,\n",
       "         -1.0611e-03, -2.9608e-04,  9.0814e-04, -1.0501e-03, -3.3861e-04,\n",
       "         -5.5392e-04,  3.4821e-03,  4.7365e-04, -1.8809e-03,  3.7219e-04,\n",
       "         -9.2463e-04,  1.2888e-03, -1.8102e-03, -2.7659e-03,  7.5606e-04,\n",
       "          1.4811e-03,  2.3439e-03,  9.6875e-06,  3.6125e-03,  2.2068e-04,\n",
       "          4.2275e-03,  1.2306e-03,  4.8257e-04,  2.6877e-03,  1.3747e-04,\n",
       "          4.7021e-04,  8.6336e-04,  2.8222e-04,  1.2787e-03,  1.1211e-03,\n",
       "         -1.9670e-04,  1.5191e-03,  2.9114e-03, -1.2101e-03, -2.1541e-04,\n",
       "         -6.3281e-04,  2.6304e-03,  1.2236e-03, -1.2879e-03, -1.4184e-03,\n",
       "          3.7953e-03,  5.0479e-04,  2.5668e-03,  4.8150e-04,  1.9305e-03,\n",
       "         -3.6165e-03,  1.2008e-03, -2.1114e-03,  4.1727e-04, -9.8285e-04,\n",
       "          1.3568e-03,  2.9762e-03,  1.4411e-03, -3.3852e-04, -2.0923e-03,\n",
       "         -1.1407e-03, -7.0109e-04, -2.7953e-03,  1.8804e-03, -4.1725e-03,\n",
       "          2.8689e-03,  1.7555e-03, -1.4764e-03,  8.7752e-04, -5.3935e-04,\n",
       "         -2.1638e-03,  4.1951e-05, -6.2177e-04, -2.1070e-03, -1.8386e-03,\n",
       "         -1.8089e-03,  3.3458e-03, -6.8489e-04,  4.0434e-03,  4.6076e-03,\n",
       "          2.6196e-03, -1.9363e-04, -2.1746e-04,  2.1214e-03,  1.7474e-05,\n",
       "         -9.3870e-04,  6.8819e-04, -3.1626e-03, -9.5045e-04,  1.9806e-03,\n",
       "          3.9780e-04,  1.8105e-03,  4.6615e-05, -2.3674e-03,  2.3101e-03,\n",
       "         -4.6395e-04,  5.6089e-04,  2.0444e-03,  1.3868e-03, -1.0938e-04,\n",
       "         -2.1917e-03, -1.4762e-03,  1.4925e-03, -1.9697e-03,  8.8289e-04,\n",
       "         -1.0674e-04, -8.5005e-04, -2.9103e-03, -2.6854e-03,  5.3662e-04,\n",
       "         -3.7228e-03, -2.2007e-04,  8.1948e-04, -1.1127e-03,  2.5739e-04,\n",
       "         -3.5823e-04, -8.3768e-04,  6.4477e-04, -7.8584e-04, -1.4137e-03,\n",
       "          2.7013e-04, -8.8131e-04, -2.4212e-03,  1.6496e-04,  1.7730e-03,\n",
       "         -1.6493e-03, -2.9197e-04,  5.6104e-04, -3.6875e-04,  1.0348e-03,\n",
       "         -1.7620e-03, -1.0796e-03,  3.1995e-04,  1.0008e-03, -5.9892e-04,\n",
       "         -1.1309e-03,  5.2170e-04, -2.0909e-03, -1.1342e-03,  1.1140e-03,\n",
       "         -1.7626e-03,  8.6023e-04, -4.1007e-04, -2.7914e-03, -5.7390e-04,\n",
       "         -6.5247e-04,  2.1511e-03,  2.1871e-03, -5.7655e-04, -2.1496e-03,\n",
       "          9.8537e-04, -3.4386e-04,  1.0605e-03,  1.5454e-04, -1.1227e-03,\n",
       "         -1.8732e-03,  1.1586e-03, -1.7041e-03,  1.0901e-03,  1.0314e-03,\n",
       "          2.0983e-03,  3.0946e-04, -1.6624e-03,  2.3821e-03,  2.2314e-03,\n",
       "          9.6095e-04,  2.2671e-03,  1.1266e-03, -2.6345e-03, -2.1044e-03,\n",
       "          1.6542e-03, -2.1809e-03,  7.1706e-04, -1.8146e-03, -1.2018e-03,\n",
       "          2.2774e-03,  3.2035e-04,  5.0942e-04, -1.8847e-03, -7.8383e-04,\n",
       "         -1.1233e-04, -5.8340e-05,  4.6077e-04, -6.8750e-05, -9.6327e-04,\n",
       "          2.4035e-03,  3.2336e-04,  1.1975e-03, -1.3576e-03,  6.0902e-04,\n",
       "         -8.3157e-04, -6.6531e-04,  1.3972e-03,  2.2950e-03, -1.6776e-03,\n",
       "         -1.2260e-03, -3.5797e-03, -1.5388e-05, -7.5375e-04,  2.9953e-03,\n",
       "          2.6404e-03,  7.1562e-04, -1.8023e-04, -7.4289e-04, -1.5396e-03,\n",
       "          1.7347e-03, -1.9905e-03,  4.8463e-03, -1.2171e-03, -1.2064e-03,\n",
       "         -2.2076e-03,  9.4852e-04, -8.3927e-06, -3.0333e-03,  1.4372e-03,\n",
       "          1.2297e-03,  1.5434e-03,  7.8152e-04, -1.0717e-03, -1.7045e-04,\n",
       "          1.6448e-03,  4.7451e-04, -8.0063e-04, -2.0844e-03,  1.7383e-04,\n",
       "         -9.2402e-04, -1.1344e-03, -2.3531e-05, -7.5991e-04, -2.3915e-03,\n",
       "          2.5212e-04,  2.7103e-04, -9.6547e-04, -3.0342e-03, -3.9626e-04,\n",
       "         -2.7010e-03, -1.7943e-03,  2.4736e-03, -2.5345e-04,  7.5647e-04,\n",
       "          2.2499e-03, -9.4067e-06, -1.6288e-03,  1.0694e-03, -2.0464e-03,\n",
       "         -1.5915e-03, -1.9719e-04, -1.9226e-03, -8.8333e-04,  8.0225e-04,\n",
       "         -3.1661e-04, -1.6601e-03,  4.3098e-04, -3.5952e-03,  2.6522e-03,\n",
       "          4.4286e-04, -8.3733e-04, -8.4094e-04, -1.3219e-03, -1.3502e-03,\n",
       "          1.4243e-03, -1.9234e-03, -1.8125e-03,  4.1477e-04,  2.0095e-03,\n",
       "         -1.5721e-03, -1.7953e-03, -4.6096e-04, -7.1178e-04,  6.2932e-05,\n",
       "          6.9038e-04, -4.2445e-04, -9.5111e-04,  7.2314e-04, -1.3120e-03,\n",
       "          2.5488e-03,  5.3639e-04, -1.2855e-03,  1.2635e-04, -1.4782e-03,\n",
       "         -1.3140e-03, -1.0205e-03, -9.8290e-04, -1.2114e-04, -1.9110e-03,\n",
       "          9.8590e-04,  1.0289e-03, -3.9495e-04, -2.0980e-03,  3.6720e-03,\n",
       "         -8.6788e-04, -2.3849e-03, -7.3241e-04,  1.8703e-03, -5.6212e-04,\n",
       "          1.0544e-04,  1.6224e-03,  1.7215e-03, -1.2952e-04,  2.5462e-04,\n",
       "          9.6360e-05, -2.9675e-04, -1.2887e-03,  5.7862e-03, -1.1459e-03,\n",
       "         -5.9369e-04,  7.6580e-04,  8.9095e-04,  2.6459e-03,  3.7534e-03,\n",
       "         -1.5066e-03, -1.0979e-03, -6.6517e-04,  1.0593e-03,  1.7786e-03,\n",
       "         -1.4689e-03, -1.3189e-03,  1.1463e-03, -1.9162e-03,  3.1887e-04,\n",
       "         -2.8760e-03, -1.0306e-03,  1.0793e-03,  5.6343e-04, -2.6618e-03,\n",
       "         -1.7350e-03, -1.2871e-03, -2.7211e-03, -2.5373e-03, -6.2192e-05,\n",
       "          7.1735e-04, -8.8985e-04, -2.5972e-03,  9.9242e-04,  4.8208e-04,\n",
       "         -1.1002e-03, -1.6900e-03, -8.8933e-04, -1.5453e-03, -2.0839e-03,\n",
       "         -1.5131e-03, -2.3665e-03, -1.4082e-03, -1.0966e-03, -9.4966e-04,\n",
       "         -3.2464e-03,  5.5196e-04, -1.7452e-03, -2.9273e-04, -3.5852e-03,\n",
       "          1.0252e-04,  2.4863e-03,  7.8785e-04, -5.0491e-04, -6.7504e-04,\n",
       "         -6.5868e-04, -4.0840e-03,  2.5301e-04,  1.5360e-03, -6.2759e-04,\n",
       "         -1.5111e-03,  2.3682e-03,  3.7259e-03, -2.5149e-03,  2.8002e-03,\n",
       "         -2.7049e-05, -2.8518e-03, -1.4769e-03,  1.2823e-03,  3.6810e-04,\n",
       "         -9.2108e-04,  5.3845e-04, -1.6424e-03, -1.6198e-03, -7.9428e-04,\n",
       "          1.6216e-03, -3.4185e-03,  1.2946e-03, -2.5752e-04, -6.7780e-04,\n",
       "         -1.9305e-04, -1.5266e-03, -2.4757e-03,  1.7096e-03, -1.1203e-03,\n",
       "         -5.5650e-04, -1.6490e-03, -3.9308e-03, -1.5007e-03, -2.2808e-03,\n",
       "         -1.9720e-03,  7.0036e-04, -5.5674e-04,  2.7335e-04, -1.2525e-03,\n",
       "         -7.2644e-05, -4.4264e-04, -1.2921e-03,  3.4462e-04,  5.5111e-04,\n",
       "          5.9861e-04, -9.4597e-04,  3.1783e-03, -1.4757e-04,  2.1335e-03,\n",
       "         -1.2762e-03,  4.1311e-04,  1.9641e-03,  1.4431e-03, -2.8552e-04,\n",
       "         -6.8858e-04, -1.2946e-03, -1.5547e-03, -1.1784e-03,  6.1284e-04,\n",
       "          4.9074e-04, -3.2329e-04,  2.9229e-04,  1.3394e-03, -2.5065e-03,\n",
       "         -3.5768e-04,  2.7922e-03,  1.0880e-04,  7.3546e-04,  2.4607e-03,\n",
       "         -5.5678e-04,  2.3405e-03,  7.1886e-04, -1.3619e-03, -2.1329e-03,\n",
       "         -1.5621e-03,  1.2278e-03, -4.8667e-04,  2.9155e-03, -7.8002e-04,\n",
       "         -1.1657e-03,  3.9511e-03,  3.8763e-03, -1.4627e-03,  9.1289e-04,\n",
       "         -2.7637e-04,  1.3010e-03]),\n",
       " 'transformer.resblocks.5.mlp.c_fc.weight': tensor([[ 0.0336,  0.0161,  0.0345,  ...,  0.0252,  0.0392,  0.0549],\n",
       "         [ 0.0070,  0.0008, -0.0700,  ..., -0.0138,  0.0379,  0.0089],\n",
       "         [ 0.0812,  0.0042, -0.0431,  ...,  0.0022, -0.0076, -0.0410],\n",
       "         ...,\n",
       "         [-0.0270,  0.0256,  0.0233,  ..., -0.0248,  0.0042, -0.0119],\n",
       "         [ 0.0151,  0.0544, -0.0220,  ...,  0.0146, -0.0113,  0.0724],\n",
       "         [ 0.0412, -0.0234, -0.0588,  ...,  0.0412, -0.0055,  0.0221]]),\n",
       " 'transformer.resblocks.5.mlp.c_fc.bias': tensor([ 4.0883e-02, -9.1978e-05,  2.5642e-02,  ...,  1.4060e-02,\n",
       "          6.9389e-03, -1.3042e-02]),\n",
       " 'transformer.resblocks.5.mlp.c_proj.weight': tensor([[ 0.0194, -0.0029,  0.0048,  ..., -0.0087, -0.0044,  0.0078],\n",
       "         [-0.0129,  0.0094, -0.0171,  ...,  0.0082,  0.0005, -0.0093],\n",
       "         [ 0.0047, -0.0021, -0.0014,  ...,  0.0114,  0.0169,  0.0147],\n",
       "         ...,\n",
       "         [ 0.0172,  0.0096, -0.0260,  ..., -0.0019,  0.0114,  0.0114],\n",
       "         [ 0.0049, -0.0044, -0.0003,  ..., -0.0051, -0.0020, -0.0068],\n",
       "         [ 0.0016, -0.0034, -0.0149,  ...,  0.0093, -0.0028,  0.0049]]),\n",
       " 'transformer.resblocks.5.mlp.c_proj.bias': tensor([ 3.8090e-03, -4.1596e-03, -2.1126e-03, -1.9086e-02,  1.4347e-02,\n",
       "          1.8959e-02,  1.9611e-02, -1.0601e-03,  1.8811e-02, -6.1465e-03,\n",
       "         -1.9813e-02, -9.1634e-03, -2.1539e-02,  1.6845e-02, -1.7739e-02,\n",
       "         -1.0974e-03,  6.8480e-03,  4.2489e-04,  1.4333e-02, -2.2813e-03,\n",
       "         -1.0348e-02, -2.1638e-02, -3.6405e-03, -1.2041e-02,  6.0415e-03,\n",
       "          1.5879e-03,  1.2608e-02, -1.1951e-02, -7.8406e-03, -1.6992e-02,\n",
       "          1.9214e-02, -2.0333e-02, -5.6055e-03, -4.4738e-03, -1.1284e-03,\n",
       "         -1.4783e-02,  1.1312e-02, -1.2825e-03,  1.8161e-02,  1.3580e-02,\n",
       "         -1.4278e-02,  1.2920e-02, -5.5141e-03, -1.0879e-02,  5.2243e-04,\n",
       "         -6.9158e-03,  1.8913e-02,  5.6466e-03,  1.4029e-02, -4.2405e-03,\n",
       "          7.1747e-03, -7.0743e-03,  5.0692e-03,  1.8230e-02, -1.5899e-02,\n",
       "         -1.1411e-02, -1.6672e-03, -1.3367e-02,  2.1590e-02,  1.8930e-02,\n",
       "         -1.4586e-02,  2.0584e-02,  4.4462e-03,  1.1363e-02, -1.5325e-02,\n",
       "         -1.2942e-02, -6.7491e-03,  1.0581e-02, -1.9033e-03,  3.0806e-03,\n",
       "         -1.9375e-02,  2.1014e-02, -5.7601e-03, -1.3609e-03, -1.3348e-02,\n",
       "          9.1070e-03,  8.2516e-03, -1.7258e-02, -2.8320e-04,  1.4252e-02,\n",
       "         -7.9824e-03, -2.2561e-02,  5.7997e-03,  2.4409e-03, -6.1711e-03,\n",
       "          1.9373e-02,  9.1259e-03,  2.0207e-02,  1.8310e-02, -1.8033e-02,\n",
       "         -1.3589e-02, -1.9370e-02,  8.4455e-03,  1.9145e-02, -1.8218e-02,\n",
       "          6.4616e-03,  8.3324e-04, -5.4322e-05,  9.0735e-03, -1.2850e-03,\n",
       "         -1.5253e-02,  9.9492e-03,  8.7724e-03, -1.1035e-05, -1.1699e-02,\n",
       "         -6.0629e-03, -1.6242e-02, -2.1311e-02,  6.7254e-03, -1.9538e-02,\n",
       "         -1.4780e-02, -1.7555e-02,  1.8692e-02,  1.3827e-02,  9.3521e-04,\n",
       "          7.4023e-03,  8.8523e-03,  1.4557e-02,  1.8890e-02, -5.4012e-03,\n",
       "         -1.3738e-02, -1.3236e-02, -1.2766e-02, -2.7142e-03, -1.0606e-02,\n",
       "         -1.1979e-03,  2.0043e-02,  1.9527e-02,  1.0299e-02,  1.4480e-02,\n",
       "          1.7364e-02,  3.0036e-03,  3.7225e-03, -2.3799e-02, -1.6144e-02,\n",
       "         -2.1028e-02,  3.8878e-03,  6.4287e-03, -1.4469e-02, -1.8779e-02,\n",
       "          1.8176e-02,  6.4430e-03, -6.7397e-03,  1.8341e-02, -1.8699e-02,\n",
       "         -1.9779e-02,  1.1175e-02, -7.4339e-03,  3.5250e-03, -1.0117e-02,\n",
       "         -1.5436e-02, -1.7274e-02, -7.1374e-03,  2.3563e-03, -3.9995e-03,\n",
       "          1.0279e-02, -1.3981e-03,  2.0949e-02, -2.0001e-02,  1.7554e-02,\n",
       "          3.5580e-03, -3.5980e-03, -5.8791e-04, -1.2671e-02,  1.4552e-02,\n",
       "         -1.2418e-02,  1.0073e-02,  6.7185e-03, -8.2379e-03, -7.9689e-03,\n",
       "          1.9410e-02, -1.0295e-02, -2.4536e-03, -2.0021e-02, -2.6817e-03,\n",
       "          2.1496e-03, -4.6988e-05, -1.8607e-02,  1.9773e-02,  1.2189e-02,\n",
       "          1.7689e-03,  7.6384e-03, -2.2975e-03,  7.0018e-03,  7.5158e-03,\n",
       "         -1.1665e-02,  2.2654e-02, -1.5865e-02, -5.2565e-03, -7.3047e-03,\n",
       "          1.0380e-02, -9.8965e-03,  2.5838e-03, -1.1678e-02, -2.0953e-03,\n",
       "         -1.5992e-02,  2.8877e-03, -5.3310e-03,  2.1546e-02, -7.9872e-03,\n",
       "         -2.1761e-02,  1.0032e-02,  1.5858e-02,  1.4627e-02,  1.8201e-02,\n",
       "         -9.8026e-03,  8.7961e-03, -7.3303e-03,  1.1017e-02, -5.1714e-03,\n",
       "          2.2429e-02, -2.2956e-03,  7.7240e-03, -1.1715e-02, -1.6188e-02,\n",
       "          2.0449e-02, -1.3874e-02, -5.2474e-03,  1.1036e-02, -1.7534e-02,\n",
       "         -1.2994e-02,  8.4376e-05,  1.9454e-03, -1.1780e-02, -4.1247e-03,\n",
       "          2.0510e-02,  1.7848e-02,  2.0902e-02, -1.7749e-02, -1.5446e-02,\n",
       "          1.1925e-02,  1.5438e-02,  1.5633e-02, -1.0935e-02,  6.3756e-03,\n",
       "         -1.6675e-02, -6.7894e-03, -8.6980e-03,  1.2976e-02, -1.6028e-02,\n",
       "          2.0244e-03, -1.8727e-02,  9.3745e-03, -1.9121e-02,  7.0188e-03,\n",
       "         -1.7654e-02, -9.7794e-04, -1.3307e-02,  1.0886e-02, -2.0101e-02,\n",
       "          4.4043e-03, -4.9631e-04, -5.9189e-03,  1.3854e-02,  1.3318e-02,\n",
       "         -2.2312e-02, -8.9422e-03, -1.5272e-02,  2.9871e-03,  1.0729e-02,\n",
       "          1.7433e-02,  4.6101e-03,  3.8148e-04, -8.9168e-04, -9.6080e-04,\n",
       "          9.1356e-03,  1.4179e-02, -2.1095e-02, -3.7239e-03, -8.7451e-03,\n",
       "         -8.9452e-03, -8.0809e-03, -2.0322e-02,  7.4671e-03,  1.6023e-02,\n",
       "          1.9407e-02, -8.8658e-04,  6.0438e-03, -1.6740e-02,  1.2788e-02,\n",
       "         -5.4035e-03, -1.5755e-03, -1.5483e-02,  6.0260e-03, -1.9293e-02,\n",
       "         -4.6397e-03, -1.2202e-02,  1.5525e-02,  1.1013e-02, -1.6415e-02,\n",
       "          1.9954e-02,  1.1500e-02, -2.1562e-02, -1.5366e-02,  8.7364e-04,\n",
       "         -1.6324e-02, -1.1521e-03,  6.8791e-03, -1.3494e-02,  3.4048e-03,\n",
       "         -1.2053e-02,  1.4370e-02, -1.0340e-02, -1.4651e-02, -1.7750e-02,\n",
       "          1.3460e-02, -5.0230e-04,  5.5665e-03,  1.0493e-02,  1.1956e-02,\n",
       "          2.2573e-02, -7.8582e-04,  3.7918e-03, -1.1742e-02, -8.5383e-03,\n",
       "          2.2725e-02, -9.3342e-03,  1.0213e-02, -1.2831e-02, -1.9531e-02,\n",
       "         -1.2853e-02, -5.1911e-03, -1.3342e-02,  8.2190e-03,  1.1353e-02,\n",
       "         -1.1442e-02,  8.2604e-04,  2.8016e-03, -2.3358e-02, -1.4147e-02,\n",
       "          1.3303e-02,  2.2017e-02,  2.1469e-02, -5.3185e-03,  2.2081e-02,\n",
       "          1.9036e-02, -8.7346e-03,  9.6920e-03, -1.8282e-02,  1.8689e-03,\n",
       "         -4.5616e-04,  6.0398e-03, -4.9255e-03, -1.4578e-02, -8.0688e-03,\n",
       "          1.2054e-02,  7.2638e-03,  8.6440e-03,  2.0915e-02, -1.7219e-03,\n",
       "         -1.4431e-02, -1.2651e-02, -1.6610e-02, -1.9500e-03, -4.1261e-03,\n",
       "          1.4390e-02, -1.2450e-02,  1.4674e-02, -2.0732e-02, -3.4284e-03,\n",
       "         -1.6823e-02,  1.5775e-02,  4.5943e-03,  4.8888e-03, -1.5155e-02,\n",
       "          2.1379e-02, -4.8557e-03,  1.0173e-02, -6.2110e-03, -1.9876e-02,\n",
       "          5.5664e-03,  1.5518e-02,  3.3092e-03, -2.1361e-04, -1.5704e-02,\n",
       "          2.6927e-03,  4.6150e-03,  8.9225e-03,  1.0427e-02,  8.1215e-03,\n",
       "          1.9410e-02, -1.0127e-02, -2.3668e-02, -1.1372e-02,  1.7025e-03,\n",
       "         -1.7538e-02,  3.7005e-03, -1.7435e-02,  1.4258e-02,  7.8761e-03,\n",
       "          8.7090e-03,  2.0649e-02,  6.7657e-03,  4.2213e-03, -1.5721e-02,\n",
       "         -1.0276e-02, -6.8664e-03, -3.8594e-03,  2.9068e-03,  1.7547e-02,\n",
       "         -1.5841e-03,  6.5688e-03,  1.9734e-02,  1.2215e-02,  1.4766e-02,\n",
       "          1.9389e-02,  8.8871e-03,  1.8344e-02, -1.4390e-02,  1.0544e-02,\n",
       "         -1.4916e-02,  6.4034e-03,  2.3711e-03,  8.0650e-03, -1.1318e-02,\n",
       "         -4.0346e-04,  1.4524e-03,  9.4952e-03,  2.0760e-02,  1.9974e-02,\n",
       "         -2.1388e-03,  2.0576e-02, -1.0907e-02, -1.1123e-02, -1.6530e-02,\n",
       "         -1.7327e-02, -7.6474e-03,  1.0771e-02,  6.8253e-03, -1.4292e-02,\n",
       "          1.6140e-02, -6.1912e-03, -3.1153e-03, -2.0943e-02, -2.4618e-02,\n",
       "          1.2208e-02,  2.0005e-02, -1.3029e-02,  1.1567e-02, -1.2359e-02,\n",
       "         -3.3070e-03, -1.7491e-02, -4.3642e-03,  6.1965e-03,  1.1719e-02,\n",
       "         -2.0396e-02, -2.2779e-02,  2.5208e-03, -2.2619e-03,  1.6329e-02,\n",
       "         -1.3380e-02,  2.1292e-02, -8.8586e-03,  1.5592e-03, -1.2660e-02,\n",
       "         -9.7686e-03, -4.3734e-03,  1.9922e-02, -1.2932e-02, -5.2034e-03,\n",
       "         -1.2238e-02,  9.3815e-03,  9.8133e-04,  1.4320e-02,  4.3137e-03,\n",
       "         -1.9694e-02, -1.3650e-02,  5.2425e-03, -5.6277e-03, -2.1921e-02,\n",
       "          1.3699e-02,  9.2494e-03, -2.5063e-02, -2.7118e-03, -1.3567e-02,\n",
       "          1.7669e-02,  1.8575e-02,  9.2918e-03, -1.2977e-02, -1.4528e-02,\n",
       "         -9.8587e-03, -9.6838e-03, -9.8678e-03,  3.8500e-03,  9.9164e-03,\n",
       "          8.6798e-04, -5.5980e-03, -1.0264e-02,  1.1863e-02, -8.0126e-03,\n",
       "         -2.7185e-03,  1.2965e-02, -1.3274e-03, -1.5832e-02, -2.0323e-02,\n",
       "         -1.9357e-02,  1.3547e-02,  7.9921e-03, -1.5348e-02, -1.7474e-02,\n",
       "         -1.6059e-02,  4.5195e-03,  6.4089e-03, -1.4105e-02, -1.0152e-02,\n",
       "         -5.0543e-03, -3.2164e-03, -1.8223e-03,  1.1142e-02, -8.4963e-03,\n",
       "          1.1451e-02,  4.3079e-03]),\n",
       " 'transformer.resblocks.5.ln_2.weight': tensor([1.0020, 1.0010, 1.0021, 1.0020, 1.0027, 0.9974, 1.0051, 1.0006, 0.9996,\n",
       "         1.0023, 1.0030, 1.0033, 0.9999, 1.0028, 0.9991, 1.0010, 1.0018, 1.0026,\n",
       "         1.0029, 0.9999, 1.0037, 1.0012, 1.0009, 1.0007, 0.9999, 1.0006, 1.0033,\n",
       "         1.0023, 1.0032, 1.0005, 1.0029, 1.0032, 1.0010, 1.0001, 1.0042, 1.0039,\n",
       "         1.0001, 1.0027, 0.9993, 1.0021, 1.0003, 1.0012, 1.0041, 1.0020, 1.0035,\n",
       "         1.0037, 1.0020, 1.0006, 1.0028, 1.0017, 1.0046, 1.0044, 1.0001, 0.9995,\n",
       "         1.0040, 1.0022, 1.0050, 1.0007, 0.9989, 1.0035, 1.0023, 0.9996, 0.9993,\n",
       "         1.0024, 1.0048, 1.0003, 0.9989, 1.0017, 1.0020, 1.0014, 1.0030, 0.9980,\n",
       "         1.0018, 1.0002, 1.0040, 1.0018, 1.0026, 1.0011, 1.0020, 1.0007, 1.0011,\n",
       "         1.0005, 1.0005, 0.9980, 1.0020, 1.0020, 1.0012, 1.0022, 1.0013, 1.0029,\n",
       "         1.0033, 1.0023, 1.0003, 1.0002, 1.0011, 0.9976, 1.0017, 1.0018, 1.0016,\n",
       "         1.0013, 1.0027, 1.0020, 1.0016, 1.0025, 1.0031, 0.9979, 1.0013, 1.0022,\n",
       "         1.0033, 1.0016, 1.0020, 1.0007, 0.9981, 0.9980, 1.0002, 0.9987, 1.0019,\n",
       "         1.0023, 1.0016, 1.0000, 1.0032, 1.0025, 1.0004, 1.0014, 1.0031, 1.0013,\n",
       "         1.0011, 1.0023, 1.0031, 1.0040, 1.0032, 0.9999, 1.0017, 1.0021, 1.0011,\n",
       "         1.0006, 1.0025, 1.0023, 1.0033, 1.0005, 1.0046, 1.0016, 1.0046, 1.0023,\n",
       "         0.9993, 1.0004, 1.0015, 1.0004, 1.0023, 1.0012, 1.0005, 0.9989, 0.9975,\n",
       "         1.0022, 0.9996, 0.9999, 0.9995, 1.0005, 0.9988, 1.0008, 1.0007, 1.0021,\n",
       "         0.9997, 1.0007, 1.0023, 0.9996, 1.0053, 1.0004, 0.9993, 0.9988, 1.0010,\n",
       "         1.0020, 1.0063, 1.0039, 0.9982, 0.9995, 1.0003, 1.0006, 1.0034, 1.0045,\n",
       "         1.0035, 1.0029, 1.0015, 1.0018, 0.9986, 1.0010, 1.0019, 1.0013, 1.0026,\n",
       "         1.0019, 1.0016, 1.0032, 1.0019, 1.0029, 1.0007, 1.0021, 0.9990, 0.9964,\n",
       "         1.0035, 1.0021, 1.0015, 1.0045, 1.0006, 1.0070, 1.0007, 1.0017, 1.0010,\n",
       "         1.0055, 1.0037, 1.0022, 1.0046, 1.0005, 1.0006, 1.0002, 0.9983, 1.0026,\n",
       "         1.0031, 1.0012, 0.9998, 1.0020, 1.0025, 1.0012, 1.0009, 1.0011, 1.0014,\n",
       "         1.0031, 0.9995, 1.0020, 1.0031, 0.9982, 1.0008, 1.0034, 1.0032, 1.0034,\n",
       "         1.0023, 1.0005, 1.0012, 1.0005, 1.0023, 1.0009, 1.0024, 1.0026, 1.0022,\n",
       "         1.0011, 1.0012, 1.0064, 1.0023, 1.0018, 1.0000, 1.0050, 1.0053, 1.0021,\n",
       "         0.9990, 1.0025, 0.9979, 1.0002, 1.0004, 0.9991, 1.0051, 1.0007, 1.0036,\n",
       "         1.0017, 1.0035, 1.0007, 1.0005, 1.0011, 1.0014, 1.0070, 1.0028, 0.9986,\n",
       "         1.0039, 1.0003, 1.0025, 1.0015, 1.0030, 0.9994, 1.0026, 1.0034, 1.0035,\n",
       "         0.9995, 1.0009, 1.0008, 1.0021, 1.0012, 1.0055, 1.0014, 1.0047, 1.0009,\n",
       "         1.0024, 1.0004, 1.0030, 0.9998, 1.0041, 0.9992, 1.0028, 1.0011, 1.0008,\n",
       "         1.0020, 1.0009, 1.0010, 1.0017, 1.0033, 1.0029, 1.0001, 1.0013, 1.0023,\n",
       "         1.0014, 1.0021, 0.9998, 0.9980, 1.0016, 1.0033, 1.0008, 1.0016, 1.0007,\n",
       "         1.0017, 1.0022, 1.0007, 1.0047, 0.9985, 0.9983, 1.0044, 1.0027, 1.0018,\n",
       "         1.0055, 0.9990, 1.0023, 1.0036, 1.0022, 1.0030, 1.0018, 1.0064, 1.0009,\n",
       "         1.0027, 0.9999, 1.0037, 1.0048, 1.0015, 1.0026, 1.0004, 1.0004, 1.0003,\n",
       "         1.0015, 1.0009, 1.0034, 1.0024, 0.9987, 0.9998, 1.0014, 1.0016, 1.0017,\n",
       "         1.0019, 1.0023, 1.0028, 1.0023, 1.0042, 1.0015, 1.0033, 1.0020, 1.0007,\n",
       "         0.9996, 1.0000, 1.0018, 1.0004, 1.0032, 1.0010, 1.0026, 1.0039, 1.0021,\n",
       "         1.0032, 1.0010, 1.0027, 0.9997, 1.0031, 1.0029, 1.0031, 0.9997, 1.0014,\n",
       "         1.0038, 1.0020, 1.0032, 1.0023, 1.0001, 1.0035, 1.0010, 0.9990, 1.0015,\n",
       "         1.0043, 1.0011, 0.9981, 1.0022, 1.0053, 0.9998, 1.0022, 1.0024, 1.0003,\n",
       "         0.9987, 1.0020, 0.9997, 1.0018, 1.0023, 1.0026, 1.0025, 1.0023, 1.0025,\n",
       "         1.0040, 0.9994, 1.0017, 1.0011, 1.0033, 0.9993, 1.0069, 1.0064, 1.0030,\n",
       "         0.9993, 0.9989, 1.0012, 1.0045, 1.0024, 1.0031, 1.0044, 1.0027, 1.0010,\n",
       "         1.0008, 1.0032, 1.0020, 1.0037, 1.0015, 1.0038, 1.0006, 1.0025, 1.0026,\n",
       "         1.0045, 1.0015, 1.0019, 1.0042, 1.0016, 1.0025, 0.9981, 1.0029, 1.0023,\n",
       "         0.9996, 1.0035, 0.9996, 1.0054, 1.0034, 1.0007, 0.9992, 1.0036, 1.0005,\n",
       "         1.0008, 1.0039, 1.0050, 0.9997, 1.0015, 1.0008, 1.0040, 1.0001, 1.0000,\n",
       "         1.0008, 1.0034, 1.0009, 1.0021, 1.0024, 1.0029, 1.0010, 0.9977, 1.0009,\n",
       "         1.0023, 1.0044, 0.9991, 1.0032, 1.0004, 1.0012, 0.9987, 1.0046, 1.0027,\n",
       "         1.0022, 1.0017, 0.9981, 1.0006, 1.0053, 1.0015, 1.0021, 1.0036, 1.0004,\n",
       "         1.0016, 1.0013, 1.0015, 1.0038, 1.0023, 1.0009, 1.0026, 1.0032, 1.0032,\n",
       "         0.9996, 1.0016, 1.0001, 0.9981, 1.0030, 1.0029, 1.0031, 1.0015, 1.0025,\n",
       "         0.9980, 1.0033, 1.0034, 1.0005, 1.0019, 1.0047, 1.0031, 0.9987]),\n",
       " 'transformer.resblocks.5.ln_2.bias': tensor([-9.0884e-04, -1.9027e-04, -2.7100e-03,  4.9528e-04, -6.0181e-05,\n",
       "          3.1716e-03,  1.7020e-03, -1.5907e-05,  4.1872e-03, -4.2924e-03,\n",
       "          2.9429e-03,  1.4913e-03,  3.6586e-04,  2.3062e-03,  2.4874e-03,\n",
       "         -1.5238e-03, -5.3704e-04, -2.4679e-03,  1.9239e-03,  2.9048e-03,\n",
       "          2.0402e-03, -9.1698e-04, -2.1338e-03,  1.4147e-03, -1.1278e-03,\n",
       "          3.2269e-03,  2.5108e-04,  1.8703e-03,  2.6752e-04,  2.5941e-03,\n",
       "          1.4391e-04, -1.2329e-04, -2.9890e-03,  8.7915e-04,  1.2008e-03,\n",
       "         -3.3620e-03, -4.4466e-03, -3.7024e-03,  1.1545e-03,  1.2697e-03,\n",
       "         -2.2816e-03,  1.3734e-03,  1.9631e-03,  1.6375e-03, -7.4098e-04,\n",
       "         -5.0369e-04,  4.3523e-03, -2.1332e-03,  9.8970e-05, -2.9724e-04,\n",
       "          3.6881e-03, -1.6156e-03,  4.3931e-04, -3.6534e-03,  3.9748e-03,\n",
       "         -9.3974e-04,  1.1177e-03,  8.0541e-05, -1.2078e-03, -4.8920e-04,\n",
       "         -1.6754e-03,  1.0295e-03, -9.8796e-04,  2.6496e-03,  1.1107e-03,\n",
       "          6.1025e-04,  3.5343e-04, -1.5881e-03,  3.8690e-04,  2.9496e-04,\n",
       "         -2.3871e-03,  6.3959e-04,  3.9535e-04, -1.1929e-04, -3.3075e-04,\n",
       "         -2.1746e-03, -3.5349e-03,  2.8929e-03, -1.3589e-03,  1.5311e-03,\n",
       "         -3.0872e-03, -2.7129e-05, -5.8717e-04, -2.6766e-03,  1.3788e-03,\n",
       "         -3.4388e-03,  2.0013e-04, -6.7213e-04, -1.7264e-03, -9.4141e-04,\n",
       "         -1.9832e-03, -1.6087e-03, -2.4151e-03,  3.2521e-03,  8.6852e-04,\n",
       "          1.7642e-03,  1.0211e-03, -3.3793e-04,  6.2881e-03,  1.9690e-03,\n",
       "         -2.3905e-03,  9.9892e-04, -1.7138e-03, -2.1245e-03,  4.1309e-04,\n",
       "          3.1094e-03, -1.0427e-03,  2.3466e-03,  9.9025e-04,  5.8880e-04,\n",
       "         -8.8320e-04, -2.9557e-03, -1.6627e-03,  1.5545e-03, -3.7993e-04,\n",
       "          4.3170e-03, -1.3997e-03,  4.4055e-04,  3.7634e-03, -1.5426e-03,\n",
       "          1.3002e-04,  1.4409e-03, -3.4840e-03, -6.9016e-04,  3.4099e-03,\n",
       "         -7.6833e-04, -9.6206e-05, -2.0434e-03, -7.8534e-04,  8.7624e-05,\n",
       "          4.2476e-03, -1.0876e-03,  3.0614e-03,  2.8146e-03, -1.5099e-03,\n",
       "          1.8782e-03,  3.7443e-03,  2.5087e-04,  5.3347e-04,  9.5293e-04,\n",
       "          9.5607e-04, -2.7803e-03, -2.8016e-03,  2.1329e-03, -3.3470e-03,\n",
       "          3.9157e-04,  3.3357e-03, -2.4398e-04, -7.2925e-05, -1.3951e-03,\n",
       "         -4.0274e-04, -2.0745e-03, -4.5059e-03, -4.1616e-03,  9.1876e-04,\n",
       "          4.2466e-03, -1.2654e-03,  3.6637e-03, -1.9929e-03, -2.9446e-04,\n",
       "          2.6652e-03,  6.7160e-04, -3.5338e-03, -1.5324e-03,  2.7106e-04,\n",
       "         -2.9312e-03, -1.9063e-03,  5.7745e-04,  1.8981e-03,  4.2065e-04,\n",
       "         -2.8906e-03,  1.6657e-03,  1.0129e-03, -3.2997e-03,  6.6278e-04,\n",
       "          4.3292e-04, -9.4953e-05, -1.3218e-03, -5.8418e-04, -1.9722e-03,\n",
       "          1.0902e-04, -6.7627e-04, -2.1197e-03, -6.3976e-04, -4.5179e-03,\n",
       "          1.0518e-03, -1.1268e-03,  2.5376e-04,  1.2477e-03,  3.4065e-04,\n",
       "         -4.1449e-04, -1.6217e-03,  4.9629e-04,  2.3493e-03,  1.7945e-03,\n",
       "          4.6344e-03, -1.7849e-03, -3.3456e-03,  7.0369e-04,  6.7255e-04,\n",
       "          1.0460e-03, -3.1407e-03, -9.4938e-04,  2.5072e-03,  8.3210e-04,\n",
       "         -1.5457e-03,  1.1778e-03, -3.5521e-04, -9.6580e-04, -1.3338e-03,\n",
       "         -2.2342e-03,  8.3124e-04,  3.6752e-04, -9.6484e-04,  2.1562e-03,\n",
       "          3.2101e-03, -1.3516e-04,  7.4689e-04, -4.6812e-05, -2.7305e-03,\n",
       "         -4.0250e-03, -3.3398e-03, -1.4590e-03, -1.0953e-03, -1.8552e-03,\n",
       "         -2.9155e-03,  1.5401e-03,  1.5783e-03,  1.5342e-03,  1.7024e-04,\n",
       "          2.0293e-03,  2.9739e-03,  2.4061e-04, -6.0409e-04,  1.4966e-03,\n",
       "          8.4825e-04,  7.1610e-04,  4.5624e-03, -3.1818e-03, -4.6241e-04,\n",
       "          1.4192e-03, -1.1548e-03, -2.8208e-03,  5.9435e-04,  7.3246e-04,\n",
       "          1.9392e-04, -7.8024e-04, -2.4202e-03,  3.5516e-03, -1.5948e-03,\n",
       "         -1.4112e-03, -8.7754e-04, -1.5242e-03,  3.8183e-03, -1.3750e-03,\n",
       "          1.4232e-03, -1.1282e-03,  4.5063e-03, -2.2797e-03,  6.6488e-04,\n",
       "         -4.6561e-03,  2.1516e-03, -2.5067e-03,  4.2069e-03,  8.1673e-04,\n",
       "          5.4139e-04,  9.2375e-04,  6.7484e-04,  1.4864e-03,  2.1844e-04,\n",
       "          3.4254e-04,  2.0529e-03,  9.8918e-04, -1.0825e-03, -2.3911e-03,\n",
       "          5.8073e-04,  3.1863e-04, -7.6349e-04,  3.8705e-03,  4.3094e-03,\n",
       "          4.2025e-04, -2.9869e-03,  1.5503e-03, -1.1076e-04,  7.1724e-04,\n",
       "         -1.5756e-03,  8.3998e-04, -1.1542e-03,  3.7681e-05, -7.5506e-04,\n",
       "         -2.2595e-04, -1.8354e-04,  1.2994e-03, -2.6004e-03, -1.0272e-03,\n",
       "          9.2352e-04,  3.3713e-03, -2.1151e-03,  3.8917e-04, -7.0793e-04,\n",
       "         -5.8810e-04, -1.1626e-03, -6.4970e-04,  2.0296e-03, -1.9565e-04,\n",
       "          1.8171e-03,  1.0049e-03, -2.2931e-03,  6.2347e-04,  6.6053e-04,\n",
       "         -1.2754e-03, -4.4548e-04, -5.5478e-04, -2.2941e-03,  2.0199e-03,\n",
       "          1.6408e-03, -3.5582e-03, -2.5796e-04,  1.4972e-05, -2.3827e-03,\n",
       "         -1.5964e-03,  1.4962e-03, -4.2195e-03,  1.5883e-03,  2.1722e-03,\n",
       "         -1.2420e-03, -2.1150e-03, -2.3708e-03,  2.4108e-04, -1.6319e-03,\n",
       "          5.5591e-04, -1.3273e-03, -2.2680e-03,  2.1610e-03, -3.6489e-03,\n",
       "         -2.6363e-04, -1.9647e-03, -1.3957e-03, -2.5083e-03, -1.6854e-03,\n",
       "          4.2435e-04, -2.6498e-03,  1.0452e-03,  1.7236e-03,  1.7440e-03,\n",
       "         -1.5026e-03, -2.7112e-03,  2.0204e-03,  1.1071e-03, -3.0428e-03,\n",
       "          1.2605e-03,  2.0015e-04,  5.5835e-04, -7.8461e-04,  1.6795e-03,\n",
       "         -1.0148e-03,  5.8090e-04,  8.4262e-04, -7.6969e-04,  2.3378e-04,\n",
       "          6.7356e-04,  2.1927e-03, -1.6904e-03, -2.3378e-03,  4.0100e-03,\n",
       "         -2.2614e-03, -6.1921e-04, -6.2155e-04,  4.7289e-03,  3.2574e-03,\n",
       "         -2.0299e-03,  7.0954e-04,  1.9032e-03, -2.5570e-03, -1.5261e-03,\n",
       "         -2.7100e-03,  1.4848e-03, -4.4515e-03,  1.5916e-03,  6.5220e-04,\n",
       "         -1.8287e-03, -2.1001e-04, -3.6695e-04, -1.8408e-03, -1.8816e-03,\n",
       "          2.2016e-03,  2.8681e-03,  1.9079e-03, -9.0320e-04, -2.9604e-03,\n",
       "          8.0524e-04,  5.9613e-04,  2.0625e-04,  2.8540e-03,  6.5363e-04,\n",
       "         -3.5402e-03,  4.1626e-04, -4.9558e-04, -2.8930e-03, -5.3819e-04,\n",
       "         -2.1905e-03, -7.4310e-04, -2.5181e-03, -4.0639e-04,  5.3912e-04,\n",
       "         -8.1275e-04, -1.0332e-03,  2.4601e-04,  2.0110e-03,  3.8152e-03,\n",
       "         -1.5560e-03, -1.3350e-03, -1.7724e-03, -4.5620e-04, -3.9848e-03,\n",
       "          3.6227e-03, -9.1968e-04,  1.1157e-03,  1.8140e-03, -1.9566e-03,\n",
       "          1.4356e-03,  1.9705e-04, -2.3533e-03,  3.7163e-03,  1.7969e-05,\n",
       "         -1.7960e-03, -1.9754e-03,  1.4388e-04, -4.5255e-04, -1.1713e-03,\n",
       "          7.9793e-04,  2.2082e-04,  5.8940e-04,  1.2261e-03,  6.6576e-04,\n",
       "          1.8095e-03, -3.4615e-03,  1.8652e-03, -4.8815e-03, -9.9917e-04,\n",
       "          1.0988e-03, -1.0448e-03, -2.3791e-04, -1.3281e-03, -1.9832e-03,\n",
       "          1.4040e-03,  1.7462e-03,  2.1176e-03,  8.5506e-04, -3.5226e-03,\n",
       "         -3.3427e-03, -8.6475e-04,  1.9757e-03, -1.0109e-03, -1.3913e-03,\n",
       "          5.4112e-04, -7.1495e-04, -1.1568e-03, -4.0678e-03, -1.5151e-03,\n",
       "          4.8029e-04,  7.9286e-04, -5.1158e-04, -1.6498e-03,  1.6033e-03,\n",
       "          3.3382e-04,  2.5187e-03,  1.1503e-04,  1.2931e-03, -1.7986e-04,\n",
       "          1.3324e-03,  5.6666e-04,  2.6387e-03,  2.5293e-03,  2.8141e-03,\n",
       "         -1.7295e-03,  1.8542e-03,  2.3362e-03,  1.1692e-03,  1.4312e-03,\n",
       "          1.5117e-03,  1.1446e-03, -1.7873e-03, -5.8891e-04,  1.0929e-03,\n",
       "         -1.6191e-03,  1.9628e-05,  2.4966e-03, -1.0720e-03,  2.5349e-03,\n",
       "         -1.1955e-03, -1.7606e-03, -4.3826e-03,  1.2377e-03, -2.9239e-03,\n",
       "         -3.0837e-04, -2.8555e-05,  3.0504e-03, -2.7659e-03,  7.0058e-04,\n",
       "          1.5659e-03,  2.6227e-03,  1.2355e-03, -9.8912e-04,  1.0599e-03,\n",
       "         -2.8789e-03, -2.4480e-03, -5.0647e-04,  9.7929e-04,  1.8917e-03,\n",
       "          3.4700e-04,  7.7309e-04]),\n",
       " 'transformer.resblocks.6.attn.in_proj_weight': tensor([[ 0.0069,  0.0295,  0.0017,  ..., -0.0727,  0.0752,  0.0243],\n",
       "         [-0.0311,  0.0139, -0.0090,  ..., -0.0145, -0.0353,  0.0622],\n",
       "         [ 0.0327,  0.0372, -0.0078,  ...,  0.0353,  0.0139, -0.0309],\n",
       "         ...,\n",
       "         [ 0.0275,  0.0271,  0.0441,  ...,  0.0102, -0.0150,  0.0169],\n",
       "         [ 0.0393,  0.0030, -0.0182,  ..., -0.0280, -0.0134, -0.0418],\n",
       "         [ 0.1527, -0.0326, -0.0288,  ...,  0.0535,  0.0385,  0.0437]]),\n",
       " 'transformer.resblocks.6.attn.in_proj_bias': tensor([ 0.0017, -0.0013, -0.0066,  ...,  0.0006, -0.0005,  0.0007]),\n",
       " 'transformer.resblocks.6.attn.out_proj.weight': tensor([[-0.0008, -0.0019, -0.0103,  ..., -0.0074,  0.0177,  0.0072],\n",
       "         [ 0.0011, -0.0010,  0.0165,  ..., -0.0006,  0.0080, -0.0036],\n",
       "         [ 0.0178, -0.0186, -0.0036,  ..., -0.0069, -0.0081,  0.0042],\n",
       "         ...,\n",
       "         [ 0.0040,  0.0218,  0.0023,  ...,  0.0142, -0.0033,  0.0095],\n",
       "         [ 0.0266,  0.0024,  0.0179,  ..., -0.0021,  0.0150,  0.0174],\n",
       "         [-0.0017, -0.0129,  0.0019,  ..., -0.0035,  0.0020, -0.0023]]),\n",
       " 'transformer.resblocks.6.attn.out_proj.bias': tensor([ 2.0508e-04, -3.0794e-03, -4.6834e-04, -8.4849e-04, -6.3076e-04,\n",
       "         -3.0238e-03,  4.3836e-04,  2.4507e-03, -3.0958e-03,  1.2710e-03,\n",
       "         -3.6333e-03, -2.0705e-03, -9.1563e-04, -1.3542e-03, -3.9915e-03,\n",
       "          1.5207e-03, -1.0283e-04,  1.6157e-03, -3.3184e-04, -7.3750e-04,\n",
       "         -1.4907e-03, -3.5607e-04,  1.1018e-03, -2.0806e-03, -1.3600e-03,\n",
       "         -4.0190e-04, -1.2666e-03, -1.1133e-03, -1.8364e-04, -1.4102e-03,\n",
       "          1.1142e-03,  8.3249e-04,  2.2077e-03, -2.7928e-03, -6.2596e-04,\n",
       "          9.9763e-04,  8.7195e-04,  1.1257e-03, -3.4146e-04, -2.5388e-03,\n",
       "          1.2509e-03,  1.7470e-03,  1.5162e-03,  5.1715e-04, -2.2342e-03,\n",
       "          1.4096e-03, -2.5287e-03,  4.1299e-04, -6.3473e-04,  1.1911e-03,\n",
       "         -8.3568e-04, -1.0546e-03, -1.5558e-03,  2.4967e-03,  2.3592e-03,\n",
       "          8.8475e-04, -5.7517e-04, -1.0017e-03,  6.8013e-04, -7.1211e-04,\n",
       "         -1.5269e-03, -9.9965e-05,  3.3933e-04,  4.8626e-05, -7.6201e-04,\n",
       "         -9.2229e-04,  9.6043e-04,  1.8072e-03, -4.1179e-04,  1.6724e-03,\n",
       "         -2.5165e-04,  2.9559e-04, -1.8275e-03, -1.0816e-03, -6.5803e-04,\n",
       "          1.8124e-03, -1.8451e-03, -1.7052e-03, -1.5881e-03,  1.5096e-03,\n",
       "          1.2480e-03, -1.7860e-03,  6.6785e-05,  1.9653e-03,  1.5371e-03,\n",
       "         -3.6293e-05, -1.1971e-03,  1.1399e-03,  4.1835e-04, -9.1725e-04,\n",
       "          4.3804e-04, -8.5000e-04,  1.4722e-03, -1.5494e-03, -1.9148e-03,\n",
       "         -3.3000e-03,  1.0014e-03, -6.9906e-04, -1.7789e-03,  2.4449e-04,\n",
       "          4.0783e-04, -1.6287e-03,  5.2006e-04, -1.2134e-03, -1.2882e-03,\n",
       "         -2.1080e-03, -2.4605e-03, -3.9552e-04,  9.4411e-04,  3.3956e-05,\n",
       "          2.3454e-03,  9.8902e-04,  1.6146e-03,  1.1261e-03, -7.9547e-04,\n",
       "         -3.3803e-03,  1.5337e-03,  1.6727e-03, -2.2044e-03, -6.2191e-04,\n",
       "          6.9240e-04, -8.4413e-04,  1.4755e-03, -9.5377e-04, -1.3331e-03,\n",
       "          7.6600e-04, -1.5233e-03, -1.6945e-03, -1.3685e-03,  4.7884e-04,\n",
       "         -9.7400e-05,  2.2476e-03, -7.0158e-04, -2.0688e-03, -7.8060e-04,\n",
       "         -4.3569e-04, -1.2377e-03,  2.0474e-03, -6.1916e-04,  1.0016e-03,\n",
       "          4.1921e-04,  2.3206e-03,  1.0956e-03,  2.7685e-03,  5.3420e-04,\n",
       "          1.7941e-03, -1.6830e-03,  3.6992e-07,  1.1057e-03,  5.1384e-04,\n",
       "          1.5358e-03,  4.2724e-03,  4.2490e-03,  1.6352e-03,  3.4686e-04,\n",
       "          1.0181e-04,  1.5139e-03, -7.8131e-04,  7.0144e-04, -6.6710e-04,\n",
       "         -1.8439e-03, -8.6632e-04,  2.6699e-03,  2.0709e-04, -1.1344e-03,\n",
       "          9.6107e-04,  2.2891e-03, -8.2469e-04, -6.1785e-04, -1.5253e-03,\n",
       "          1.2377e-03, -2.8491e-04, -1.0748e-03,  1.1440e-03,  2.3468e-03,\n",
       "          1.2827e-03, -1.9217e-03,  5.0294e-04,  1.0987e-03, -4.2075e-04,\n",
       "         -2.4817e-03,  4.9626e-04,  3.1243e-04, -2.0981e-03,  4.1469e-03,\n",
       "          3.4515e-04,  1.2494e-03, -4.5234e-05,  1.1143e-03, -2.4325e-03,\n",
       "          3.2208e-04,  1.2092e-03, -2.1297e-04,  1.6143e-03, -5.9992e-04,\n",
       "         -1.2697e-03,  1.8111e-03,  2.1776e-03,  2.7587e-03,  2.8248e-03,\n",
       "         -3.6326e-05, -1.3663e-03,  1.2702e-03,  2.1475e-03,  2.5445e-04,\n",
       "          3.1575e-04, -1.3504e-03, -2.8388e-04,  1.5863e-03,  3.8455e-04,\n",
       "          1.8569e-03, -9.3392e-06, -2.6104e-04,  1.3834e-03, -2.3648e-03,\n",
       "         -1.7941e-03,  4.4984e-04, -2.2360e-03, -1.1732e-03,  6.1190e-04,\n",
       "         -7.2400e-04, -1.9572e-06,  1.3713e-03,  6.0965e-04,  1.2420e-03,\n",
       "          1.2708e-03, -3.7852e-04, -1.2063e-03,  8.9643e-04,  1.5076e-03,\n",
       "         -1.6426e-03,  1.0595e-03,  8.2437e-04, -3.9876e-04,  1.1461e-03,\n",
       "         -3.7732e-04,  2.2989e-03, -2.6702e-03,  1.1521e-03,  9.1037e-04,\n",
       "         -1.9614e-03,  1.8235e-03,  2.1833e-03,  1.5195e-03, -2.1088e-03,\n",
       "         -3.0986e-04,  5.4452e-04,  3.5343e-04,  8.2279e-05, -1.1295e-03,\n",
       "          1.0843e-03, -1.7352e-03,  1.4773e-04, -1.1150e-03, -7.5941e-04,\n",
       "         -9.8894e-04,  3.5783e-04, -1.5040e-03, -1.3590e-04, -3.1770e-03,\n",
       "          1.1950e-03, -2.6722e-04,  8.0561e-04, -2.5052e-03, -1.1035e-03,\n",
       "          2.8507e-03,  8.8636e-04,  9.1249e-04, -2.6594e-04, -1.4645e-03,\n",
       "          8.8857e-04, -7.4439e-04, -1.5956e-03,  1.0256e-03,  1.3916e-03,\n",
       "         -1.2673e-03,  2.7674e-03, -7.7240e-04,  2.5192e-03, -8.4098e-04,\n",
       "         -2.5344e-04,  2.5616e-03, -1.4316e-03, -1.6435e-03, -2.1624e-03,\n",
       "          1.4756e-03,  2.9123e-04,  1.2768e-03,  1.5978e-03,  1.9917e-04,\n",
       "         -2.0121e-04,  1.7745e-03, -1.3549e-03,  3.6457e-03,  8.9301e-04,\n",
       "          1.0694e-03, -2.0754e-03,  1.2116e-03, -6.2055e-04, -1.1973e-03,\n",
       "         -2.1928e-03,  1.1208e-03,  1.2803e-03, -3.1791e-03, -1.2576e-04,\n",
       "         -2.0912e-03,  2.7416e-04,  3.7214e-04, -1.8860e-03, -2.2156e-03,\n",
       "          1.2259e-03,  1.7145e-03, -8.4629e-04, -1.8426e-03,  7.2493e-04,\n",
       "          1.7442e-03,  2.6773e-03,  1.0202e-03,  6.4215e-04,  7.0856e-04,\n",
       "          2.1979e-03, -6.7861e-04, -4.6834e-04,  6.3390e-04, -2.5541e-03,\n",
       "          1.1176e-03,  2.3606e-03,  8.7618e-04, -2.6880e-03, -2.1616e-03,\n",
       "         -2.2808e-04,  2.0325e-03,  7.0560e-04, -6.3096e-04,  8.2355e-04,\n",
       "         -2.9774e-04, -5.1276e-04,  7.9383e-04,  7.4674e-04,  2.3244e-03,\n",
       "         -2.6050e-03,  2.4449e-03,  6.2950e-04, -7.8576e-04, -2.5035e-03,\n",
       "          5.9613e-06, -1.9561e-03, -8.1080e-05, -3.9712e-04,  1.2839e-03,\n",
       "         -1.5041e-03, -3.0656e-03, -1.3597e-03, -9.8619e-04, -5.4261e-04,\n",
       "          4.5023e-04,  2.4324e-03,  2.2158e-03,  7.0942e-04, -2.1619e-03,\n",
       "          5.6739e-04, -7.0340e-04,  2.2978e-03,  2.7275e-03,  1.2526e-04,\n",
       "          2.9509e-03, -3.5754e-04, -3.2237e-03,  1.6751e-03, -1.1765e-03,\n",
       "         -7.7585e-04,  1.3595e-03,  8.7937e-04, -4.8603e-04,  1.9753e-04,\n",
       "          2.3211e-03, -6.0359e-04,  1.6521e-03, -1.5586e-03, -1.8082e-03,\n",
       "          1.4035e-03,  1.2947e-03, -1.7242e-03, -2.5348e-04, -1.5229e-03,\n",
       "         -7.1510e-04,  1.6274e-03, -3.0615e-04,  2.5902e-03,  1.6434e-03,\n",
       "         -7.2422e-04,  4.1567e-04, -7.9479e-04,  4.6990e-04,  3.0072e-03,\n",
       "          1.9428e-03, -5.7190e-04,  1.8320e-03,  2.3552e-04,  3.6175e-04,\n",
       "         -4.9311e-04, -1.3195e-03, -1.1063e-03, -5.0903e-04, -5.2086e-04,\n",
       "          1.4099e-03,  9.2701e-04,  1.3788e-03,  1.1992e-03,  1.1857e-03,\n",
       "         -3.0324e-05, -2.1611e-03,  5.7260e-04,  2.4895e-03,  9.5739e-04,\n",
       "         -2.1018e-03,  1.6947e-03,  1.3340e-03, -9.1813e-05,  2.4051e-03,\n",
       "         -1.2018e-03,  6.9215e-04,  1.1251e-03, -1.6917e-03, -1.6620e-03,\n",
       "          8.8575e-05, -7.9218e-04, -5.5959e-04,  1.6613e-03,  6.7280e-05,\n",
       "         -1.7697e-03,  5.0618e-04,  1.2825e-03, -1.4082e-03, -2.8925e-03,\n",
       "         -1.0693e-03,  1.6568e-03, -2.4279e-03,  2.4788e-03, -6.4841e-04,\n",
       "         -1.1618e-03,  3.9957e-04,  3.1935e-05, -1.7488e-04, -1.6788e-03,\n",
       "         -2.6126e-03, -8.7234e-04,  1.1066e-04, -2.9970e-04,  8.9691e-04,\n",
       "          4.1254e-04,  9.3746e-04, -1.2993e-03,  3.4044e-03,  1.3471e-03,\n",
       "         -1.6571e-03,  1.3493e-03,  2.9637e-03,  2.1467e-03,  4.7166e-04,\n",
       "          1.0036e-03, -1.4513e-03,  1.3870e-03,  2.6421e-03,  1.1203e-03,\n",
       "         -8.0526e-05, -3.5573e-05,  1.4064e-03,  2.7455e-04, -2.5209e-03,\n",
       "         -1.8062e-03,  4.0525e-04, -2.7672e-03, -1.0573e-03, -1.8248e-03,\n",
       "          1.7071e-04, -2.6198e-03, -1.0166e-03, -9.2369e-04, -1.1520e-03,\n",
       "         -1.9335e-03, -1.5882e-03,  9.9207e-04,  4.9213e-04, -1.2790e-03,\n",
       "          3.7445e-04, -1.0675e-03, -1.7800e-03,  2.1808e-03,  7.3595e-04,\n",
       "          1.0796e-03, -9.7810e-05,  1.5770e-03, -2.3834e-03, -5.8275e-04,\n",
       "          2.0998e-04,  1.7639e-03, -9.9857e-05,  2.2913e-04, -3.2932e-03,\n",
       "         -7.4216e-04,  1.3488e-04, -8.1569e-04,  1.7931e-03, -2.5495e-03,\n",
       "          1.4924e-03, -7.5945e-04, -1.1888e-03,  1.6798e-03, -5.1629e-04,\n",
       "         -1.7270e-03, -2.3394e-04]),\n",
       " 'transformer.resblocks.6.ln_1.weight': tensor([0.9992, 1.0012, 0.9993, 0.9995, 1.0012, 1.0018, 0.9959, 1.0005, 0.9985,\n",
       "         1.0020, 0.9988, 0.9999, 0.9984, 1.0036, 0.9970, 1.0017, 0.9993, 1.0049,\n",
       "         1.0018, 0.9973, 1.0013, 0.9992, 0.9996, 0.9973, 0.9994, 1.0006, 1.0007,\n",
       "         0.9991, 1.0029, 0.9997, 1.0015, 1.0016, 1.0002, 0.9996, 1.0012, 1.0003,\n",
       "         0.9977, 0.9991, 0.9964, 1.0006, 1.0001, 0.9991, 0.9969, 0.9958, 0.9979,\n",
       "         0.9992, 0.9993, 0.9996, 1.0014, 0.9970, 1.0024, 1.0025, 0.9988, 0.9981,\n",
       "         0.9986, 1.0019, 1.0002, 1.0013, 0.9983, 0.9954, 0.9959, 0.9988, 0.9979,\n",
       "         0.9971, 1.0022, 1.0028, 0.9986, 0.9981, 1.0016, 0.9989, 1.0025, 1.0026,\n",
       "         0.9989, 0.9973, 0.9997, 0.9995, 0.9983, 1.0000, 0.9974, 1.0004, 0.9964,\n",
       "         0.9969, 1.0024, 1.0032, 0.9997, 0.9993, 0.9993, 0.9997, 0.9970, 1.0014,\n",
       "         0.9999, 0.9997, 1.0009, 1.0017, 0.9966, 0.9970, 1.0019, 1.0031, 0.9995,\n",
       "         1.0021, 1.0020, 1.0015, 1.0020, 0.9986, 1.0010, 0.9984, 0.9964, 1.0014,\n",
       "         0.9987, 0.9977, 0.9971, 1.0040, 0.9972, 1.0010, 0.9983, 0.9989, 1.0006,\n",
       "         1.0011, 1.0045, 1.0001, 0.9991, 0.9996, 0.9976, 0.9973, 0.9988, 0.9991,\n",
       "         0.9983, 1.0008, 0.9971, 1.0018, 1.0023, 0.9997, 0.9985, 0.9983, 0.9999,\n",
       "         1.0001, 0.9965, 1.0013, 1.0019, 0.9986, 1.0024, 1.0004, 1.0029, 1.0005,\n",
       "         0.9988, 1.0000, 0.9981, 0.9998, 1.0014, 0.9986, 0.9997, 0.9999, 0.9985,\n",
       "         1.0012, 1.0009, 1.0006, 0.9998, 0.9997, 0.9974, 1.0008, 1.0001, 0.9989,\n",
       "         1.0013, 1.0011, 1.0031, 1.0002, 1.0026, 0.9996, 0.9991, 0.9990, 0.9997,\n",
       "         0.9986, 1.0013, 0.9974, 0.9992, 1.0027, 1.0033, 0.9979, 0.9987, 0.9988,\n",
       "         1.0011, 1.0011, 0.9999, 1.0010, 1.0013, 1.0025, 0.9971, 0.9978, 1.0010,\n",
       "         1.0009, 0.9976, 0.9997, 1.0012, 1.0009, 0.9992, 0.9983, 0.9994, 0.9984,\n",
       "         0.9983, 0.9994, 1.0001, 0.9995, 0.9991, 1.0017, 1.0042, 1.0025, 1.0010,\n",
       "         0.9986, 1.0011, 1.0003, 0.9997, 1.0012, 1.0006, 1.0000, 0.9990, 0.9976,\n",
       "         0.9979, 0.9979, 0.9999, 1.0001, 0.9979, 0.9990, 1.0009, 0.9985, 0.9993,\n",
       "         1.0044, 1.0008, 0.9982, 1.0007, 0.9962, 0.9996, 0.9983, 0.9958, 1.0016,\n",
       "         0.9985, 0.9976, 1.0003, 0.9989, 0.9995, 1.0005, 0.9977, 0.9985, 0.9973,\n",
       "         0.9988, 1.0023, 1.0004, 1.0034, 0.9952, 1.0009, 0.9995, 0.9997, 1.0005,\n",
       "         0.9971, 1.0009, 0.9987, 1.0008, 1.0012, 1.0004, 1.0032, 1.0008, 1.0028,\n",
       "         1.0019, 1.0039, 1.0027, 0.9987, 0.9991, 0.9995, 0.9976, 0.9986, 0.9981,\n",
       "         0.9974, 0.9990, 1.0021, 0.9996, 0.9973, 1.0014, 1.0001, 1.0035, 0.9991,\n",
       "         0.9990, 1.0013, 1.0020, 0.9984, 0.9988, 0.9997, 0.9996, 1.0032, 0.9973,\n",
       "         1.0004, 0.9996, 0.9977, 0.9998, 0.9969, 0.9967, 1.0014, 1.0001, 0.9967,\n",
       "         0.9984, 0.9966, 0.9985, 0.9992, 0.9978, 1.0014, 1.0006, 0.9997, 1.0032,\n",
       "         0.9987, 0.9967, 0.9982, 0.9993, 0.9998, 1.0023, 1.0002, 1.0009, 0.9994,\n",
       "         0.9991, 0.9988, 1.0015, 1.0027, 1.0036, 0.9966, 1.0011, 0.9996, 1.0000,\n",
       "         1.0058, 1.0047, 0.9998, 0.9962, 0.9999, 0.9979, 0.9993, 0.9976, 0.9994,\n",
       "         1.0037, 1.0015, 1.0007, 0.9996, 0.9978, 1.0012, 0.9975, 1.0009, 1.0019,\n",
       "         0.9977, 1.0027, 1.0007, 0.9993, 1.0007, 0.9984, 0.9988, 1.0042, 1.0019,\n",
       "         1.0012, 0.9995, 1.0006, 1.0016, 0.9995, 0.9972, 1.0015, 1.0027, 1.0009,\n",
       "         0.9974, 0.9994, 0.9991, 0.9995, 1.0005, 0.9964, 1.0009, 1.0001, 0.9995,\n",
       "         1.0000, 0.9987, 0.9980, 0.9994, 0.9997, 1.0015, 0.9982, 1.0012, 0.9983,\n",
       "         0.9993, 0.9986, 0.9970, 1.0016, 1.0003, 1.0020, 0.9991, 0.9969, 0.9971,\n",
       "         0.9968, 0.9974, 0.9984, 1.0008, 1.0029, 1.0021, 0.9997, 0.9971, 1.0002,\n",
       "         1.0005, 0.9992, 1.0000, 1.0029, 0.9990, 0.9964, 0.9981, 0.9963, 0.9994,\n",
       "         0.9999, 1.0005, 0.9976, 0.9963, 1.0012, 0.9998, 0.9962, 0.9981, 1.0003,\n",
       "         0.9992, 0.9981, 1.0005, 0.9963, 0.9982, 0.9958, 1.0017, 0.9991, 0.9989,\n",
       "         1.0010, 0.9968, 0.9988, 1.0013, 1.0008, 0.9996, 0.9971, 1.0009, 0.9966,\n",
       "         0.9981, 1.0010, 0.9972, 0.9998, 0.9992, 0.9987, 0.9989, 1.0031, 0.9983,\n",
       "         1.0003, 1.0018, 0.9982, 0.9975, 0.9993, 0.9951, 0.9992, 0.9993, 0.9986,\n",
       "         0.9992, 1.0027, 1.0020, 0.9996, 0.9987, 0.9994, 1.0015, 0.9986, 1.0002,\n",
       "         1.0011, 0.9999, 0.9996, 0.9974, 0.9998, 0.9982, 0.9988, 1.0017, 1.0002,\n",
       "         1.0025, 0.9994, 0.9988, 0.9989, 1.0016, 0.9985, 1.0016, 1.0023, 1.0009,\n",
       "         1.0002, 0.9985, 1.0023, 1.0004, 0.9962, 1.0000, 1.0031, 1.0004, 1.0001,\n",
       "         1.0010, 1.0008, 0.9966, 0.9993, 1.0008, 0.9973, 0.9986, 1.0014, 1.0002,\n",
       "         1.0002, 1.0002, 0.9993, 1.0009, 1.0004, 1.0009, 1.0026, 1.0004, 0.9979,\n",
       "         0.9980, 0.9984, 0.9996, 0.9973, 0.9991, 1.0025, 0.9989, 0.9993]),\n",
       " 'transformer.resblocks.6.ln_1.bias': tensor([-9.1834e-04,  2.9750e-03,  7.5157e-04, -5.8513e-04, -8.9851e-04,\n",
       "          1.1630e-03, -1.7378e-03,  2.2301e-03, -1.1212e-03,  9.6058e-04,\n",
       "          1.4485e-03,  8.0267e-04, -1.0186e-03,  1.3167e-03,  1.9644e-04,\n",
       "         -3.3268e-03,  3.6903e-04,  3.4783e-04,  1.6615e-03, -2.6836e-04,\n",
       "          5.9366e-04,  1.6438e-03, -8.0952e-04,  2.3626e-03, -9.6004e-04,\n",
       "          5.2220e-04, -2.0389e-03, -1.8103e-03, -9.7100e-04, -1.7852e-03,\n",
       "         -8.1757e-04, -1.1387e-03,  1.9164e-03, -1.0926e-03, -1.4295e-04,\n",
       "          4.3175e-03,  1.7911e-03,  1.3034e-03,  1.9381e-04, -5.0299e-04,\n",
       "          2.7228e-03, -2.3466e-03, -1.1493e-03, -1.5817e-03,  3.8706e-03,\n",
       "         -4.9253e-05,  1.1624e-03, -3.0318e-03, -5.8766e-04,  2.1963e-03,\n",
       "         -2.3077e-03,  1.4175e-03,  1.8021e-03, -2.4631e-03, -7.3215e-04,\n",
       "         -1.6463e-04,  7.6541e-04,  9.6274e-05,  1.2794e-03, -5.8386e-04,\n",
       "         -8.2250e-05,  1.1036e-04, -5.6415e-04,  1.1933e-03,  8.7529e-04,\n",
       "         -2.7142e-04,  9.9583e-05,  2.1742e-03,  1.2846e-03,  3.4213e-04,\n",
       "         -4.5647e-04, -1.7739e-03,  1.7540e-04,  2.6178e-03, -9.8319e-04,\n",
       "          2.1681e-03,  7.1944e-04,  2.2296e-03,  8.1193e-04,  2.4812e-03,\n",
       "         -7.9215e-04,  3.0419e-03, -1.0793e-03,  2.1487e-03, -2.1629e-03,\n",
       "          4.1834e-03, -1.2028e-03, -1.1217e-03,  3.2335e-03, -1.3973e-03,\n",
       "          2.0665e-04,  1.2011e-03, -1.0126e-03,  1.5740e-03, -1.1502e-03,\n",
       "          2.8461e-03, -1.1698e-03,  2.5014e-03, -7.2987e-05, -7.5490e-04,\n",
       "         -5.2962e-04, -1.1621e-04, -2.2594e-04,  8.7174e-04, -1.0317e-03,\n",
       "         -1.9902e-03,  3.3209e-03, -8.3813e-04, -2.2413e-04, -1.9922e-03,\n",
       "         -3.3721e-03,  1.6272e-03, -1.2010e-05,  4.3876e-03, -1.9587e-03,\n",
       "         -2.7444e-04,  1.9258e-03, -7.5815e-04,  1.4553e-03, -2.2876e-03,\n",
       "          7.9966e-04, -1.3714e-04,  1.2717e-03, -4.7197e-04, -2.2787e-03,\n",
       "          7.7849e-04,  1.5804e-03, -3.2508e-03,  3.4062e-03,  7.7736e-04,\n",
       "         -1.6868e-05,  8.7500e-04,  2.0207e-03,  1.3817e-03,  1.4407e-03,\n",
       "          1.0420e-03,  3.1084e-03, -1.0659e-03,  4.9591e-04, -9.6019e-04,\n",
       "         -2.3024e-03, -3.3995e-03, -2.4620e-03, -6.8370e-04, -4.9444e-04,\n",
       "         -7.2462e-04,  2.8953e-03,  2.4663e-03,  1.6465e-03,  1.0911e-03,\n",
       "          1.0694e-03, -2.1900e-03, -2.2987e-04,  2.7596e-03, -1.5126e-03,\n",
       "         -3.6557e-04,  1.6544e-03, -1.0693e-03, -1.3527e-03, -1.2126e-04,\n",
       "         -2.2284e-03,  2.7267e-03, -7.4686e-04,  3.9206e-04, -3.1315e-03,\n",
       "          5.3144e-04, -2.1839e-03,  7.4830e-06, -1.2349e-03, -1.2243e-03,\n",
       "         -1.8501e-03,  1.9500e-04,  1.1981e-03, -4.0692e-03, -3.0834e-03,\n",
       "         -3.8958e-03, -7.7885e-04,  1.2732e-03,  1.1359e-03, -4.8756e-04,\n",
       "          2.8045e-03,  1.8208e-03,  2.7398e-03, -3.3105e-03,  2.7054e-03,\n",
       "         -1.0353e-03,  1.2412e-03, -6.7453e-04, -3.4024e-03,  1.4341e-03,\n",
       "         -6.1690e-05, -1.5011e-03,  8.3335e-04,  2.5208e-04, -1.0607e-03,\n",
       "         -3.4106e-04,  6.9778e-04, -1.8089e-03, -3.3059e-03,  4.2472e-03,\n",
       "          2.2764e-03, -1.3526e-03,  4.5047e-04, -2.2468e-04, -2.8492e-03,\n",
       "          5.3791e-04, -2.9178e-04, -1.5694e-03, -1.8273e-03, -8.6027e-04,\n",
       "         -5.0111e-04, -4.7532e-04, -8.8595e-04,  7.2974e-04,  9.4662e-04,\n",
       "          2.2956e-03, -4.5828e-04, -2.0770e-03, -1.9897e-04,  3.2993e-04,\n",
       "          1.3930e-03,  7.4331e-04,  3.3206e-03, -1.8020e-05,  7.7539e-04,\n",
       "         -5.0405e-03, -4.0603e-04,  1.2214e-04, -1.2886e-03, -5.7746e-04,\n",
       "          2.5227e-03,  3.8107e-04, -2.4737e-03, -6.0943e-04,  8.5666e-04,\n",
       "         -2.7515e-05,  1.4337e-04, -1.2183e-03, -5.4309e-03, -2.7828e-04,\n",
       "          9.7056e-04, -2.3041e-04, -1.7919e-03, -7.8228e-05,  1.0037e-03,\n",
       "          5.8033e-04, -7.4463e-04, -2.2101e-03, -3.7571e-03, -2.2629e-03,\n",
       "          1.3240e-03,  5.5391e-04,  1.8043e-03, -1.3657e-03, -1.4780e-03,\n",
       "         -4.6202e-04, -1.9047e-03, -1.3646e-03, -1.4138e-03, -1.4058e-03,\n",
       "          1.5434e-03, -1.1090e-04, -1.6529e-03,  9.6807e-04, -2.7960e-03,\n",
       "          1.3497e-03, -4.6570e-04, -2.9576e-03,  2.1871e-03,  4.9132e-04,\n",
       "          1.7476e-03, -7.9021e-04,  1.0892e-03, -4.7591e-03,  1.4681e-05,\n",
       "          1.1338e-03, -8.4852e-04,  9.4428e-04, -5.6075e-04,  1.7979e-03,\n",
       "          3.2163e-03,  2.6538e-03,  1.3144e-03, -1.7641e-03,  5.9799e-04,\n",
       "         -4.6201e-04,  3.5505e-04, -4.3731e-03,  1.0988e-04, -2.8879e-04,\n",
       "         -2.4762e-04,  4.8618e-04,  2.1291e-04,  1.0227e-03, -1.5248e-03,\n",
       "          6.8373e-04,  3.2433e-03, -8.5302e-04, -4.2111e-03,  1.0401e-03,\n",
       "          3.1636e-03,  1.4638e-03,  1.5723e-03,  3.2270e-04, -2.5349e-03,\n",
       "         -1.0664e-04, -1.1492e-03,  6.0636e-04, -4.8275e-04, -1.3950e-03,\n",
       "         -1.3911e-03,  4.7657e-04, -1.7877e-03, -1.1988e-03,  1.5518e-03,\n",
       "         -9.3161e-04,  2.2834e-03, -1.2601e-04,  1.5020e-03,  1.7111e-03,\n",
       "         -4.1301e-03,  5.0017e-04, -2.1663e-04, -4.9960e-04,  1.9492e-03,\n",
       "         -4.9325e-04, -2.3065e-04, -5.6615e-05,  6.1820e-04, -2.0207e-04,\n",
       "          1.6936e-03,  1.5032e-03,  1.7376e-04, -9.1807e-04,  4.2002e-04,\n",
       "          4.0430e-04, -8.6955e-04,  1.6194e-03,  1.3036e-03, -1.2895e-03,\n",
       "         -6.8566e-04,  1.0964e-03, -2.3626e-04,  1.4712e-04,  1.9336e-03,\n",
       "          1.0939e-03,  2.5943e-03,  1.7047e-03,  1.6686e-03,  1.3320e-03,\n",
       "         -3.5208e-04, -9.6333e-04,  4.0470e-04,  1.5572e-03,  2.8481e-03,\n",
       "         -2.0192e-03,  6.4797e-04,  1.4698e-03, -1.8916e-03, -9.4559e-04,\n",
       "         -1.6384e-03,  2.6670e-04, -3.2159e-04, -1.2036e-03,  1.5781e-03,\n",
       "         -2.2429e-03,  3.7984e-04, -7.7406e-04, -1.8010e-03,  1.1837e-03,\n",
       "          2.4921e-03, -5.7146e-04, -8.6878e-04, -2.1876e-04, -4.9498e-04,\n",
       "          8.4881e-04, -1.1648e-03,  6.9044e-04, -4.3662e-04, -7.9484e-04,\n",
       "          4.8442e-05, -1.2562e-03, -1.4452e-03, -3.2599e-03,  1.5282e-03,\n",
       "         -7.2558e-04, -1.7098e-04, -1.0620e-05, -3.3725e-03, -1.9722e-03,\n",
       "         -2.0756e-03, -2.7747e-03, -3.8991e-04,  2.8841e-04, -9.2613e-04,\n",
       "         -1.0688e-04, -5.6416e-04, -1.9157e-04, -2.9359e-04,  1.2520e-03,\n",
       "          3.9567e-04,  1.9538e-03, -5.4543e-04,  1.8926e-05,  7.3982e-04,\n",
       "          9.5673e-04,  2.8351e-03, -1.0565e-03, -2.4171e-03, -8.7047e-05,\n",
       "          2.7323e-03,  2.2701e-03, -2.0427e-04, -3.0421e-03, -1.6957e-03,\n",
       "         -8.1214e-04,  1.2104e-03,  2.4319e-04,  2.0937e-03, -4.6292e-03,\n",
       "          1.6336e-03, -2.0347e-03, -2.2728e-05,  2.1289e-03, -1.3057e-03,\n",
       "          6.2925e-04, -1.3537e-03,  3.2763e-04,  4.6691e-03, -3.2698e-03,\n",
       "          2.0916e-03,  1.6588e-04, -2.3994e-03, -5.4629e-04,  1.1889e-03,\n",
       "          2.4402e-03, -1.9144e-03, -1.3669e-03,  1.7769e-03,  6.5185e-04,\n",
       "          4.6812e-05, -1.1519e-04,  1.0839e-03, -1.5442e-03,  6.5436e-04,\n",
       "          1.0085e-03, -2.8998e-03, -1.1022e-03,  5.6742e-06,  3.0075e-03,\n",
       "         -7.0158e-04,  2.1150e-03, -2.0793e-03,  1.5596e-03, -1.8785e-04,\n",
       "          3.9686e-04, -1.0454e-03,  1.0243e-03,  8.4500e-04,  1.8264e-04,\n",
       "         -3.2820e-03,  6.6741e-04, -5.9016e-04,  3.3162e-04, -1.6949e-03,\n",
       "          3.3331e-03,  5.0710e-04,  8.2652e-04, -1.6355e-03,  2.0257e-03,\n",
       "          4.1406e-03, -1.8941e-03, -2.8552e-03,  8.8385e-04,  1.0682e-03,\n",
       "          1.9991e-03, -1.0803e-03, -2.3341e-03,  4.0353e-04,  4.3801e-04,\n",
       "         -1.1740e-03, -1.8485e-03, -1.5142e-04,  2.7664e-03, -1.5403e-03,\n",
       "          4.1112e-04, -2.8094e-03,  1.0767e-03,  9.4863e-04, -2.9718e-03,\n",
       "         -5.8336e-05,  1.3849e-03, -1.3674e-03,  3.7635e-04,  9.1004e-04,\n",
       "          2.6050e-04,  1.7198e-03, -1.2656e-04, -1.6692e-03,  4.9713e-04,\n",
       "          1.9798e-03, -3.1656e-03, -2.3291e-03, -1.1046e-03,  6.8452e-04,\n",
       "         -6.8983e-04,  6.2997e-04,  5.8856e-04, -8.0944e-04,  4.1309e-04,\n",
       "          1.2512e-03,  5.5337e-04]),\n",
       " 'transformer.resblocks.6.mlp.c_fc.weight': tensor([[-0.0209,  0.0428, -0.0362,  ..., -0.0637, -0.0086,  0.0682],\n",
       "         [-0.0444, -0.0392, -0.0574,  ...,  0.0253, -0.0283, -0.0445],\n",
       "         [ 0.0343, -0.0215,  0.0183,  ...,  0.0526, -0.0312, -0.0555],\n",
       "         ...,\n",
       "         [-0.0200, -0.0149,  0.0097,  ..., -0.0445,  0.0187, -0.0312],\n",
       "         [-0.0227, -0.0143, -0.0202,  ...,  0.0028,  0.0286, -0.0689],\n",
       "         [ 0.0333, -0.0238,  0.0928,  ...,  0.0198,  0.0633,  0.0048]]),\n",
       " 'transformer.resblocks.6.mlp.c_fc.bias': tensor([-0.0180,  0.0323, -0.0442,  ..., -0.0332,  0.0227,  0.0351]),\n",
       " 'transformer.resblocks.6.mlp.c_proj.weight': tensor([[-1.2150e-02,  3.8463e-03,  1.0110e-02,  ...,  2.4118e-03,\n",
       "          -4.4035e-03,  9.2073e-05],\n",
       "         [-8.7751e-04,  2.8965e-03,  1.2525e-02,  ...,  1.1389e-02,\n",
       "          -4.9393e-03,  4.5326e-03],\n",
       "         [ 2.7733e-03, -1.1926e-02,  4.1723e-03,  ..., -5.6820e-05,\n",
       "           1.2400e-03, -9.7740e-03],\n",
       "         ...,\n",
       "         [ 1.5711e-02, -1.6587e-03,  1.2353e-02,  ...,  6.6583e-03,\n",
       "          -6.8475e-03,  1.4588e-03],\n",
       "         [ 6.4034e-03,  1.1464e-02, -4.2806e-03,  ..., -1.2713e-02,\n",
       "          -1.5040e-02, -3.3174e-03],\n",
       "         [ 6.6056e-03,  1.0737e-02,  4.8021e-03,  ..., -3.2035e-03,\n",
       "          -1.2946e-02,  1.0876e-02]]),\n",
       " 'transformer.resblocks.6.mlp.c_proj.bias': tensor([ 2.1000e-02,  5.3720e-03,  8.4958e-03, -7.6604e-03, -1.7028e-02,\n",
       "          3.0675e-03,  2.8685e-03, -4.9645e-03,  5.4799e-04,  5.7984e-03,\n",
       "          4.2161e-03,  4.7624e-03,  1.8357e-02,  2.0797e-02,  6.7659e-03,\n",
       "          4.0498e-04,  2.3292e-03,  9.5782e-03,  1.4918e-02, -1.3740e-03,\n",
       "          5.3634e-03,  1.3829e-02, -2.7449e-03,  4.4380e-03,  6.7872e-03,\n",
       "          7.9193e-03,  1.1273e-02,  2.7730e-03,  9.3195e-03,  2.7952e-03,\n",
       "          1.9787e-02, -9.0506e-03, -1.2837e-02,  1.8257e-02,  2.0736e-02,\n",
       "          2.2524e-02, -7.6897e-03,  1.7767e-02, -2.1931e-03,  2.8246e-03,\n",
       "         -1.4306e-02, -5.1496e-03, -1.7610e-02, -2.0456e-03, -2.3767e-02,\n",
       "         -5.9120e-03, -1.6063e-02,  4.7442e-03, -1.0011e-02, -1.0657e-02,\n",
       "         -2.2068e-02,  1.6672e-02, -2.0558e-02,  2.0107e-02, -5.0413e-03,\n",
       "         -1.9999e-02, -2.0535e-02,  2.2187e-03, -2.1493e-02, -1.3197e-02,\n",
       "          2.9981e-03,  3.5250e-03, -1.9243e-02,  1.5122e-02,  8.6325e-03,\n",
       "         -2.1484e-02,  2.0103e-02, -1.0536e-02, -1.8955e-02, -7.8324e-03,\n",
       "          8.6724e-03,  5.4602e-03, -8.0769e-03, -6.4056e-03, -1.3578e-02,\n",
       "          1.8647e-02, -1.7778e-02, -2.1706e-02, -1.8595e-02,  1.5447e-02,\n",
       "          3.0526e-03, -1.5086e-02,  1.6206e-02,  3.5476e-03,  5.8623e-03,\n",
       "          3.1561e-03,  1.8628e-02, -1.5097e-02, -1.4631e-02,  2.0416e-03,\n",
       "         -4.9609e-03, -1.8614e-02,  9.3293e-03, -2.3125e-02,  1.7517e-02,\n",
       "          2.7580e-03, -7.3760e-03, -6.2085e-03,  1.4772e-02, -9.0074e-03,\n",
       "          9.1441e-03,  1.8564e-02, -1.2143e-02,  6.3608e-03,  3.5774e-03,\n",
       "          1.4910e-02,  1.3514e-02,  1.5976e-02, -1.8968e-02,  1.2858e-02,\n",
       "          4.2495e-03,  7.2548e-05, -2.8279e-03, -9.7841e-03,  1.2629e-03,\n",
       "         -1.7093e-03,  6.1518e-03, -1.4706e-03, -8.5233e-03,  1.0594e-02,\n",
       "         -2.0546e-02, -2.8787e-03, -9.0987e-04,  6.7448e-03,  1.5459e-02,\n",
       "          8.3889e-03,  6.7042e-03, -1.2194e-02, -6.8665e-03, -9.9250e-03,\n",
       "          1.5356e-02,  1.8615e-02, -7.9848e-03, -6.2040e-03,  1.2737e-02,\n",
       "          7.7180e-03,  6.4849e-03, -1.9514e-02,  5.4245e-03,  2.0981e-02,\n",
       "          3.3549e-03, -7.7429e-03, -6.6785e-03,  1.5662e-02,  1.3366e-02,\n",
       "          2.1914e-02,  8.3002e-03, -1.7622e-02, -1.9491e-02,  5.1493e-04,\n",
       "          1.5276e-02,  1.8783e-02,  1.2378e-02,  6.1712e-03, -1.0908e-02,\n",
       "         -2.2328e-03, -3.9371e-03,  8.5986e-03, -2.0610e-02,  2.0663e-02,\n",
       "         -7.4536e-03, -1.5612e-02,  7.0011e-03,  1.3268e-02,  1.7211e-02,\n",
       "         -1.6708e-02, -1.3069e-02, -4.2138e-03,  1.6488e-02,  1.2200e-02,\n",
       "         -1.1909e-03, -1.2509e-02, -1.0791e-02,  2.2439e-02, -1.6507e-02,\n",
       "          2.2712e-02,  1.7073e-02, -1.8764e-02,  1.8493e-02,  6.3574e-03,\n",
       "         -1.0541e-02,  7.9035e-03,  9.1070e-03,  1.3473e-02,  2.5125e-02,\n",
       "         -1.6904e-02, -1.0065e-03,  1.6453e-02,  1.7018e-03, -1.4583e-02,\n",
       "         -1.1832e-02,  6.0530e-03,  6.3917e-03,  1.3574e-02, -1.9310e-02,\n",
       "         -7.3347e-03,  1.7001e-02,  2.2821e-02, -9.6911e-04,  2.2404e-02,\n",
       "         -1.8478e-02,  1.0412e-02,  1.1093e-02, -2.2344e-03, -1.1121e-05,\n",
       "          1.0210e-02, -1.4439e-02,  1.2825e-02, -9.3694e-03, -2.0669e-02,\n",
       "          1.3731e-02, -4.5955e-03,  9.4696e-03, -1.3567e-02, -1.9262e-02,\n",
       "         -1.2429e-02, -1.3275e-02, -1.3317e-03,  1.9615e-02, -1.5147e-02,\n",
       "         -1.4167e-02,  9.9753e-03,  1.5754e-02, -1.9860e-02, -1.2246e-02,\n",
       "          1.1620e-02,  9.8453e-03, -2.1339e-02, -2.8654e-04,  2.2522e-02,\n",
       "         -1.6098e-02, -3.3525e-03,  1.5002e-02,  1.7516e-02,  1.3474e-02,\n",
       "          1.2901e-02,  1.3709e-02, -1.3445e-03,  8.2213e-03,  1.4341e-02,\n",
       "         -1.3435e-03, -1.2745e-03, -6.9170e-03,  2.1884e-02,  2.6808e-03,\n",
       "         -1.3456e-02, -4.0318e-03,  1.9599e-03,  2.0231e-02,  2.0900e-02,\n",
       "         -1.6798e-02,  1.9430e-02,  1.7895e-02,  1.0312e-02, -4.3690e-03,\n",
       "          9.1486e-03,  7.5038e-03,  9.8644e-03,  1.9761e-02, -1.6411e-02,\n",
       "          9.8824e-03, -1.0446e-02,  1.6041e-02, -1.9918e-02, -1.7185e-02,\n",
       "          3.7878e-03,  2.0915e-02,  1.6967e-02,  6.0616e-03,  6.6424e-03,\n",
       "         -5.6451e-03, -1.4942e-02,  9.8808e-04, -1.5767e-02, -9.2987e-03,\n",
       "          1.3929e-02, -1.4707e-02,  1.3497e-02,  7.7256e-03,  4.9445e-03,\n",
       "         -1.9534e-02, -1.0494e-02,  1.8523e-02,  1.1586e-02,  1.7546e-02,\n",
       "         -6.7108e-03, -3.4798e-03, -7.9624e-03,  1.3111e-03, -1.6302e-03,\n",
       "          1.8402e-02, -1.9793e-02,  1.6032e-02, -1.7008e-02,  6.4996e-03,\n",
       "         -1.2728e-03, -6.5543e-03,  2.1030e-03, -1.9264e-02, -1.0045e-02,\n",
       "         -7.9453e-03, -6.4090e-03, -5.8321e-03, -6.9987e-03, -2.1238e-02,\n",
       "         -9.6199e-03, -1.8835e-03,  1.5784e-02,  1.3597e-02,  6.9875e-03,\n",
       "          9.9547e-03,  2.3419e-02,  5.2241e-03,  7.9242e-03,  2.9796e-03,\n",
       "         -2.6139e-03, -7.9200e-03,  1.8842e-02, -1.7043e-02,  1.4872e-02,\n",
       "         -3.0974e-03,  1.7570e-02,  1.5187e-02, -1.5011e-02, -1.1897e-02,\n",
       "         -5.2594e-03,  1.8056e-02, -1.8046e-03,  1.3807e-02,  2.0256e-03,\n",
       "          1.5667e-02, -1.6443e-03,  4.6019e-04, -1.8690e-03, -9.9383e-03,\n",
       "         -6.7331e-03, -9.2208e-03, -2.1011e-02,  5.1374e-03,  6.2569e-04,\n",
       "         -1.8119e-02,  1.9689e-02, -7.0133e-03, -2.2009e-03, -6.1029e-04,\n",
       "          1.9842e-02, -1.7509e-02, -3.1327e-03, -8.2407e-04, -3.1380e-03,\n",
       "         -5.5466e-03, -1.0346e-02,  6.0444e-04,  1.8419e-02,  2.0867e-02,\n",
       "         -4.6549e-03,  4.1401e-03,  2.3049e-02,  1.2078e-02,  1.4979e-02,\n",
       "         -9.7566e-03, -1.3549e-02,  3.2893e-03,  1.9039e-02,  1.1516e-02,\n",
       "          9.9839e-04,  4.5046e-03,  5.0443e-03,  1.7930e-02,  1.2763e-02,\n",
       "         -5.2344e-03, -8.6712e-03,  1.0762e-02, -5.4484e-03,  1.7626e-02,\n",
       "         -4.9081e-03,  1.5467e-02,  1.9977e-02,  1.3592e-02, -7.6322e-03,\n",
       "         -4.9984e-04, -1.3835e-02, -1.9084e-04, -1.8490e-02, -2.2876e-02,\n",
       "         -6.2792e-03,  2.1549e-02,  1.9102e-02,  2.9892e-03,  5.7421e-04,\n",
       "         -1.9832e-02,  5.5475e-03, -1.0578e-02,  9.1698e-03,  7.1075e-03,\n",
       "         -8.4840e-03, -1.2635e-02, -4.8388e-03, -6.6500e-03,  1.2046e-02,\n",
       "          2.0066e-02, -1.6787e-02, -1.8837e-03,  1.3347e-02, -1.5424e-02,\n",
       "         -1.9212e-02, -1.3309e-02, -1.9439e-02,  1.0749e-02,  7.8048e-03,\n",
       "          7.9492e-03, -1.1156e-02, -7.3189e-03,  3.7825e-04, -1.4241e-02,\n",
       "         -5.7460e-03, -1.8977e-02, -1.2281e-02,  1.4806e-02, -1.8565e-02,\n",
       "         -1.0817e-04,  1.8202e-02,  1.0237e-02, -1.6914e-02, -9.1061e-03,\n",
       "          1.6153e-02,  4.5330e-03, -1.9146e-02,  1.9782e-03,  1.6547e-02,\n",
       "         -1.2339e-03,  2.2404e-02, -4.0294e-03, -7.8429e-03,  4.6733e-03,\n",
       "          1.5322e-02, -1.2707e-03,  7.4841e-04,  1.2173e-03, -1.3571e-02,\n",
       "         -1.6684e-03,  1.3912e-02,  1.7704e-02, -1.4535e-02,  1.5463e-02,\n",
       "          4.6260e-03, -1.2654e-02,  6.1942e-03,  2.2164e-02,  2.0125e-02,\n",
       "         -4.4063e-03, -1.8801e-02,  1.7249e-02,  2.3292e-02,  1.8991e-02,\n",
       "          2.2216e-03,  6.3398e-03,  8.0816e-03,  1.3227e-03,  8.1049e-03,\n",
       "         -3.2768e-03,  1.9291e-02,  1.2137e-02, -9.8602e-03,  2.2856e-02,\n",
       "         -7.6263e-03, -1.9462e-02, -6.1907e-03,  1.6915e-02, -2.1915e-02,\n",
       "         -1.0217e-02, -1.9128e-02, -1.3925e-02, -1.7647e-02, -9.0676e-03,\n",
       "         -6.8943e-03,  1.2946e-02, -2.0036e-02,  1.4598e-03,  1.9919e-02,\n",
       "          4.4314e-03,  1.4285e-02, -6.6416e-04,  1.1391e-02,  1.3711e-02,\n",
       "          4.6837e-03,  1.2315e-02, -1.1289e-03,  2.3167e-02, -6.1943e-03,\n",
       "          5.9997e-03, -3.3956e-04,  1.1636e-02,  5.6993e-03, -2.2041e-02,\n",
       "         -8.0908e-03, -3.9949e-03,  1.9827e-03, -2.1122e-02,  1.7425e-03,\n",
       "          1.5285e-03,  6.7130e-03,  1.7415e-04,  3.9827e-03, -6.5802e-03,\n",
       "         -1.8083e-02, -2.8957e-03,  5.5248e-03, -9.8323e-03,  1.1170e-02,\n",
       "          9.9087e-03,  8.2170e-03]),\n",
       " 'transformer.resblocks.6.ln_2.weight': tensor([1.0023, 0.9978, 0.9982, 1.0019, 1.0001, 1.0005, 1.0010, 1.0011, 0.9998,\n",
       "         1.0010, 1.0003, 1.0009, 0.9981, 1.0031, 0.9956, 1.0011, 1.0028, 1.0037,\n",
       "         1.0022, 1.0008, 1.0030, 1.0009, 1.0042, 1.0005, 0.9998, 1.0060, 1.0008,\n",
       "         1.0008, 0.9993, 1.0019, 1.0057, 1.0032, 0.9990, 1.0012, 1.0031, 0.9985,\n",
       "         1.0023, 1.0023, 1.0004, 0.9995, 1.0017, 1.0013, 0.9999, 1.0027, 0.9980,\n",
       "         0.9993, 1.0019, 1.0030, 1.0005, 1.0033, 1.0018, 1.0046, 1.0015, 1.0004,\n",
       "         1.0030, 1.0011, 1.0019, 1.0040, 0.9998, 1.0022, 0.9978, 0.9992, 0.9998,\n",
       "         1.0030, 1.0012, 1.0008, 1.0012, 1.0021, 1.0002, 1.0038, 1.0016, 0.9999,\n",
       "         1.0012, 1.0048, 0.9990, 1.0029, 0.9928, 1.0011, 1.0061, 0.9984, 0.9968,\n",
       "         1.0023, 1.0036, 0.9988, 1.0001, 1.0037, 1.0034, 1.0018, 1.0024, 1.0004,\n",
       "         1.0038, 1.0045, 0.9982, 1.0011, 1.0042, 0.9966, 0.9993, 0.9979, 1.0029,\n",
       "         0.9997, 1.0041, 1.0021, 1.0027, 0.9998, 1.0043, 1.0016, 1.0011, 1.0014,\n",
       "         1.0016, 1.0004, 0.9991, 1.0028, 0.9985, 0.9978, 1.0029, 0.9992, 1.0026,\n",
       "         1.0022, 0.9997, 1.0006, 0.9999, 1.0028, 0.9989, 1.0037, 1.0023, 1.0002,\n",
       "         1.0033, 0.9996, 1.0024, 0.9997, 1.0029, 0.9979, 1.0043, 0.9996, 0.9999,\n",
       "         1.0054, 1.0028, 1.0007, 1.0036, 0.9988, 0.9996, 1.0035, 1.0032, 0.9966,\n",
       "         1.0002, 1.0027, 1.0009, 1.0033, 1.0016, 1.0010, 0.9992, 0.9992, 0.9972,\n",
       "         1.0029, 1.0031, 1.0013, 0.9984, 1.0015, 1.0028, 1.0072, 1.0020, 0.9980,\n",
       "         0.9987, 1.0037, 1.0010, 0.9991, 1.0011, 0.9981, 1.0032, 1.0000, 1.0012,\n",
       "         1.0023, 1.0059, 1.0025, 1.0017, 1.0016, 1.0000, 0.9998, 1.0024, 1.0036,\n",
       "         0.9991, 1.0002, 1.0010, 0.9978, 1.0002, 1.0028, 1.0040, 1.0009, 1.0028,\n",
       "         1.0004, 1.0000, 1.0030, 1.0013, 1.0009, 1.0015, 1.0046, 1.0057, 1.0011,\n",
       "         1.0003, 1.0006, 1.0041, 1.0039, 1.0007, 1.0020, 0.9994, 1.0016, 1.0046,\n",
       "         1.0067, 1.0047, 1.0031, 1.0041, 1.0028, 1.0003, 1.0023, 1.0026, 1.0025,\n",
       "         0.9985, 0.9993, 1.0035, 1.0008, 1.0011, 1.0012, 1.0015, 1.0052, 1.0016,\n",
       "         1.0030, 1.0009, 0.9995, 1.0005, 0.9992, 1.0006, 1.0022, 1.0003, 1.0013,\n",
       "         1.0025, 1.0001, 0.9985, 0.9998, 0.9994, 1.0014, 1.0028, 1.0043, 1.0005,\n",
       "         1.0035, 1.0004, 1.0015, 1.0024, 1.0023, 1.0011, 1.0031, 1.0020, 1.0018,\n",
       "         0.9982, 1.0000, 0.9996, 1.0022, 1.0020, 0.9957, 0.9989, 0.9987, 1.0021,\n",
       "         1.0022, 1.0037, 0.9977, 1.0019, 1.0009, 1.0002, 1.0027, 0.9982, 1.0015,\n",
       "         0.9994, 0.9993, 1.0003, 1.0011, 0.9997, 0.9980, 0.9995, 1.0044, 0.9965,\n",
       "         0.9991, 1.0000, 0.9993, 1.0020, 1.0035, 1.0017, 1.0005, 0.9998, 1.0021,\n",
       "         1.0030, 1.0023, 1.0005, 1.0034, 1.0024, 0.9999, 1.0053, 1.0017, 1.0006,\n",
       "         1.0008, 1.0018, 1.0010, 0.9978, 1.0018, 1.0020, 0.9984, 1.0020, 1.0010,\n",
       "         1.0043, 1.0025, 0.9992, 0.9979, 1.0021, 1.0049, 1.0029, 1.0024, 1.0005,\n",
       "         1.0009, 0.9994, 0.9970, 1.0036, 1.0026, 1.0023, 1.0029, 1.0006, 1.0036,\n",
       "         1.0035, 0.9998, 1.0023, 1.0028, 1.0010, 1.0035, 0.9999, 1.0020, 1.0037,\n",
       "         1.0047, 0.9991, 1.0016, 1.0015, 1.0021, 1.0030, 1.0040, 0.9989, 1.0038,\n",
       "         1.0016, 1.0033, 1.0025, 1.0050, 1.0009, 1.0002, 0.9985, 1.0027, 1.0020,\n",
       "         0.9987, 1.0021, 1.0046, 1.0011, 1.0044, 1.0002, 1.0022, 1.0004, 1.0000,\n",
       "         1.0043, 1.0034, 1.0002, 0.9988, 1.0017, 0.9986, 1.0052, 1.0017, 1.0028,\n",
       "         0.9997, 1.0034, 0.9999, 0.9992, 0.9992, 1.0021, 1.0010, 1.0002, 1.0032,\n",
       "         0.9992, 1.0015, 1.0017, 0.9992, 1.0016, 0.9968, 1.0030, 1.0025, 0.9979,\n",
       "         0.9998, 1.0000, 1.0012, 1.0022, 1.0026, 1.0011, 0.9997, 1.0019, 1.0004,\n",
       "         1.0019, 0.9986, 1.0034, 1.0018, 1.0047, 0.9999, 1.0031, 1.0038, 1.0018,\n",
       "         1.0005, 1.0015, 0.9995, 0.9987, 1.0025, 1.0022, 1.0005, 1.0036, 1.0018,\n",
       "         1.0039, 1.0005, 1.0027, 1.0018, 1.0003, 0.9987, 1.0014, 1.0019, 1.0025,\n",
       "         1.0010, 1.0006, 1.0020, 1.0010, 1.0020, 1.0004, 1.0018, 1.0008, 1.0006,\n",
       "         0.9997, 1.0050, 1.0017, 1.0038, 0.9997, 1.0013, 1.0020, 1.0021, 1.0023,\n",
       "         1.0003, 1.0018, 1.0031, 1.0037, 0.9993, 1.0001, 1.0019, 1.0003, 1.0023,\n",
       "         1.0022, 1.0019, 1.0025, 0.9966, 1.0023, 1.0008, 1.0009, 1.0033, 0.9989,\n",
       "         0.9973, 1.0023, 1.0009, 1.0010, 0.9997, 1.0032, 1.0005, 1.0022, 1.0016,\n",
       "         1.0040, 0.9993, 1.0024, 1.0026, 0.9992, 1.0012, 1.0018, 1.0007, 1.0001,\n",
       "         1.0011, 0.9998, 0.9986, 1.0018, 1.0038, 1.0043, 1.0037, 1.0005, 1.0005,\n",
       "         1.0003, 1.0016, 1.0021, 1.0012, 0.9987, 1.0026, 1.0013, 1.0028, 1.0043,\n",
       "         1.0010, 1.0007, 1.0018, 1.0018, 0.9998, 1.0026, 1.0020, 1.0004, 1.0026,\n",
       "         1.0000, 1.0001, 0.9981, 0.9987, 1.0019, 1.0041, 0.9997, 1.0015]),\n",
       " 'transformer.resblocks.6.ln_2.bias': tensor([ 1.3904e-03,  5.0823e-03, -7.5259e-04, -5.0005e-04,  7.9263e-04,\n",
       "          7.1926e-04,  4.1060e-05,  7.2485e-04, -6.8250e-04, -1.8095e-03,\n",
       "          2.6283e-03,  2.7163e-04, -1.2882e-03, -1.0774e-03,  5.0673e-03,\n",
       "         -2.0086e-03, -2.7258e-04, -2.0370e-03, -1.5661e-03, -1.1314e-03,\n",
       "         -3.1780e-03,  1.5042e-03,  1.3795e-03,  6.6221e-04,  9.4652e-04,\n",
       "          2.2777e-03,  3.4187e-04,  6.8277e-04, -1.0296e-03,  7.6257e-04,\n",
       "          3.2657e-03,  2.1522e-03, -1.4613e-03,  1.0049e-03,  8.0189e-04,\n",
       "         -2.6602e-03, -1.9458e-04,  2.0922e-03, -3.9050e-03, -2.1574e-03,\n",
       "         -4.2460e-03, -8.5562e-04, -1.9989e-03, -3.2052e-03,  5.1260e-03,\n",
       "         -1.6086e-03,  3.9294e-03,  1.4618e-03,  1.7777e-03,  2.8305e-03,\n",
       "          1.5886e-03, -1.8835e-03, -5.8716e-04,  6.6717e-04,  1.8245e-04,\n",
       "         -2.6453e-03,  1.5306e-03, -3.1146e-03,  1.8969e-03, -3.3724e-04,\n",
       "          1.3051e-03, -1.1926e-04, -5.1736e-04, -2.5362e-03, -2.0775e-03,\n",
       "         -1.7084e-03, -1.8651e-03, -3.3501e-04, -5.4460e-04,  7.6243e-04,\n",
       "          2.5684e-03,  7.6752e-04, -8.5973e-04, -3.5081e-03,  3.3404e-03,\n",
       "         -2.3982e-03,  3.9279e-03,  3.0111e-04,  1.1058e-03, -1.2555e-03,\n",
       "         -1.4649e-03, -6.4062e-04,  7.0016e-04, -2.2877e-03, -2.2621e-03,\n",
       "         -9.6563e-04, -1.7807e-03, -1.8312e-03,  5.1713e-05,  4.3143e-03,\n",
       "          7.8990e-04, -2.4246e-05,  4.8844e-04,  7.9899e-04, -7.4040e-04,\n",
       "          3.1956e-03,  3.0307e-03,  2.4763e-03,  8.8931e-04, -2.7346e-03,\n",
       "         -1.6739e-03, -1.5973e-03,  1.2027e-03, -3.8521e-04, -1.5159e-03,\n",
       "          3.1525e-04, -4.7294e-05,  4.1643e-03,  7.4542e-04,  5.8934e-04,\n",
       "         -5.8517e-03, -6.3811e-04, -1.7782e-03, -2.1970e-03,  2.2326e-03,\n",
       "          2.6429e-03, -1.0546e-03,  3.6197e-04,  8.6329e-04,  7.8590e-04,\n",
       "         -8.8247e-04,  2.8805e-03, -2.2718e-03, -4.1804e-03,  1.1139e-03,\n",
       "         -3.0741e-05,  7.1542e-04, -1.4478e-03, -1.3500e-03,  2.3401e-04,\n",
       "         -2.6888e-03, -4.7863e-03, -1.0174e-03,  3.5101e-03,  1.6133e-03,\n",
       "          1.5411e-03,  3.0949e-03,  1.7343e-03, -2.2790e-03, -2.8349e-03,\n",
       "         -1.1687e-03, -1.7025e-03, -7.8457e-04, -3.2401e-03, -3.4952e-03,\n",
       "         -3.5640e-04,  2.4118e-03, -2.4386e-03, -1.8838e-04,  1.3612e-03,\n",
       "          9.6691e-04, -1.7687e-03, -4.8867e-03,  4.5187e-04,  1.1265e-03,\n",
       "          2.6118e-04,  2.9557e-03,  1.4093e-04, -8.1737e-04, -2.7522e-03,\n",
       "          1.6993e-04,  1.0081e-03, -1.7027e-03,  8.1129e-04,  1.4617e-03,\n",
       "         -1.6249e-03,  1.6795e-03, -1.2846e-04,  2.3157e-04,  1.7479e-03,\n",
       "         -1.9305e-03, -2.0815e-03,  1.0624e-03, -1.9132e-04,  9.9153e-04,\n",
       "          9.9969e-04, -2.6996e-03, -1.1801e-03,  4.8870e-05,  2.5348e-03,\n",
       "         -2.6206e-04,  1.9571e-03, -2.7182e-03,  4.9548e-03, -3.0856e-03,\n",
       "          1.5565e-03,  2.5465e-03,  6.6554e-05,  1.0095e-03,  1.6737e-03,\n",
       "         -4.3657e-03,  1.7456e-03,  1.9461e-04, -2.8629e-03,  6.0186e-04,\n",
       "         -3.4011e-03,  1.6255e-03, -7.1203e-04,  4.2104e-04,  2.8145e-03,\n",
       "          2.2965e-03, -2.7832e-03, -4.7263e-03, -2.8006e-03, -6.6933e-04,\n",
       "         -4.8703e-04, -2.1431e-03, -2.7462e-03,  1.2277e-03,  2.1289e-03,\n",
       "         -2.8753e-03, -1.1334e-03,  2.3425e-03, -8.7678e-04, -1.2948e-03,\n",
       "          9.5300e-04,  1.3609e-03, -1.9170e-03, -2.5804e-03, -4.8763e-04,\n",
       "         -4.5866e-04,  9.3683e-04, -1.5070e-03, -5.8321e-03,  8.6387e-05,\n",
       "         -2.7469e-03,  1.9146e-03,  3.8408e-03,  8.7894e-04,  1.0119e-03,\n",
       "         -1.2254e-03, -6.3048e-04, -1.8320e-04,  8.1679e-05, -5.6389e-04,\n",
       "         -9.0615e-04, -2.1941e-03,  5.1960e-03, -2.4234e-03, -1.4415e-03,\n",
       "         -2.3196e-03, -1.8255e-03, -7.1343e-04,  2.3368e-03,  2.7019e-03,\n",
       "         -9.1866e-04,  4.3087e-04, -1.3967e-03,  2.0878e-03, -3.6390e-03,\n",
       "         -3.0550e-04,  7.5205e-04,  1.4393e-03,  9.2830e-04,  1.4815e-03,\n",
       "          2.0550e-03,  3.9690e-03,  4.4960e-03,  4.0001e-04,  1.1700e-03,\n",
       "         -3.8085e-04, -2.8420e-03, -9.2026e-04,  2.3198e-03,  1.3538e-03,\n",
       "          1.4501e-03, -1.3469e-03, -6.6710e-04, -2.2549e-03, -1.1704e-04,\n",
       "         -5.5874e-04,  1.5417e-03,  2.0567e-03,  3.2329e-03,  6.2895e-04,\n",
       "          3.1603e-03, -1.7205e-03, -1.7276e-03, -2.3642e-03,  2.5539e-03,\n",
       "          7.4958e-04, -4.2557e-03,  5.3110e-03,  7.2040e-04,  3.1035e-03,\n",
       "          1.5516e-03, -4.1690e-04, -3.3834e-03,  1.2420e-03,  8.0496e-04,\n",
       "         -8.0698e-04, -1.0035e-03,  1.9720e-03, -4.4758e-04, -3.2711e-04,\n",
       "         -1.0683e-03,  1.6402e-03,  8.2796e-04, -2.6970e-03, -7.7879e-04,\n",
       "          3.7030e-03,  4.0541e-03,  1.1126e-03,  1.6286e-03, -2.2881e-03,\n",
       "          4.0407e-03, -2.2325e-03, -1.1650e-04,  5.3199e-04,  8.1135e-05,\n",
       "         -3.7270e-03, -1.4447e-04, -2.5050e-03, -4.0004e-04, -3.4620e-03,\n",
       "          1.4309e-03, -1.7439e-03, -2.4036e-03, -1.8149e-03, -2.1475e-03,\n",
       "         -2.5604e-03, -6.6086e-04,  1.9343e-03, -7.4263e-04, -1.8017e-03,\n",
       "          3.4093e-03, -2.4872e-03,  1.3220e-03,  2.1555e-03, -1.8847e-05,\n",
       "         -2.9882e-04, -1.6643e-03,  1.0677e-03,  2.0056e-03, -2.7275e-03,\n",
       "         -1.2414e-03,  2.2193e-04,  2.5420e-03, -8.1313e-04, -1.8710e-03,\n",
       "          2.6872e-03, -2.9613e-03, -5.0735e-04, -1.4141e-03,  7.9352e-04,\n",
       "          1.3003e-03, -4.9452e-03, -6.6394e-04,  8.5415e-04, -2.7084e-03,\n",
       "          1.6489e-03,  4.7015e-03,  2.1571e-03, -3.7719e-03, -1.3598e-03,\n",
       "         -1.6929e-05, -1.0146e-03,  1.3685e-03, -5.2356e-05,  1.6898e-03,\n",
       "         -1.8948e-03,  1.4025e-03, -1.7991e-03, -3.4595e-03,  4.6639e-05,\n",
       "         -1.4090e-03,  1.0018e-03, -1.8062e-04,  1.8802e-03, -3.1626e-03,\n",
       "          1.0529e-04,  8.2418e-04, -8.8503e-04,  2.0983e-03,  1.5845e-03,\n",
       "         -2.0665e-03, -1.8480e-03,  9.3430e-04, -4.0268e-05,  2.2229e-03,\n",
       "         -2.2267e-03, -2.5660e-03,  1.5885e-03,  7.9323e-04,  2.0648e-03,\n",
       "         -1.4089e-03, -4.8236e-03,  7.4344e-05, -2.0544e-03, -1.5495e-03,\n",
       "         -3.0614e-03,  3.4569e-03, -3.8566e-03, -1.8954e-03, -1.1088e-03,\n",
       "         -1.3962e-03, -3.1746e-04, -2.0500e-03, -1.9432e-03, -1.9303e-03,\n",
       "          4.0640e-04, -6.0053e-04,  3.4556e-04,  1.0715e-04,  2.7547e-05,\n",
       "         -1.4284e-03, -2.4241e-03, -3.8013e-05, -3.4589e-03, -2.8891e-03,\n",
       "         -1.2209e-03, -1.1950e-03, -4.9170e-05, -2.5566e-03, -1.7921e-03,\n",
       "          1.5004e-03,  5.6539e-04, -1.4772e-04,  2.6730e-03, -5.2319e-03,\n",
       "         -8.4782e-04, -9.2961e-04, -2.0248e-03,  4.5233e-05,  2.0280e-03,\n",
       "          2.2191e-04,  4.2110e-04, -1.7546e-03,  2.2626e-03,  1.2292e-03,\n",
       "          8.4429e-04, -2.2649e-03,  9.8558e-04, -2.0974e-03, -7.7735e-05,\n",
       "          7.8511e-04, -3.0234e-04,  1.5605e-03,  4.2168e-03,  2.2630e-03,\n",
       "         -1.2846e-03,  7.1816e-05, -9.3514e-04,  6.4853e-04,  1.0094e-03,\n",
       "          3.8533e-03,  1.8874e-03, -2.2118e-03, -2.3349e-03, -2.8671e-03,\n",
       "          9.3927e-04, -1.7419e-03,  1.7933e-03, -4.1248e-03,  2.5427e-03,\n",
       "          2.0523e-04,  1.8814e-04,  2.7167e-03, -1.4241e-03,  4.9761e-04,\n",
       "          8.5467e-04, -2.7764e-03,  2.3800e-03, -3.4184e-03,  3.5569e-03,\n",
       "         -2.1925e-03, -2.9657e-03, -5.0151e-04,  3.6005e-03, -6.8474e-04,\n",
       "          2.2570e-03, -2.6953e-03,  5.2678e-04, -2.7693e-03, -6.6789e-04,\n",
       "         -3.9034e-03,  1.3335e-03,  3.5865e-03, -1.7888e-03,  1.9858e-03,\n",
       "          8.6701e-04, -2.1198e-03,  2.4936e-03, -2.4504e-03, -2.3439e-04,\n",
       "         -9.5308e-04, -2.1308e-04,  5.7731e-04, -3.2196e-03, -4.6873e-03,\n",
       "         -3.3877e-03, -1.2798e-03,  1.4184e-03, -5.8959e-04,  3.2670e-03,\n",
       "          2.7266e-03,  2.5965e-03,  4.9435e-04,  1.5431e-03, -4.0392e-04,\n",
       "          3.6614e-03, -1.8892e-03, -6.0349e-04, -3.2721e-03,  2.4122e-03,\n",
       "          1.2197e-03,  3.5084e-03,  4.9866e-04,  2.6954e-03,  2.0435e-03,\n",
       "          8.3571e-04,  1.2982e-03]),\n",
       " 'transformer.resblocks.7.attn.in_proj_weight': tensor([[ 0.0399, -0.0095, -0.0381,  ..., -0.0305, -0.0418,  0.0512],\n",
       "         [ 0.0595, -0.0159, -0.0369,  ..., -0.0737, -0.0150,  0.0113],\n",
       "         [ 0.0318,  0.1095,  0.0365,  ..., -0.0309, -0.0069, -0.0498],\n",
       "         ...,\n",
       "         [ 0.0227,  0.0378, -0.0664,  ..., -0.0497, -0.1504,  0.0043],\n",
       "         [-0.0284,  0.0157, -0.0182,  ..., -0.0339,  0.0279, -0.0425],\n",
       "         [ 0.0023, -0.0629, -0.0264,  ...,  0.0096, -0.0782, -0.0662]]),\n",
       " 'transformer.resblocks.7.attn.in_proj_bias': tensor([-0.0032,  0.0016,  0.0024,  ...,  0.0019, -0.0028,  0.0005]),\n",
       " 'transformer.resblocks.7.attn.out_proj.weight': tensor([[ 1.6820e-02, -2.2952e-03,  2.6935e-05,  ..., -5.1608e-03,\n",
       "          -7.5535e-03, -6.8509e-03],\n",
       "         [-1.0837e-03,  1.1355e-02,  8.6768e-03,  ..., -4.8113e-03,\n",
       "          -7.5551e-03,  1.4494e-03],\n",
       "         [-9.6055e-04,  1.1259e-02, -1.5487e-02,  ..., -5.1615e-03,\n",
       "           9.0393e-03,  1.7616e-04],\n",
       "         ...,\n",
       "         [-1.2212e-02,  3.2373e-03, -2.0335e-04,  ...,  1.0513e-04,\n",
       "          -2.6122e-03, -6.0605e-03],\n",
       "         [ 3.0833e-03,  6.4879e-03,  8.5312e-03,  ...,  1.0216e-02,\n",
       "          -2.6032e-04,  1.2561e-02],\n",
       "         [-1.0128e-03, -1.0627e-02,  5.3552e-03,  ...,  2.3091e-03,\n",
       "          -5.0702e-03,  2.9256e-03]]),\n",
       " 'transformer.resblocks.7.attn.out_proj.bias': tensor([-3.9819e-04, -3.8318e-03, -5.5199e-04, -5.1916e-04, -8.9043e-04,\n",
       "         -2.8451e-03,  5.7345e-04,  2.2992e-03, -3.3761e-03,  1.6109e-03,\n",
       "         -4.2975e-03, -2.3779e-03, -6.7658e-04, -1.0397e-03, -4.9534e-03,\n",
       "          1.8425e-03,  4.6207e-05,  1.7820e-03, -3.5684e-06, -7.5321e-04,\n",
       "         -5.7099e-04,  1.5105e-05,  9.2331e-04, -2.0060e-03, -1.6483e-03,\n",
       "         -8.4738e-04, -1.2815e-03, -1.0161e-03, -4.5511e-04, -1.7005e-03,\n",
       "          8.2019e-04,  4.3463e-04,  2.7603e-03, -3.2844e-03, -5.6122e-04,\n",
       "          1.1818e-03,  9.4543e-04,  7.0376e-04,  1.2054e-04, -2.7155e-03,\n",
       "          1.5587e-03,  2.0498e-03,  1.8776e-03,  1.2528e-03, -3.0615e-03,\n",
       "          1.5858e-03, -3.1626e-03,  6.0260e-04, -9.2631e-04,  1.1137e-03,\n",
       "         -1.1751e-03, -6.2387e-04, -1.5733e-03,  2.3609e-03,  2.4212e-03,\n",
       "          9.7373e-04, -7.0809e-04, -5.9722e-04,  1.9515e-04, -8.3702e-04,\n",
       "         -1.4440e-03, -4.0560e-05,  3.4080e-04,  4.1183e-04, -2.5280e-04,\n",
       "         -6.0550e-04,  1.1616e-03,  1.7030e-03, -5.8386e-04,  1.8107e-03,\n",
       "         -9.6982e-04, -1.7030e-05, -1.9326e-03, -3.6258e-04, -9.5265e-04,\n",
       "          1.9339e-03, -2.3758e-03, -1.8579e-03, -1.9023e-03,  2.2640e-03,\n",
       "          1.4414e-03, -1.7165e-03,  3.1715e-04,  2.1610e-03,  1.6938e-03,\n",
       "          2.2309e-04, -8.5346e-04,  1.0906e-03,  7.1493e-04, -1.6498e-03,\n",
       "          1.2321e-04, -1.1210e-03,  1.4704e-03, -1.6965e-03, -1.9810e-03,\n",
       "         -4.0899e-03,  2.8190e-04, -1.2063e-03, -1.8592e-03,  9.6540e-04,\n",
       "          7.3564e-04, -1.4508e-03,  3.9133e-04, -9.1503e-04, -7.5057e-04,\n",
       "         -2.2539e-03, -2.8740e-03, -1.0049e-03,  9.6771e-04, -2.2415e-04,\n",
       "          3.1384e-03,  9.6722e-04,  2.0070e-03,  1.4284e-03, -1.1860e-03,\n",
       "         -3.8340e-03,  1.4940e-03,  1.5651e-03, -2.5084e-03, -9.6590e-04,\n",
       "          6.1975e-04, -1.0847e-03,  1.7948e-03, -6.3962e-04, -1.3794e-03,\n",
       "          7.2389e-04, -1.5151e-03, -1.6658e-03, -1.5444e-03,  4.5474e-05,\n",
       "         -1.0951e-04,  3.1220e-03, -2.8401e-04, -1.9812e-03, -9.4048e-04,\n",
       "         -7.1998e-04, -1.3800e-03,  1.8774e-03, -7.0258e-04,  1.2773e-03,\n",
       "          4.0150e-04,  2.2237e-03,  1.1531e-03,  3.4931e-03,  1.1365e-03,\n",
       "          1.8875e-03, -2.3339e-03,  3.5777e-04,  1.2206e-03,  6.1132e-05,\n",
       "          1.6864e-03,  4.4399e-03,  5.3925e-03,  1.6582e-03, -1.4435e-04,\n",
       "          9.1961e-05,  9.8262e-04, -1.0037e-03,  8.3742e-04, -6.0580e-06,\n",
       "         -1.7504e-03, -9.5973e-04,  2.9595e-03, -2.2912e-04, -1.4550e-03,\n",
       "          1.1688e-03,  2.7027e-03, -9.4053e-04, -4.2323e-04, -2.1023e-03,\n",
       "          1.4968e-03, -2.1895e-04, -1.5843e-03,  9.5818e-04,  2.1351e-03,\n",
       "          1.2732e-03, -1.4651e-03,  3.9424e-04,  9.9279e-04, -8.1609e-04,\n",
       "         -2.6441e-03,  1.5293e-04,  9.2124e-04, -2.7788e-03,  4.4486e-03,\n",
       "          7.4901e-05,  4.3115e-04, -4.2311e-04,  1.0481e-03, -2.4980e-03,\n",
       "          6.4601e-04,  8.6797e-04, -5.1933e-04,  1.7219e-03, -7.9563e-04,\n",
       "         -8.5007e-04,  1.6733e-03,  2.5283e-03,  2.7971e-03,  2.7748e-03,\n",
       "         -2.2035e-04, -1.2351e-03,  1.6866e-03,  2.1878e-03,  2.7039e-04,\n",
       "          3.5577e-04, -1.1506e-03,  9.1984e-06,  1.2173e-03,  2.3236e-04,\n",
       "          2.1244e-03,  5.9183e-04, -1.2591e-06,  1.5358e-03, -2.0936e-03,\n",
       "         -2.2168e-03,  2.8967e-04, -2.0494e-03, -9.0640e-04,  8.4875e-04,\n",
       "         -7.7876e-04, -4.4094e-04,  1.6488e-03,  1.9308e-03,  1.1284e-03,\n",
       "          1.2325e-03, -4.7150e-04, -2.0355e-03,  8.1498e-04,  1.6363e-03,\n",
       "         -1.6216e-03,  9.8537e-04,  6.8681e-04, -4.2502e-04,  1.4579e-03,\n",
       "         -2.9698e-04,  2.5660e-03, -3.2636e-03,  1.4054e-03,  9.9965e-04,\n",
       "         -1.7047e-03,  2.0401e-03,  2.5054e-03,  1.5453e-03, -2.8016e-03,\n",
       "         -2.5289e-04,  5.2652e-04,  7.3915e-04, -2.3934e-04, -1.0128e-03,\n",
       "          9.0722e-04, -1.9112e-03,  2.1342e-04, -1.0700e-03, -1.2778e-03,\n",
       "         -1.1558e-03, -3.8683e-04, -2.3505e-03, -1.5388e-04, -3.3465e-03,\n",
       "          1.2400e-03,  4.9914e-04,  1.0328e-03, -3.2660e-03, -1.6743e-03,\n",
       "          2.6033e-03,  1.0251e-03,  1.2021e-03, -3.6692e-04, -1.6747e-03,\n",
       "          9.1371e-04, -9.8202e-04, -1.8466e-03,  5.9961e-04,  1.1966e-03,\n",
       "         -1.1664e-03,  3.4559e-03, -7.0705e-04,  3.0867e-03, -1.3123e-03,\n",
       "         -7.6927e-04,  3.2925e-03, -1.9908e-03, -1.5780e-03, -2.7072e-03,\n",
       "          1.1331e-03,  3.7777e-04,  1.6036e-03,  1.8088e-03,  2.0839e-04,\n",
       "         -1.7419e-04,  1.8160e-03, -1.6388e-03,  4.0295e-03,  6.7245e-04,\n",
       "          1.3077e-03, -2.0684e-03,  8.7800e-04, -7.7816e-04, -1.1655e-03,\n",
       "         -2.5635e-03,  8.7471e-04,  1.0818e-03, -3.9118e-03, -1.0144e-04,\n",
       "         -2.5156e-03,  3.2294e-04,  1.7695e-04, -2.2824e-03, -2.0654e-03,\n",
       "          1.5667e-03,  2.0853e-03, -7.3808e-04, -1.9341e-03,  1.3149e-03,\n",
       "          1.9237e-03,  3.0376e-03,  1.2772e-03,  9.9954e-04,  1.1519e-03,\n",
       "          2.4655e-03, -7.2872e-04, -1.1007e-03,  6.0965e-04, -2.1124e-03,\n",
       "          6.8951e-04,  2.7744e-03,  6.2780e-04, -3.2484e-03, -1.9927e-03,\n",
       "         -1.5086e-04,  1.7041e-03,  5.4118e-04, -8.5018e-04,  1.7690e-03,\n",
       "         -2.7168e-04, -3.0955e-04,  4.5838e-04,  9.4048e-04,  2.6482e-03,\n",
       "         -3.1672e-03,  2.7901e-03,  7.2997e-04, -5.5460e-04, -2.8317e-03,\n",
       "         -8.2582e-07, -1.9463e-03,  1.0740e-05, -3.8530e-04,  1.4939e-03,\n",
       "         -1.6528e-03, -3.7955e-03, -1.8936e-03, -7.7472e-05, -2.3100e-04,\n",
       "          6.0307e-04,  2.9044e-03,  2.4809e-03,  9.7174e-04, -2.7053e-03,\n",
       "          9.4228e-04, -8.2222e-04,  2.6517e-03,  3.2654e-03,  3.0861e-05,\n",
       "          2.9234e-03, -1.8538e-04, -3.6861e-03,  1.5229e-03, -8.4946e-04,\n",
       "         -9.4569e-04,  1.2976e-03,  7.7250e-04, -7.5794e-04,  1.0679e-04,\n",
       "          3.0663e-03, -6.9127e-04,  1.9967e-03, -1.5817e-03, -1.9216e-03,\n",
       "          2.0838e-03,  1.9878e-03, -1.7762e-03, -5.2882e-04, -1.8313e-03,\n",
       "         -6.4027e-04,  2.1556e-03, -7.2608e-04,  3.1515e-03,  1.8905e-03,\n",
       "         -3.5197e-04,  8.1531e-05, -2.2421e-04,  1.1349e-03,  3.0664e-03,\n",
       "          2.3214e-03, -6.4349e-04,  2.0866e-03,  3.7302e-04,  3.8211e-05,\n",
       "         -6.1455e-04, -1.7479e-03, -7.2157e-04, -5.4445e-04, -6.0545e-04,\n",
       "          1.6667e-03,  1.3100e-03,  1.2162e-03,  1.6294e-03,  1.5556e-03,\n",
       "         -4.0787e-04, -2.5535e-03,  7.6322e-04,  2.6362e-03,  9.0509e-04,\n",
       "         -2.1964e-03,  1.8571e-03,  1.1546e-03, -4.1321e-04,  2.8543e-03,\n",
       "         -1.1439e-03,  7.0412e-04,  1.2531e-03, -1.5021e-03, -1.8647e-03,\n",
       "          2.3995e-04, -5.9447e-04, -2.7655e-04,  1.2147e-03,  4.4832e-05,\n",
       "         -1.8243e-03,  8.8506e-04,  1.4165e-03, -8.4224e-04, -2.8522e-03,\n",
       "         -1.3459e-03,  1.5478e-03, -2.6712e-03,  1.8666e-03, -1.0377e-03,\n",
       "         -1.1297e-03,  7.1985e-04, -1.6033e-04, -4.1030e-04, -1.8821e-03,\n",
       "         -2.9617e-03, -6.1972e-04,  9.4953e-04,  4.6664e-04,  1.2211e-03,\n",
       "          4.5845e-04,  8.2189e-04, -1.2936e-03,  4.2385e-03,  8.1185e-04,\n",
       "         -2.0558e-03,  7.4325e-04,  2.3943e-03,  2.2098e-03,  5.4448e-04,\n",
       "          6.7412e-04, -1.0702e-03,  1.1143e-03,  3.2297e-03,  4.8587e-04,\n",
       "          6.0252e-04,  3.4018e-05,  1.4084e-03, -8.6771e-05, -2.2822e-03,\n",
       "         -2.0741e-03,  1.3800e-03, -3.0630e-03, -5.5828e-04, -1.7992e-03,\n",
       "          9.1543e-04, -2.8833e-03, -1.3933e-03, -8.4650e-04, -1.7561e-03,\n",
       "         -2.3696e-03, -1.0547e-03,  7.1874e-04,  5.0801e-04, -1.0807e-03,\n",
       "          8.7500e-04, -1.0923e-03, -1.8054e-03,  2.5055e-03,  1.1778e-03,\n",
       "          1.1925e-03, -5.1578e-04,  1.2046e-03, -2.7356e-03, -1.2583e-03,\n",
       "         -5.7667e-05,  1.2917e-03, -1.2719e-04, -7.2294e-05, -3.2951e-03,\n",
       "         -1.1442e-03,  6.3539e-04, -9.5798e-04,  2.4920e-03, -2.9164e-03,\n",
       "          1.2391e-03, -1.4543e-03, -1.1180e-03,  1.0185e-03, -5.9698e-04,\n",
       "         -1.6739e-03, -4.1489e-04]),\n",
       " 'transformer.resblocks.7.ln_1.weight': tensor([0.9989, 0.9999, 0.9982, 1.0014, 1.0001, 1.0008, 0.9975, 0.9992, 0.9971,\n",
       "         0.9987, 0.9977, 0.9998, 0.9989, 0.9998, 0.9992, 0.9985, 0.9979, 1.0009,\n",
       "         1.0001, 0.9975, 1.0000, 0.9994, 1.0004, 0.9995, 1.0003, 0.9992, 0.9970,\n",
       "         1.0001, 0.9998, 0.9975, 0.9987, 0.9990, 0.9998, 0.9983, 0.9992, 1.0015,\n",
       "         0.9982, 1.0039, 0.9980, 0.9995, 0.9973, 0.9972, 1.0000, 0.9991, 0.9997,\n",
       "         0.9982, 0.9942, 0.9978, 0.9972, 0.9990, 0.9989, 0.9988, 1.0002, 0.9992,\n",
       "         1.0010, 0.9984, 0.9990, 1.0020, 1.0003, 0.9983, 1.0010, 0.9972, 1.0012,\n",
       "         0.9994, 1.0012, 0.9999, 0.9999, 0.9985, 0.9989, 0.9986, 0.9970, 0.9998,\n",
       "         1.0003, 0.9989, 1.0022, 0.9999, 0.9992, 1.0027, 0.9990, 0.9996, 0.9986,\n",
       "         1.0026, 1.0009, 1.0002, 1.0013, 0.9991, 0.9996, 1.0023, 1.0003, 1.0014,\n",
       "         1.0001, 0.9986, 0.9985, 0.9991, 1.0000, 0.9978, 1.0018, 0.9962, 1.0013,\n",
       "         1.0014, 0.9992, 0.9996, 0.9998, 1.0006, 0.9990, 1.0006, 0.9966, 0.9979,\n",
       "         1.0010, 1.0027, 0.9967, 1.0032, 0.9987, 1.0004, 0.9999, 0.9975, 1.0008,\n",
       "         1.0007, 0.9970, 0.9980, 0.9993, 1.0011, 1.0002, 0.9981, 0.9984, 0.9997,\n",
       "         0.9986, 0.9992, 1.0007, 0.9985, 1.0000, 0.9984, 1.0019, 1.0009, 1.0000,\n",
       "         0.9987, 0.9987, 1.0014, 0.9988, 0.9959, 1.0033, 1.0007, 0.9997, 0.9992,\n",
       "         1.0017, 0.9982, 0.9972, 1.0021, 1.0004, 1.0006, 1.0010, 0.9993, 0.9971,\n",
       "         1.0016, 0.9997, 1.0006, 1.0050, 1.0005, 0.9984, 0.9998, 1.0032, 1.0001,\n",
       "         0.9979, 0.9990, 1.0022, 0.9997, 1.0001, 0.9990, 0.9962, 1.0021, 0.9990,\n",
       "         0.9993, 1.0004, 0.9980, 0.9987, 1.0027, 0.9986, 1.0007, 0.9965, 0.9990,\n",
       "         1.0005, 0.9990, 0.9997, 0.9989, 0.9999, 0.9987, 0.9987, 1.0015, 0.9991,\n",
       "         0.9988, 1.0022, 0.9993, 1.0001, 0.9983, 0.9988, 1.0015, 1.0012, 0.9977,\n",
       "         0.9983, 1.0006, 0.9963, 0.9989, 0.9995, 0.9987, 1.0016, 0.9984, 1.0018,\n",
       "         0.9989, 1.0015, 1.0023, 1.0011, 1.0023, 1.0022, 0.9965, 0.9992, 0.9979,\n",
       "         1.0002, 1.0005, 1.0000, 0.9964, 1.0002, 1.0038, 1.0009, 0.9994, 0.9998,\n",
       "         1.0013, 0.9982, 1.0005, 0.9990, 0.9981, 0.9985, 1.0018, 0.9987, 0.9996,\n",
       "         0.9981, 0.9998, 1.0022, 0.9994, 1.0050, 0.9997, 1.0025, 0.9995, 0.9975,\n",
       "         0.9977, 0.9982, 1.0032, 0.9966, 0.9998, 0.9994, 1.0035, 1.0003, 0.9999,\n",
       "         1.0001, 1.0022, 1.0021, 1.0011, 0.9993, 0.9965, 0.9999, 0.9990, 1.0018,\n",
       "         1.0003, 1.0006, 0.9998, 0.9988, 1.0008, 0.9992, 1.0002, 0.9986, 0.9992,\n",
       "         0.9987, 1.0015, 0.9988, 1.0004, 0.9973, 0.9997, 0.9991, 1.0025, 1.0017,\n",
       "         1.0024, 0.9978, 0.9997, 0.9989, 1.0034, 0.9994, 1.0025, 1.0004, 0.9993,\n",
       "         0.9972, 0.9990, 0.9987, 1.0010, 1.0003, 0.9977, 0.9997, 1.0000, 1.0000,\n",
       "         0.9984, 0.9987, 0.9962, 0.9986, 0.9990, 1.0021, 0.9997, 0.9989, 1.0017,\n",
       "         0.9968, 1.0033, 0.9986, 1.0003, 0.9997, 0.9976, 1.0013, 1.0007, 1.0005,\n",
       "         0.9990, 0.9979, 1.0005, 0.9986, 1.0003, 1.0008, 0.9953, 0.9981, 0.9967,\n",
       "         0.9994, 0.9997, 0.9960, 1.0022, 0.9973, 0.9996, 1.0014, 1.0015, 1.0004,\n",
       "         0.9989, 0.9980, 1.0017, 0.9985, 0.9980, 1.0007, 0.9981, 0.9977, 1.0001,\n",
       "         0.9993, 0.9993, 0.9997, 0.9963, 1.0026, 0.9982, 0.9995, 0.9987, 1.0018,\n",
       "         0.9981, 0.9989, 1.0010, 0.9998, 0.9993, 1.0002, 0.9988, 1.0049, 0.9969,\n",
       "         1.0007, 1.0006, 0.9996, 0.9976, 1.0006, 1.0027, 1.0017, 1.0033, 1.0022,\n",
       "         0.9999, 0.9968, 0.9980, 0.9979, 0.9976, 0.9972, 1.0021, 0.9996, 1.0012,\n",
       "         0.9984, 1.0003, 1.0012, 1.0015, 1.0036, 0.9985, 0.9983, 1.0004, 1.0022,\n",
       "         1.0007, 0.9982, 1.0013, 0.9992, 1.0013, 1.0001, 1.0022, 0.9975, 0.9984,\n",
       "         1.0003, 1.0011, 1.0003, 0.9989, 0.9985, 0.9998, 0.9996, 0.9952, 1.0029,\n",
       "         0.9988, 0.9979, 1.0043, 0.9994, 0.9993, 1.0015, 0.9997, 0.9965, 1.0024,\n",
       "         1.0008, 1.0001, 0.9991, 1.0016, 1.0000, 0.9997, 1.0020, 1.0022, 1.0013,\n",
       "         1.0009, 0.9986, 1.0003, 0.9985, 1.0019, 1.0026, 1.0007, 0.9996, 0.9980,\n",
       "         1.0019, 0.9983, 1.0002, 0.9975, 1.0010, 0.9979, 1.0023, 0.9971, 0.9938,\n",
       "         0.9961, 1.0008, 0.9981, 1.0013, 1.0013, 1.0018, 1.0023, 0.9957, 1.0005,\n",
       "         1.0012, 1.0016, 1.0009, 0.9975, 0.9969, 1.0016, 1.0028, 0.9981, 0.9986,\n",
       "         1.0012, 0.9997, 0.9991, 0.9974, 1.0031, 1.0017, 0.9980, 0.9972, 1.0001,\n",
       "         1.0017, 0.9990, 1.0003, 1.0004, 0.9993, 0.9993, 0.9978, 0.9969, 0.9989,\n",
       "         0.9994, 0.9995, 1.0021, 1.0003, 1.0019, 1.0012, 1.0007, 1.0015, 1.0024,\n",
       "         1.0005, 1.0004, 0.9996, 0.9997, 0.9988, 0.9981, 0.9992, 0.9978, 0.9987,\n",
       "         0.9989, 1.0008, 1.0001, 0.9984, 1.0022, 1.0009, 0.9993, 0.9988, 1.0007,\n",
       "         0.9991, 0.9973, 0.9995, 1.0012, 0.9994, 1.0020, 0.9990, 0.9976]),\n",
       " 'transformer.resblocks.7.ln_1.bias': tensor([ 2.0320e-03, -1.7922e-03,  4.0021e-04, -2.0042e-03,  2.8473e-03,\n",
       "         -1.5266e-03, -1.2667e-03, -6.5435e-04,  6.4542e-04, -4.6413e-04,\n",
       "          5.6355e-04, -3.4063e-05, -1.6121e-03, -1.7892e-03, -3.9349e-04,\n",
       "         -2.3024e-04, -1.7017e-03, -2.1490e-03, -1.2485e-03,  1.2416e-03,\n",
       "         -2.9901e-03, -2.1983e-03, -8.4973e-04, -1.7382e-03, -2.6510e-04,\n",
       "          9.4000e-04,  4.8750e-04, -4.3096e-03,  1.6892e-03,  1.0433e-03,\n",
       "          5.2640e-04,  3.5532e-03, -1.3929e-04,  1.6433e-03, -2.1684e-03,\n",
       "          1.9582e-03,  3.7676e-04,  1.5083e-03,  5.9787e-04,  6.2456e-04,\n",
       "          1.8305e-03,  1.1128e-04,  1.1021e-03, -2.6475e-03,  2.0259e-03,\n",
       "          7.1229e-04,  1.9458e-03, -1.1063e-03,  2.4447e-03, -1.3976e-03,\n",
       "          5.1040e-05, -2.3352e-03,  5.3075e-06,  1.2503e-03,  1.9241e-03,\n",
       "          2.9853e-03,  5.3544e-04, -9.2063e-04,  5.1144e-04, -5.6661e-04,\n",
       "         -2.7631e-03, -7.6363e-04,  1.0835e-03,  3.6606e-04, -2.1602e-03,\n",
       "         -2.6601e-03,  1.6795e-03, -1.1023e-04,  1.7052e-03,  2.5237e-03,\n",
       "          2.0618e-03,  2.5362e-03, -1.2015e-03, -1.6272e-03, -2.5954e-03,\n",
       "          2.8150e-03, -2.4185e-03,  7.9843e-04, -2.6343e-04, -1.8355e-03,\n",
       "          4.7094e-04,  1.6388e-04, -1.4286e-03,  1.5441e-03,  2.2073e-03,\n",
       "         -5.2407e-04, -9.7781e-04,  4.5601e-03, -2.2501e-03, -5.5466e-04,\n",
       "          1.6430e-03,  3.0208e-03, -4.9106e-04,  1.5724e-03,  1.3942e-03,\n",
       "          8.1938e-04,  4.6947e-03,  2.6876e-03,  6.8105e-04, -1.0879e-03,\n",
       "         -7.0655e-04, -2.1079e-04, -1.0355e-03, -7.6838e-04, -1.8002e-03,\n",
       "         -1.7104e-03,  2.2747e-03,  2.2033e-03, -6.6864e-04,  9.8742e-04,\n",
       "         -1.4470e-03,  1.5210e-03, -4.0560e-04,  2.0596e-03,  4.3675e-04,\n",
       "          1.1788e-03,  3.6689e-04,  1.7976e-03,  1.4027e-03,  5.8972e-04,\n",
       "          1.9000e-03,  6.6979e-04,  7.9909e-05,  2.2331e-03, -3.1835e-03,\n",
       "          2.7989e-03, -1.9552e-03,  2.2917e-04,  2.9229e-04,  3.2141e-03,\n",
       "          2.8323e-03,  2.9402e-04, -1.7882e-03, -2.4310e-03, -1.7016e-03,\n",
       "         -1.5108e-03,  5.8372e-04,  4.6397e-04,  1.4464e-03,  4.9794e-04,\n",
       "          1.1553e-03, -1.5104e-03,  5.1018e-04, -1.8409e-04, -1.9495e-03,\n",
       "         -1.3573e-03,  1.6488e-03,  4.6605e-04, -4.1101e-04,  2.3599e-03,\n",
       "         -1.1394e-03, -2.2092e-05, -1.9459e-03,  3.1797e-03,  2.0282e-03,\n",
       "          1.4837e-04,  1.6124e-03,  1.1574e-03,  3.6396e-05, -1.8564e-03,\n",
       "         -3.6733e-03, -2.5840e-03,  8.9227e-05,  8.8079e-04,  2.5561e-04,\n",
       "          1.9507e-03,  5.0797e-04,  9.6775e-04, -9.5387e-04,  2.8386e-03,\n",
       "          1.5103e-04,  5.5282e-04,  2.6098e-03,  1.4934e-04, -1.3070e-03,\n",
       "          3.2473e-04,  5.2426e-06,  1.6936e-03,  1.1577e-03,  2.1500e-03,\n",
       "         -1.5998e-03,  1.3378e-03,  1.5056e-03, -2.5531e-04, -1.9010e-03,\n",
       "          1.0827e-03,  1.7663e-03,  3.1631e-03, -8.4689e-04, -2.4377e-03,\n",
       "         -7.1471e-04, -1.1574e-03,  1.7584e-03,  1.0794e-03,  4.7235e-04,\n",
       "          6.7629e-04, -1.3178e-03, -5.7702e-04, -1.3085e-03, -1.5577e-03,\n",
       "          3.4497e-04,  1.2083e-04,  3.0203e-03,  2.8706e-03, -2.4722e-03,\n",
       "         -7.8357e-04, -6.8072e-04, -1.9080e-03,  1.7813e-03, -3.9390e-04,\n",
       "          7.1894e-04,  7.9952e-05, -4.7079e-03, -1.1275e-03,  3.7464e-03,\n",
       "          6.0170e-04, -1.5791e-03, -1.4001e-03,  5.1835e-04, -3.3510e-03,\n",
       "          1.8822e-03, -2.8967e-04,  1.8266e-03, -3.0752e-03, -1.0339e-05,\n",
       "          2.3111e-03, -5.6727e-04,  1.6198e-03,  1.2058e-04, -1.4460e-04,\n",
       "         -1.6730e-03,  1.8777e-03,  1.3640e-03,  1.1074e-03, -1.8435e-03,\n",
       "          2.5143e-04,  1.9609e-03, -6.6717e-04,  1.5774e-03,  1.0556e-03,\n",
       "         -2.2808e-03, -5.6494e-04, -1.1652e-03, -1.4393e-03,  1.9837e-03,\n",
       "          9.5355e-04, -1.5826e-03, -9.5056e-04,  1.4270e-03, -2.3088e-04,\n",
       "          1.4733e-03, -9.7105e-04, -1.7898e-03, -1.4089e-03, -2.4562e-03,\n",
       "         -4.0551e-04,  2.2200e-03,  2.3506e-03, -3.2267e-03, -5.2424e-04,\n",
       "          7.4626e-04, -2.8496e-03,  4.8047e-04,  1.5930e-03,  3.2471e-03,\n",
       "          1.5144e-03, -1.0332e-03, -1.6424e-03,  1.6735e-03,  2.9354e-03,\n",
       "          3.0607e-03,  1.1950e-03,  3.1713e-05,  5.4518e-04,  2.1375e-03,\n",
       "         -2.3552e-03, -8.2433e-04,  4.6477e-04,  5.6305e-06,  2.7753e-03,\n",
       "          2.2296e-03,  1.9213e-03, -2.2394e-03, -2.9987e-03,  3.9398e-04,\n",
       "          1.4690e-03, -3.7478e-04, -3.4393e-04, -5.7264e-04, -1.5722e-03,\n",
       "          9.5841e-04,  1.0103e-03, -1.6263e-03, -4.3260e-05,  2.3264e-03,\n",
       "          4.3947e-05, -1.7336e-03,  7.8844e-04,  2.4524e-03,  1.5455e-03,\n",
       "         -8.9441e-04, -8.9381e-04,  1.7294e-03,  1.2197e-03,  2.7186e-03,\n",
       "         -1.7272e-03,  6.7713e-04, -1.7904e-04, -2.9344e-04, -1.9920e-03,\n",
       "          1.5480e-04, -3.6945e-03,  2.5907e-04, -5.8250e-04,  3.1941e-04,\n",
       "          3.4557e-04, -1.0992e-03,  2.3617e-03,  9.1598e-04, -9.6741e-04,\n",
       "          1.4661e-03, -5.0238e-04,  2.3644e-03,  9.3185e-05, -2.1807e-03,\n",
       "          1.6821e-03, -6.8206e-04, -1.2269e-03,  5.9496e-04, -7.3095e-04,\n",
       "          2.2570e-04,  2.4384e-03, -9.9975e-04, -7.3818e-04, -2.2465e-03,\n",
       "         -1.8719e-03, -1.8095e-03, -1.3883e-03, -1.7207e-03, -2.2811e-03,\n",
       "          2.0585e-03,  3.0854e-04, -1.7601e-03, -1.8994e-03,  3.2010e-04,\n",
       "         -1.0445e-06,  2.3583e-03, -3.3365e-04, -9.1056e-04, -1.6722e-04,\n",
       "         -2.2818e-03, -8.0573e-04, -7.6916e-04, -7.6875e-04, -6.9327e-04,\n",
       "         -7.1521e-04, -1.9801e-03, -7.0758e-04, -2.0839e-03,  3.1293e-03,\n",
       "          8.4652e-04, -2.0688e-03, -1.3035e-03, -1.5473e-04,  1.3895e-03,\n",
       "          3.4974e-03, -3.2994e-03,  3.2226e-04,  2.0864e-03, -4.2771e-04,\n",
       "          2.0081e-03, -1.1319e-03,  8.3233e-04,  9.9283e-04, -6.8646e-04,\n",
       "         -2.4112e-04,  5.0449e-03, -1.7066e-03, -6.8627e-04,  2.1760e-04,\n",
       "         -4.3038e-04, -2.1232e-03, -3.6925e-03,  1.6677e-05,  1.8170e-04,\n",
       "          1.5815e-03,  2.1651e-03,  4.0138e-03, -1.8345e-03,  6.4254e-04,\n",
       "         -3.5347e-03, -2.1362e-03,  2.1708e-03, -1.1329e-03,  5.0949e-04,\n",
       "          1.1133e-03, -3.1674e-04, -2.6845e-03, -7.4443e-05,  3.6961e-03,\n",
       "         -4.2566e-04,  3.1854e-03, -2.7999e-03, -1.8135e-04, -1.6857e-05,\n",
       "         -1.9475e-03, -3.0743e-04,  2.7884e-03,  7.4274e-04,  3.8419e-04,\n",
       "          3.4648e-03,  2.2152e-03, -3.4181e-03, -7.5771e-04,  2.6286e-03,\n",
       "         -1.1446e-03, -2.6241e-03, -3.4562e-04, -4.0939e-04,  2.2500e-03,\n",
       "          2.2681e-04,  2.0187e-04,  6.4323e-04, -3.1811e-03, -5.1512e-04,\n",
       "         -3.4299e-03, -1.8116e-03,  7.0650e-04,  1.1243e-03, -2.2963e-03,\n",
       "         -2.0712e-03,  1.0474e-03,  1.7499e-03, -1.8651e-03, -7.1609e-05,\n",
       "         -4.4313e-04,  8.7182e-04, -2.9375e-04,  7.1642e-04, -4.0331e-04,\n",
       "         -2.4263e-04, -1.5559e-03,  1.3847e-03,  1.5110e-03, -1.1859e-04,\n",
       "         -9.3843e-04, -4.4505e-03, -2.9920e-03, -4.0895e-03,  7.7613e-04,\n",
       "         -2.1970e-03,  2.4108e-03, -4.6736e-03, -1.4984e-03,  2.0776e-03,\n",
       "          1.8840e-03,  3.1184e-03,  9.2320e-04,  6.9927e-04, -1.4350e-03,\n",
       "         -4.9161e-04,  3.0556e-04, -1.2497e-03,  2.0497e-04,  3.0525e-03,\n",
       "         -2.7538e-04,  2.2674e-03,  7.0017e-04,  1.1541e-03, -1.1829e-03,\n",
       "         -6.3075e-04, -4.4999e-03, -4.4160e-04, -1.0133e-03, -7.5997e-04,\n",
       "         -1.4456e-03, -2.2265e-03, -1.2515e-04,  3.8643e-03,  3.0225e-03,\n",
       "          1.7108e-03, -3.1991e-03, -6.9382e-05,  3.3899e-03, -1.8038e-03,\n",
       "         -1.6369e-03, -2.3636e-03, -7.2418e-04,  1.0089e-03,  1.8823e-03,\n",
       "          4.6688e-03,  3.9390e-03, -6.2385e-04, -1.8827e-04,  1.3847e-03,\n",
       "         -9.5296e-04,  1.8876e-03,  5.0723e-04, -3.0868e-04, -1.1132e-03,\n",
       "          1.5531e-04, -2.6300e-03, -7.8754e-04, -1.0612e-03,  7.9299e-05,\n",
       "         -1.3194e-03,  8.7837e-04, -1.9139e-03,  1.2831e-03, -1.3014e-04,\n",
       "         -1.4470e-03,  2.6933e-03]),\n",
       " 'transformer.resblocks.7.mlp.c_fc.weight': tensor([[-3.4201e-03, -5.4096e-02,  3.3410e-02,  ..., -1.0417e-02,\n",
       "           3.4700e-02,  5.2310e-02],\n",
       "         [-7.8602e-02, -4.2287e-02,  7.8941e-02,  ...,  2.8237e-02,\n",
       "          -3.9403e-02, -1.0712e-05],\n",
       "         [-3.2273e-02,  1.8318e-02,  3.9952e-02,  ...,  4.8266e-02,\n",
       "           1.0541e-03,  3.3863e-02],\n",
       "         ...,\n",
       "         [ 4.3499e-02,  1.5058e-02,  6.3088e-02,  ..., -3.5174e-03,\n",
       "          -1.0595e-02,  4.1267e-02],\n",
       "         [-1.0215e-02,  6.6352e-03,  4.6744e-02,  ...,  5.1681e-02,\n",
       "           1.0934e-03,  2.5526e-02],\n",
       "         [ 1.1569e-02, -6.4613e-02, -1.8136e-02,  ...,  8.4668e-03,\n",
       "           1.4296e-02,  1.3836e-02]]),\n",
       " 'transformer.resblocks.7.mlp.c_fc.bias': tensor([-0.0103,  0.0292, -0.0300,  ...,  0.0288,  0.0279,  0.0334]),\n",
       " 'transformer.resblocks.7.mlp.c_proj.weight': tensor([[ 0.0138,  0.0103, -0.0002,  ...,  0.0065,  0.0033,  0.0119],\n",
       "         [-0.0010,  0.0044, -0.0035,  ..., -0.0047, -0.0030, -0.0065],\n",
       "         [ 0.0042, -0.0024,  0.0130,  ...,  0.0015, -0.0018, -0.0065],\n",
       "         ...,\n",
       "         [-0.0183,  0.0206,  0.0109,  ..., -0.0073, -0.0006, -0.0237],\n",
       "         [-0.0031,  0.0137, -0.0038,  ...,  0.0125, -0.0141, -0.0101],\n",
       "         [-0.0114,  0.0067,  0.0065,  ...,  0.0077, -0.0027, -0.0156]]),\n",
       " 'transformer.resblocks.7.mlp.c_proj.bias': tensor([ 7.7501e-03,  1.1521e-02,  4.4149e-03, -9.0982e-03, -8.7880e-03,\n",
       "          1.1039e-03,  5.9313e-04, -8.4914e-03, -1.8308e-02,  2.1268e-02,\n",
       "          3.8509e-03,  4.5741e-03,  7.3159e-03, -2.8994e-03, -2.1733e-02,\n",
       "          2.2327e-02,  7.1907e-03,  6.6275e-03, -1.1074e-02,  1.7205e-02,\n",
       "         -4.3663e-03, -1.4299e-02, -2.0827e-02, -1.7923e-02, -7.7432e-03,\n",
       "         -1.3869e-02, -2.4158e-03, -8.2610e-03,  5.8217e-03,  7.1557e-03,\n",
       "         -9.7962e-03,  4.2627e-03, -1.3086e-03, -2.2180e-02, -6.0731e-03,\n",
       "          2.3310e-02, -3.9683e-03,  9.9008e-03, -1.3973e-02, -2.1857e-03,\n",
       "          1.7490e-03, -1.5042e-03, -6.6040e-03, -2.0174e-03, -1.9926e-02,\n",
       "         -4.5032e-03, -2.0696e-02, -1.1382e-04,  8.3889e-03, -6.8034e-03,\n",
       "          6.6005e-03, -4.5885e-03, -1.0428e-02,  2.2848e-02, -1.7033e-02,\n",
       "         -1.5698e-02,  8.9810e-03, -1.8110e-02, -9.7176e-04, -1.7857e-02,\n",
       "          1.9521e-02,  1.5516e-02,  1.6791e-02, -1.1015e-02,  5.5459e-03,\n",
       "         -1.8475e-02,  1.7124e-02,  1.0496e-02,  1.9909e-02,  4.4970e-03,\n",
       "         -2.1162e-02, -2.0320e-02,  1.4425e-02, -1.4811e-02,  1.4827e-02,\n",
       "         -1.3113e-02,  1.6913e-02, -1.7170e-02,  1.5700e-02,  2.0784e-02,\n",
       "         -5.1159e-03, -1.0723e-02,  1.4881e-02,  1.5214e-02,  1.8050e-02,\n",
       "          1.7932e-02,  1.9036e-02,  1.0550e-02,  4.8186e-03,  6.1304e-03,\n",
       "          1.3763e-02, -6.9763e-03, -1.4023e-02,  1.9316e-02, -1.1575e-02,\n",
       "          1.3268e-02, -1.2515e-02,  1.3522e-03,  6.9298e-03,  1.8155e-02,\n",
       "          1.3659e-04, -1.7673e-02,  1.6747e-02,  8.1338e-03, -2.1262e-02,\n",
       "          1.1697e-03,  6.2510e-03,  1.0447e-02,  5.2362e-03,  5.7210e-03,\n",
       "         -1.2718e-03, -1.1315e-03,  9.7783e-03,  1.1301e-02,  1.9058e-02,\n",
       "          8.2059e-03,  1.9673e-02,  1.6166e-02, -5.9793e-04, -7.7103e-03,\n",
       "         -1.7375e-02,  8.0357e-03, -9.6775e-03,  7.6598e-04,  8.9598e-03,\n",
       "         -3.0041e-03, -2.2593e-03,  1.7749e-02, -2.2413e-03, -2.2047e-02,\n",
       "         -1.9656e-02,  1.0989e-02,  1.2407e-02, -1.6272e-02, -1.0330e-02,\n",
       "         -5.5695e-03,  1.4602e-02,  2.9028e-03, -2.2635e-02, -3.3909e-03,\n",
       "          1.5721e-02,  2.3120e-02, -1.2798e-03,  2.2009e-02, -9.3337e-03,\n",
       "          1.9146e-03,  4.4618e-03,  2.2540e-02,  1.6400e-02, -7.1988e-03,\n",
       "          1.7423e-02,  1.3282e-02,  2.1873e-02,  1.5743e-02, -1.1909e-02,\n",
       "         -1.9593e-02,  1.9891e-02,  1.9419e-02,  1.5865e-02,  1.3180e-02,\n",
       "          1.6509e-02,  6.7229e-03, -1.0194e-02,  1.5659e-02, -1.9705e-02,\n",
       "          1.1314e-02,  1.7812e-02, -2.2459e-02,  7.5248e-03,  9.1068e-03,\n",
       "          1.4766e-02,  3.1895e-03, -5.8879e-03,  8.8066e-03, -2.7366e-03,\n",
       "         -4.6823e-03,  1.7506e-02, -1.8959e-02,  1.2891e-02,  1.5334e-02,\n",
       "          1.6621e-03,  4.7301e-03, -5.7934e-03, -1.8350e-02,  4.9664e-03,\n",
       "          4.4580e-04, -2.0371e-02, -5.5473e-03, -1.7022e-02, -8.6679e-03,\n",
       "         -1.0861e-02,  8.3696e-03, -3.9068e-03,  1.1415e-02,  5.4639e-03,\n",
       "          3.7666e-03,  2.0058e-03,  6.5812e-04,  2.0339e-02,  2.2863e-02,\n",
       "          1.3557e-02, -2.1282e-02, -1.6661e-02,  2.8513e-03, -8.6451e-03,\n",
       "         -1.5118e-02, -8.9492e-03, -6.3042e-03, -1.8264e-02, -2.1557e-03,\n",
       "          1.1821e-02, -1.1662e-02, -1.6129e-02,  2.1142e-02, -1.3449e-02,\n",
       "         -2.2120e-03,  1.6930e-02,  3.6893e-03,  3.7194e-05, -5.5222e-03,\n",
       "         -1.5688e-03, -9.3621e-03, -5.1144e-03,  1.6690e-02,  1.6354e-02,\n",
       "         -1.3592e-02,  4.9145e-03, -5.5035e-03, -4.5252e-03, -4.3751e-03,\n",
       "         -1.8311e-02, -1.6729e-02,  6.0467e-04,  1.5372e-02,  1.1984e-02,\n",
       "         -5.5605e-04,  1.9998e-02, -2.4433e-02,  1.2512e-02, -1.2873e-02,\n",
       "         -1.3957e-02, -8.7496e-03, -1.6302e-02,  8.1679e-03,  1.3372e-02,\n",
       "         -5.0703e-03,  1.3492e-02,  9.1404e-03,  1.1271e-02, -1.5602e-02,\n",
       "         -3.3182e-03, -5.3828e-03, -3.8672e-03,  2.0195e-02, -1.9042e-02,\n",
       "         -1.3063e-02,  1.6067e-02, -6.9351e-03,  1.7557e-02, -9.8479e-03,\n",
       "         -8.0518e-04,  1.1461e-02, -2.9958e-03, -1.9208e-02,  1.5356e-02,\n",
       "         -5.5010e-03,  1.7357e-02, -1.1552e-02, -2.2147e-02, -2.3466e-02,\n",
       "         -6.9565e-03,  1.5793e-02,  1.5664e-02,  1.6774e-02,  2.0971e-02,\n",
       "         -4.2417e-03, -1.7514e-02,  1.8050e-02, -1.3049e-02,  1.1857e-02,\n",
       "         -4.6650e-03, -1.3092e-03, -2.1130e-02, -1.7289e-02, -1.9797e-02,\n",
       "          1.8274e-02,  2.0327e-02, -1.9819e-02,  1.8895e-02, -1.3756e-03,\n",
       "          9.5653e-03,  5.7632e-03,  5.1888e-04,  1.4526e-02, -1.5052e-02,\n",
       "          2.3370e-02,  1.1136e-02,  1.6466e-02,  1.2197e-02, -1.4254e-02,\n",
       "          1.6626e-02,  1.0272e-02,  1.5809e-02,  1.4578e-02,  1.3714e-02,\n",
       "          5.9540e-03, -7.7550e-04,  2.0382e-02,  9.2402e-04, -6.9004e-03,\n",
       "          3.1039e-04, -1.2745e-02, -2.0325e-02, -1.8291e-03, -1.1005e-02,\n",
       "          3.5538e-03,  3.0309e-03, -1.4370e-02,  1.8563e-02,  2.0403e-03,\n",
       "          1.0716e-02, -5.5673e-03, -8.4243e-03, -7.8701e-03, -1.3654e-02,\n",
       "         -8.6774e-03, -1.5651e-03, -1.0711e-02, -1.0042e-02, -2.5391e-03,\n",
       "          1.6916e-02, -2.0346e-03, -2.1085e-02, -2.1297e-02,  8.1231e-03,\n",
       "         -1.9422e-02,  1.1655e-02, -4.1244e-03, -1.6554e-02,  9.8880e-03,\n",
       "         -1.1424e-02,  4.1030e-05,  1.0042e-02,  2.0256e-02, -3.6828e-03,\n",
       "         -1.6749e-02, -4.3920e-03,  1.1353e-02,  1.8793e-02,  1.1024e-02,\n",
       "          8.8371e-03,  1.1338e-03,  1.2558e-02,  9.5333e-03,  5.7636e-03,\n",
       "          1.6392e-02,  6.0834e-03,  2.1668e-02, -9.4733e-03, -5.0338e-03,\n",
       "         -1.8313e-03, -2.1716e-02,  2.3168e-02,  4.4032e-04,  1.6471e-02,\n",
       "         -1.5926e-02,  1.2098e-03,  2.7813e-03,  1.6144e-02,  1.2215e-02,\n",
       "         -2.3641e-03, -1.7897e-02, -1.2949e-02, -6.5923e-03,  1.7077e-02,\n",
       "         -5.6415e-06, -6.9492e-03,  5.5361e-03,  6.1453e-03, -9.2794e-03,\n",
       "         -1.3834e-02,  1.4012e-02,  4.6694e-03,  1.8115e-02, -2.1806e-02,\n",
       "          1.0293e-02,  2.2468e-02, -9.7537e-03, -1.6200e-02, -1.1292e-02,\n",
       "         -5.5068e-03,  1.1505e-02, -1.6263e-03, -8.8299e-04,  1.0110e-02,\n",
       "         -1.6524e-03, -1.7737e-02, -1.3312e-02, -1.5228e-02,  1.3871e-02,\n",
       "          3.8842e-03, -8.8679e-03,  1.7775e-02,  1.7968e-02,  1.0917e-02,\n",
       "          1.6111e-02,  3.6764e-03, -1.5971e-02, -1.6030e-02, -2.6048e-03,\n",
       "         -8.5848e-03,  1.0080e-02, -1.8167e-03,  1.0220e-02,  6.4939e-03,\n",
       "         -1.9162e-02,  2.0504e-02,  6.2166e-03, -8.9227e-03, -1.2971e-03,\n",
       "         -2.2492e-02, -7.8262e-03, -1.7859e-02, -1.1599e-02, -1.3822e-02,\n",
       "          1.8604e-02,  1.1188e-02,  7.2233e-03, -7.3859e-03,  1.6045e-02,\n",
       "          2.2091e-03, -7.5225e-03,  4.9921e-04, -2.2155e-02, -1.7904e-02,\n",
       "          1.8160e-02, -1.1758e-02, -2.2378e-02,  1.9612e-02, -6.5366e-03,\n",
       "         -1.6161e-02,  5.5155e-03,  1.7024e-02,  7.1478e-03, -7.9722e-03,\n",
       "          2.0189e-03, -5.4247e-03,  1.3208e-02,  1.5061e-02,  1.4764e-02,\n",
       "          1.4497e-02, -7.1008e-03,  1.9598e-02,  2.5882e-02,  8.3715e-03,\n",
       "          1.5412e-02, -8.2332e-03,  2.2013e-02,  2.2643e-02,  1.5311e-03,\n",
       "          8.2362e-03,  8.1234e-03,  8.1466e-03,  1.3338e-02,  1.4934e-02,\n",
       "          6.4060e-03,  1.3413e-02, -8.2868e-03, -1.7086e-02,  2.5803e-03,\n",
       "         -2.0614e-03, -1.4570e-02,  3.2954e-03,  4.8009e-03, -1.0354e-02,\n",
       "         -2.0329e-03, -8.4972e-03,  1.7939e-02,  9.5257e-03,  6.2259e-03,\n",
       "         -7.3757e-03, -8.6740e-03, -1.1874e-02, -1.7982e-02, -2.1480e-02,\n",
       "          6.6930e-03, -6.0872e-03,  1.6493e-02,  2.1069e-02,  1.5428e-02,\n",
       "          2.0733e-02, -5.5872e-03,  2.1745e-03, -1.6835e-02,  1.4912e-02,\n",
       "          9.0465e-03,  7.3972e-04, -1.4034e-03,  1.5362e-02, -2.3037e-02,\n",
       "         -9.8951e-03, -2.1249e-02, -1.6253e-02,  5.0940e-03, -6.9756e-03,\n",
       "         -9.4507e-03, -1.0201e-03, -1.9022e-02,  1.8077e-02,  9.9246e-03,\n",
       "         -1.6853e-02, -2.0082e-02]),\n",
       " 'transformer.resblocks.7.ln_2.weight': tensor([1.0057, 0.9982, 0.9996, 1.0025, 1.0004, 0.9998, 1.0026, 1.0013, 0.9995,\n",
       "         1.0026, 0.9975, 1.0010, 1.0004, 1.0015, 0.9959, 1.0023, 1.0017, 1.0030,\n",
       "         1.0036, 1.0028, 1.0044, 1.0007, 1.0037, 1.0035, 1.0037, 1.0039, 1.0045,\n",
       "         0.9981, 1.0058, 1.0000, 1.0024, 1.0004, 0.9980, 0.9998, 1.0020, 0.9999,\n",
       "         1.0006, 1.0053, 1.0004, 1.0017, 1.0035, 1.0047, 1.0038, 1.0022, 0.9966,\n",
       "         1.0014, 0.9969, 1.0038, 1.0020, 1.0011, 0.9996, 1.0027, 1.0029, 1.0013,\n",
       "         0.9980, 1.0030, 0.9997, 1.0031, 0.9989, 1.0003, 1.0007, 0.9980, 1.0006,\n",
       "         1.0026, 1.0025, 1.0024, 1.0013, 1.0023, 1.0020, 1.0031, 1.0015, 1.0003,\n",
       "         1.0004, 1.0022, 1.0033, 1.0045, 0.9985, 1.0036, 1.0009, 1.0016, 1.0020,\n",
       "         0.9990, 1.0024, 0.9998, 1.0015, 1.0020, 1.0020, 1.0029, 0.9995, 1.0008,\n",
       "         1.0009, 1.0042, 1.0003, 1.0040, 1.0003, 0.9973, 0.9983, 0.9983, 1.0017,\n",
       "         1.0022, 1.0044, 1.0007, 1.0033, 1.0000, 1.0072, 0.9978, 0.9990, 1.0011,\n",
       "         1.0001, 1.0035, 0.9998, 1.0021, 1.0006, 0.9990, 1.0033, 0.9977, 0.9995,\n",
       "         1.0009, 1.0030, 0.9998, 1.0030, 1.0020, 1.0024, 1.0041, 1.0031, 1.0011,\n",
       "         1.0007, 0.9990, 1.0007, 1.0049, 1.0037, 0.9992, 0.9996, 0.9982, 1.0007,\n",
       "         1.0032, 0.9993, 0.9991, 1.0038, 0.9997, 1.0003, 1.0010, 1.0020, 1.0000,\n",
       "         1.0038, 0.9984, 0.9993, 0.9984, 1.0041, 1.0020, 1.0004, 1.0007, 0.9964,\n",
       "         1.0006, 1.0002, 0.9993, 0.9980, 1.0015, 1.0019, 1.0053, 1.0008, 1.0007,\n",
       "         1.0001, 1.0023, 1.0033, 1.0012, 1.0029, 0.9985, 1.0037, 1.0026, 1.0025,\n",
       "         1.0015, 1.0012, 1.0054, 1.0006, 1.0005, 0.9999, 1.0035, 1.0036, 0.9995,\n",
       "         1.0038, 1.0028, 1.0035, 0.9978, 0.9973, 0.9988, 1.0012, 1.0009, 1.0004,\n",
       "         0.9992, 1.0008, 0.9978, 1.0023, 1.0035, 0.9993, 1.0024, 1.0020, 0.9995,\n",
       "         0.9992, 0.9991, 1.0021, 1.0008, 1.0032, 1.0045, 0.9989, 0.9996, 1.0017,\n",
       "         1.0062, 0.9982, 0.9978, 1.0018, 1.0026, 1.0014, 1.0012, 1.0007, 1.0031,\n",
       "         1.0022, 1.0021, 1.0003, 0.9991, 1.0038, 1.0015, 1.0038, 1.0055, 1.0064,\n",
       "         1.0030, 0.9990, 1.0027, 1.0026, 0.9985, 1.0014, 1.0000, 1.0039, 1.0030,\n",
       "         1.0006, 0.9985, 0.9991, 0.9976, 1.0017, 1.0005, 1.0028, 1.0002, 1.0025,\n",
       "         1.0016, 0.9997, 1.0028, 1.0022, 0.9994, 0.9966, 1.0030, 1.0022, 1.0004,\n",
       "         0.9983, 0.9967, 1.0016, 1.0024, 1.0009, 1.0004, 1.0011, 0.9984, 1.0032,\n",
       "         0.9998, 1.0024, 1.0010, 1.0010, 1.0012, 1.0012, 1.0045, 1.0003, 1.0029,\n",
       "         1.0011, 1.0032, 1.0030, 1.0018, 1.0008, 0.9996, 1.0016, 1.0015, 1.0016,\n",
       "         1.0011, 1.0011, 0.9991, 1.0006, 1.0035, 1.0028, 0.9990, 1.0019, 1.0002,\n",
       "         1.0018, 0.9996, 1.0020, 1.0004, 1.0018, 0.9992, 1.0026, 1.0058, 0.9988,\n",
       "         1.0022, 1.0004, 1.0018, 1.0022, 1.0053, 1.0032, 0.9962, 0.9997, 1.0010,\n",
       "         1.0010, 1.0047, 0.9986, 0.9990, 1.0017, 0.9969, 1.0048, 1.0059, 1.0029,\n",
       "         0.9982, 1.0025, 1.0011, 1.0031, 1.0024, 1.0033, 1.0028, 1.0006, 1.0023,\n",
       "         1.0016, 1.0019, 1.0009, 1.0003, 0.9964, 0.9977, 1.0028, 1.0019, 1.0005,\n",
       "         1.0042, 1.0008, 1.0049, 1.0065, 0.9987, 1.0030, 1.0018, 0.9992, 0.9996,\n",
       "         0.9997, 1.0049, 0.9991, 1.0054, 0.9997, 1.0015, 1.0024, 1.0017, 1.0016,\n",
       "         0.9972, 1.0002, 0.9995, 1.0039, 1.0039, 1.0015, 0.9996, 1.0006, 1.0026,\n",
       "         1.0047, 1.0018, 0.9991, 0.9998, 1.0028, 0.9972, 1.0045, 1.0002, 1.0003,\n",
       "         1.0005, 0.9979, 1.0005, 1.0055, 0.9974, 1.0035, 0.9975, 1.0003, 1.0002,\n",
       "         1.0016, 1.0040, 0.9964, 0.9983, 1.0016, 1.0038, 1.0000, 1.0005, 0.9975,\n",
       "         1.0016, 1.0023, 1.0018, 1.0016, 1.0036, 1.0027, 1.0022, 0.9992, 1.0027,\n",
       "         1.0016, 1.0009, 1.0018, 1.0027, 1.0010, 1.0010, 1.0022, 1.0026, 1.0014,\n",
       "         1.0018, 1.0034, 1.0022, 0.9998, 1.0005, 1.0040, 0.9972, 1.0026, 1.0013,\n",
       "         1.0018, 0.9993, 1.0033, 1.0020, 1.0001, 0.9995, 1.0006, 1.0000, 1.0043,\n",
       "         1.0011, 0.9989, 1.0042, 1.0051, 1.0073, 1.0006, 1.0005, 0.9981, 1.0025,\n",
       "         1.0026, 1.0015, 0.9981, 1.0004, 0.9977, 0.9999, 1.0007, 1.0008, 1.0011,\n",
       "         1.0015, 1.0043, 1.0038, 1.0018, 0.9990, 1.0003, 1.0015, 1.0021, 1.0027,\n",
       "         1.0004, 0.9988, 1.0027, 0.9972, 1.0001, 1.0042, 1.0038, 1.0009, 0.9998,\n",
       "         1.0006, 1.0029, 1.0010, 1.0024, 1.0022, 1.0012, 1.0030, 1.0039, 1.0000,\n",
       "         1.0053, 1.0013, 0.9991, 1.0009, 0.9984, 1.0057, 1.0007, 1.0033, 1.0033,\n",
       "         0.9989, 0.9979, 1.0038, 1.0012, 1.0037, 0.9987, 1.0009, 1.0018, 1.0025,\n",
       "         1.0015, 0.9992, 1.0031, 0.9997, 0.9984, 0.9987, 1.0048, 1.0064, 1.0035,\n",
       "         1.0017, 0.9985, 1.0032, 1.0004, 0.9999, 1.0004, 1.0025, 0.9959, 0.9980,\n",
       "         0.9999, 1.0022, 1.0014, 0.9992, 1.0006, 1.0022, 1.0011, 1.0006]),\n",
       " 'transformer.resblocks.7.ln_2.bias': tensor([ 1.9120e-03,  1.2492e-03, -5.8806e-04,  8.4214e-04, -6.3248e-04,\n",
       "          8.7263e-04,  5.6638e-04, -1.6202e-03,  4.6000e-04, -1.5942e-03,\n",
       "          5.1197e-03, -1.7961e-03,  1.4005e-03,  1.8323e-03,  3.5560e-03,\n",
       "          1.9761e-04,  9.4254e-04, -3.5774e-04,  1.6101e-03,  2.4259e-04,\n",
       "         -2.3225e-03, -2.2924e-03, -7.7881e-04, -1.8179e-03, -1.8751e-03,\n",
       "          1.9847e-03,  2.4645e-04,  8.9932e-04, -1.8609e-03, -1.2666e-03,\n",
       "         -2.7455e-03, -5.7041e-04, -2.5682e-03,  1.7253e-03, -1.2598e-03,\n",
       "         -1.2743e-03, -5.7353e-04,  3.3213e-03, -3.9758e-04,  8.5673e-04,\n",
       "          1.3007e-04,  1.0920e-03, -1.4911e-03, -1.7997e-03,  6.4929e-03,\n",
       "         -5.2686e-04,  3.0211e-03,  2.0728e-03,  1.9070e-03, -3.8237e-03,\n",
       "         -1.3407e-04, -1.2122e-03, -1.1095e-04, -4.5896e-05, -4.4074e-03,\n",
       "         -3.7924e-03,  5.1420e-04,  1.4520e-03,  1.9568e-03,  9.1357e-04,\n",
       "          3.6880e-03,  1.8749e-03,  5.3206e-04, -5.6108e-04,  1.2993e-03,\n",
       "          3.0152e-03, -1.3803e-03,  2.9151e-03, -5.1207e-05,  7.1753e-04,\n",
       "         -1.7015e-03,  2.0345e-04, -7.3411e-04,  1.2014e-03,  1.0551e-03,\n",
       "         -2.8677e-03,  1.8198e-03, -7.5679e-04,  2.7250e-03,  1.5404e-03,\n",
       "         -2.4561e-03,  7.5497e-04, -1.4155e-03,  1.5603e-05, -4.9766e-04,\n",
       "         -1.5393e-03,  2.5770e-03,  3.9162e-04,  1.2604e-04,  6.5901e-04,\n",
       "         -1.8593e-03, -3.3863e-04,  1.1151e-03,  2.2445e-03, -6.9571e-04,\n",
       "          1.7082e-03, -4.3673e-04, -2.9651e-04, -8.0461e-04,  5.6500e-04,\n",
       "         -1.3454e-03,  2.4969e-03,  1.0694e-03, -4.3777e-04, -7.3547e-04,\n",
       "          1.5813e-03,  2.9946e-03,  3.5659e-04, -5.9267e-04, -3.1411e-05,\n",
       "         -2.8763e-03, -1.5256e-03, -3.6360e-03, -7.4161e-04, -3.1873e-04,\n",
       "          4.0024e-03, -9.1847e-04,  1.2296e-03,  1.5498e-03,  1.7173e-03,\n",
       "          9.1786e-04,  3.5519e-03, -1.1775e-03,  4.0397e-04, -9.0017e-04,\n",
       "          1.4531e-03,  3.2710e-04,  1.2749e-04, -1.9136e-04,  3.0239e-03,\n",
       "         -3.7914e-04, -2.5354e-03,  7.2393e-04,  2.2390e-03,  2.7021e-03,\n",
       "          1.4682e-03,  6.0539e-04, -4.0369e-03, -1.5326e-03, -6.7095e-04,\n",
       "         -8.7197e-04, -2.3181e-03,  4.6580e-04, -1.0428e-03, -1.8766e-03,\n",
       "         -3.5411e-03,  1.3414e-03, -1.6377e-03,  2.8507e-03, -2.8273e-04,\n",
       "         -1.8367e-03, -7.9291e-04, -5.2094e-03, -2.1215e-03,  2.7875e-03,\n",
       "          2.0541e-03, -1.3425e-04,  1.1698e-03,  3.0444e-04, -2.5967e-03,\n",
       "          6.1871e-04, -2.4468e-04, -1.2429e-03, -2.2461e-03, -2.0541e-03,\n",
       "         -1.5016e-03, -1.7050e-04, -2.6099e-04, -3.6723e-04, -5.5588e-04,\n",
       "         -8.3147e-04, -4.2610e-04, -1.0464e-03,  1.8815e-04, -1.2253e-03,\n",
       "          1.6486e-03,  1.6133e-03, -1.7371e-03,  8.3811e-04,  2.2423e-03,\n",
       "          2.8407e-05, -4.6400e-04, -1.4318e-03,  2.9455e-03, -3.7566e-03,\n",
       "         -1.9562e-04,  2.2581e-03, -2.2640e-03,  1.0601e-03,  1.8086e-03,\n",
       "         -5.4166e-04,  8.8206e-05, -2.1155e-04, -1.3451e-03,  2.3655e-03,\n",
       "          5.6030e-04, -8.2784e-04, -2.1690e-03, -2.9484e-03, -2.5504e-03,\n",
       "          8.0360e-04,  1.2431e-03,  1.9707e-03,  5.4794e-05,  1.7710e-03,\n",
       "         -2.9335e-03,  5.0606e-04, -3.1616e-03,  5.4617e-04,  2.0311e-03,\n",
       "         -1.5369e-03,  9.3986e-04, -9.7640e-04, -8.1794e-04,  1.3674e-03,\n",
       "          2.4387e-03,  1.9163e-03,  1.3560e-03, -1.4945e-04, -1.9651e-04,\n",
       "          1.0527e-03, -1.4578e-03,  7.3006e-04, -4.4798e-04, -6.7746e-04,\n",
       "         -4.4904e-04,  4.5711e-03,  1.3912e-03, -1.8513e-03, -3.0024e-03,\n",
       "          6.6640e-04, -1.5043e-03,  5.0375e-04,  4.4867e-03,  2.3527e-03,\n",
       "         -1.3393e-03, -3.6365e-03,  2.0285e-03, -3.3149e-03, -2.1530e-03,\n",
       "          2.7684e-03, -1.9653e-03,  4.6456e-04,  8.9723e-04, -9.2491e-04,\n",
       "          9.6960e-04, -9.4532e-04, -4.4090e-04,  2.9413e-03, -2.3455e-04,\n",
       "         -2.9623e-03,  1.2482e-03,  1.7868e-03,  4.8039e-04, -2.3751e-03,\n",
       "         -7.9053e-05, -9.7774e-04,  7.9209e-04, -2.4311e-03,  3.4949e-03,\n",
       "         -1.9601e-03, -3.6739e-03, -8.2952e-04, -5.2944e-04,  4.4487e-04,\n",
       "          3.8432e-04,  1.1598e-03, -6.8343e-05, -8.1613e-04,  3.0847e-03,\n",
       "         -8.4628e-04,  3.3119e-04,  5.4071e-04, -1.4910e-03,  1.1351e-03,\n",
       "         -1.1661e-03, -9.5368e-04, -9.0554e-04,  5.3729e-04,  9.2795e-04,\n",
       "         -9.4316e-04, -2.0839e-03,  1.7247e-03,  9.7889e-04,  2.1270e-03,\n",
       "         -2.1545e-03, -9.3409e-04,  4.7550e-04, -1.1113e-03,  2.2536e-03,\n",
       "          1.0413e-04,  9.0252e-05,  2.3343e-03, -2.2110e-03,  2.3065e-04,\n",
       "         -1.2827e-03,  7.2048e-04,  2.6654e-04, -2.7386e-04,  2.2993e-04,\n",
       "         -7.8872e-04, -5.0626e-04, -1.2525e-03,  1.8430e-03,  2.2162e-03,\n",
       "         -1.8582e-03, -7.1528e-04,  1.3029e-03, -2.3908e-03,  2.7339e-03,\n",
       "         -9.1241e-04, -5.4813e-03,  6.0945e-04, -2.2711e-03, -5.2585e-05,\n",
       "         -2.6648e-03, -2.2443e-04,  1.0455e-03, -1.0759e-03,  2.3658e-03,\n",
       "          4.3654e-04,  1.6060e-03,  4.4394e-05, -1.1013e-03,  3.4354e-03,\n",
       "          1.8467e-03, -3.6484e-03,  2.7788e-03,  5.5702e-03,  1.3736e-03,\n",
       "         -1.4555e-03,  8.0085e-04, -8.4502e-04,  1.3127e-03,  2.8132e-03,\n",
       "          1.3430e-03, -2.1127e-03,  1.9602e-05,  2.0588e-03,  1.2084e-05,\n",
       "          1.3575e-03, -4.1643e-03,  2.8339e-03,  2.8535e-04,  1.3957e-03,\n",
       "          1.0561e-03,  1.0192e-03, -1.8807e-03,  2.8019e-03,  3.1152e-04,\n",
       "          2.3694e-03,  2.6905e-03,  3.1683e-03, -5.6225e-04, -1.7797e-03,\n",
       "         -3.3340e-04,  1.3396e-03, -2.6562e-03, -4.3867e-05, -1.5814e-04,\n",
       "          3.8643e-04, -2.6260e-04, -2.1966e-03,  8.9152e-04,  7.5778e-04,\n",
       "         -2.8088e-03,  2.4761e-03,  2.6303e-03, -2.8053e-03,  1.0967e-03,\n",
       "          2.1238e-03, -3.2309e-04,  7.6329e-04,  8.5271e-04,  1.5480e-03,\n",
       "         -2.1446e-03, -5.9337e-04, -9.8239e-04,  1.4395e-03, -3.0633e-04,\n",
       "         -3.5888e-03, -2.1517e-03, -9.4621e-04,  2.2264e-03, -1.0735e-03,\n",
       "          6.7853e-04, -1.0057e-03, -3.9307e-04, -1.4717e-03, -4.1628e-04,\n",
       "         -4.2796e-04, -7.2240e-04, -3.5783e-04,  2.1011e-03, -1.7636e-03,\n",
       "          6.3273e-04,  5.8108e-04, -5.3317e-04, -3.6578e-04,  7.6042e-04,\n",
       "         -7.3929e-04,  1.5052e-03, -2.2562e-03,  1.9232e-04, -2.7005e-03,\n",
       "          6.3607e-04,  7.0060e-04,  2.3685e-03, -5.5065e-04, -4.0131e-04,\n",
       "          4.6369e-04, -4.5697e-04,  4.0042e-05, -1.4553e-03, -6.3581e-04,\n",
       "          2.3597e-03, -2.2494e-04, -2.3406e-03,  3.5790e-03, -1.6609e-03,\n",
       "          1.1873e-03, -2.1310e-03, -1.5300e-03,  9.6360e-04,  8.2269e-04,\n",
       "          5.1708e-05,  8.8518e-04,  2.2833e-03, -2.1042e-03, -1.0700e-03,\n",
       "          2.8675e-03, -1.3462e-03, -1.0593e-04,  9.5254e-04,  4.5843e-03,\n",
       "         -3.3589e-03, -3.7919e-03,  2.9196e-03,  5.4165e-04, -2.0252e-03,\n",
       "          3.7900e-03, -3.3844e-03, -4.9694e-03, -1.4083e-03,  6.0636e-04,\n",
       "          5.5152e-05,  6.1362e-04, -1.1180e-03,  7.0835e-04, -1.4174e-03,\n",
       "          1.6602e-03, -1.8444e-03,  1.7963e-03, -2.1529e-03, -1.5856e-03,\n",
       "          1.5749e-03, -2.4170e-04, -1.2826e-03, -3.7903e-05,  2.8740e-04,\n",
       "         -1.9094e-03,  3.7335e-04, -1.0764e-03, -2.9125e-03, -2.1148e-03,\n",
       "          4.2542e-03,  4.7824e-04,  1.7564e-04,  3.9243e-04, -4.5776e-04,\n",
       "          8.1830e-04,  1.0525e-03,  2.5196e-03,  2.2938e-03,  1.2798e-03,\n",
       "          1.4187e-03,  1.5529e-05,  2.6297e-03,  3.3451e-03, -6.0974e-04,\n",
       "          3.0147e-03,  1.8991e-03, -2.9369e-03, -1.1343e-03,  1.1698e-03,\n",
       "          1.4808e-03,  3.9475e-04,  8.1485e-04,  7.4996e-05, -2.2718e-03,\n",
       "         -1.1164e-03,  4.9322e-03,  8.3725e-04, -1.9057e-03, -2.8127e-03,\n",
       "         -2.2218e-03, -1.5169e-03,  1.6441e-03,  2.8409e-04, -6.3511e-04,\n",
       "          3.4977e-03,  1.2757e-03,  4.4879e-03, -5.7025e-03,  4.0898e-03,\n",
       "          2.1438e-03,  2.8192e-03, -1.3882e-04, -2.0930e-03,  3.9915e-06,\n",
       "          1.4398e-03,  1.6673e-04]),\n",
       " 'transformer.resblocks.8.attn.in_proj_weight': tensor([[-0.0199, -0.0390, -0.0022,  ..., -0.0306, -0.0280,  0.0313],\n",
       "         [-0.0130, -0.0180, -0.0315,  ..., -0.0592,  0.0135,  0.0946],\n",
       "         [-0.0206, -0.0341, -0.0170,  ...,  0.0402, -0.0132,  0.0429],\n",
       "         ...,\n",
       "         [ 0.0031,  0.0033,  0.0183,  ..., -0.0062,  0.0098,  0.0114],\n",
       "         [ 0.0697,  0.0272,  0.0701,  ..., -0.0113, -0.0014, -0.0295],\n",
       "         [-0.0572, -0.0092,  0.0140,  ...,  0.0052, -0.0153,  0.0539]]),\n",
       " 'transformer.resblocks.8.attn.in_proj_bias': tensor([ 0.0043, -0.0022,  0.0048,  ..., -0.0025, -0.0009,  0.0001]),\n",
       " 'transformer.resblocks.8.attn.out_proj.weight': tensor([[ 0.0059, -0.0032, -0.0047,  ..., -0.0055,  0.0032,  0.0037],\n",
       "         [ 0.0069,  0.0075, -0.0084,  ..., -0.0043,  0.0005, -0.0272],\n",
       "         [ 0.0033,  0.0053,  0.0043,  ..., -0.0025, -0.0042, -0.0060],\n",
       "         ...,\n",
       "         [-0.0046, -0.0151, -0.0138,  ..., -0.0019,  0.0065,  0.0005],\n",
       "         [-0.0104, -0.0096, -0.0082,  ...,  0.0001,  0.0028, -0.0081],\n",
       "         [-0.0031, -0.0137, -0.0044,  ...,  0.0098,  0.0069, -0.0084]]),\n",
       " 'transformer.resblocks.8.attn.out_proj.bias': tensor([-7.5088e-04, -4.1215e-03, -2.7986e-04, -7.5313e-04, -8.7992e-04,\n",
       "         -3.1680e-03,  4.1795e-04,  2.6085e-03, -3.3220e-03,  1.9130e-03,\n",
       "         -4.9135e-03, -2.2089e-03, -7.8226e-04, -1.5349e-03, -5.7918e-03,\n",
       "          2.0038e-03, -2.3169e-04,  1.7661e-03, -1.1575e-04, -6.6859e-04,\n",
       "         -3.2614e-04,  5.1868e-04,  4.6861e-04, -1.7215e-03, -1.0502e-03,\n",
       "         -1.0918e-03, -1.1704e-03, -8.6645e-04, -4.4895e-04, -1.6742e-03,\n",
       "          8.7701e-04,  1.0110e-04,  3.1767e-03, -3.8010e-03, -3.8578e-04,\n",
       "          1.4436e-03,  1.2265e-03,  4.0943e-04,  4.9981e-04, -2.9316e-03,\n",
       "          1.6461e-03,  2.1485e-03,  1.8588e-03,  1.3084e-03, -4.1341e-03,\n",
       "          1.5852e-03, -3.3758e-03,  5.3569e-04, -8.4986e-04,  1.5456e-03,\n",
       "         -1.3072e-03, -7.2867e-04, -1.4411e-03,  2.7890e-03,  3.0571e-03,\n",
       "          1.0905e-03, -6.5820e-04, -7.4281e-04, -2.0700e-05, -8.9554e-04,\n",
       "         -1.7591e-03, -4.2172e-05,  5.1202e-04,  3.5948e-04, -2.3024e-04,\n",
       "         -7.7941e-04,  1.6460e-03,  1.1297e-03, -8.4512e-04,  2.0354e-03,\n",
       "         -9.0771e-04, -1.3851e-04, -2.1037e-03, -2.1650e-04, -1.1825e-03,\n",
       "          2.1632e-03, -2.0603e-03, -1.9447e-03, -2.3338e-03,  2.2761e-03,\n",
       "          1.4224e-03, -2.2040e-03,  1.0431e-03,  1.9576e-03,  2.1480e-03,\n",
       "          5.3965e-04, -9.8324e-04,  8.9811e-04,  4.6117e-04, -1.8797e-03,\n",
       "          6.3118e-04, -8.4910e-04,  1.6309e-03, -1.8920e-03, -2.0496e-03,\n",
       "         -4.1903e-03,  1.0795e-04, -1.1026e-03, -1.6979e-03,  6.9304e-04,\n",
       "          8.9130e-04, -1.6642e-03,  2.1926e-04, -7.5152e-04, -9.2006e-04,\n",
       "         -2.5496e-03, -3.5757e-03, -9.3415e-04,  1.1972e-03, -4.3425e-04,\n",
       "          3.2998e-03,  1.0092e-03,  2.8292e-03,  1.4283e-03, -1.2632e-03,\n",
       "         -4.3852e-03,  1.4373e-03,  1.4103e-03, -2.6626e-03, -1.1260e-03,\n",
       "          5.9041e-04, -1.4949e-03,  1.6817e-03, -5.9769e-04, -1.4060e-03,\n",
       "          4.9453e-04, -1.6805e-03, -1.6478e-03, -1.6786e-03, -4.8873e-04,\n",
       "          8.1055e-05,  3.4317e-03, -1.7139e-04, -2.5160e-03, -9.9964e-04,\n",
       "         -1.2445e-03, -1.5174e-03,  2.3421e-03, -7.2579e-04,  1.0308e-03,\n",
       "          1.8239e-04,  1.9187e-03,  1.1141e-03,  3.8904e-03,  1.1871e-03,\n",
       "          2.4316e-03, -2.3469e-03,  7.8041e-04,  8.4203e-04,  4.4440e-04,\n",
       "          2.0720e-03,  4.6538e-03,  5.9407e-03,  2.0007e-03, -8.6581e-04,\n",
       "         -3.1707e-04,  1.0019e-03, -1.4777e-03,  3.4953e-04,  4.3186e-04,\n",
       "         -2.0448e-03, -1.1081e-03,  2.9860e-03,  2.6339e-04, -1.1109e-03,\n",
       "          1.2473e-03,  2.4881e-03, -6.2657e-04, -3.6149e-04, -1.9706e-03,\n",
       "          1.3580e-03, -3.5482e-04, -1.8819e-03,  7.5881e-04,  2.0324e-03,\n",
       "          1.4252e-03, -1.8064e-03,  1.3033e-04,  7.4081e-04, -1.4457e-03,\n",
       "         -2.5632e-03, -9.1864e-05,  1.1523e-03, -3.1515e-03,  4.4531e-03,\n",
       "          1.7183e-04,  4.9161e-04, -9.3560e-05,  1.1338e-03, -2.4531e-03,\n",
       "          5.3039e-04,  8.3936e-04, -5.0972e-04,  1.6954e-03, -1.3612e-03,\n",
       "         -1.4429e-03,  1.7077e-03,  2.6938e-03,  3.5668e-03,  3.0519e-03,\n",
       "         -2.7610e-04, -1.3063e-03,  1.6120e-03,  1.9108e-03, -1.0299e-05,\n",
       "          7.9035e-04, -1.4979e-03,  3.6782e-04,  1.5387e-03,  2.0479e-04,\n",
       "          2.4463e-03,  4.4679e-04,  2.0351e-04,  1.7215e-03, -2.2351e-03,\n",
       "         -2.3857e-03,  1.7345e-04, -2.1132e-03, -1.1637e-03,  7.6857e-04,\n",
       "         -7.4310e-04, -5.7398e-04,  1.7465e-03,  2.0850e-03,  1.0713e-03,\n",
       "          8.7834e-04, -1.0527e-03, -2.3174e-03,  1.3264e-03,  2.0983e-03,\n",
       "         -1.7831e-03,  1.2568e-03,  3.8993e-04, -1.0445e-03,  1.3463e-03,\n",
       "         -2.7488e-05,  2.9013e-03, -3.5317e-03,  1.5285e-03,  1.5055e-03,\n",
       "         -1.9364e-03,  2.3873e-03,  2.7803e-03,  1.4750e-03, -2.8179e-03,\n",
       "          2.2766e-06,  5.7687e-04,  6.5108e-04, -5.5292e-04, -1.5659e-03,\n",
       "          1.1543e-03, -2.0754e-03,  3.1925e-05, -1.3312e-03, -9.0403e-04,\n",
       "         -1.1197e-03, -3.1332e-04, -2.6012e-03, -1.7880e-04, -3.5430e-03,\n",
       "          1.1849e-03,  9.7065e-04,  1.2233e-03, -3.2471e-03, -1.7098e-03,\n",
       "          3.2111e-03,  6.7787e-04,  1.2156e-03, -2.3596e-04, -1.7529e-03,\n",
       "          7.3365e-04, -1.1942e-03, -1.6633e-03,  4.4752e-04,  1.1460e-03,\n",
       "         -6.4971e-04,  3.4459e-03, -6.9413e-04,  3.3123e-03, -1.5355e-03,\n",
       "         -4.0367e-04,  3.8304e-03, -2.2801e-03, -1.7519e-03, -2.5877e-03,\n",
       "          1.3885e-03,  5.5208e-04,  1.6270e-03,  2.2230e-03,  5.0538e-05,\n",
       "         -6.1470e-04,  1.5152e-03, -2.2644e-03,  4.2629e-03,  5.3693e-04,\n",
       "          1.7953e-03, -1.9778e-03,  9.9922e-04, -4.9695e-04, -1.0830e-03,\n",
       "         -2.7842e-03,  9.2829e-04,  1.4244e-03, -4.1778e-03, -1.5082e-04,\n",
       "         -2.3097e-03,  5.1455e-04,  9.2164e-05, -1.7145e-03, -2.3479e-03,\n",
       "          2.0055e-03,  3.0175e-03, -7.8908e-04, -1.8762e-03,  1.0212e-03,\n",
       "          2.5633e-03,  3.1657e-03,  1.2033e-03,  8.1420e-04,  8.9397e-04,\n",
       "          2.2561e-03, -6.6971e-04, -1.0839e-03,  3.6879e-04, -2.6843e-03,\n",
       "          4.4143e-04,  3.2669e-03,  9.7305e-05, -3.5577e-03, -2.0503e-03,\n",
       "         -4.0241e-04,  1.3312e-03,  5.9867e-04, -9.5821e-04,  1.4606e-03,\n",
       "         -3.9150e-04, -8.3486e-05,  3.0661e-05,  7.7368e-04,  2.4957e-03,\n",
       "         -3.1554e-03,  3.1509e-03,  1.5540e-04, -6.2749e-04, -3.3949e-03,\n",
       "         -2.3094e-05, -2.3799e-03, -2.3954e-05, -6.9617e-04,  1.4589e-03,\n",
       "         -1.7219e-03, -4.0343e-03, -2.0929e-03, -2.8055e-04, -5.9420e-05,\n",
       "          6.7868e-04,  3.2262e-03,  2.5852e-03,  9.1279e-04, -2.6120e-03,\n",
       "          1.3758e-03, -5.9323e-04,  2.6597e-03,  3.1406e-03, -6.9335e-05,\n",
       "          2.9350e-03, -5.1246e-04, -4.2060e-03,  1.9819e-03, -8.3635e-04,\n",
       "         -1.4163e-03,  1.3663e-03,  6.7933e-04, -5.5298e-04,  5.8443e-05,\n",
       "          3.4074e-03, -9.6412e-04,  2.5697e-03, -1.6236e-03, -1.8739e-03,\n",
       "          2.5361e-03,  2.2433e-03, -1.6707e-03, -9.3940e-04, -1.2647e-03,\n",
       "         -7.6588e-04,  2.6858e-03, -7.3159e-04,  3.4922e-03,  1.9223e-03,\n",
       "         -9.7492e-05, -9.6776e-05, -2.0691e-05,  9.2268e-04,  2.9894e-03,\n",
       "          2.2615e-03, -1.0317e-03,  2.0414e-03,  2.9349e-04,  2.0795e-04,\n",
       "         -3.5416e-04, -1.5311e-03, -4.4431e-04, -5.4772e-04, -5.4872e-04,\n",
       "          1.6816e-03,  1.2324e-03,  8.4623e-04,  1.7830e-03,  1.7781e-03,\n",
       "         -6.0813e-04, -3.0706e-03,  4.5846e-04,  2.9127e-03,  1.0635e-03,\n",
       "         -2.4559e-03,  1.9201e-03,  1.7880e-03, -4.9191e-04,  2.6120e-03,\n",
       "         -1.3095e-03,  1.0637e-03,  1.5843e-03, -1.8324e-03, -1.7486e-03,\n",
       "          1.8175e-04, -5.4948e-04, -4.7101e-04,  1.3786e-03,  4.3408e-04,\n",
       "         -2.2917e-03,  1.2766e-03,  1.4910e-03, -1.1825e-03, -3.2026e-03,\n",
       "         -1.1818e-03,  1.7177e-03, -2.6488e-03,  1.7486e-03, -1.1486e-03,\n",
       "         -1.1773e-03,  1.1026e-03,  6.3230e-04, -1.6026e-04, -2.0645e-03,\n",
       "         -3.0014e-03, -5.8258e-04,  1.2579e-03, -8.9603e-05,  1.5746e-03,\n",
       "          3.6639e-04,  9.5191e-04, -1.1475e-03,  4.8608e-03,  1.4041e-03,\n",
       "         -1.8979e-03,  6.0331e-04,  2.2175e-03,  2.2810e-03,  3.7191e-04,\n",
       "          8.5956e-04, -8.7847e-04,  1.1989e-03,  3.4148e-03,  5.7100e-04,\n",
       "          1.8740e-04,  1.7292e-04,  1.0939e-03, -1.8261e-05, -1.9975e-03,\n",
       "         -2.0012e-03,  1.4140e-03, -3.2583e-03, -1.2976e-03, -1.9569e-03,\n",
       "          7.9959e-04, -2.8244e-03, -1.7611e-03, -1.5449e-03, -1.2937e-03,\n",
       "         -2.9809e-03, -1.7163e-03,  1.2332e-03,  6.8275e-04, -1.2549e-03,\n",
       "          5.7204e-04, -1.1632e-03, -2.0352e-03,  2.4815e-03,  1.5128e-03,\n",
       "          1.2496e-03, -1.0011e-03,  1.2965e-03, -2.4007e-03, -9.7461e-04,\n",
       "          5.5440e-04,  1.6628e-03, -2.1075e-04, -8.2909e-05, -3.2202e-03,\n",
       "         -1.4370e-03,  8.1343e-04, -1.6466e-03,  3.1991e-03, -3.1078e-03,\n",
       "          7.4268e-04, -1.7794e-03, -1.3222e-03,  1.0766e-03, -4.0325e-04,\n",
       "         -1.6060e-03, -4.4681e-04]),\n",
       " 'transformer.resblocks.8.ln_1.weight': tensor([0.9985, 1.0016, 0.9978, 1.0005, 0.9993, 0.9994, 1.0020, 0.9987, 0.9985,\n",
       "         0.9967, 0.9996, 1.0017, 1.0007, 0.9980, 0.9976, 1.0001, 1.0007, 1.0040,\n",
       "         0.9979, 0.9991, 0.9997, 0.9988, 1.0027, 1.0007, 0.9997, 0.9985, 0.9988,\n",
       "         0.9989, 1.0007, 0.9999, 0.9982, 1.0006, 0.9980, 0.9995, 1.0001, 0.9978,\n",
       "         0.9980, 0.9990, 0.9964, 1.0027, 1.0009, 0.9967, 0.9995, 0.9996, 0.9998,\n",
       "         1.0034, 1.0012, 0.9978, 0.9999, 0.9981, 0.9986, 0.9992, 1.0010, 0.9963,\n",
       "         1.0007, 0.9976, 0.9999, 0.9994, 0.9973, 0.9986, 1.0011, 0.9979, 1.0044,\n",
       "         0.9972, 0.9997, 0.9973, 0.9966, 0.9980, 0.9999, 1.0000, 0.9990, 1.0048,\n",
       "         0.9990, 0.9991, 0.9964, 1.0013, 1.0012, 0.9984, 1.0015, 0.9994, 0.9998,\n",
       "         0.9979, 1.0007, 1.0010, 0.9980, 1.0006, 0.9997, 0.9988, 1.0009, 0.9997,\n",
       "         1.0014, 0.9982, 1.0019, 0.9997, 0.9992, 0.9986, 0.9990, 1.0023, 1.0013,\n",
       "         1.0000, 0.9998, 1.0033, 0.9985, 1.0002, 0.9969, 0.9989, 0.9961, 1.0001,\n",
       "         0.9966, 1.0004, 0.9982, 0.9988, 0.9972, 0.9989, 0.9989, 0.9981, 1.0018,\n",
       "         0.9992, 0.9994, 1.0015, 0.9983, 1.0012, 0.9969, 0.9983, 0.9995, 0.9994,\n",
       "         0.9978, 1.0003, 0.9997, 0.9987, 1.0009, 0.9990, 0.9995, 1.0020, 1.0021,\n",
       "         0.9974, 0.9995, 0.9999, 1.0010, 0.9992, 0.9997, 1.0004, 0.9996, 1.0005,\n",
       "         1.0016, 0.9983, 0.9988, 0.9974, 0.9970, 0.9996, 0.9970, 0.9969, 1.0006,\n",
       "         1.0003, 1.0012, 1.0026, 0.9989, 0.9989, 0.9975, 0.9996, 0.9966, 0.9986,\n",
       "         0.9996, 1.0002, 0.9981, 0.9985, 1.0017, 0.9998, 1.0003, 0.9997, 1.0003,\n",
       "         0.9983, 1.0007, 0.9967, 1.0001, 0.9997, 0.9997, 1.0019, 0.9982, 1.0002,\n",
       "         1.0016, 1.0000, 1.0003, 1.0012, 0.9979, 1.0004, 0.9988, 0.9977, 0.9971,\n",
       "         0.9997, 1.0011, 1.0023, 0.9981, 0.9994, 0.9997, 0.9978, 1.0025, 0.9971,\n",
       "         0.9960, 1.0005, 0.9975, 0.9997, 0.9983, 1.0036, 0.9994, 0.9978, 1.0005,\n",
       "         0.9989, 0.9923, 1.0002, 1.0032, 1.0009, 1.0035, 0.9968, 0.9991, 0.9988,\n",
       "         0.9994, 0.9993, 1.0010, 0.9998, 0.9998, 1.0004, 0.9987, 0.9992, 0.9998,\n",
       "         1.0013, 0.9971, 0.9961, 1.0023, 0.9987, 0.9963, 1.0008, 0.9987, 0.9983,\n",
       "         1.0018, 0.9997, 0.9996, 0.9986, 1.0008, 1.0001, 0.9998, 1.0007, 1.0013,\n",
       "         0.9983, 0.9982, 1.0045, 1.0023, 0.9978, 0.9995, 1.0006, 1.0023, 0.9989,\n",
       "         1.0011, 0.9979, 1.0004, 0.9988, 1.0002, 0.9982, 1.0006, 0.9969, 0.9996,\n",
       "         0.9982, 0.9982, 0.9999, 1.0005, 0.9979, 1.0026, 1.0028, 0.9981, 1.0006,\n",
       "         0.9969, 0.9998, 1.0003, 1.0046, 0.9999, 1.0008, 1.0011, 1.0031, 0.9970,\n",
       "         1.0047, 0.9965, 0.9956, 0.9990, 1.0000, 1.0023, 0.9997, 0.9987, 0.9979,\n",
       "         0.9992, 0.9990, 1.0014, 1.0012, 0.9958, 0.9958, 0.9999, 0.9991, 0.9994,\n",
       "         1.0015, 0.9979, 1.0002, 0.9992, 0.9995, 0.9993, 0.9991, 1.0035, 1.0012,\n",
       "         1.0001, 0.9984, 1.0014, 0.9991, 0.9992, 1.0008, 0.9996, 1.0009, 1.0016,\n",
       "         0.9984, 0.9982, 1.0006, 0.9986, 0.9977, 1.0016, 1.0010, 0.9979, 0.9987,\n",
       "         1.0013, 1.0014, 0.9987, 0.9996, 1.0004, 1.0031, 1.0019, 1.0006, 1.0016,\n",
       "         0.9988, 0.9982, 0.9973, 1.0009, 0.9986, 0.9977, 0.9977, 0.9995, 1.0008,\n",
       "         0.9987, 0.9982, 0.9980, 0.9991, 1.0003, 1.0020, 0.9986, 1.0003, 0.9983,\n",
       "         1.0026, 1.0007, 0.9999, 1.0007, 1.0004, 0.9999, 1.0019, 1.0013, 0.9989,\n",
       "         1.0006, 0.9995, 1.0008, 0.9992, 0.9964, 1.0019, 0.9993, 1.0026, 0.9968,\n",
       "         0.9980, 0.9967, 1.0021, 0.9998, 1.0002, 1.0017, 1.0011, 0.9992, 0.9985,\n",
       "         1.0008, 0.9965, 0.9986, 1.0015, 0.9999, 0.9981, 0.9999, 0.9977, 0.9982,\n",
       "         0.9978, 0.9978, 1.0002, 1.0005, 1.0000, 1.0025, 1.0007, 0.9985, 0.9993,\n",
       "         0.9985, 1.0007, 0.9988, 1.0001, 0.9989, 0.9998, 0.9989, 1.0002, 1.0066,\n",
       "         1.0004, 0.9996, 1.0001, 1.0004, 0.9983, 0.9998, 0.9976, 1.0001, 0.9976,\n",
       "         0.9997, 0.9975, 0.9972, 1.0035, 1.0026, 0.9997, 0.9996, 0.9981, 1.0009,\n",
       "         0.9996, 1.0008, 1.0004, 1.0006, 0.9996, 0.9989, 0.9977, 0.9985, 0.9975,\n",
       "         0.9989, 0.9995, 0.9999, 0.9992, 1.0020, 1.0020, 1.0002, 0.9964, 0.9990,\n",
       "         0.9991, 0.9983, 0.9977, 1.0000, 0.9981, 0.9965, 1.0024, 1.0003, 0.9992,\n",
       "         0.9968, 1.0011, 1.0000, 0.9981, 0.9976, 1.0018, 1.0002, 0.9997, 1.0001,\n",
       "         1.0010, 1.0018, 1.0012, 1.0003, 1.0019, 0.9999, 1.0020, 0.9977, 1.0002,\n",
       "         1.0014, 1.0033, 0.9998, 0.9999, 0.9986, 1.0011, 1.0013, 1.0025, 1.0001,\n",
       "         0.9990, 0.9994, 1.0019, 1.0001, 0.9994, 0.9995, 0.9993, 1.0005, 1.0026,\n",
       "         0.9973, 1.0002, 1.0002, 1.0027, 1.0014, 1.0022, 0.9993, 0.9996, 0.9990,\n",
       "         0.9997, 0.9997, 1.0016, 0.9989, 0.9991, 0.9996, 0.9977, 1.0004, 0.9974,\n",
       "         0.9986, 0.9981, 1.0009, 1.0002, 0.9994, 1.0015, 0.9993, 0.9998]),\n",
       " 'transformer.resblocks.8.ln_1.bias': tensor([ 1.7554e-03, -3.4255e-04, -1.2469e-03,  5.1212e-04,  1.7684e-03,\n",
       "          3.7930e-04,  2.4636e-03, -8.8878e-04, -1.7339e-04,  7.1302e-04,\n",
       "          9.9177e-04, -1.8310e-03,  4.7154e-04,  2.7610e-03,  1.0618e-03,\n",
       "         -9.1348e-04,  2.8281e-03,  1.9481e-03,  4.1020e-04, -7.8919e-04,\n",
       "          1.3362e-03, -4.1136e-04,  2.9831e-03, -7.5625e-04, -2.0958e-03,\n",
       "          1.5424e-03, -1.5263e-04, -1.5481e-03,  1.4259e-03, -4.4752e-04,\n",
       "          2.5212e-03,  3.5393e-03, -2.0387e-05, -7.8517e-04, -2.5678e-03,\n",
       "          8.0045e-04, -2.4182e-03,  2.2535e-04, -2.5302e-03, -6.0511e-04,\n",
       "         -8.6176e-04, -3.4416e-04,  1.5165e-03, -3.8960e-04, -4.0563e-04,\n",
       "          3.7125e-04, -1.2790e-03,  1.8602e-03, -1.3851e-03,  7.0339e-04,\n",
       "          1.8849e-04,  6.2753e-04, -7.6842e-04, -2.7325e-03,  2.8809e-04,\n",
       "          3.3790e-03,  8.5420e-04, -1.0227e-03,  1.1467e-03, -1.3062e-03,\n",
       "         -8.7341e-04, -1.5504e-03, -1.0022e-03,  1.7087e-03, -9.2086e-04,\n",
       "         -3.1408e-03, -2.3934e-03,  1.7149e-03,  2.9036e-03, -1.3921e-03,\n",
       "          4.8849e-04,  2.6091e-03, -1.3422e-03, -1.0214e-04, -5.1097e-04,\n",
       "         -6.9969e-04, -2.9229e-03,  1.1370e-03,  1.2797e-03, -2.3368e-03,\n",
       "          2.4474e-03,  3.9452e-03, -2.9240e-03,  3.3970e-03, -3.4015e-03,\n",
       "         -1.8396e-03,  9.0683e-04,  2.0714e-03,  1.8678e-03,  1.5060e-03,\n",
       "         -1.9325e-03, -7.4419e-04, -1.9918e-03,  1.5872e-03,  2.6943e-03,\n",
       "          2.0149e-03,  3.1870e-03,  1.3759e-03, -1.1122e-03,  2.3130e-03,\n",
       "         -5.6304e-04, -1.9052e-03,  5.3025e-04,  3.8221e-04,  8.2987e-04,\n",
       "         -1.0333e-03,  2.6341e-03, -4.6176e-04,  6.7071e-04,  2.8713e-03,\n",
       "         -9.0422e-04,  2.3176e-03, -3.5710e-03,  1.0484e-04, -5.2454e-04,\n",
       "          1.5137e-03,  2.9843e-03,  5.2676e-04,  1.1721e-04, -4.7385e-04,\n",
       "         -6.0900e-04,  1.3476e-03,  2.5883e-03, -6.1227e-04, -1.3598e-03,\n",
       "          2.1363e-03,  6.4574e-04, -6.6287e-04,  1.9602e-03,  5.5521e-04,\n",
       "         -1.5110e-03,  3.7905e-04, -1.6277e-03,  5.0070e-04, -1.5905e-03,\n",
       "          1.8757e-03,  1.7117e-03,  6.0522e-04,  6.5585e-04,  3.2475e-03,\n",
       "          1.9368e-03,  3.0950e-03,  1.1897e-03,  1.7249e-03, -4.4579e-05,\n",
       "         -2.4648e-03, -6.7771e-04, -2.4356e-03,  5.8244e-04, -1.1809e-03,\n",
       "         -1.8412e-03, -2.6032e-03,  1.1208e-03,  1.2068e-03,  1.9758e-03,\n",
       "          5.6659e-04,  2.3635e-03,  2.1455e-03,  4.3871e-03,  1.5136e-03,\n",
       "          2.4384e-03,  5.0318e-04, -8.5749e-04, -2.2416e-03, -1.2985e-03,\n",
       "         -1.5042e-03,  2.6065e-03, -2.0367e-03,  1.8541e-04, -8.1939e-04,\n",
       "          1.1067e-03,  1.7870e-03,  3.7164e-03,  2.8775e-04, -4.9225e-04,\n",
       "         -1.3708e-03,  1.7894e-03,  2.7499e-03,  1.4065e-03,  2.6756e-03,\n",
       "         -1.4340e-03,  2.3279e-03,  5.9727e-04, -3.5028e-03, -2.2691e-03,\n",
       "          6.7652e-04, -2.5652e-03, -1.1015e-03, -1.9202e-03, -1.8654e-03,\n",
       "          1.7592e-03, -7.6925e-04, -7.9422e-04, -1.6563e-03,  1.9041e-03,\n",
       "          4.8240e-03,  1.1815e-03, -2.8695e-03, -2.3707e-03,  7.3021e-04,\n",
       "          7.3322e-04, -8.2270e-04, -3.0806e-03,  2.5779e-03, -1.2678e-03,\n",
       "         -7.9720e-04,  4.0845e-03, -1.3217e-03, -6.5619e-04, -2.6962e-03,\n",
       "         -1.6351e-03,  1.4906e-03, -9.1845e-04, -1.6578e-03, -1.3483e-03,\n",
       "         -1.9218e-03, -1.5391e-03, -7.8243e-04,  3.0657e-04,  4.4955e-04,\n",
       "         -5.1165e-04,  1.9941e-03,  7.3734e-04, -1.1146e-03,  6.3176e-04,\n",
       "          1.4303e-03,  7.1342e-04,  1.5370e-03, -2.0783e-03, -5.7946e-04,\n",
       "          3.0715e-04, -1.5821e-04,  1.6664e-03,  4.5140e-04, -2.0370e-05,\n",
       "         -8.0454e-04,  1.8656e-03,  1.7253e-03,  1.3343e-03, -1.7120e-03,\n",
       "         -2.6997e-04, -1.6835e-03, -1.7315e-03, -3.8691e-04,  2.2078e-03,\n",
       "         -1.7352e-03, -1.2119e-03,  4.3650e-04, -8.2815e-05,  3.0438e-03,\n",
       "         -9.9277e-04, -3.6549e-04,  2.4048e-04,  8.9315e-04, -2.3770e-03,\n",
       "         -6.0113e-05,  1.5078e-03,  1.2127e-03,  9.2260e-04, -2.2677e-04,\n",
       "          2.8800e-03, -1.4244e-03,  7.1246e-04, -1.7678e-04, -2.9943e-03,\n",
       "         -3.1375e-03,  5.2665e-04,  2.0186e-03, -6.2666e-04, -1.6452e-03,\n",
       "          5.1436e-03,  5.1936e-04, -1.9691e-03,  3.3277e-03,  1.2102e-03,\n",
       "         -1.4804e-03,  2.0044e-03, -3.4629e-04, -1.3353e-03,  2.3291e-03,\n",
       "         -1.8288e-03,  9.5517e-04,  4.2138e-05, -3.1758e-03, -2.1357e-03,\n",
       "         -8.0902e-04, -9.4339e-04, -3.3955e-04, -3.3575e-04, -1.7946e-03,\n",
       "          3.5585e-03,  2.0134e-03,  1.2291e-03, -2.9829e-03,  1.7386e-03,\n",
       "         -2.8437e-03, -1.3730e-03, -9.2133e-04, -1.6294e-03, -8.7390e-04,\n",
       "          7.3660e-04,  1.4752e-03, -1.9162e-03, -7.0156e-04, -3.7347e-03,\n",
       "         -2.5022e-03, -2.0762e-04,  4.0883e-04, -3.2200e-03,  1.2694e-03,\n",
       "         -2.9907e-03, -2.1447e-03, -2.5335e-03, -6.6748e-04,  2.0514e-03,\n",
       "         -3.4885e-04, -1.4504e-03,  2.2549e-03,  2.3646e-03, -4.9175e-05,\n",
       "          1.5952e-03, -3.7937e-03, -1.6541e-04,  5.9269e-04,  2.0899e-03,\n",
       "          1.7578e-03,  5.5904e-04,  2.9900e-04, -2.1545e-03, -2.9481e-03,\n",
       "          1.6211e-03,  1.7964e-03,  1.1621e-03, -1.0329e-03, -1.1267e-03,\n",
       "         -1.2000e-03,  4.6054e-04,  2.2313e-03, -4.9175e-04,  4.3168e-04,\n",
       "         -1.6278e-03,  1.0319e-03,  2.5757e-03,  8.5052e-04,  8.1947e-06,\n",
       "          1.6906e-03, -4.3064e-04,  9.0541e-04, -2.0326e-04,  5.4492e-04,\n",
       "         -2.5906e-03, -3.6587e-03, -2.5103e-03,  2.1978e-03,  1.0132e-03,\n",
       "         -6.5585e-04, -1.1067e-03,  3.3459e-03,  3.0213e-04, -9.0204e-05,\n",
       "         -2.3661e-03, -3.3638e-03,  3.2007e-03,  1.4123e-03, -4.7776e-04,\n",
       "          3.0727e-03,  3.6695e-04, -5.9886e-05, -3.7457e-04, -3.5739e-03,\n",
       "          2.8971e-03,  6.9539e-04,  9.4288e-05, -1.7535e-03,  1.4389e-04,\n",
       "         -1.4928e-03,  3.7813e-03, -3.7771e-03, -8.1038e-04, -8.0082e-05,\n",
       "          3.8727e-04,  1.9237e-03, -7.0319e-04,  3.6197e-04, -2.0821e-03,\n",
       "         -8.1812e-04, -5.2618e-04,  1.6517e-03, -1.3448e-03, -4.5094e-04,\n",
       "         -3.2686e-03,  1.3874e-03, -5.2511e-04,  9.4665e-04,  3.8608e-04,\n",
       "         -4.7459e-04,  2.1502e-03,  1.5141e-03, -4.4034e-04, -6.7343e-04,\n",
       "         -2.7146e-03, -2.3826e-03,  3.1487e-04, -1.1791e-03, -1.3298e-04,\n",
       "          2.9529e-04,  4.1617e-04,  2.3376e-03, -8.3989e-04, -7.3513e-04,\n",
       "          6.9623e-04,  1.1328e-03,  1.7982e-03, -2.0694e-03, -5.8281e-04,\n",
       "         -1.1198e-05,  2.7226e-04, -2.3765e-03, -1.8782e-03,  8.1995e-04,\n",
       "          2.7207e-04, -1.9458e-03, -2.9372e-04,  2.5892e-03, -2.8758e-03,\n",
       "         -8.8319e-04, -2.1169e-03,  1.2426e-03,  9.6645e-04, -1.5097e-03,\n",
       "         -1.2302e-04,  3.6870e-04,  4.9388e-04, -3.9205e-04, -8.2648e-04,\n",
       "          1.6654e-03,  3.4462e-03, -2.6696e-03,  2.9715e-03,  3.7751e-03,\n",
       "         -3.3576e-03,  2.7478e-05, -2.8428e-03,  1.8633e-04, -6.6374e-04,\n",
       "         -1.9411e-03, -2.1111e-03,  1.4428e-03,  3.4771e-03, -2.0295e-03,\n",
       "         -2.6866e-03,  1.3360e-03, -3.6227e-03, -1.5911e-03, -1.7382e-03,\n",
       "         -2.3004e-03,  8.6282e-05,  1.5839e-03,  5.0591e-05,  1.5532e-03,\n",
       "          2.4694e-04, -2.6621e-03,  2.2665e-03,  2.6214e-03,  1.7056e-03,\n",
       "          9.1693e-04, -1.4841e-03,  2.3331e-03,  8.2665e-04, -1.2086e-03,\n",
       "         -9.8235e-04, -2.2228e-03, -1.0828e-03,  2.1091e-03,  9.1162e-05,\n",
       "          1.2987e-05, -9.8188e-04,  1.3175e-03,  2.7379e-03, -1.4231e-03,\n",
       "          3.4077e-04,  2.2651e-03, -1.1162e-03,  1.5756e-03, -1.6275e-04,\n",
       "          2.0570e-03,  4.5783e-04, -1.4666e-03, -8.3990e-04, -2.5423e-03,\n",
       "          1.3955e-03,  9.4057e-04, -1.2196e-03, -1.6521e-03,  8.7766e-04,\n",
       "         -2.1678e-03, -1.2072e-03, -3.1242e-04, -5.8134e-05, -1.1689e-03,\n",
       "         -2.5901e-04, -3.8586e-04,  5.8553e-04, -1.0774e-03,  5.4122e-04,\n",
       "          7.4513e-04,  5.6596e-04,  9.1702e-04,  4.7898e-04, -1.7284e-03,\n",
       "         -3.1765e-03,  7.9225e-04]),\n",
       " 'transformer.resblocks.8.mlp.c_fc.weight': tensor([[ 0.0098, -0.0310,  0.0159,  ...,  0.0134, -0.0613, -0.0115],\n",
       "         [ 0.0469, -0.0302,  0.0518,  ...,  0.0108, -0.0337, -0.0401],\n",
       "         [ 0.0179,  0.0088, -0.0161,  ...,  0.0142, -0.0555,  0.0086],\n",
       "         ...,\n",
       "         [ 0.0064,  0.0231, -0.0181,  ..., -0.0105,  0.0243, -0.0124],\n",
       "         [-0.0283,  0.0682, -0.0346,  ...,  0.0309,  0.0514,  0.0605],\n",
       "         [ 0.0065, -0.0354,  0.0581,  ...,  0.0263,  0.0254,  0.0388]]),\n",
       " 'transformer.resblocks.8.mlp.c_fc.bias': tensor([-0.0154,  0.0229, -0.0074,  ...,  0.0208, -0.0067,  0.0270]),\n",
       " 'transformer.resblocks.8.mlp.c_proj.weight': tensor([[-0.0018,  0.0131,  0.0023,  ...,  0.0064,  0.0131, -0.0074],\n",
       "         [ 0.0047,  0.0050, -0.0075,  ...,  0.0130,  0.0154,  0.0028],\n",
       "         [-0.0061, -0.0248,  0.0005,  ..., -0.0015,  0.0091, -0.0012],\n",
       "         ...,\n",
       "         [-0.0052, -0.0063, -0.0098,  ..., -0.0155, -0.0036, -0.0042],\n",
       "         [-0.0051, -0.0009, -0.0052,  ...,  0.0046, -0.0169, -0.0048],\n",
       "         [ 0.0157, -0.0076,  0.0040,  ..., -0.0037, -0.0096,  0.0033]]),\n",
       " 'transformer.resblocks.8.mlp.c_proj.bias': tensor([-2.5296e-05,  8.6021e-03,  1.3509e-02, -1.9486e-02, -1.5424e-02,\n",
       "          4.5935e-03, -1.0283e-02,  3.1246e-03,  1.1389e-02, -1.6186e-02,\n",
       "          1.0969e-02, -1.7684e-02,  1.2311e-02,  1.1781e-02, -2.4166e-03,\n",
       "          1.4870e-02,  3.5363e-03,  1.1675e-02,  1.7892e-02, -2.2365e-02,\n",
       "          1.5376e-02, -5.1089e-03,  1.5077e-02, -4.9583e-03, -2.1563e-02,\n",
       "         -1.7730e-02, -3.8386e-03, -6.2204e-03, -1.2548e-02,  1.3751e-02,\n",
       "          1.5446e-02,  1.0139e-02,  1.1244e-02, -2.0505e-03, -3.2003e-04,\n",
       "          1.4739e-02, -1.4690e-02, -1.4687e-02,  1.0855e-02,  2.2336e-03,\n",
       "         -2.6166e-03,  2.4343e-02,  9.0913e-03,  2.0481e-02,  9.0070e-04,\n",
       "         -4.5352e-03, -3.8400e-03,  9.9056e-03, -6.0087e-03, -1.5780e-02,\n",
       "          1.9703e-02, -1.1155e-02, -1.8136e-02, -4.9052e-04, -5.2416e-03,\n",
       "          1.1952e-02,  9.5873e-03, -1.4051e-02,  1.1995e-02,  3.2768e-03,\n",
       "          8.7809e-03,  8.4083e-03, -4.4597e-03, -2.2330e-03,  1.3251e-02,\n",
       "         -9.5220e-03, -7.3724e-03,  1.3579e-02,  4.5898e-03,  2.1847e-02,\n",
       "         -1.7694e-02,  5.0799e-03, -1.9598e-02,  1.8166e-02,  3.3949e-03,\n",
       "         -6.8485e-03, -1.5138e-02, -3.0354e-03, -1.6731e-02, -1.1320e-02,\n",
       "          1.1755e-02, -5.1039e-04,  2.5000e-03, -1.4587e-02, -9.8266e-03,\n",
       "         -1.8397e-02, -2.1240e-04,  1.6256e-02,  2.5381e-03,  1.0204e-02,\n",
       "          8.5116e-03, -2.4179e-03,  4.2320e-03,  9.1121e-03, -2.2982e-02,\n",
       "         -2.6524e-04, -1.1595e-02, -2.0501e-02, -9.9712e-03,  5.5208e-03,\n",
       "         -1.2575e-02,  1.3868e-03, -9.3514e-03, -1.3693e-02,  6.8373e-03,\n",
       "         -1.3376e-02, -9.7235e-03, -1.8643e-03, -1.0536e-02,  1.2372e-02,\n",
       "          2.1946e-02, -5.7998e-03, -6.0372e-04, -7.9270e-03,  2.9536e-03,\n",
       "          1.1471e-02, -1.9438e-02, -8.2673e-04, -1.7543e-02, -4.7492e-03,\n",
       "          2.1362e-02, -4.1490e-03, -1.2381e-03,  5.1900e-03,  1.3324e-02,\n",
       "          1.5838e-02,  1.9331e-02, -1.2606e-02, -1.8106e-02,  1.8511e-02,\n",
       "         -5.6298e-03,  1.9445e-02, -1.9334e-02,  1.3539e-02,  1.1533e-02,\n",
       "         -2.2985e-03, -1.1511e-02,  3.8627e-03, -1.5096e-02, -7.0906e-03,\n",
       "         -4.9443e-03, -6.6826e-03,  3.0095e-03,  7.3400e-03,  4.1833e-03,\n",
       "         -3.4438e-03,  6.5548e-03, -3.6550e-03, -1.6677e-02,  1.5536e-02,\n",
       "          1.3185e-02, -4.2559e-04,  5.8391e-03, -1.7615e-02,  1.8660e-02,\n",
       "         -1.3060e-02,  4.4043e-03,  3.3244e-03,  1.6191e-02,  7.9887e-03,\n",
       "         -7.9712e-03,  4.6968e-03, -1.4745e-02, -8.1538e-03,  8.4228e-03,\n",
       "          1.6311e-02,  8.2582e-03,  1.1071e-03,  6.0352e-05,  6.9719e-03,\n",
       "         -1.5524e-02,  1.4085e-02, -2.3126e-03,  1.6733e-02,  2.0642e-02,\n",
       "          2.5641e-03, -1.9750e-02,  7.2296e-03,  4.4264e-03,  2.8410e-03,\n",
       "          1.0550e-02,  1.8747e-02, -1.0921e-03,  1.2847e-02, -1.3882e-02,\n",
       "         -1.6426e-02,  1.5133e-02, -1.3916e-02, -4.1368e-03,  1.9288e-02,\n",
       "          1.2809e-02,  2.1561e-02,  7.3134e-03,  1.5963e-02,  1.5582e-02,\n",
       "          5.6684e-03,  1.7123e-02,  1.7916e-02,  1.5152e-02, -1.8025e-02,\n",
       "          2.0785e-02, -2.1441e-02,  1.2084e-02,  8.8457e-03, -7.8324e-03,\n",
       "         -1.0896e-02,  2.5737e-03,  1.3905e-02, -1.6007e-02, -7.1276e-03,\n",
       "         -2.4402e-03,  1.8548e-02, -1.9991e-02,  6.5569e-03,  6.1018e-03,\n",
       "         -9.8370e-03, -7.5330e-03, -2.3999e-02,  4.7469e-03,  1.7039e-02,\n",
       "          1.0566e-02, -1.0323e-02, -7.1299e-03, -4.8594e-03, -1.1746e-02,\n",
       "         -1.5483e-02, -1.8939e-02, -7.4793e-03, -1.1454e-03, -7.9531e-03,\n",
       "          7.6518e-03,  1.1496e-03, -4.5839e-03, -2.0057e-02,  1.0912e-02,\n",
       "         -1.3454e-02, -1.8296e-02, -4.9068e-03,  5.0681e-04,  2.2901e-02,\n",
       "          4.3768e-03,  2.1500e-02,  2.5201e-03, -1.5483e-02, -1.4534e-02,\n",
       "          1.8654e-03, -5.1788e-03,  2.1909e-02, -5.9493e-03, -3.1705e-03,\n",
       "          1.6816e-02,  7.8604e-03,  2.9902e-03,  1.5535e-02, -7.6229e-03,\n",
       "         -2.0848e-02, -3.9915e-03, -1.4286e-02, -2.1400e-02,  1.6540e-02,\n",
       "          7.7310e-03,  1.4569e-02,  1.0788e-02, -1.7900e-02,  6.9386e-03,\n",
       "         -1.5391e-02,  7.2268e-03,  1.5428e-02,  6.3120e-03, -7.2554e-03,\n",
       "         -5.6801e-03,  9.0941e-03,  1.1391e-03,  9.3069e-03, -2.5844e-03,\n",
       "         -1.7281e-02,  1.3072e-02, -6.2806e-05,  2.2382e-02, -3.6676e-03,\n",
       "          2.1498e-02, -8.7163e-03, -8.9692e-03,  9.1741e-03,  1.2994e-02,\n",
       "          7.9646e-03, -1.6388e-02,  1.5559e-02, -9.4793e-03,  3.0806e-03,\n",
       "         -2.1487e-02, -3.5285e-03,  1.7738e-02,  2.6495e-02, -1.8327e-02,\n",
       "          2.0031e-02, -1.3733e-03,  2.9605e-03, -6.1840e-03, -1.0016e-02,\n",
       "          2.4413e-03, -4.1577e-03,  4.5541e-03, -3.5612e-03, -1.1554e-02,\n",
       "         -8.0544e-03,  1.1912e-03,  1.0377e-02,  4.9575e-04, -3.9223e-03,\n",
       "          1.7815e-02,  1.4421e-02, -1.1417e-02, -4.5244e-03, -1.0392e-02,\n",
       "         -6.3348e-03, -1.6638e-03,  1.6716e-02, -4.5065e-04, -4.5892e-03,\n",
       "          1.8934e-02, -2.2016e-02, -9.9505e-03, -1.9585e-02,  6.8153e-03,\n",
       "          4.4764e-03,  1.5217e-02,  2.4721e-03,  7.5749e-03, -1.2341e-02,\n",
       "          1.0360e-02,  2.1054e-02, -4.3803e-03, -1.7476e-02,  1.0579e-02,\n",
       "         -6.3715e-03, -1.3974e-03,  4.7588e-03,  6.4768e-03,  1.6025e-02,\n",
       "          1.4402e-02, -5.3973e-03,  1.3127e-02, -3.9363e-03,  1.4417e-02,\n",
       "          2.6110e-03,  1.7072e-02,  1.4200e-02, -7.6260e-03,  9.7839e-05,\n",
       "          1.9094e-02,  1.4153e-02,  9.9108e-03,  1.3226e-02,  8.5815e-03,\n",
       "          1.8344e-03, -8.7436e-03, -4.6355e-06,  1.0057e-02,  1.4250e-02,\n",
       "         -1.6502e-02, -2.1466e-02,  1.6449e-02,  1.8114e-02, -9.5615e-03,\n",
       "          2.0137e-02, -1.3479e-02, -1.7048e-02, -5.8458e-03, -3.2632e-03,\n",
       "         -4.1597e-03,  7.9217e-03,  1.3028e-02,  1.9513e-02, -2.1358e-02,\n",
       "         -4.6091e-03, -1.9996e-02,  1.9680e-02, -7.2644e-03,  1.4588e-03,\n",
       "          2.0885e-03,  2.3671e-02,  1.1612e-02, -4.3854e-03,  1.4317e-02,\n",
       "          6.3890e-03, -3.8229e-03,  1.1197e-02,  2.0539e-02,  1.0107e-02,\n",
       "         -5.6684e-03,  1.4989e-02, -1.8309e-02, -3.0256e-03,  7.9633e-03,\n",
       "         -1.3849e-02, -2.2256e-02, -7.2263e-03,  9.6629e-03,  1.7325e-02,\n",
       "         -8.0054e-04,  7.3092e-03, -6.7865e-03,  8.1452e-03, -1.6100e-02,\n",
       "         -8.9073e-03,  2.4580e-03,  2.2387e-02,  2.2089e-04,  2.2251e-02,\n",
       "         -1.3657e-02,  7.6111e-03,  6.7229e-03,  2.3329e-02, -1.3540e-02,\n",
       "         -1.4386e-02, -5.8769e-03, -1.8972e-02, -2.1518e-02, -5.2199e-03,\n",
       "         -7.4214e-04, -1.0193e-03, -4.2887e-03, -8.7210e-04,  6.4497e-03,\n",
       "         -1.6150e-02,  3.9284e-03, -1.4470e-02, -9.2229e-03, -1.9891e-02,\n",
       "         -9.5994e-03,  4.4013e-03,  1.2373e-02,  5.6705e-03, -1.5298e-02,\n",
       "         -9.2325e-03,  4.2320e-04, -8.1421e-03,  7.3816e-03,  7.0503e-04,\n",
       "          1.8965e-02,  1.4160e-02, -4.0777e-03, -1.7622e-02,  1.5129e-02,\n",
       "         -2.2517e-02,  1.0590e-02, -1.4044e-02,  1.7436e-02,  1.2645e-02,\n",
       "         -1.0009e-02,  1.7115e-02, -6.7669e-03, -6.4543e-03, -3.2441e-04,\n",
       "          5.7290e-03, -1.2059e-02,  1.1901e-02, -1.2084e-02,  2.1627e-02,\n",
       "          2.1650e-02,  4.5685e-04,  1.6858e-02,  2.0557e-02, -1.4864e-02,\n",
       "         -1.9009e-02,  1.2304e-02,  5.2896e-03, -2.4220e-04, -8.4042e-03,\n",
       "          1.1559e-03, -1.8430e-02, -2.4951e-02,  1.7329e-04, -1.7949e-05,\n",
       "          9.1971e-03, -2.0937e-02,  7.3502e-03, -1.5045e-02,  1.6585e-02,\n",
       "         -2.0505e-02, -1.2106e-02,  2.2075e-03, -1.1999e-02,  2.0355e-02,\n",
       "         -1.6940e-02, -1.8720e-02, -1.0461e-02,  2.1524e-02, -1.5426e-03,\n",
       "         -9.0384e-03, -9.4214e-03, -1.5787e-02, -1.5093e-02, -2.2314e-02,\n",
       "          2.0403e-02,  1.7757e-02, -2.0176e-02, -8.2192e-03, -3.7660e-03,\n",
       "         -2.0168e-02,  9.3564e-03, -1.1622e-02,  1.7172e-02, -1.3025e-02,\n",
       "          1.7522e-02,  9.0045e-03, -6.8383e-03,  1.9593e-02, -3.7601e-03,\n",
       "          1.9526e-02, -3.4076e-03]),\n",
       " 'transformer.resblocks.8.ln_2.weight': tensor([0.9979, 0.9971, 0.9995, 1.0005, 1.0029, 1.0006, 0.9999, 1.0007, 0.9984,\n",
       "         0.9983, 1.0000, 1.0009, 0.9988, 0.9993, 0.9973, 1.0004, 1.0024, 1.0013,\n",
       "         1.0020, 1.0010, 0.9978, 1.0033, 1.0036, 1.0013, 0.9982, 0.9983, 1.0021,\n",
       "         0.9980, 1.0026, 1.0019, 1.0028, 1.0011, 1.0020, 1.0018, 0.9997, 1.0008,\n",
       "         1.0009, 1.0028, 1.0017, 1.0002, 0.9999, 0.9988, 1.0005, 1.0004, 0.9993,\n",
       "         1.0035, 0.9984, 1.0039, 0.9981, 0.9986, 1.0002, 1.0043, 0.9999, 0.9978,\n",
       "         0.9999, 1.0031, 1.0039, 1.0004, 0.9995, 1.0013, 0.9994, 0.9986, 0.9984,\n",
       "         1.0007, 0.9979, 1.0005, 0.9994, 1.0016, 1.0021, 1.0008, 1.0043, 0.9992,\n",
       "         0.9994, 1.0033, 0.9994, 1.0020, 0.9977, 1.0026, 1.0001, 1.0020, 0.9994,\n",
       "         1.0046, 1.0003, 0.9963, 0.9977, 0.9998, 1.0029, 1.0054, 1.0030, 1.0017,\n",
       "         1.0012, 1.0009, 1.0025, 1.0033, 1.0031, 0.9980, 1.0000, 0.9995, 0.9986,\n",
       "         1.0015, 1.0018, 0.9999, 0.9972, 0.9975, 1.0020, 0.9995, 0.9996, 1.0019,\n",
       "         1.0018, 0.9998, 0.9992, 1.0031, 0.9989, 1.0015, 1.0036, 0.9955, 1.0006,\n",
       "         1.0017, 0.9994, 0.9990, 1.0011, 1.0011, 1.0026, 1.0024, 1.0038, 0.9981,\n",
       "         1.0030, 0.9984, 1.0027, 1.0054, 1.0022, 0.9976, 0.9986, 0.9975, 1.0036,\n",
       "         1.0033, 0.9993, 1.0020, 1.0014, 1.0022, 1.0012, 0.9975, 0.9990, 0.9981,\n",
       "         1.0019, 0.9998, 0.9996, 1.0012, 1.0050, 1.0027, 1.0011, 0.9934, 0.9953,\n",
       "         0.9997, 1.0017, 1.0044, 1.0008, 1.0009, 1.0012, 1.0033, 1.0013, 0.9990,\n",
       "         1.0003, 1.0016, 0.9988, 0.9960, 1.0032, 0.9973, 1.0010, 1.0009, 0.9992,\n",
       "         0.9994, 1.0032, 1.0026, 0.9999, 1.0015, 1.0017, 1.0014, 1.0018, 1.0008,\n",
       "         1.0035, 1.0025, 1.0013, 1.0018, 0.9946, 1.0016, 1.0028, 1.0010, 1.0017,\n",
       "         0.9982, 1.0024, 0.9973, 1.0037, 0.9967, 1.0005, 1.0001, 1.0016, 0.9976,\n",
       "         0.9973, 0.9980, 1.0012, 0.9992, 1.0039, 0.9994, 1.0007, 1.0000, 1.0014,\n",
       "         1.0030, 1.0025, 1.0008, 1.0025, 1.0011, 0.9989, 0.9991, 0.9987, 1.0028,\n",
       "         0.9984, 1.0021, 1.0008, 1.0022, 1.0001, 1.0001, 1.0024, 1.0033, 0.9968,\n",
       "         1.0036, 1.0027, 0.9967, 1.0000, 1.0036, 1.0010, 1.0019, 1.0020, 1.0012,\n",
       "         1.0034, 1.0029, 0.9977, 0.9982, 1.0020, 0.9993, 1.0020, 1.0003, 1.0006,\n",
       "         0.9996, 1.0018, 1.0045, 1.0026, 0.9990, 1.0021, 1.0020, 1.0063, 1.0037,\n",
       "         1.0013, 1.0012, 1.0006, 1.0015, 1.0031, 0.9993, 0.9994, 0.9957, 1.0011,\n",
       "         1.0023, 1.0025, 0.9972, 1.0002, 0.9979, 1.0012, 1.0026, 0.9998, 1.0025,\n",
       "         0.9997, 0.9997, 0.9989, 1.0004, 1.0013, 1.0004, 0.9975, 1.0040, 0.9977,\n",
       "         1.0005, 1.0014, 0.9996, 0.9971, 1.0030, 0.9999, 0.9968, 1.0018, 1.0006,\n",
       "         1.0012, 0.9980, 1.0019, 0.9990, 1.0001, 0.9954, 1.0024, 1.0067, 0.9993,\n",
       "         1.0015, 1.0031, 0.9993, 0.9969, 0.9996, 1.0002, 1.0005, 1.0025, 0.9999,\n",
       "         1.0001, 1.0048, 1.0002, 1.0006, 1.0041, 1.0012, 1.0024, 1.0039, 1.0037,\n",
       "         1.0038, 0.9994, 0.9983, 1.0014, 1.0031, 0.9989, 0.9981, 0.9994, 1.0013,\n",
       "         1.0005, 1.0010, 0.9998, 1.0016, 0.9957, 1.0029, 1.0004, 1.0022, 1.0001,\n",
       "         1.0004, 1.0002, 1.0010, 1.0023, 1.0011, 1.0031, 1.0008, 0.9988, 1.0011,\n",
       "         0.9987, 1.0007, 1.0016, 1.0013, 1.0003, 1.0025, 1.0026, 1.0017, 1.0007,\n",
       "         0.9975, 1.0000, 1.0009, 1.0049, 1.0016, 1.0001, 0.9992, 0.9996, 0.9970,\n",
       "         1.0039, 1.0014, 0.9985, 0.9990, 1.0017, 1.0005, 1.0037, 0.9958, 0.9974,\n",
       "         1.0011, 1.0054, 1.0002, 1.0037, 1.0007, 1.0026, 0.9997, 0.9984, 0.9992,\n",
       "         1.0015, 1.0018, 0.9990, 1.0012, 0.9990, 0.9982, 1.0015, 0.9993, 0.9965,\n",
       "         1.0011, 0.9978, 0.9986, 1.0030, 1.0042, 0.9978, 1.0020, 0.9989, 1.0012,\n",
       "         1.0019, 0.9981, 0.9977, 1.0001, 1.0006, 0.9984, 1.0044, 1.0008, 1.0019,\n",
       "         1.0010, 1.0003, 1.0007, 0.9992, 1.0022, 1.0031, 1.0001, 1.0036, 0.9995,\n",
       "         0.9993, 0.9999, 0.9999, 0.9998, 1.0008, 0.9972, 0.9993, 1.0018, 1.0025,\n",
       "         0.9994, 0.9980, 1.0032, 1.0024, 1.0051, 0.9992, 1.0005, 1.0016, 1.0015,\n",
       "         0.9998, 1.0005, 1.0000, 0.9994, 1.0011, 1.0030, 1.0006, 1.0003, 0.9984,\n",
       "         0.9995, 1.0021, 0.9988, 1.0025, 1.0009, 1.0006, 1.0023, 1.0026, 0.9994,\n",
       "         1.0029, 0.9977, 0.9986, 0.9974, 1.0024, 0.9999, 0.9991, 0.9972, 0.9980,\n",
       "         1.0008, 1.0051, 0.9981, 1.0034, 0.9990, 0.9992, 1.0008, 1.0034, 0.9982,\n",
       "         1.0027, 0.9976, 1.0002, 1.0043, 1.0008, 1.0020, 0.9993, 0.9995, 1.0039,\n",
       "         0.9990, 0.9976, 0.9991, 1.0008, 0.9991, 1.0029, 1.0045, 1.0010, 1.0031,\n",
       "         1.0017, 1.0010, 0.9997, 1.0026, 1.0008, 0.9987, 1.0019, 0.9986, 1.0042,\n",
       "         1.0033, 1.0010, 1.0021, 1.0003, 0.9985, 1.0019, 1.0064, 0.9996, 0.9992,\n",
       "         0.9995, 1.0011, 1.0027, 1.0017, 1.0022, 1.0001, 0.9974, 0.9987]),\n",
       " 'transformer.resblocks.8.ln_2.bias': tensor([ 2.3011e-03,  2.5127e-03, -1.7098e-03, -9.5917e-04, -1.2735e-03,\n",
       "         -8.7483e-05,  1.2196e-03, -1.0831e-03, -9.8001e-04, -7.4484e-04,\n",
       "          1.1132e-03,  4.1934e-03, -3.4075e-04,  7.0580e-04,  1.5773e-04,\n",
       "         -1.7331e-03, -2.1581e-03, -2.4673e-03,  1.9193e-03,  1.7352e-03,\n",
       "         -2.4851e-03, -1.2136e-03,  1.2043e-04, -1.9191e-03,  2.4586e-04,\n",
       "          1.0937e-03,  1.9631e-04,  2.2133e-03,  1.9286e-03,  1.5427e-03,\n",
       "         -2.0090e-03, -3.8428e-04,  2.5001e-03, -7.3364e-04,  7.7987e-04,\n",
       "         -1.2650e-03, -2.7140e-03,  7.3956e-04, -2.3368e-03, -7.5897e-04,\n",
       "         -1.8110e-05, -1.0108e-03, -1.5585e-03, -2.1222e-03,  1.0353e-03,\n",
       "         -1.4157e-03,  1.4018e-05, -1.6121e-03,  1.5480e-04,  1.2041e-03,\n",
       "          1.6011e-03, -2.6031e-04, -2.3672e-04, -3.4007e-03,  4.6770e-04,\n",
       "         -4.7875e-03,  2.7161e-03,  1.5053e-03, -1.3829e-03,  1.5180e-03,\n",
       "          3.3027e-03, -2.2938e-03,  1.4283e-04, -5.4528e-04, -2.2669e-03,\n",
       "          5.9171e-05, -2.5232e-03,  1.9072e-04,  1.5001e-03,  2.0051e-03,\n",
       "         -2.9396e-04,  9.1459e-04,  1.9216e-03, -1.7287e-03, -4.4068e-04,\n",
       "         -2.3827e-03,  1.5486e-03,  1.9353e-03,  4.9485e-04,  3.2687e-04,\n",
       "         -2.4475e-03, -7.5320e-06, -1.6034e-04, -2.3269e-03, -4.7197e-04,\n",
       "          2.6825e-03, -1.9015e-04,  1.2920e-03, -6.4671e-05,  2.5925e-03,\n",
       "          6.9577e-04,  9.4135e-04,  9.4233e-05,  2.3024e-05, -2.2326e-03,\n",
       "          1.3020e-03,  2.8989e-03,  2.4743e-04,  2.0695e-03, -9.6136e-04,\n",
       "          8.5689e-04,  9.9830e-04, -2.7555e-03,  1.1846e-03, -8.3171e-04,\n",
       "          1.4157e-03,  1.2360e-03, -3.1704e-04,  7.6072e-04,  1.1961e-03,\n",
       "         -1.2548e-03,  1.4143e-03, -2.6876e-03,  3.2265e-03,  2.2365e-03,\n",
       "          3.4006e-03, -1.5755e-03, -6.6756e-04,  1.7010e-03,  2.0767e-04,\n",
       "          2.0364e-03,  4.1720e-03,  8.1718e-05,  2.3296e-03, -6.1511e-04,\n",
       "         -4.2302e-04, -2.8007e-04,  2.6034e-03, -5.3874e-03, -7.3165e-04,\n",
       "          7.9796e-04, -1.2762e-03,  1.8492e-03,  4.1444e-03, -3.8130e-03,\n",
       "         -3.6220e-03,  1.1032e-03,  1.5070e-03,  1.5484e-03, -6.5102e-04,\n",
       "         -2.9310e-03, -5.7443e-03, -3.0446e-03, -6.1908e-04, -3.5396e-03,\n",
       "          1.4684e-04,  3.2196e-03, -4.4634e-04, -6.2507e-04, -1.1959e-03,\n",
       "          9.4363e-04, -6.0624e-03, -4.8345e-03, -1.0864e-03, -7.6390e-04,\n",
       "         -7.1857e-04,  1.4274e-03,  1.7342e-03, -8.8981e-04,  5.9688e-04,\n",
       "         -7.1088e-04, -1.6846e-03, -3.0978e-03, -9.0981e-04,  1.9010e-03,\n",
       "         -1.0041e-03, -6.4508e-04, -9.5461e-04,  2.8803e-04, -2.7789e-03,\n",
       "         -2.5258e-03, -1.2042e-03,  2.7835e-03,  6.4306e-04, -6.7749e-04,\n",
       "          9.5592e-04, -4.4445e-04, -3.0641e-03,  2.9518e-03,  2.5887e-04,\n",
       "         -8.4060e-04,  1.6207e-03, -2.0011e-03,  9.3856e-04, -4.0013e-03,\n",
       "          1.9947e-03, -1.1795e-03, -1.1101e-03,  2.0091e-05,  3.2921e-03,\n",
       "         -5.8335e-04, -1.4088e-03, -2.8567e-03, -3.3054e-03,  9.8485e-04,\n",
       "          1.2550e-03, -1.1250e-03, -2.9060e-03, -6.3516e-03, -2.8632e-04,\n",
       "          1.5171e-03,  2.4654e-03, -1.3080e-03, -8.0783e-04, -4.8405e-03,\n",
       "         -1.4247e-03,  9.6909e-04, -4.3409e-03,  1.0504e-03, -1.5964e-03,\n",
       "         -2.2171e-03, -3.4153e-04,  1.6238e-03, -6.7550e-05,  4.0060e-04,\n",
       "         -2.2271e-03,  2.7284e-03, -9.3363e-04,  6.9929e-04,  1.2584e-03,\n",
       "          3.6967e-03,  5.1197e-04, -9.2422e-04, -3.0239e-03, -2.8477e-04,\n",
       "         -1.1025e-03, -4.7654e-04,  4.2901e-03,  5.1810e-04,  1.9671e-04,\n",
       "          7.2710e-04,  5.8677e-03, -1.0180e-03,  2.7254e-03, -3.0994e-04,\n",
       "         -3.0674e-03, -2.0908e-03,  4.4594e-03, -2.3005e-03, -4.2505e-04,\n",
       "         -1.0046e-03, -2.0580e-03, -7.9234e-05,  2.2607e-03, -2.9511e-04,\n",
       "         -1.0309e-03, -9.5994e-04, -1.7391e-03, -1.7058e-03,  1.0450e-03,\n",
       "         -8.4951e-04,  1.8070e-03, -7.6619e-04,  3.3608e-04,  6.7552e-04,\n",
       "         -4.9295e-04,  1.0356e-03,  1.1623e-04,  7.3234e-04,  1.8644e-03,\n",
       "         -4.2196e-03, -2.5269e-03,  3.0237e-03,  2.9964e-03,  1.0808e-03,\n",
       "          2.7707e-04, -1.8622e-03, -3.4797e-03,  1.7023e-03,  2.0536e-04,\n",
       "         -1.9898e-03,  2.3049e-03,  1.6806e-03, -6.1741e-04, -2.6792e-04,\n",
       "          8.3595e-04, -2.7336e-03, -1.6403e-03, -5.5917e-03,  2.0565e-03,\n",
       "         -3.0893e-03, -8.6169e-04,  2.9296e-03, -7.4359e-04,  3.6472e-03,\n",
       "         -5.0286e-03,  3.1248e-03, -2.4380e-03, -9.8484e-04,  1.8533e-03,\n",
       "         -1.3626e-03, -2.5225e-03, -5.0611e-04, -3.0911e-03, -1.2588e-03,\n",
       "         -1.3413e-03,  8.7885e-04, -1.3439e-03,  4.4958e-05,  8.0927e-04,\n",
       "          2.5100e-03,  1.1108e-03, -2.1967e-03, -1.1868e-03, -1.5497e-03,\n",
       "          1.9211e-03, -2.9929e-03,  8.0506e-04,  6.9635e-04,  2.5967e-03,\n",
       "         -8.3096e-04,  8.6623e-04, -1.8483e-03, -9.6527e-04,  1.2019e-03,\n",
       "          2.2758e-03, -6.4545e-04, -3.1267e-03, -6.0004e-04, -5.5635e-04,\n",
       "         -4.7371e-03, -3.0549e-06, -4.2792e-04, -1.1816e-03, -5.8148e-04,\n",
       "          1.0296e-03, -6.2120e-03, -1.3526e-03,  5.4416e-03, -1.3122e-03,\n",
       "          6.4585e-04,  1.0699e-03, -6.4224e-04, -5.1055e-04,  7.9158e-04,\n",
       "         -2.2742e-03, -5.0969e-03, -1.1540e-03, -1.2046e-03, -8.2521e-04,\n",
       "          2.9673e-03, -2.1139e-03, -6.1241e-05,  3.5101e-04,  1.0398e-03,\n",
       "          2.5963e-03, -1.7563e-03,  1.0135e-03, -4.9608e-04, -2.3422e-03,\n",
       "          2.1585e-03,  2.1936e-03,  3.4960e-04, -1.6799e-03,  8.1529e-04,\n",
       "         -1.1467e-03, -1.0628e-03, -2.0779e-03, -1.0394e-03,  1.5767e-05,\n",
       "          1.0253e-03, -1.7827e-03, -5.4474e-04, -4.3795e-04, -7.0778e-04,\n",
       "         -3.1101e-04, -4.5856e-04,  5.2532e-03, -2.7651e-03, -1.0200e-03,\n",
       "          4.4005e-04, -1.3951e-03, -1.9548e-03,  4.8686e-03,  1.3928e-03,\n",
       "         -3.6204e-03,  1.0028e-04, -1.5836e-03, -8.5770e-04,  3.9760e-03,\n",
       "         -2.8459e-03,  8.5329e-04,  2.2546e-03, -1.2419e-03,  2.5461e-03,\n",
       "          1.2835e-03, -3.9443e-03,  2.3949e-03, -4.2037e-03,  1.6183e-03,\n",
       "         -2.1869e-03,  1.2254e-03,  2.5127e-03,  1.1541e-03, -3.7045e-03,\n",
       "          7.5064e-04, -9.5913e-04, -1.5132e-03,  7.5692e-04, -1.0666e-03,\n",
       "         -2.8910e-03,  4.4359e-03, -2.6082e-04, -2.4501e-04, -2.5003e-03,\n",
       "         -8.7393e-04, -3.6243e-04, -4.3460e-03, -8.7239e-04, -2.4072e-03,\n",
       "         -2.5282e-03, -2.2690e-04, -7.7598e-05, -1.6894e-03, -1.3158e-03,\n",
       "          2.1724e-03, -4.4613e-03, -1.5624e-03, -1.6514e-03, -2.4076e-03,\n",
       "          1.3766e-03, -2.8732e-03, -2.0709e-03,  3.1379e-03,  1.8511e-03,\n",
       "         -2.2130e-03, -3.2951e-03,  1.8696e-03,  2.5044e-04, -1.0773e-03,\n",
       "         -4.0514e-04, -1.4749e-03,  1.5049e-03, -8.6080e-04,  3.4484e-03,\n",
       "          1.9927e-04, -9.5371e-04,  4.1490e-03, -3.0351e-03,  7.6022e-04,\n",
       "         -6.5889e-04,  4.8237e-05, -1.3898e-03,  1.3811e-03,  1.7152e-03,\n",
       "         -1.2794e-03, -6.9724e-04,  2.3479e-03,  2.7309e-03, -2.6751e-03,\n",
       "         -4.3004e-03, -4.4666e-03,  1.3423e-03, -2.0584e-03,  2.7461e-04,\n",
       "          2.2630e-03, -2.7613e-03, -2.8348e-03, -2.8607e-03,  3.6467e-03,\n",
       "         -1.7426e-03,  2.0524e-03,  1.2506e-03, -4.0578e-03, -4.2209e-05,\n",
       "          7.7902e-04, -1.7668e-03, -2.1940e-03,  1.8394e-04,  3.1900e-03,\n",
       "          9.7551e-04,  3.1049e-04, -1.4804e-03,  2.5510e-03,  4.3037e-03,\n",
       "          1.7963e-03, -2.8899e-04, -7.3728e-04,  9.0286e-04, -1.3698e-03,\n",
       "          3.2556e-03,  5.3199e-03,  7.1726e-04,  1.0728e-03,  6.2453e-04,\n",
       "          1.0623e-03, -2.7081e-04,  2.6997e-03, -5.4729e-04,  5.0365e-05,\n",
       "         -1.0921e-03,  3.9381e-04, -2.3578e-04,  1.0873e-03,  3.6798e-03,\n",
       "          2.7517e-03,  1.2051e-03,  1.7532e-03,  6.4707e-04,  2.5986e-03,\n",
       "          5.2898e-03,  1.6435e-03,  1.7517e-03, -2.2567e-03, -7.5296e-04,\n",
       "         -2.7974e-03,  2.2436e-03, -4.8046e-03, -1.4909e-03, -6.2556e-04,\n",
       "          5.5841e-03,  6.1170e-04]),\n",
       " 'transformer.resblocks.9.attn.in_proj_weight': tensor([[ 0.0071,  0.0530, -0.0220,  ..., -0.0666,  0.0309, -0.0225],\n",
       "         [ 0.0244,  0.0708,  0.0133,  ...,  0.0113, -0.0081,  0.0320],\n",
       "         [ 0.0296,  0.0087,  0.0470,  ...,  0.0020,  0.0155,  0.0470],\n",
       "         ...,\n",
       "         [-0.0092,  0.0050,  0.0205,  ...,  0.0542,  0.0326,  0.0224],\n",
       "         [-0.0479,  0.0339, -0.0130,  ...,  0.0269,  0.0277, -0.0006],\n",
       "         [ 0.0115,  0.0367,  0.0759,  ...,  0.0317, -0.0082, -0.0673]]),\n",
       " 'transformer.resblocks.9.attn.in_proj_bias': tensor([ 0.0005,  0.0058,  0.0005,  ..., -0.0003,  0.0011, -0.0019]),\n",
       " 'transformer.resblocks.9.attn.out_proj.weight': tensor([[-0.0121,  0.0140,  0.0105,  ..., -0.0002, -0.0134,  0.0114],\n",
       "         [ 0.0042, -0.0101,  0.0046,  ..., -0.0106,  0.0022,  0.0071],\n",
       "         [-0.0031,  0.0144,  0.0031,  ..., -0.0046, -0.0071, -0.0061],\n",
       "         ...,\n",
       "         [ 0.0132, -0.0124,  0.0209,  ..., -0.0105, -0.0043, -0.0039],\n",
       "         [ 0.0055, -0.0024,  0.0040,  ...,  0.0083,  0.0012,  0.0075],\n",
       "         [ 0.0054,  0.0123,  0.0020,  ..., -0.0012, -0.0033, -0.0086]]),\n",
       " 'transformer.resblocks.9.attn.out_proj.bias': tensor([-7.8611e-04, -4.1480e-03, -3.0011e-04, -6.8723e-04, -8.0057e-04,\n",
       "         -3.0540e-03,  2.3493e-04,  2.3846e-03, -3.2709e-03,  2.0080e-03,\n",
       "         -4.8667e-03, -2.5177e-03, -6.7211e-04, -1.7566e-03, -6.0076e-03,\n",
       "          2.1253e-03, -2.9371e-04,  1.9740e-03, -3.4398e-04, -7.1077e-04,\n",
       "         -2.4873e-04,  7.7036e-04,  4.3105e-04, -1.2572e-03, -8.1285e-04,\n",
       "         -1.2987e-03, -1.2425e-03, -1.1216e-03, -8.4757e-04, -1.8711e-03,\n",
       "          9.0067e-04,  1.9751e-04,  2.7130e-03, -3.5064e-03, -3.6596e-04,\n",
       "          1.2567e-03,  1.1117e-03, -1.7535e-05,  8.7396e-04, -2.9942e-03,\n",
       "          1.4582e-03,  2.3016e-03,  1.8208e-03,  1.4081e-03, -4.2146e-03,\n",
       "          1.6593e-03, -3.4737e-03,  6.7375e-04, -5.2942e-04,  1.5587e-03,\n",
       "         -1.1432e-03, -7.8037e-04, -1.2426e-03,  2.5533e-03,  3.0394e-03,\n",
       "          1.2950e-03, -1.1692e-03, -1.0374e-03, -4.3428e-05, -9.2296e-04,\n",
       "         -2.1219e-03,  2.4965e-04,  5.7147e-04,  5.3037e-04, -2.2471e-04,\n",
       "         -8.1872e-04,  1.8596e-03,  6.4585e-04, -1.4406e-03,  1.6076e-03,\n",
       "         -8.6549e-04, -1.8957e-04, -2.3363e-03,  1.8604e-04, -9.2358e-04,\n",
       "          1.9913e-03, -1.7313e-03, -1.9960e-03, -2.4263e-03,  1.9654e-03,\n",
       "          1.7256e-03, -2.2199e-03,  8.4835e-04,  1.9309e-03,  2.0012e-03,\n",
       "          1.7630e-04, -6.3123e-04,  6.0444e-04,  2.5830e-04, -2.2353e-03,\n",
       "          3.4738e-04, -7.8883e-04,  1.7279e-03, -1.8614e-03, -1.7901e-03,\n",
       "         -4.4490e-03, -3.3805e-04, -1.3516e-03, -1.8508e-03,  5.2330e-04,\n",
       "          8.0987e-04, -1.6114e-03,  4.5499e-04, -5.7093e-04, -6.8121e-04,\n",
       "         -3.0382e-03, -3.6870e-03, -9.7791e-04,  9.9437e-04, -3.7880e-04,\n",
       "          2.9346e-03,  1.0344e-03,  3.0610e-03,  9.9701e-04, -1.6245e-03,\n",
       "         -4.8327e-03,  1.4109e-03,  1.8418e-03, -3.0078e-03, -1.0848e-03,\n",
       "          5.5593e-04, -1.8832e-03,  1.9018e-03, -6.9477e-04, -1.3577e-03,\n",
       "          5.7366e-04, -1.5297e-03, -2.4021e-03, -1.0777e-03, -6.2343e-05,\n",
       "         -2.2302e-04,  3.7643e-03, -4.0780e-04, -2.6528e-03, -5.2126e-04,\n",
       "         -1.0929e-03, -1.3222e-03,  2.1803e-03, -1.1100e-03,  8.4854e-04,\n",
       "          3.5640e-04,  1.9047e-03,  1.3330e-03,  3.4678e-03,  1.3545e-03,\n",
       "          2.5211e-03, -2.4516e-03,  8.3894e-04,  6.4599e-04,  8.1361e-04,\n",
       "          1.9790e-03,  5.0189e-03,  6.4045e-03,  1.8526e-03, -7.4895e-04,\n",
       "         -2.1268e-04,  1.0986e-03, -1.7473e-03,  3.2026e-04,  3.6565e-04,\n",
       "         -1.9972e-03, -1.1629e-03,  3.4312e-03,  2.1887e-04, -1.2603e-03,\n",
       "          1.3928e-03,  2.2412e-03, -2.0530e-04, -3.4147e-04, -1.9398e-03,\n",
       "          1.4270e-03, -2.9408e-04, -2.0041e-03,  4.3909e-04,  2.1544e-03,\n",
       "          1.1626e-03, -1.5744e-03,  5.2899e-04,  3.0048e-04, -1.2257e-03,\n",
       "         -2.3803e-03, -5.5006e-04,  1.3050e-03, -3.1782e-03,  4.7754e-03,\n",
       "         -1.6954e-04,  6.2265e-04, -2.6615e-04,  1.2474e-03, -2.6062e-03,\n",
       "          6.5303e-04,  6.6679e-04, -3.5746e-04,  1.8069e-03, -1.3851e-03,\n",
       "         -1.5126e-03,  1.7162e-03,  3.0673e-03,  4.0881e-03,  3.1419e-03,\n",
       "         -4.5901e-04, -1.6145e-03,  1.6310e-03,  1.8206e-03,  8.3910e-04,\n",
       "          6.8242e-04, -1.3410e-03,  6.8103e-04,  1.2708e-03,  2.8450e-04,\n",
       "          2.5869e-03,  1.0707e-04,  6.4327e-05,  1.7671e-03, -2.0129e-03,\n",
       "         -1.8560e-03, -2.4180e-04, -1.8629e-03, -1.4995e-03,  6.7737e-04,\n",
       "         -8.5262e-04, -6.0399e-04,  1.6931e-03,  2.1784e-03,  1.1098e-03,\n",
       "          1.0216e-03, -1.1089e-03, -2.7018e-03,  1.1234e-03,  1.8278e-03,\n",
       "         -1.7061e-03,  4.5633e-04,  8.4204e-04, -1.3034e-03,  1.5979e-03,\n",
       "          3.6736e-04,  2.8701e-03, -3.6834e-03,  1.7847e-03,  1.6305e-03,\n",
       "         -1.5567e-03,  2.7016e-03,  2.9293e-03,  1.5380e-03, -2.7369e-03,\n",
       "         -7.8759e-06,  7.1163e-04,  6.8528e-04, -2.6645e-04, -1.5904e-03,\n",
       "          1.0195e-03, -2.4039e-03, -2.0806e-04, -1.5456e-03, -6.8009e-04,\n",
       "         -1.0517e-03, -2.6950e-04, -2.1766e-03, -8.1920e-05, -3.5171e-03,\n",
       "          1.7170e-03,  1.0195e-03,  8.6754e-04, -3.5845e-03, -1.8818e-03,\n",
       "          2.9158e-03,  7.9358e-04,  1.1655e-03, -5.5007e-04, -1.6214e-03,\n",
       "          1.0503e-03, -1.1938e-03, -1.7770e-03,  3.0406e-04,  1.1696e-03,\n",
       "         -6.9941e-04,  3.5522e-03, -6.7200e-04,  3.6206e-03, -1.6155e-03,\n",
       "         -1.2714e-04,  3.9540e-03, -2.6591e-03, -1.8175e-03, -2.8097e-03,\n",
       "          1.6254e-03,  3.3664e-05,  1.5040e-03,  2.1071e-03,  1.5885e-04,\n",
       "         -5.8692e-04,  1.4273e-03, -1.8397e-03,  4.6336e-03,  4.9601e-04,\n",
       "          1.9672e-03, -1.7646e-03,  1.2096e-03, -6.5631e-04, -1.0617e-03,\n",
       "         -3.0868e-03,  7.2848e-04,  1.6563e-03, -4.0415e-03,  7.0546e-05,\n",
       "         -2.2786e-03,  6.5707e-04,  3.1266e-04, -1.2061e-03, -2.5763e-03,\n",
       "          2.0127e-03,  2.8253e-03, -8.3781e-04, -1.7494e-03,  7.0285e-04,\n",
       "          2.4080e-03,  2.9233e-03,  1.5793e-03,  7.3420e-04,  1.0373e-03,\n",
       "          2.8758e-03, -7.6425e-04, -7.4812e-04,  2.6871e-04, -2.6187e-03,\n",
       "          4.0995e-04,  3.6620e-03, -6.2353e-05, -3.8682e-03, -1.7416e-03,\n",
       "         -7.7765e-04,  1.4781e-03,  5.1475e-04, -7.0830e-04,  1.4190e-03,\n",
       "          8.7530e-06,  1.5957e-04, -3.0286e-04,  9.5570e-04,  2.0586e-03,\n",
       "         -3.3950e-03,  3.3213e-03,  1.8419e-04, -4.7686e-04, -3.4185e-03,\n",
       "         -1.2990e-04, -2.5225e-03, -7.5702e-05, -9.4387e-04,  1.4657e-03,\n",
       "         -1.7333e-03, -4.1498e-03, -2.0604e-03, -3.4823e-04, -9.5471e-05,\n",
       "          6.5622e-04,  3.2280e-03,  2.8719e-03,  1.1439e-03, -2.6127e-03,\n",
       "          1.4552e-03, -4.5915e-04,  2.8438e-03,  3.1832e-03, -3.4060e-04,\n",
       "          2.8292e-03, -2.9171e-04, -4.7096e-03,  1.9523e-03, -8.1116e-04,\n",
       "         -1.1814e-03,  1.4816e-03,  7.6737e-04, -4.5386e-04, -1.8599e-04,\n",
       "          3.3315e-03, -1.1037e-03,  2.7790e-03, -1.7423e-03, -2.1437e-03,\n",
       "          2.6565e-03,  1.9918e-03, -1.8396e-03, -3.9405e-04, -1.1888e-03,\n",
       "         -1.1219e-03,  3.0707e-03, -1.0311e-03,  3.8089e-03,  1.5058e-03,\n",
       "          1.4502e-04, -1.7201e-04, -4.7239e-04,  9.2947e-04,  3.3036e-03,\n",
       "          2.1717e-03, -1.0697e-03,  2.2744e-03,  2.5030e-04,  4.1695e-04,\n",
       "         -2.9914e-04, -1.5817e-03, -4.5082e-04, -6.4402e-04, -6.7233e-05,\n",
       "          1.7743e-03,  1.0094e-03,  1.2060e-03,  1.9997e-03,  1.9396e-03,\n",
       "         -5.7845e-04, -3.3891e-03,  4.5955e-04,  2.7885e-03,  9.2034e-04,\n",
       "         -2.6154e-03,  2.3598e-03,  2.1128e-03, -4.0450e-04,  2.7576e-03,\n",
       "         -1.0943e-03,  1.3163e-03,  1.8052e-03, -2.2236e-03, -1.9684e-03,\n",
       "          4.6789e-04, -4.1862e-04, -4.6908e-04,  8.1592e-04,  6.1381e-04,\n",
       "         -2.0136e-03,  1.8366e-03,  1.5293e-03, -8.9606e-04, -3.5539e-03,\n",
       "         -1.2271e-03,  2.1205e-03, -3.0467e-03,  1.7062e-03, -1.3713e-03,\n",
       "         -1.3806e-03,  8.1670e-04,  8.9146e-04, -4.0484e-04, -2.3157e-03,\n",
       "         -2.8471e-03, -4.9800e-04,  1.0201e-03, -3.8955e-04,  1.7154e-03,\n",
       "          4.0343e-04,  1.2511e-03, -1.0663e-03,  5.0573e-03,  1.3353e-03,\n",
       "         -2.2325e-03,  5.9787e-04,  2.6029e-03,  2.3766e-03,  1.7810e-04,\n",
       "          1.0429e-03, -1.2016e-03,  9.3334e-04,  3.8050e-03,  3.1848e-04,\n",
       "          1.5734e-04,  1.1522e-04,  1.0060e-03,  6.8471e-05, -2.2441e-03,\n",
       "         -2.0246e-03,  1.3180e-03, -2.6064e-03, -1.3630e-03, -1.8851e-03,\n",
       "          5.1216e-04, -2.7899e-03, -1.4826e-03, -1.6202e-03, -1.0776e-03,\n",
       "         -3.3202e-03, -2.3606e-03,  1.1621e-03,  4.8774e-04, -1.1435e-03,\n",
       "          7.2484e-04, -1.1843e-03, -2.0213e-03,  2.3767e-03,  1.8097e-03,\n",
       "          8.8568e-04, -1.0737e-03,  1.2722e-03, -2.3800e-03, -1.9715e-03,\n",
       "          3.4783e-04,  1.6229e-03, -5.2220e-04,  2.2582e-04, -3.1544e-03,\n",
       "         -2.0679e-03,  7.2191e-04, -1.7975e-03,  3.4353e-03, -2.9721e-03,\n",
       "          1.3420e-03, -2.0939e-03, -9.5144e-04,  1.2197e-03, -3.6529e-04,\n",
       "         -1.7273e-03, -6.3125e-04]),\n",
       " 'transformer.resblocks.9.ln_1.weight': tensor([1.0008, 1.0000, 1.0005, 0.9994, 0.9983, 0.9997, 1.0002, 0.9994, 0.9997,\n",
       "         1.0020, 0.9992, 1.0001, 1.0014, 1.0013, 0.9965, 0.9994, 1.0011, 0.9990,\n",
       "         0.9992, 1.0012, 0.9988, 0.9997, 1.0002, 1.0012, 0.9990, 1.0012, 0.9996,\n",
       "         0.9981, 0.9969, 0.9976, 1.0017, 0.9998, 0.9998, 0.9997, 0.9972, 1.0001,\n",
       "         1.0028, 1.0006, 0.9983, 0.9962, 1.0027, 0.9976, 1.0013, 0.9993, 1.0005,\n",
       "         0.9984, 1.0024, 0.9977, 0.9997, 0.9997, 0.9993, 1.0002, 0.9982, 1.0021,\n",
       "         1.0001, 0.9987, 0.9995, 1.0003, 0.9981, 1.0001, 0.9983, 0.9981, 0.9990,\n",
       "         0.9991, 1.0023, 0.9963, 0.9985, 0.9993, 0.9975, 1.0011, 0.9994, 0.9963,\n",
       "         0.9987, 1.0001, 1.0012, 1.0007, 1.0009, 0.9999, 1.0009, 1.0001, 0.9974,\n",
       "         0.9999, 0.9987, 1.0003, 0.9987, 1.0004, 1.0005, 1.0004, 0.9989, 1.0038,\n",
       "         0.9991, 0.9996, 0.9989, 0.9974, 0.9966, 0.9986, 1.0003, 1.0005, 0.9997,\n",
       "         1.0001, 0.9968, 0.9999, 0.9987, 1.0015, 1.0001, 0.9972, 0.9984, 0.9993,\n",
       "         1.0017, 0.9973, 0.9999, 1.0004, 0.9984, 1.0014, 1.0005, 0.9992, 1.0006,\n",
       "         0.9976, 0.9979, 0.9974, 0.9993, 1.0010, 1.0015, 1.0015, 1.0020, 0.9984,\n",
       "         0.9982, 0.9944, 0.9991, 0.9994, 0.9980, 0.9959, 1.0005, 1.0001, 1.0001,\n",
       "         0.9998, 1.0000, 0.9994, 0.9989, 0.9995, 0.9974, 1.0002, 0.9978, 1.0019,\n",
       "         0.9996, 0.9979, 0.9989, 1.0006, 0.9990, 0.9977, 0.9989, 0.9990, 0.9987,\n",
       "         1.0015, 0.9989, 1.0000, 0.9989, 0.9994, 0.9983, 0.9988, 0.9991, 1.0007,\n",
       "         0.9967, 1.0024, 1.0007, 1.0014, 1.0011, 1.0002, 1.0008, 1.0003, 0.9998,\n",
       "         0.9992, 0.9970, 1.0006, 0.9988, 0.9987, 1.0007, 0.9996, 1.0000, 1.0004,\n",
       "         1.0009, 1.0003, 1.0012, 0.9958, 1.0000, 0.9980, 0.9998, 1.0007, 0.9974,\n",
       "         1.0006, 1.0020, 0.9994, 0.9965, 0.9974, 0.9980, 1.0003, 1.0001, 0.9956,\n",
       "         0.9997, 0.9984, 0.9990, 0.9974, 1.0004, 1.0011, 0.9994, 0.9996, 0.9996,\n",
       "         1.0000, 0.9989, 0.9995, 1.0011, 0.9987, 1.0026, 0.9997, 0.9995, 1.0015,\n",
       "         0.9989, 0.9996, 0.9976, 0.9999, 1.0025, 1.0005, 0.9992, 0.9997, 0.9999,\n",
       "         0.9964, 0.9998, 0.9993, 0.9997, 0.9955, 1.0014, 1.0019, 0.9959, 1.0005,\n",
       "         0.9978, 1.0040, 1.0009, 0.9979, 0.9984, 1.0013, 0.9998, 0.9957, 0.9986,\n",
       "         0.9975, 1.0009, 1.0021, 0.9999, 1.0004, 0.9981, 0.9982, 1.0001, 0.9971,\n",
       "         0.9978, 1.0008, 1.0008, 0.9996, 1.0003, 0.9999, 1.0023, 1.0006, 0.9996,\n",
       "         1.0007, 1.0027, 0.9973, 0.9972, 1.0006, 1.0009, 0.9994, 0.9990, 0.9995,\n",
       "         0.9982, 0.9979, 0.9984, 0.9988, 0.9980, 1.0003, 0.9990, 0.9979, 1.0002,\n",
       "         0.9987, 0.9995, 0.9983, 1.0006, 0.9976, 1.0007, 1.0013, 0.9981, 0.9999,\n",
       "         0.9994, 1.0010, 1.0005, 1.0021, 0.9992, 0.9968, 1.0001, 1.0016, 0.9991,\n",
       "         1.0013, 1.0001, 0.9983, 0.9985, 0.9987, 0.9988, 0.9993, 1.0003, 1.0013,\n",
       "         1.0000, 1.0003, 1.0010, 0.9977, 0.9991, 0.9977, 1.0001, 1.0009, 0.9998,\n",
       "         0.9990, 0.9998, 1.0018, 0.9991, 0.9973, 0.9993, 0.9997, 1.0005, 0.9991,\n",
       "         0.9986, 0.9996, 0.9998, 1.0002, 1.0005, 1.0010, 0.9993, 0.9965, 0.9996,\n",
       "         0.9976, 0.9981, 1.0005, 0.9997, 1.0005, 1.0011, 1.0028, 0.9985, 0.9980,\n",
       "         1.0010, 1.0007, 0.9987, 0.9983, 1.0004, 0.9987, 1.0000, 1.0004, 0.9998,\n",
       "         0.9972, 1.0003, 0.9986, 1.0000, 0.9975, 1.0008, 0.9988, 0.9987, 0.9979,\n",
       "         1.0004, 0.9990, 0.9977, 0.9976, 0.9991, 0.9998, 0.9962, 0.9987, 1.0016,\n",
       "         0.9975, 1.0001, 1.0001, 0.9991, 0.9995, 1.0022, 1.0014, 0.9970, 0.9993,\n",
       "         0.9974, 0.9993, 1.0003, 1.0006, 1.0002, 1.0009, 1.0016, 0.9993, 0.9977,\n",
       "         0.9989, 0.9976, 1.0005, 1.0002, 0.9981, 0.9972, 0.9994, 0.9959, 0.9980,\n",
       "         0.9998, 0.9984, 0.9985, 1.0028, 0.9987, 1.0001, 1.0004, 0.9974, 0.9995,\n",
       "         0.9988, 1.0010, 0.9960, 0.9982, 0.9978, 1.0008, 0.9970, 0.9975, 0.9995,\n",
       "         1.0002, 0.9973, 0.9991, 1.0003, 1.0015, 0.9985, 1.0009, 0.9979, 0.9991,\n",
       "         1.0001, 0.9979, 0.9992, 0.9998, 0.9991, 0.9987, 0.9973, 1.0005, 1.0004,\n",
       "         1.0011, 1.0009, 0.9983, 1.0015, 0.9984, 0.9994, 1.0021, 1.0001, 0.9991,\n",
       "         0.9994, 0.9993, 0.9983, 0.9991, 0.9982, 1.0001, 0.9966, 0.9976, 1.0025,\n",
       "         0.9992, 0.9996, 1.0000, 0.9995, 1.0002, 0.9985, 1.0041, 0.9970, 1.0000,\n",
       "         1.0001, 0.9955, 0.9974, 0.9997, 0.9988, 0.9988, 0.9995, 0.9987, 1.0010,\n",
       "         0.9973, 0.9973, 0.9985, 0.9986, 0.9999, 0.9989, 1.0022, 1.0004, 1.0004,\n",
       "         0.9994, 0.9991, 1.0000, 0.9980, 0.9996, 1.0005, 1.0019, 0.9996, 1.0011,\n",
       "         0.9975, 0.9990, 0.9988, 0.9966, 1.0020, 0.9975, 0.9985, 0.9995, 0.9967,\n",
       "         0.9984, 0.9982, 0.9991, 0.9996, 0.9960, 0.9964, 0.9995, 1.0001, 0.9976,\n",
       "         0.9992, 0.9988, 0.9970, 0.9971, 0.9971, 1.0032, 1.0014, 0.9970]),\n",
       " 'transformer.resblocks.9.ln_1.bias': tensor([-2.5122e-03, -9.8939e-04,  3.7425e-04,  1.7820e-04,  1.1536e-03,\n",
       "         -5.3252e-04,  1.1213e-03,  2.1719e-04, -1.0934e-03,  1.7561e-03,\n",
       "         -5.8584e-04, -5.6394e-04, -2.0281e-03,  1.0530e-03,  2.4026e-03,\n",
       "         -2.5284e-04,  2.2617e-03, -1.6733e-03,  1.1295e-03, -8.4974e-04,\n",
       "          1.2684e-03,  1.0822e-03, -2.2196e-04, -1.7618e-03, -1.4690e-03,\n",
       "          2.2251e-04,  2.0873e-03,  2.3814e-03,  4.0387e-03,  1.8322e-03,\n",
       "          1.5101e-03, -3.3172e-05,  1.3302e-03,  5.4639e-04, -9.9552e-04,\n",
       "          1.4876e-03,  3.4256e-03,  2.7663e-03, -1.9083e-05, -7.1345e-04,\n",
       "          1.4109e-03, -4.9972e-04,  5.7013e-04, -1.1491e-03, -1.6786e-05,\n",
       "         -2.5613e-05, -1.1724e-03,  1.1895e-03, -1.4677e-03, -1.2281e-03,\n",
       "         -3.0019e-03,  2.4888e-04, -1.1227e-03,  2.5671e-03, -1.4128e-03,\n",
       "          1.6486e-03,  2.6760e-03,  1.4965e-03,  1.5820e-03, -3.1236e-03,\n",
       "          1.3084e-03, -6.7979e-04, -2.6292e-04, -2.8277e-04,  1.1530e-03,\n",
       "         -2.2884e-03, -4.1678e-04,  9.6123e-04,  2.5520e-03,  1.5925e-03,\n",
       "         -8.8099e-04, -2.0015e-04,  1.3746e-03, -1.2659e-03, -2.2137e-03,\n",
       "          2.6484e-03, -2.7943e-03, -7.3754e-04, -4.2957e-04,  1.6504e-03,\n",
       "         -3.6025e-03,  2.7787e-04,  2.5782e-03,  1.6461e-03,  1.0953e-03,\n",
       "          1.8185e-03, -2.1410e-03,  8.2708e-04,  2.0014e-03, -7.0549e-04,\n",
       "          2.6721e-03, -3.3214e-05, -3.0666e-03,  6.2275e-04,  6.9286e-04,\n",
       "         -2.8118e-04,  1.7571e-03,  2.1345e-03,  8.5836e-04, -1.6451e-03,\n",
       "         -6.2965e-04, -1.9204e-03, -1.1948e-04, -1.6280e-03, -9.6365e-04,\n",
       "          2.1883e-03,  2.9282e-03,  1.9840e-03,  8.0503e-04, -1.4672e-03,\n",
       "          1.4579e-03, -1.6525e-03, -2.1874e-03,  1.3315e-04,  1.4677e-03,\n",
       "         -7.1084e-04, -3.5540e-04, -3.8359e-03,  8.1082e-04,  5.7810e-04,\n",
       "         -2.1028e-03,  1.2635e-03, -4.5860e-03, -1.2122e-03, -7.7872e-04,\n",
       "          1.0386e-03, -7.7895e-04,  3.0909e-03,  4.7952e-04, -2.4783e-03,\n",
       "          2.9987e-03, -3.8877e-03,  1.2744e-03,  4.5732e-04, -8.5277e-04,\n",
       "         -1.0369e-04, -8.5490e-04,  8.8838e-04,  8.5809e-04,  1.3134e-03,\n",
       "          4.5325e-04,  2.3570e-03,  1.1921e-05,  4.1952e-03,  7.2734e-04,\n",
       "         -7.9702e-04,  7.0547e-04, -7.9735e-05,  2.0398e-03, -7.7074e-04,\n",
       "         -2.3249e-03,  2.5895e-04, -2.2248e-05,  9.6939e-04,  2.0558e-03,\n",
       "         -1.1008e-03, -2.1477e-03,  2.0426e-04,  1.7181e-03,  1.9230e-03,\n",
       "         -2.0209e-03, -1.3397e-04, -3.8503e-03,  1.1349e-03, -1.1507e-03,\n",
       "         -2.1321e-03,  1.8874e-03, -1.3604e-03,  7.0462e-04,  1.7717e-03,\n",
       "         -5.4590e-04,  1.7062e-03, -1.2946e-03,  7.2141e-04, -2.2096e-03,\n",
       "          1.2745e-03, -1.8409e-03,  1.1933e-03,  2.2997e-03, -4.0153e-04,\n",
       "         -7.9400e-04,  3.1041e-03,  1.8018e-03,  1.7943e-03,  1.4023e-03,\n",
       "          2.2521e-03, -8.1436e-05,  1.7957e-03, -3.7255e-03, -2.6587e-04,\n",
       "         -1.8840e-03,  8.6949e-04,  1.8923e-04, -5.0981e-05, -6.1847e-04,\n",
       "          1.5037e-04,  1.6932e-03, -2.8964e-03, -1.8175e-04, -1.8675e-04,\n",
       "          1.9735e-03,  1.8190e-03,  5.8971e-04,  1.1452e-03, -3.5918e-03,\n",
       "          1.4429e-03, -2.7554e-03, -9.9320e-04,  2.6314e-03,  1.1403e-04,\n",
       "         -1.6787e-03,  3.3700e-03,  2.9769e-05, -1.4824e-03, -8.2636e-04,\n",
       "         -1.8369e-03,  2.1070e-03, -1.1020e-03,  1.2013e-03, -2.8159e-03,\n",
       "         -2.0945e-03, -7.3199e-04,  1.4748e-03,  2.0008e-03, -1.2422e-03,\n",
       "         -2.5011e-03,  8.7002e-04,  1.0096e-03,  1.3674e-03,  9.6742e-04,\n",
       "         -1.6623e-03,  2.7270e-03, -5.1116e-03,  2.4214e-04, -3.8584e-03,\n",
       "         -6.3175e-05,  1.7666e-03, -2.1237e-04, -1.9707e-03, -1.0320e-03,\n",
       "         -1.5284e-03, -3.3974e-03, -6.2389e-04, -3.4507e-03, -1.0319e-03,\n",
       "          1.7909e-03, -9.6002e-04, -6.0763e-04,  3.5356e-04, -5.6868e-04,\n",
       "          1.4863e-03,  2.5875e-03,  3.9756e-03,  9.2188e-04, -1.6262e-03,\n",
       "         -1.2646e-04,  4.3742e-04, -2.0651e-03, -3.8404e-03, -1.9922e-03,\n",
       "         -1.6365e-03,  4.8256e-05,  5.4465e-04,  5.8185e-04,  1.0491e-03,\n",
       "          1.3292e-03,  6.0834e-04,  3.1091e-03,  2.9236e-04, -4.0141e-04,\n",
       "          2.1710e-03, -1.5824e-03,  7.4065e-04,  9.0715e-04,  1.8699e-04,\n",
       "          6.4879e-04,  3.0180e-04,  1.6260e-03,  1.2363e-03, -6.8325e-04,\n",
       "          1.4090e-03,  5.4005e-04,  2.4461e-04,  1.2653e-03, -1.3432e-03,\n",
       "          5.6183e-04,  3.3729e-03,  1.8024e-03,  1.5221e-04, -3.1924e-03,\n",
       "          1.3799e-03,  2.6436e-03, -2.1234e-03, -2.2320e-03,  1.2597e-03,\n",
       "         -6.0972e-04, -4.1352e-04, -1.0039e-03,  8.9655e-04, -5.0892e-04,\n",
       "          5.9679e-04,  1.8297e-03, -2.1961e-04, -8.2165e-04,  1.5423e-05,\n",
       "         -5.9891e-04,  7.9079e-04, -2.9705e-03, -3.4269e-03,  1.0914e-03,\n",
       "          1.0275e-03, -5.8274e-04,  7.8863e-04, -3.3397e-04,  2.6644e-04,\n",
       "         -2.2779e-03,  2.1703e-04, -1.0250e-03,  8.0153e-04, -1.4227e-03,\n",
       "         -2.0847e-03,  1.1218e-03, -2.4730e-03, -3.8880e-04,  2.2984e-04,\n",
       "          7.5271e-04, -2.0936e-03,  1.3984e-03, -1.1765e-03, -1.2307e-03,\n",
       "          1.3247e-03, -2.7106e-03,  3.5718e-04, -1.4638e-03, -1.1326e-03,\n",
       "         -1.0853e-03,  1.6772e-03,  2.1720e-03, -6.4550e-04,  3.7383e-03,\n",
       "          7.8948e-04, -1.9897e-03, -1.2806e-03, -1.0001e-03,  1.0326e-03,\n",
       "          5.2657e-04, -2.9657e-04, -1.4543e-03,  2.3595e-03,  1.2311e-03,\n",
       "         -2.5627e-03,  1.3710e-03, -1.7807e-03,  2.9020e-03, -5.7158e-04,\n",
       "          5.5823e-04,  1.3832e-03,  5.5496e-04, -1.4565e-03,  7.7002e-04,\n",
       "         -1.0058e-03, -3.8155e-04, -4.5505e-04,  3.5841e-05,  2.2066e-03,\n",
       "          1.3079e-03, -1.8725e-03,  1.8863e-04,  2.4668e-03,  2.0177e-04,\n",
       "         -7.9645e-04, -1.7289e-03,  1.1578e-03, -2.3165e-03,  1.1315e-03,\n",
       "          3.1009e-03,  1.9127e-03,  1.0272e-03,  3.8863e-03,  1.7740e-04,\n",
       "          1.5161e-03,  2.3566e-03, -2.5426e-03, -2.5906e-03, -1.3866e-03,\n",
       "          2.7244e-03, -2.0403e-03,  5.8892e-04, -1.6296e-03,  1.4828e-03,\n",
       "         -1.8331e-03,  6.3121e-04,  1.8053e-03,  6.2400e-04, -2.7714e-03,\n",
       "          5.1312e-04, -5.4314e-04, -1.9850e-03, -8.0483e-04, -1.2862e-03,\n",
       "          6.5499e-04, -2.3766e-03, -6.3500e-04,  1.5445e-03, -1.7349e-03,\n",
       "         -1.8822e-03,  2.6612e-03,  7.1173e-04, -1.7081e-03, -1.0238e-03,\n",
       "          2.0647e-03,  1.8995e-03, -1.8472e-03,  1.9064e-03,  1.3531e-03,\n",
       "          4.4532e-04,  2.3215e-03, -1.7460e-03,  1.5138e-03, -1.6918e-03,\n",
       "         -3.1573e-03, -1.3248e-03, -3.6109e-04,  3.1507e-03,  7.4756e-04,\n",
       "         -1.8573e-03,  7.6462e-04, -5.7291e-04,  3.4348e-03,  8.0063e-05,\n",
       "         -2.9462e-03, -1.6033e-03, -3.4334e-04, -1.5155e-03,  4.1905e-06,\n",
       "          2.0799e-04, -1.6815e-03,  1.7354e-04,  2.1625e-03,  3.6212e-03,\n",
       "          2.2882e-03,  2.6394e-03, -2.0493e-03,  1.2362e-03,  1.0222e-03,\n",
       "          4.7410e-04, -1.5867e-03, -3.1306e-04, -2.3531e-05,  1.3452e-03,\n",
       "          1.6324e-03,  1.8940e-03, -1.1418e-03,  1.3039e-03, -1.2169e-03,\n",
       "          5.6007e-04,  3.3995e-03, -3.4980e-03,  7.7966e-04, -1.2161e-03,\n",
       "          1.8229e-04,  2.2028e-04,  6.3075e-04, -5.9731e-04,  3.0043e-03,\n",
       "          8.8150e-04,  2.8817e-03,  2.0601e-03, -1.4818e-03, -1.0599e-03,\n",
       "         -2.2001e-04,  5.2275e-04, -1.7140e-03, -2.2727e-03, -2.5421e-03,\n",
       "          2.8810e-04, -1.1355e-04, -6.0503e-06,  1.0779e-03, -2.0491e-03,\n",
       "          8.7947e-04,  5.1919e-04,  9.0334e-04,  3.7385e-03, -8.7178e-04,\n",
       "         -2.3767e-03, -2.4082e-03, -3.0285e-04,  1.8024e-03, -3.1669e-03,\n",
       "          3.3725e-03,  4.1942e-04, -8.7873e-04, -2.1823e-03,  3.0090e-03,\n",
       "         -1.5718e-03,  1.3291e-03,  3.5033e-03, -3.4828e-03, -3.5906e-04,\n",
       "          3.6412e-03,  1.0598e-03,  2.1177e-04, -3.3228e-04, -1.0653e-03,\n",
       "         -3.5435e-03,  1.7310e-03,  1.1530e-03, -6.9369e-04,  6.1169e-04,\n",
       "         -2.7985e-03,  1.6681e-03]),\n",
       " 'transformer.resblocks.9.mlp.c_fc.weight': tensor([[-0.0006,  0.0314, -0.0108,  ..., -0.0067, -0.0381,  0.0177],\n",
       "         [-0.0528,  0.0002,  0.0136,  ...,  0.0098, -0.0114,  0.0085],\n",
       "         [-0.0056,  0.0011, -0.0020,  ...,  0.0067, -0.0038, -0.0033],\n",
       "         ...,\n",
       "         [-0.0206,  0.0458, -0.0230,  ..., -0.0064,  0.0121,  0.0254],\n",
       "         [-0.0681, -0.0012, -0.0235,  ...,  0.0080,  0.0134, -0.0090],\n",
       "         [-0.0054, -0.0433, -0.0105,  ...,  0.0496,  0.0346, -0.0305]]),\n",
       " 'transformer.resblocks.9.mlp.c_fc.bias': tensor([ 0.0034, -0.0173, -0.0041,  ..., -0.0034,  0.0182,  0.0225]),\n",
       " 'transformer.resblocks.9.mlp.c_proj.weight': tensor([[-0.0143, -0.0116, -0.0003,  ...,  0.0112,  0.0027, -0.0115],\n",
       "         [-0.0077, -0.0050,  0.0021,  ..., -0.0033,  0.0020,  0.0015],\n",
       "         [-0.0189,  0.0180,  0.0016,  ..., -0.0055,  0.0022,  0.0061],\n",
       "         ...,\n",
       "         [-0.0091,  0.0011, -0.0015,  ...,  0.0011,  0.0097,  0.0037],\n",
       "         [ 0.0060,  0.0120,  0.0026,  ...,  0.0107,  0.0133, -0.0087],\n",
       "         [ 0.0072,  0.0102,  0.0122,  ..., -0.0033, -0.0095, -0.0034]]),\n",
       " 'transformer.resblocks.9.mlp.c_proj.bias': tensor([ 1.3594e-02, -1.7226e-02,  2.0395e-02, -2.0240e-02, -2.1428e-03,\n",
       "         -7.5193e-03, -1.6387e-02,  1.5236e-02, -8.7959e-03, -1.5047e-02,\n",
       "          1.3360e-02, -1.2737e-02,  3.3362e-03,  1.5142e-02,  3.8681e-03,\n",
       "          1.3630e-02,  6.5661e-03,  1.3491e-03, -1.3587e-02,  8.5095e-03,\n",
       "          1.1826e-03, -1.0308e-04, -3.6969e-04,  2.4223e-03, -8.4252e-03,\n",
       "          1.9814e-02, -7.0824e-03,  1.5656e-02, -2.8604e-03, -1.9526e-02,\n",
       "          1.9474e-02, -1.5642e-02, -1.0373e-02, -1.5389e-02,  9.4754e-03,\n",
       "         -5.2618e-03, -5.8066e-03,  6.2206e-03,  1.5195e-02,  1.3204e-02,\n",
       "          1.0041e-02,  1.6556e-02, -1.6670e-02, -6.4945e-03,  4.0241e-04,\n",
       "          1.1791e-02, -1.2238e-03, -1.4966e-02, -9.7969e-03,  2.1391e-02,\n",
       "         -1.9899e-02, -9.5723e-03,  1.3905e-02,  4.0122e-03, -1.1189e-02,\n",
       "          1.5012e-02, -6.4058e-03, -1.6925e-02,  1.4642e-02,  4.6512e-03,\n",
       "         -1.5929e-02,  1.7722e-02, -7.4997e-03, -1.0098e-02,  1.9114e-02,\n",
       "         -1.7610e-03,  5.2631e-04, -1.1706e-02, -1.1257e-02, -1.1858e-02,\n",
       "          1.0617e-02, -3.7388e-03,  1.4976e-02,  1.6722e-02,  6.7129e-03,\n",
       "          1.6785e-02, -1.3703e-02, -2.1074e-02,  1.9509e-02,  1.9634e-02,\n",
       "          1.8380e-02,  1.7869e-02,  8.9869e-03,  9.0545e-03,  2.3827e-02,\n",
       "         -2.0844e-02,  1.9322e-02,  1.9143e-03,  1.8526e-03, -1.4339e-02,\n",
       "         -2.0559e-02,  1.5990e-02, -9.5902e-03,  7.3212e-03,  1.2260e-02,\n",
       "          9.2384e-03,  5.5056e-03, -1.0082e-02, -9.4453e-03, -1.0909e-02,\n",
       "         -1.6439e-02, -2.7312e-03, -1.0364e-02, -2.2580e-02, -3.4481e-03,\n",
       "         -1.0899e-02,  1.0031e-02,  1.8087e-02,  1.0839e-02,  4.1904e-03,\n",
       "          3.0860e-03,  1.6423e-02, -1.7986e-02,  2.6333e-03, -2.8135e-03,\n",
       "         -8.0556e-03,  5.1493e-03,  1.1859e-02, -2.7648e-03, -1.4657e-02,\n",
       "          1.3619e-02, -1.4954e-02,  7.5382e-03,  1.1790e-02, -3.7155e-03,\n",
       "         -9.2259e-03,  1.5594e-03, -1.3249e-02, -1.6210e-02, -8.2777e-04,\n",
       "          1.4089e-02, -4.3871e-03, -1.5737e-02,  6.4903e-03,  1.4789e-02,\n",
       "          1.6066e-02, -1.0568e-05,  9.6076e-03, -3.6981e-06, -1.5567e-02,\n",
       "         -1.8483e-02,  1.5384e-02,  1.9381e-02, -7.9926e-03, -3.1353e-04,\n",
       "          1.3917e-02, -1.3900e-02,  9.9078e-03,  1.5205e-02,  1.7507e-02,\n",
       "          1.1014e-04,  2.0190e-04,  1.8697e-02, -5.0569e-03,  4.2539e-04,\n",
       "         -4.4882e-04, -1.8944e-02, -1.3887e-02, -7.0773e-03,  3.7482e-04,\n",
       "          1.9362e-03, -1.4750e-03,  1.6813e-02,  1.3687e-02,  1.8837e-02,\n",
       "         -2.0134e-02,  1.5942e-02,  5.6913e-03,  1.1386e-02,  4.8903e-04,\n",
       "          2.1649e-02, -5.9565e-03, -3.7474e-03,  7.8399e-03,  1.8330e-02,\n",
       "          9.5177e-03, -2.3265e-02, -4.7455e-03,  1.6928e-03,  1.1531e-02,\n",
       "          1.5411e-02,  2.0143e-02,  2.5332e-06,  1.0003e-02,  2.3253e-03,\n",
       "         -9.9702e-03,  2.0217e-02, -4.7755e-03,  1.9110e-02,  1.8942e-02,\n",
       "          1.6715e-02, -2.7448e-03,  1.4871e-02,  2.2192e-02, -2.0837e-02,\n",
       "          5.4008e-03, -5.3138e-03,  2.4324e-02,  1.4120e-02,  2.2497e-03,\n",
       "         -6.8945e-03,  2.3607e-03,  8.3204e-03,  5.2182e-03,  2.1371e-02,\n",
       "          1.4924e-02,  2.0060e-02,  9.4452e-03,  1.0575e-02,  2.1931e-02,\n",
       "         -1.8697e-02, -1.0831e-02,  1.1437e-03,  1.5472e-02,  1.7620e-02,\n",
       "          1.4241e-02,  7.1760e-03, -1.3658e-02,  5.8607e-05,  1.2288e-02,\n",
       "          5.1768e-03, -8.8724e-03,  1.9115e-02, -8.2624e-03,  1.2833e-02,\n",
       "          8.8482e-03, -2.9464e-03, -1.6406e-02, -1.7120e-02,  2.2120e-02,\n",
       "          1.1260e-02, -1.3956e-02,  1.6299e-02, -1.7567e-02,  6.3409e-03,\n",
       "          1.3943e-02, -1.4379e-02, -2.2223e-02,  1.7414e-02,  1.2647e-02,\n",
       "          1.5845e-02,  1.2297e-02,  1.6417e-02, -8.7175e-03, -8.4558e-03,\n",
       "         -1.4193e-02, -1.7891e-03,  9.8042e-03,  2.0722e-02, -2.0180e-02,\n",
       "          1.4811e-02, -1.5911e-02, -8.0469e-03, -9.0112e-03,  1.6293e-02,\n",
       "          1.4084e-02, -1.0169e-02, -9.3423e-03, -2.0461e-02, -1.0911e-02,\n",
       "         -1.0868e-02,  9.9257e-03,  1.3032e-02,  4.3362e-05,  1.6115e-02,\n",
       "         -1.7012e-02,  1.0948e-02, -1.3658e-02, -6.4709e-03,  6.5198e-03,\n",
       "          4.0732e-03, -2.1105e-02,  1.3548e-02, -2.4100e-03,  1.1134e-02,\n",
       "         -4.9582e-03,  1.9514e-02,  1.5629e-02, -7.9230e-03, -6.0405e-03,\n",
       "         -8.6935e-03,  1.0154e-02, -1.8030e-02, -3.1678e-03, -1.6901e-02,\n",
       "          1.4517e-02,  8.3004e-03,  1.5924e-02, -1.7876e-02,  1.5331e-03,\n",
       "          4.8790e-04,  2.2288e-05,  1.8033e-02, -8.9442e-03,  1.4344e-03,\n",
       "         -1.1872e-02, -1.5878e-02,  1.3143e-02,  6.7049e-05, -6.9156e-03,\n",
       "         -1.4242e-02, -7.5823e-03,  1.2478e-02,  3.1589e-03, -3.0071e-03,\n",
       "         -5.0616e-03, -1.8750e-02, -1.7223e-02,  1.0463e-02,  5.9775e-03,\n",
       "          2.0291e-02, -1.5514e-02, -9.2966e-03,  2.5547e-03,  1.8402e-03,\n",
       "          5.7912e-03,  1.8962e-02,  1.0023e-02,  5.4823e-03,  8.1768e-03,\n",
       "         -3.1687e-03, -2.2100e-02, -2.7723e-03, -1.6690e-02, -1.1497e-02,\n",
       "          1.7647e-02, -8.5479e-03,  5.8864e-03,  8.9731e-03,  1.9278e-02,\n",
       "         -2.1980e-02, -1.3326e-02, -3.5125e-03, -2.9403e-03,  1.5289e-02,\n",
       "          4.2017e-03, -1.4387e-02,  1.1379e-02,  4.0174e-03, -3.3218e-03,\n",
       "          1.5944e-02, -1.7019e-02, -2.1285e-02, -4.2922e-03, -2.4027e-02,\n",
       "         -2.2246e-03,  1.9490e-02, -7.9420e-04,  5.4589e-03, -9.7497e-03,\n",
       "          5.4762e-03, -1.2831e-02,  5.5239e-03, -1.8519e-02, -3.8528e-03,\n",
       "          6.5403e-03, -1.1610e-02, -8.6812e-03, -8.4317e-03,  6.2231e-03,\n",
       "         -1.5964e-02,  1.2499e-02,  5.0536e-03,  2.1204e-02,  5.5519e-03,\n",
       "          2.1629e-02, -1.1140e-02, -1.3831e-02, -1.0551e-02, -2.2176e-02,\n",
       "         -1.6802e-02,  2.9385e-03, -1.0858e-02,  2.1520e-02,  5.9862e-03,\n",
       "          2.2268e-02,  1.1973e-02,  1.0830e-02, -1.9988e-02, -1.3543e-02,\n",
       "         -9.5558e-03,  2.3259e-02, -5.4316e-03,  1.2780e-02, -8.9406e-03,\n",
       "         -1.7963e-02,  5.9804e-03,  1.6221e-02,  2.2968e-02, -1.5127e-02,\n",
       "          1.5568e-02, -2.7219e-03, -7.7408e-03, -8.4211e-03,  5.1994e-03,\n",
       "         -5.6847e-04,  1.9874e-02,  1.7312e-02, -1.9000e-03,  1.9371e-02,\n",
       "         -1.1433e-02,  1.2477e-03, -6.6515e-04, -6.8153e-03, -1.5535e-03,\n",
       "         -4.3611e-03, -1.1390e-02, -1.5340e-02,  7.9800e-03, -1.2029e-02,\n",
       "         -2.1725e-02,  6.5865e-03,  1.6466e-02,  9.6584e-03,  1.4444e-02,\n",
       "         -1.7767e-03, -1.7567e-03,  2.1185e-02,  5.4345e-03,  7.4546e-03,\n",
       "         -2.0941e-03,  8.7991e-03,  1.7344e-02, -1.7742e-02,  5.4863e-03,\n",
       "         -1.6250e-03, -6.1045e-03, -2.1026e-02,  8.5542e-04, -2.0880e-02,\n",
       "         -1.5680e-02,  2.3630e-02, -3.7485e-03, -1.0180e-02,  2.5059e-03,\n",
       "         -1.6203e-02, -8.2968e-03, -3.8120e-03,  7.3993e-04, -1.3936e-02,\n",
       "          1.8106e-02, -1.1041e-02, -1.1721e-02, -8.4967e-04, -1.4239e-03,\n",
       "         -9.3056e-03, -4.2496e-03,  1.3104e-02,  7.5932e-03, -1.6619e-02,\n",
       "          1.8735e-02,  6.7773e-03,  1.6958e-02, -5.8603e-03,  2.0846e-02,\n",
       "         -1.2743e-02,  1.3565e-02,  2.7620e-03, -3.3447e-03,  1.1606e-02,\n",
       "          8.3236e-03, -1.0174e-02,  2.4563e-03, -9.7122e-03,  4.9369e-03,\n",
       "         -2.1259e-02, -1.0895e-02,  2.6110e-03,  2.1857e-03,  1.1191e-02,\n",
       "          3.3355e-03,  6.1735e-03,  1.2886e-02, -2.0394e-02, -6.8282e-03,\n",
       "          6.0724e-03,  1.4388e-03, -3.5327e-04, -9.0956e-03, -5.0415e-03,\n",
       "         -8.9782e-03, -6.5209e-03, -2.0016e-02,  5.5579e-05,  1.7024e-02,\n",
       "          3.1646e-03, -8.0243e-03,  1.7393e-04,  2.1378e-02, -1.6309e-02,\n",
       "         -2.7070e-03, -1.5489e-02,  2.0334e-02,  1.5863e-02,  1.3246e-02,\n",
       "         -1.0777e-02,  9.4537e-03,  1.2505e-02, -1.5084e-02, -2.7012e-03,\n",
       "          1.6442e-02,  1.8037e-02,  1.0675e-02,  1.4757e-02, -1.8811e-02,\n",
       "         -1.1369e-02, -1.7856e-02,  4.9899e-03,  1.9372e-02, -8.8217e-03,\n",
       "         -2.2119e-02, -8.1893e-03]),\n",
       " 'transformer.resblocks.9.ln_2.weight': tensor([0.9993, 0.9997, 1.0005, 1.0023, 1.0036, 1.0011, 1.0011, 0.9970, 0.9979,\n",
       "         1.0029, 0.9978, 1.0008, 1.0004, 1.0028, 0.9957, 0.9970, 1.0030, 1.0030,\n",
       "         1.0005, 0.9969, 0.9992, 1.0011, 1.0015, 0.9998, 1.0021, 1.0031, 0.9992,\n",
       "         0.9990, 1.0017, 1.0017, 1.0043, 1.0033, 1.0013, 1.0015, 1.0051, 1.0000,\n",
       "         1.0009, 1.0036, 1.0025, 1.0027, 1.0011, 1.0004, 1.0003, 0.9992, 0.9985,\n",
       "         0.9989, 0.9984, 1.0005, 1.0011, 0.9974, 1.0011, 1.0007, 0.9976, 0.9977,\n",
       "         1.0012, 1.0016, 1.0015, 1.0003, 1.0005, 1.0026, 0.9994, 1.0020, 1.0000,\n",
       "         1.0002, 1.0008, 1.0009, 0.9993, 0.9990, 1.0004, 0.9999, 1.0042, 1.0017,\n",
       "         0.9984, 1.0010, 0.9991, 1.0028, 0.9983, 0.9985, 1.0007, 1.0019, 1.0007,\n",
       "         1.0031, 0.9985, 0.9992, 0.9999, 1.0010, 0.9985, 1.0044, 1.0035, 0.9987,\n",
       "         1.0015, 1.0009, 0.9988, 1.0003, 1.0016, 0.9968, 1.0005, 1.0015, 1.0013,\n",
       "         0.9994, 1.0012, 0.9997, 0.9998, 0.9973, 1.0001, 0.9980, 0.9978, 0.9991,\n",
       "         0.9999, 1.0030, 0.9991, 1.0012, 1.0035, 0.9984, 1.0021, 0.9984, 0.9998,\n",
       "         1.0016, 1.0010, 0.9995, 1.0019, 0.9981, 1.0007, 1.0009, 1.0033, 1.0022,\n",
       "         0.9955, 1.0021, 1.0011, 1.0038, 1.0026, 0.9976, 0.9987, 1.0000, 1.0000,\n",
       "         1.0036, 1.0013, 0.9962, 1.0019, 1.0029, 0.9991, 0.9993, 1.0015, 0.9986,\n",
       "         1.0007, 0.9992, 0.9971, 1.0024, 1.0045, 1.0041, 0.9994, 0.9941, 0.9922,\n",
       "         1.0029, 1.0037, 1.0009, 1.0009, 0.9977, 0.9999, 0.9993, 0.9996, 1.0009,\n",
       "         0.9983, 1.0037, 1.0012, 0.9995, 1.0018, 0.9996, 0.9995, 1.0008, 0.9995,\n",
       "         0.9979, 1.0012, 1.0013, 0.9984, 1.0003, 0.9998, 1.0004, 1.0037, 0.9987,\n",
       "         1.0018, 1.0035, 0.9992, 0.9997, 0.9957, 0.9990, 0.9975, 0.9970, 1.0028,\n",
       "         0.9998, 1.0075, 1.0027, 0.9986, 0.9984, 1.0022, 0.9994, 1.0011, 1.0002,\n",
       "         1.0001, 0.9986, 1.0020, 1.0003, 1.0020, 1.0005, 1.0004, 1.0009, 1.0012,\n",
       "         1.0067, 0.9971, 1.0020, 0.9993, 0.9982, 1.0012, 0.9997, 0.9969, 1.0003,\n",
       "         0.9981, 0.9988, 1.0010, 1.0015, 1.0008, 1.0011, 0.9991, 0.9990, 0.9983,\n",
       "         1.0041, 0.9998, 1.0014, 0.9982, 0.9997, 1.0004, 0.9972, 0.9999, 1.0009,\n",
       "         0.9986, 1.0017, 0.9981, 0.9986, 1.0016, 1.0008, 1.0006, 0.9987, 0.9995,\n",
       "         1.0027, 1.0013, 1.0030, 0.9990, 1.0030, 1.0021, 1.0019, 1.0002, 0.9972,\n",
       "         1.0018, 1.0027, 1.0012, 0.9999, 1.0010, 0.9989, 1.0008, 0.9949, 1.0022,\n",
       "         1.0019, 1.0017, 0.9972, 0.9974, 1.0002, 0.9989, 1.0006, 0.9992, 1.0033,\n",
       "         1.0026, 0.9996, 1.0023, 1.0016, 0.9990, 0.9992, 1.0016, 1.0010, 0.9957,\n",
       "         0.9968, 1.0040, 0.9996, 0.9984, 0.9986, 1.0025, 0.9983, 1.0010, 1.0007,\n",
       "         0.9994, 1.0001, 1.0029, 1.0004, 0.9978, 0.9974, 1.0009, 1.0012, 0.9989,\n",
       "         0.9987, 1.0020, 0.9986, 0.9999, 1.0014, 1.0020, 0.9964, 0.9992, 0.9973,\n",
       "         0.9998, 1.0021, 0.9970, 1.0001, 1.0031, 0.9992, 1.0042, 1.0001, 1.0023,\n",
       "         0.9955, 0.9992, 0.9981, 1.0022, 0.9988, 0.9957, 1.0014, 1.0000, 0.9999,\n",
       "         1.0002, 0.9983, 1.0001, 1.0032, 0.9962, 1.0021, 1.0008, 1.0011, 1.0000,\n",
       "         1.0003, 0.9988, 1.0022, 1.0020, 1.0027, 1.0000, 0.9987, 0.9970, 0.9984,\n",
       "         0.9990, 1.0047, 0.9996, 1.0034, 1.0001, 1.0022, 1.0000, 1.0000, 0.9963,\n",
       "         0.9991, 1.0012, 1.0028, 0.9999, 1.0041, 0.9974, 1.0023, 1.0019, 0.9980,\n",
       "         1.0015, 0.9997, 0.9975, 0.9974, 1.0032, 0.9954, 1.0027, 0.9962, 0.9980,\n",
       "         0.9988, 1.0011, 0.9997, 1.0033, 0.9988, 1.0011, 0.9956, 1.0007, 1.0001,\n",
       "         0.9990, 1.0009, 0.9989, 0.9993, 0.9986, 0.9988, 0.9983, 0.9998, 0.9994,\n",
       "         0.9978, 0.9970, 0.9999, 1.0019, 1.0013, 0.9973, 0.9992, 0.9983, 0.9996,\n",
       "         1.0002, 1.0005, 0.9959, 1.0020, 1.0019, 0.9988, 1.0021, 1.0016, 1.0010,\n",
       "         0.9992, 0.9977, 0.9988, 1.0028, 1.0044, 1.0020, 0.9987, 0.9975, 0.9979,\n",
       "         0.9966, 1.0015, 0.9997, 0.9989, 1.0009, 0.9960, 0.9986, 1.0051, 1.0006,\n",
       "         1.0013, 0.9976, 1.0025, 1.0032, 1.0044, 1.0005, 1.0004, 1.0012, 1.0024,\n",
       "         1.0001, 1.0034, 0.9986, 0.9991, 0.9987, 0.9990, 1.0014, 1.0027, 0.9979,\n",
       "         0.9987, 1.0048, 1.0029, 1.0019, 0.9990, 1.0012, 0.9978, 1.0021, 1.0008,\n",
       "         0.9980, 0.9985, 0.9993, 0.9973, 1.0007, 1.0005, 0.9999, 1.0000, 0.9992,\n",
       "         1.0000, 1.0015, 0.9972, 1.0018, 0.9965, 1.0010, 1.0009, 1.0005, 0.9973,\n",
       "         1.0045, 1.0016, 0.9996, 1.0033, 0.9991, 1.0011, 0.9990, 0.9998, 1.0012,\n",
       "         0.9968, 1.0014, 0.9974, 1.0024, 1.0015, 1.0031, 1.0019, 0.9980, 1.0038,\n",
       "         1.0019, 0.9998, 0.9996, 1.0007, 0.9992, 0.9991, 1.0011, 1.0003, 0.9993,\n",
       "         0.9982, 1.0017, 1.0020, 1.0026, 0.9993, 0.9990, 1.0024, 0.9979, 0.9967,\n",
       "         0.9984, 1.0027, 1.0009, 1.0009, 1.0005, 1.0022, 0.9997, 0.9997]),\n",
       " 'transformer.resblocks.9.ln_2.bias': tensor([-5.5749e-04, -4.1211e-04, -2.8044e-03, -1.0517e-03, -1.6044e-03,\n",
       "         -5.5340e-04,  1.6584e-03, -3.5715e-03,  2.6560e-03,  1.8588e-03,\n",
       "          3.3799e-03,  4.4106e-05,  4.1381e-04, -5.7794e-04,  4.1814e-03,\n",
       "         -4.9387e-03, -1.3386e-03, -1.8319e-03,  2.2506e-03,  1.9474e-03,\n",
       "         -8.0880e-04, -1.0092e-04,  2.6778e-04,  5.0670e-04, -1.3236e-04,\n",
       "          1.9031e-03,  3.0015e-03,  1.7771e-03,  8.4792e-04, -1.0094e-03,\n",
       "          1.4071e-03,  1.6518e-03, -5.5169e-04, -1.9924e-04, -3.0334e-04,\n",
       "          6.8834e-04, -1.1921e-03,  2.6033e-03, -6.2475e-04,  9.7145e-05,\n",
       "         -3.6210e-04, -6.5486e-04,  4.1352e-04, -2.8761e-03,  1.7699e-03,\n",
       "         -2.1539e-03,  1.9296e-03,  1.7648e-03,  6.8627e-04, -3.0086e-03,\n",
       "         -5.2093e-04, -9.3607e-05,  8.8312e-04, -3.1518e-03, -2.1537e-04,\n",
       "          2.4236e-03,  3.8924e-03,  5.3821e-04, -6.6797e-04, -1.5477e-03,\n",
       "          2.8371e-04, -1.0662e-04, -3.5944e-03, -1.2687e-03, -1.2562e-03,\n",
       "          1.8613e-03, -1.5427e-03,  1.6325e-03, -5.5508e-04, -2.4465e-03,\n",
       "         -1.4390e-03,  5.4111e-04,  2.3316e-03,  3.7558e-03, -9.2957e-04,\n",
       "          3.8946e-04,  9.6784e-04,  2.9230e-03,  1.4733e-03, -1.1559e-03,\n",
       "         -8.5728e-04, -2.9673e-04, -1.2877e-04, -2.9998e-03, -2.0808e-03,\n",
       "         -2.0063e-03,  2.2658e-03,  1.6258e-03,  2.6026e-03,  2.3840e-03,\n",
       "         -1.0167e-03,  4.8551e-04,  1.2393e-03,  2.8773e-03,  1.9352e-03,\n",
       "          8.5326e-04, -5.2992e-04,  1.0706e-03,  3.8426e-04, -1.1010e-03,\n",
       "         -1.3095e-04, -1.6846e-03, -2.8150e-04,  2.3757e-03,  1.7161e-03,\n",
       "          1.9557e-03, -3.6834e-04,  2.2939e-03,  3.5606e-04,  2.8412e-03,\n",
       "         -1.1017e-04,  3.0302e-03, -5.3214e-04, -8.2918e-04, -5.3436e-04,\n",
       "          1.7110e-03, -2.7237e-03, -1.2040e-03, -1.4687e-03,  1.5883e-03,\n",
       "         -1.1238e-03,  4.5475e-03, -1.0491e-03,  1.7668e-04, -1.2850e-03,\n",
       "         -5.4206e-04,  1.8809e-03,  7.4313e-04, -2.1741e-03, -1.8516e-03,\n",
       "         -1.6452e-03, -2.1261e-03, -1.6123e-03,  7.6620e-04,  1.1826e-03,\n",
       "         -1.0763e-03,  4.6858e-03, -3.6808e-03, -7.8054e-04, -2.2259e-04,\n",
       "         -5.1787e-04,  6.3676e-04, -3.3278e-04,  4.0974e-04, -1.0663e-03,\n",
       "         -2.0518e-03,  5.2541e-03, -1.7915e-03, -2.8863e-04,  1.7986e-03,\n",
       "          6.5056e-04, -4.5874e-03, -6.4949e-03, -5.3402e-04,  1.5140e-03,\n",
       "         -6.7780e-04,  8.4175e-04,  1.7944e-03,  4.1805e-04,  2.0145e-03,\n",
       "          3.6608e-04, -1.7059e-03, -8.8780e-04, -9.7762e-04,  2.3283e-03,\n",
       "         -1.5762e-03,  1.7227e-03,  3.5074e-04,  1.3306e-03, -1.7679e-03,\n",
       "         -1.1815e-03, -5.9922e-04,  4.1861e-03, -9.5113e-04, -1.7220e-03,\n",
       "         -1.1510e-03,  2.0442e-03, -1.8265e-03,  2.6749e-03,  3.6754e-03,\n",
       "         -3.8728e-04,  4.4735e-04,  3.0526e-03,  1.4380e-03, -2.2504e-03,\n",
       "         -1.6353e-03, -1.8387e-03, -1.0914e-03, -1.7545e-03,  3.7279e-03,\n",
       "         -9.3949e-04, -2.7544e-03,  8.5122e-05, -2.1478e-03,  5.6890e-04,\n",
       "          2.0646e-03, -7.6713e-04, -1.0705e-03, -2.5379e-03, -1.3928e-03,\n",
       "          2.0380e-03, -1.2098e-04, -7.7709e-04, -7.5926e-04, -1.4637e-03,\n",
       "         -3.2327e-04, -2.9372e-04, -3.2355e-03, -2.4065e-03,  1.3447e-03,\n",
       "         -4.5314e-03,  2.9737e-03,  7.0683e-04, -3.5248e-03,  2.2605e-03,\n",
       "         -1.5839e-04, -3.3054e-03,  2.4731e-03, -8.9606e-04,  4.7368e-04,\n",
       "          5.7189e-04,  1.5856e-03, -8.8232e-04, -4.1741e-03, -5.7111e-04,\n",
       "          1.3882e-03,  1.2523e-03,  2.2187e-03, -2.7666e-03, -5.6515e-04,\n",
       "         -1.1056e-03,  4.0920e-04,  5.5645e-04, -2.6890e-07,  1.3750e-03,\n",
       "          1.2764e-03, -3.3592e-03,  1.4702e-03, -1.8449e-03,  9.3627e-04,\n",
       "         -3.9069e-04, -2.1435e-03,  1.0757e-03,  2.3956e-03, -2.1908e-03,\n",
       "         -1.9065e-03, -2.3276e-03,  1.5186e-03, -1.0714e-03, -8.2754e-04,\n",
       "         -3.2380e-03,  2.1789e-03, -2.1829e-03,  6.5601e-04, -1.6306e-03,\n",
       "          4.9999e-04,  1.1701e-03,  9.5754e-04,  2.0962e-03,  3.8227e-03,\n",
       "         -1.7517e-03,  1.4964e-03, -1.0277e-04,  2.5202e-03,  3.4690e-03,\n",
       "          1.9838e-04, -2.8063e-03, -2.4474e-03, -1.3255e-03,  1.3972e-03,\n",
       "         -7.6243e-04,  1.4962e-03,  2.4423e-03, -2.1230e-03, -2.9975e-03,\n",
       "          1.5865e-03, -5.6143e-04,  1.0553e-03, -4.8098e-03,  3.6923e-03,\n",
       "          6.5861e-04, -1.5145e-03,  1.8683e-03,  6.6021e-04, -2.6210e-04,\n",
       "         -5.7583e-03, -1.5456e-03, -1.1932e-03, -1.4412e-03,  1.0369e-03,\n",
       "          6.3070e-05, -1.5332e-03,  2.2768e-03, -4.3977e-04,  2.7144e-03,\n",
       "          1.4577e-04,  2.4949e-03,  8.7985e-04,  2.7972e-04,  1.4485e-03,\n",
       "         -4.6226e-05,  9.2324e-04, -1.3782e-03,  3.2847e-03,  1.5472e-04,\n",
       "          4.0678e-03, -1.2568e-03,  1.0262e-06,  3.0940e-03,  1.2163e-03,\n",
       "         -1.4167e-03, -1.7332e-03,  3.3465e-04, -1.9496e-03,  2.1236e-03,\n",
       "         -3.4209e-03, -1.2902e-03, -1.4199e-03, -2.9823e-03,  3.8949e-04,\n",
       "         -6.1164e-03,  1.1319e-03, -2.1129e-03, -1.4114e-03,  6.7488e-04,\n",
       "          3.5312e-04, -2.2939e-03, -1.9650e-04,  2.1808e-03, -3.2960e-03,\n",
       "          6.6366e-04, -2.1148e-03,  5.1817e-04,  1.3287e-04, -3.0006e-03,\n",
       "         -2.2174e-03,  2.7887e-03,  9.7138e-04, -1.7554e-03, -1.5364e-03,\n",
       "          4.0408e-03, -5.7394e-03,  7.9068e-04, -2.6170e-03, -1.9097e-04,\n",
       "          4.9549e-06, -1.8044e-03, -6.9058e-04,  1.4852e-03, -2.3187e-03,\n",
       "          3.2104e-03,  1.0168e-03, -4.8989e-04, -3.6284e-03,  8.6304e-07,\n",
       "          1.2524e-03, -3.0302e-03,  4.9111e-04, -2.0895e-03,  3.8519e-04,\n",
       "         -2.8860e-03,  1.4034e-03, -1.7666e-03, -2.8999e-03,  1.7249e-03,\n",
       "         -2.0438e-03, -1.6434e-05,  3.3648e-03, -1.0071e-03,  1.9655e-03,\n",
       "          2.0948e-03, -1.2413e-03,  1.9860e-03,  5.0596e-04,  1.5928e-03,\n",
       "         -3.9778e-03,  4.6118e-04, -1.6076e-03,  1.2231e-03,  2.0342e-03,\n",
       "         -5.0050e-04,  1.2253e-03, -3.8353e-04,  1.6145e-03,  1.0301e-03,\n",
       "          5.2218e-04,  2.1574e-04, -1.5473e-04, -3.6991e-03,  6.7734e-04,\n",
       "         -3.9397e-03, -1.2296e-03,  2.4118e-03,  1.7323e-03, -4.7462e-03,\n",
       "         -2.0600e-03,  1.2640e-03, -1.7850e-03, -3.2372e-04, -2.9933e-03,\n",
       "         -3.3485e-03,  4.6514e-03, -3.0244e-03, -2.4402e-03, -3.1875e-03,\n",
       "          4.1013e-04, -6.3235e-04, -6.6369e-04,  2.6767e-03,  5.1697e-04,\n",
       "          7.6436e-04,  8.0444e-04, -2.0256e-03, -1.3848e-03, -1.0380e-03,\n",
       "          9.3165e-04, -3.2589e-03, -1.5325e-03,  7.7822e-04, -3.5403e-03,\n",
       "          2.0883e-04,  1.4412e-03, -1.2379e-03,  2.0899e-03, -1.9173e-04,\n",
       "         -3.4207e-03,  3.1557e-04,  2.6580e-03, -7.9417e-04,  3.9794e-03,\n",
       "          1.6692e-04, -1.4896e-03, -1.5604e-03,  8.9962e-04,  3.2486e-03,\n",
       "         -2.1991e-03, -2.3503e-03,  7.1663e-03,  2.1845e-04,  1.6212e-03,\n",
       "          2.3254e-03, -9.5259e-04,  9.5586e-05, -9.5357e-04,  4.4992e-04,\n",
       "          6.6284e-04,  6.1950e-04, -1.4900e-03,  3.2355e-04,  5.5849e-05,\n",
       "         -1.5439e-03, -1.3622e-03,  9.6995e-04, -1.2059e-03,  1.8832e-03,\n",
       "         -7.5228e-04, -3.1080e-04, -4.7897e-04,  8.5365e-04, -9.7744e-04,\n",
       "          3.5703e-04,  4.3449e-04, -1.8340e-03, -4.8342e-03, -2.0941e-03,\n",
       "          5.1766e-04,  3.6497e-04, -3.1338e-03,  3.0714e-03,  1.1050e-03,\n",
       "          1.4459e-03, -2.6391e-03,  1.3796e-03,  1.6647e-03,  1.9713e-03,\n",
       "         -1.5378e-03,  6.5785e-04,  3.4982e-03,  3.2832e-03,  3.3696e-03,\n",
       "         -3.7221e-04,  5.4414e-04,  1.8457e-03,  6.5083e-04,  1.7166e-03,\n",
       "         -4.6843e-03,  8.8315e-04,  2.7736e-03, -2.3002e-03,  1.9482e-03,\n",
       "         -1.4735e-04,  7.2377e-04,  1.8763e-03,  2.7661e-04,  1.5549e-04,\n",
       "         -1.0190e-03,  2.0048e-03,  3.2481e-03, -1.3517e-03,  5.7289e-04,\n",
       "          3.4193e-03, -1.5626e-03, -7.4929e-04, -2.1262e-03,  1.5078e-03,\n",
       "          2.6660e-05,  3.1715e-04, -1.0075e-03,  2.1404e-03,  1.1772e-03,\n",
       "          6.4624e-04, -4.5349e-04]),\n",
       " 'transformer.resblocks.10.attn.in_proj_weight': tensor([[-0.0539,  0.0054, -0.0994,  ...,  0.0307, -0.0360,  0.0297],\n",
       "         [-0.0427,  0.0114,  0.0219,  ..., -0.0798,  0.0696,  0.0651],\n",
       "         [-0.0008,  0.0612,  0.0027,  ...,  0.0156, -0.0502,  0.0107],\n",
       "         ...,\n",
       "         [ 0.0247,  0.0354, -0.0269,  ..., -0.0531,  0.0904, -0.0508],\n",
       "         [ 0.0043, -0.0074, -0.0476,  ...,  0.0100,  0.0110,  0.0444],\n",
       "         [ 0.0151, -0.0415,  0.0336,  ...,  0.0736, -0.0694,  0.0109]]),\n",
       " 'transformer.resblocks.10.attn.in_proj_bias': tensor([ 0.0045,  0.0009, -0.0027,  ..., -0.0025,  0.0010,  0.0014]),\n",
       " 'transformer.resblocks.10.attn.out_proj.weight': tensor([[ 0.0150, -0.0070,  0.0091,  ...,  0.0024, -0.0024, -0.0116],\n",
       "         [-0.0008,  0.0047, -0.0181,  ..., -0.0012, -0.0080, -0.0031],\n",
       "         [ 0.0102, -0.0049, -0.0115,  ...,  0.0094, -0.0061,  0.0048],\n",
       "         ...,\n",
       "         [ 0.0050, -0.0079, -0.0094,  ..., -0.0050, -0.0062, -0.0033],\n",
       "         [ 0.0019,  0.0025,  0.0191,  ..., -0.0006, -0.0062,  0.0051],\n",
       "         [ 0.0056, -0.0009, -0.0102,  ...,  0.0111, -0.0002,  0.0056]]),\n",
       " 'transformer.resblocks.10.attn.out_proj.bias': tensor([-6.3260e-04, -3.9287e-03, -6.7339e-05, -4.8920e-04, -8.9896e-04,\n",
       "         -2.7963e-03, -1.0158e-07,  2.4510e-03, -3.3942e-03,  1.8975e-03,\n",
       "         -4.7905e-03, -2.6485e-03, -5.3362e-04, -1.5688e-03, -5.8677e-03,\n",
       "          2.4729e-03, -2.4579e-04,  1.9634e-03, -8.9797e-04, -9.1079e-04,\n",
       "         -1.5368e-04,  7.9583e-04,  3.0159e-04, -6.3327e-04, -5.2529e-04,\n",
       "         -1.5601e-03, -1.3011e-03, -1.2957e-03, -9.3796e-04, -1.9270e-03,\n",
       "          7.3179e-04, -8.3103e-05,  2.5582e-03, -3.2864e-03, -2.3480e-04,\n",
       "          1.0395e-03,  1.1735e-03, -1.4647e-04,  1.0656e-03, -3.0226e-03,\n",
       "          1.5150e-03,  2.2294e-03,  1.6605e-03,  1.5003e-03, -4.0951e-03,\n",
       "          1.7749e-03, -3.4306e-03,  5.0724e-04, -3.5923e-04,  1.9962e-03,\n",
       "         -8.2687e-04, -3.7491e-04, -1.1265e-03,  2.6733e-03,  2.9149e-03,\n",
       "          9.9839e-04, -1.5968e-03, -9.4432e-04, -1.6354e-04, -8.9736e-04,\n",
       "         -1.9240e-03,  2.7448e-04,  8.1259e-04,  7.4235e-04, -1.5415e-04,\n",
       "         -1.0540e-03,  2.1865e-03,  3.1312e-04, -1.4815e-03,  1.8373e-03,\n",
       "         -4.7423e-04, -3.5503e-04, -2.3351e-03,  6.8914e-05, -9.1528e-04,\n",
       "          1.8633e-03, -1.7334e-03, -2.0510e-03, -2.6318e-03,  2.1967e-03,\n",
       "          1.6321e-03, -2.0439e-03,  7.3833e-04,  2.0162e-03,  2.1245e-03,\n",
       "          2.7523e-04, -7.2014e-04,  5.1708e-04,  2.2451e-04, -2.6814e-03,\n",
       "          2.4319e-04, -4.8551e-04,  1.7266e-03, -2.2306e-03, -2.0851e-03,\n",
       "         -4.3507e-03, -2.6034e-04, -1.4311e-03, -1.9751e-03,  7.3053e-04,\n",
       "          6.1141e-04, -1.5725e-03,  2.9594e-04, -2.8317e-04, -6.5659e-04,\n",
       "         -2.8361e-03, -3.7651e-03, -9.7380e-04,  7.8998e-04, -7.8386e-04,\n",
       "          2.7403e-03,  6.4979e-04,  3.0953e-03,  1.2619e-03, -1.6381e-03,\n",
       "         -4.9790e-03,  1.5214e-03,  1.6977e-03, -2.9717e-03, -1.0481e-03,\n",
       "          4.9457e-04, -2.2298e-03,  1.9103e-03, -7.8558e-04, -1.3702e-03,\n",
       "          8.3541e-04, -1.4960e-03, -2.2337e-03, -9.4927e-04,  4.2146e-05,\n",
       "          1.3291e-05,  3.6688e-03, -3.3189e-04, -2.5187e-03, -6.8568e-04,\n",
       "         -1.0310e-03, -1.4821e-03,  2.6169e-03, -1.1869e-03,  6.1228e-04,\n",
       "          4.7294e-04,  1.8212e-03,  1.2772e-03,  3.1710e-03,  1.5733e-03,\n",
       "          2.4635e-03, -2.7822e-03,  1.2101e-03,  6.7675e-04,  9.2402e-04,\n",
       "          1.8778e-03,  5.0203e-03,  6.7854e-03,  1.9251e-03, -6.7921e-04,\n",
       "         -7.3465e-05,  8.1236e-04, -2.0310e-03,  2.6329e-04,  2.0251e-04,\n",
       "         -1.9560e-03, -1.0168e-03,  3.4360e-03,  5.7041e-04, -1.5082e-03,\n",
       "          1.5826e-03,  2.0935e-03, -3.1233e-04, -4.3019e-04, -1.7386e-03,\n",
       "          1.1847e-03, -2.1496e-04, -2.2969e-03,  1.5530e-04,  2.0476e-03,\n",
       "          9.6740e-04, -1.6315e-03,  6.0017e-04, -4.1385e-05, -1.3086e-03,\n",
       "         -2.3263e-03, -7.3829e-04,  1.2289e-03, -3.2008e-03,  4.9947e-03,\n",
       "         -2.3969e-04,  5.4399e-04, -3.6870e-04,  1.7008e-03, -2.6616e-03,\n",
       "          7.8865e-04,  6.3357e-04, -7.4134e-04,  2.1120e-03, -1.8364e-03,\n",
       "         -1.6539e-03,  1.8697e-03,  2.9691e-03,  4.1256e-03,  3.4328e-03,\n",
       "         -6.3266e-04, -1.6193e-03,  1.4494e-03,  1.8227e-03,  1.1089e-03,\n",
       "          4.3403e-04, -1.0454e-03,  6.1637e-04,  1.3804e-03,  4.4347e-04,\n",
       "          2.6873e-03,  2.1722e-06,  4.0990e-04,  2.0018e-03, -1.9650e-03,\n",
       "         -1.5848e-03,  5.4953e-07, -1.8939e-03, -1.4691e-03,  8.9181e-04,\n",
       "         -7.0730e-04, -5.6974e-04,  1.6067e-03,  2.6653e-03,  9.4016e-04,\n",
       "          7.3921e-04, -1.2279e-03, -2.9442e-03,  1.6078e-03,  1.8166e-03,\n",
       "         -1.3689e-03,  3.8161e-04,  7.2724e-04, -1.4769e-03,  1.4314e-03,\n",
       "          5.0540e-05,  3.1004e-03, -3.7069e-03,  1.7856e-03,  1.5290e-03,\n",
       "         -1.8083e-03,  2.8387e-03,  2.8542e-03,  1.2114e-03, -2.1943e-03,\n",
       "         -5.2383e-06,  9.1777e-04,  4.2570e-04, -3.9100e-05, -1.4808e-03,\n",
       "          1.1267e-03, -2.6707e-03, -8.4496e-05, -1.5320e-03, -5.7638e-04,\n",
       "         -9.8431e-04, -2.2040e-04, -1.6939e-03,  4.5149e-05, -3.4464e-03,\n",
       "          1.7170e-03,  6.5787e-04,  8.6333e-04, -3.6305e-03, -2.0797e-03,\n",
       "          2.8367e-03,  9.0189e-04,  9.7247e-04, -3.7318e-04, -1.6311e-03,\n",
       "          1.1952e-03, -1.1213e-03, -1.9451e-03,  5.4884e-04,  1.3651e-03,\n",
       "         -7.9554e-04,  3.4386e-03, -6.5344e-04,  3.8616e-03, -1.7414e-03,\n",
       "         -2.1015e-04,  3.7602e-03, -2.7630e-03, -1.8240e-03, -2.7185e-03,\n",
       "          1.8454e-03,  1.0524e-04,  1.3290e-03,  1.9968e-03,  7.2484e-05,\n",
       "         -6.7541e-04,  1.4433e-03, -1.8791e-03,  4.5555e-03,  3.9128e-04,\n",
       "          2.0268e-03, -1.8569e-03,  1.2478e-03, -5.5091e-04, -1.3660e-03,\n",
       "         -2.8754e-03,  4.5611e-04,  1.6820e-03, -4.0388e-03,  3.7378e-05,\n",
       "         -2.6397e-03,  8.7532e-04,  3.1421e-04, -1.1594e-03, -2.8801e-03,\n",
       "          2.0174e-03,  2.8578e-03, -9.9452e-04, -1.6315e-03,  4.2734e-04,\n",
       "          2.6302e-03,  2.8875e-03,  1.6268e-03,  9.2449e-04,  9.2110e-04,\n",
       "          3.3335e-03, -7.9777e-04, -6.1892e-04,  2.2932e-04, -2.4714e-03,\n",
       "          4.2326e-04,  3.7544e-03, -2.0372e-04, -3.9295e-03, -1.3825e-03,\n",
       "         -6.3038e-04,  1.5314e-03,  5.5260e-04, -6.2721e-04,  1.5029e-03,\n",
       "          5.9815e-04,  8.5813e-05, -6.9491e-04,  1.0286e-03,  2.1877e-03,\n",
       "         -3.4912e-03,  3.6484e-03,  3.1922e-04, -2.4522e-04, -3.1397e-03,\n",
       "         -1.0072e-04, -2.4266e-03,  5.6995e-05, -1.0822e-03,  1.4311e-03,\n",
       "         -2.1056e-03, -4.1267e-03, -1.6500e-03,  1.5718e-04,  2.0384e-04,\n",
       "          2.4818e-04,  3.1711e-03,  2.7470e-03,  1.2010e-03, -2.4969e-03,\n",
       "          1.7570e-03, -5.0345e-04,  3.0786e-03,  3.0051e-03, -7.0112e-04,\n",
       "          2.7913e-03, -2.8337e-04, -5.0900e-03,  1.7914e-03, -1.0496e-03,\n",
       "         -1.0723e-03,  1.3714e-03,  6.2495e-04, -6.0036e-04, -5.0472e-04,\n",
       "          3.5993e-03, -1.0172e-03,  2.7931e-03, -1.7508e-03, -2.5092e-03,\n",
       "          2.4518e-03,  1.5634e-03, -1.5199e-03, -5.4316e-04, -1.3281e-03,\n",
       "         -1.1894e-03,  2.9938e-03, -1.2881e-03,  4.0413e-03,  1.2418e-03,\n",
       "          7.9658e-04, -1.4516e-04, -6.0413e-04,  7.5762e-04,  3.3948e-03,\n",
       "          2.3681e-03, -1.1993e-03,  2.0766e-03,  3.5109e-04,  5.2748e-04,\n",
       "         -1.1173e-04, -1.7273e-03, -2.3340e-04, -3.1815e-04,  2.7127e-04,\n",
       "          1.5826e-03,  8.9212e-04,  1.2972e-03,  1.3682e-03,  1.9024e-03,\n",
       "         -8.6800e-04, -3.3071e-03,  6.4382e-04,  2.8649e-03,  7.8792e-04,\n",
       "         -2.8390e-03,  2.5081e-03,  2.0716e-03, -5.8641e-04,  2.5989e-03,\n",
       "         -8.9173e-04,  1.2151e-03,  2.1602e-03, -2.4366e-03, -1.8430e-03,\n",
       "          6.8932e-04, -4.5770e-04, -8.3074e-04,  8.7846e-04,  3.4729e-04,\n",
       "         -1.7575e-03,  2.0494e-03,  1.8419e-03, -8.2430e-04, -3.5840e-03,\n",
       "         -7.7612e-04,  2.0077e-03, -3.4009e-03,  1.6983e-03, -1.2595e-03,\n",
       "         -1.4844e-03,  6.7994e-04,  8.3147e-04, -3.5101e-04, -2.1605e-03,\n",
       "         -2.8665e-03, -4.5050e-04,  1.3811e-03, -1.7893e-04,  1.5803e-03,\n",
       "          1.1605e-04,  1.0981e-03, -1.1139e-03,  4.8440e-03,  1.0167e-03,\n",
       "         -2.2597e-03,  3.9135e-04,  2.3380e-03,  2.0318e-03,  1.9895e-05,\n",
       "          8.5990e-04, -9.4509e-04,  1.1297e-03,  3.8484e-03,  2.9166e-04,\n",
       "          3.2413e-05,  7.0772e-05,  1.2020e-03, -5.6244e-04, -2.3816e-03,\n",
       "         -2.0115e-03,  1.3418e-03, -2.4104e-03, -1.4397e-03, -1.9255e-03,\n",
       "          5.2059e-04, -2.8245e-03, -1.6346e-03, -1.6944e-03, -1.1542e-03,\n",
       "         -3.1341e-03, -2.3919e-03,  8.8986e-04,  5.4135e-04, -1.2610e-03,\n",
       "          1.3880e-03, -1.3883e-03, -1.9709e-03,  2.2784e-03,  1.3850e-03,\n",
       "          7.8504e-04, -1.0989e-03,  9.8098e-04, -2.2975e-03, -1.9143e-03,\n",
       "          4.2583e-04,  1.3932e-03, -6.8232e-04,  5.0244e-04, -3.2959e-03,\n",
       "         -2.2384e-03,  9.7778e-04, -1.5690e-03,  3.8964e-03, -3.1630e-03,\n",
       "          1.4521e-03, -1.9605e-03, -8.8142e-04,  9.8274e-04, -5.5861e-04,\n",
       "         -1.5946e-03, -1.5927e-04]),\n",
       " 'transformer.resblocks.10.ln_1.weight': tensor([0.9992, 1.0015, 1.0029, 1.0030, 1.0011, 1.0010, 0.9972, 0.9994, 0.9987,\n",
       "         1.0030, 1.0004, 1.0004, 1.0019, 0.9995, 0.9985, 0.9974, 0.9988, 1.0013,\n",
       "         0.9977, 1.0013, 0.9970, 0.9981, 1.0015, 1.0022, 0.9979, 0.9993, 0.9970,\n",
       "         0.9985, 0.9997, 0.9965, 1.0009, 0.9986, 0.9995, 0.9985, 0.9994, 1.0026,\n",
       "         0.9991, 1.0008, 0.9977, 0.9985, 1.0017, 1.0021, 0.9984, 0.9994, 0.9997,\n",
       "         0.9979, 1.0004, 0.9994, 1.0002, 0.9965, 0.9998, 1.0018, 0.9999, 0.9990,\n",
       "         1.0001, 0.9967, 1.0000, 1.0008, 0.9996, 0.9995, 1.0008, 0.9992, 0.9979,\n",
       "         0.9997, 0.9986, 0.9984, 0.9993, 0.9997, 0.9979, 1.0005, 0.9997, 0.9981,\n",
       "         0.9994, 1.0000, 0.9951, 0.9993, 0.9970, 1.0006, 0.9997, 0.9970, 0.9993,\n",
       "         1.0006, 1.0012, 0.9983, 1.0003, 0.9962, 0.9997, 0.9998, 0.9997, 0.9961,\n",
       "         0.9959, 1.0008, 0.9991, 1.0004, 0.9963, 1.0004, 1.0002, 0.9987, 0.9975,\n",
       "         0.9972, 1.0018, 0.9999, 1.0012, 1.0001, 0.9996, 1.0003, 0.9983, 1.0027,\n",
       "         0.9986, 1.0003, 0.9997, 0.9991, 0.9982, 1.0000, 1.0001, 0.9972, 0.9998,\n",
       "         1.0026, 0.9989, 0.9982, 1.0000, 0.9979, 0.9995, 1.0010, 0.9984, 0.9985,\n",
       "         1.0017, 1.0006, 0.9981, 1.0008, 1.0009, 1.0008, 1.0001, 1.0018, 0.9965,\n",
       "         1.0014, 0.9986, 0.9987, 0.9967, 1.0003, 0.9998, 1.0006, 0.9989, 1.0021,\n",
       "         0.9981, 1.0004, 0.9974, 1.0030, 0.9988, 1.0006, 1.0012, 0.9979, 0.9996,\n",
       "         1.0000, 0.9984, 1.0006, 1.0001, 0.9993, 1.0014, 1.0009, 1.0033, 0.9989,\n",
       "         0.9986, 1.0001, 0.9992, 1.0000, 0.9990, 0.9972, 1.0006, 0.9993, 1.0006,\n",
       "         0.9971, 1.0006, 1.0018, 1.0001, 1.0010, 1.0016, 0.9970, 0.9986, 0.9989,\n",
       "         1.0002, 1.0022, 0.9972, 0.9996, 0.9979, 1.0013, 1.0017, 1.0007, 0.9983,\n",
       "         0.9995, 1.0007, 0.9998, 1.0024, 0.9968, 1.0000, 0.9994, 0.9973, 0.9987,\n",
       "         0.9978, 0.9984, 0.9980, 0.9979, 0.9998, 0.9983, 1.0033, 0.9998, 1.0022,\n",
       "         0.9978, 0.9998, 1.0009, 0.9994, 0.9999, 0.9991, 0.9978, 0.9989, 0.9998,\n",
       "         1.0003, 0.9996, 0.9991, 0.9983, 1.0026, 1.0019, 0.9978, 1.0013, 1.0004,\n",
       "         1.0012, 1.0004, 0.9982, 1.0001, 1.0000, 0.9963, 0.9985, 1.0000, 0.9993,\n",
       "         1.0002, 0.9980, 0.9995, 0.9993, 0.9993, 0.9994, 0.9990, 0.9999, 0.9989,\n",
       "         1.0001, 0.9992, 0.9969, 0.9991, 1.0008, 1.0004, 1.0001, 0.9996, 0.9977,\n",
       "         0.9991, 1.0004, 1.0005, 0.9982, 1.0002, 1.0010, 0.9971, 1.0014, 0.9999,\n",
       "         1.0009, 1.0006, 0.9990, 0.9989, 0.9989, 0.9994, 1.0016, 0.9967, 0.9993,\n",
       "         0.9995, 0.9995, 0.9970, 0.9988, 0.9990, 0.9981, 0.9987, 1.0025, 0.9977,\n",
       "         1.0002, 0.9975, 1.0028, 0.9985, 1.0002, 0.9977, 1.0015, 0.9966, 0.9989,\n",
       "         0.9999, 0.9995, 1.0029, 1.0000, 0.9995, 1.0000, 0.9996, 1.0018, 0.9980,\n",
       "         1.0026, 0.9989, 0.9972, 1.0006, 1.0013, 1.0000, 1.0011, 1.0014, 0.9986,\n",
       "         0.9964, 0.9996, 1.0000, 0.9965, 0.9992, 1.0003, 1.0037, 1.0006, 0.9996,\n",
       "         0.9995, 0.9997, 0.9973, 0.9989, 1.0011, 0.9989, 0.9971, 0.9980, 1.0002,\n",
       "         1.0014, 0.9997, 1.0006, 1.0019, 0.9992, 1.0000, 0.9966, 0.9972, 0.9971,\n",
       "         0.9995, 0.9999, 1.0013, 1.0008, 0.9998, 0.9998, 0.9979, 0.9986, 1.0000,\n",
       "         0.9979, 0.9998, 1.0003, 1.0015, 1.0006, 0.9985, 1.0008, 0.9982, 0.9997,\n",
       "         0.9989, 1.0003, 0.9994, 0.9987, 1.0011, 1.0009, 0.9982, 0.9979, 0.9988,\n",
       "         1.0013, 1.0003, 0.9946, 1.0018, 1.0018, 1.0001, 0.9990, 1.0005, 1.0024,\n",
       "         0.9985, 0.9991, 1.0002, 0.9980, 0.9995, 0.9976, 0.9999, 0.9990, 1.0015,\n",
       "         0.9981, 0.9986, 1.0012, 1.0015, 1.0013, 1.0005, 0.9975, 1.0025, 1.0005,\n",
       "         0.9954, 0.9984, 0.9991, 0.9985, 0.9982, 1.0004, 0.9980, 0.9986, 0.9999,\n",
       "         1.0008, 1.0032, 1.0016, 0.9982, 1.0019, 1.0008, 1.0001, 1.0030, 1.0008,\n",
       "         1.0003, 0.9986, 0.9988, 1.0003, 0.9997, 1.0016, 1.0002, 1.0003, 0.9997,\n",
       "         0.9990, 0.9968, 0.9968, 0.9998, 0.9964, 1.0000, 0.9992, 0.9988, 0.9989,\n",
       "         0.9987, 1.0010, 0.9988, 1.0002, 0.9986, 0.9972, 1.0015, 1.0001, 0.9981,\n",
       "         0.9966, 0.9981, 0.9993, 1.0017, 1.0012, 0.9998, 1.0007, 1.0006, 0.9995,\n",
       "         0.9974, 0.9984, 0.9955, 1.0017, 0.9972, 0.9987, 0.9999, 1.0002, 0.9998,\n",
       "         1.0016, 0.9976, 1.0010, 0.9975, 0.9978, 0.9985, 0.9972, 0.9996, 1.0003,\n",
       "         1.0007, 0.9980, 1.0024, 0.9984, 1.0017, 0.9992, 0.9978, 0.9972, 0.9992,\n",
       "         1.0010, 0.9975, 0.9992, 0.9999, 0.9997, 0.9977, 0.9980, 0.9991, 0.9988,\n",
       "         0.9983, 1.0012, 1.0002, 1.0002, 1.0031, 1.0008, 0.9993, 0.9992, 1.0004,\n",
       "         0.9980, 1.0002, 1.0015, 0.9990, 1.0007, 0.9982, 0.9990, 0.9989, 1.0015,\n",
       "         1.0003, 1.0014, 0.9986, 0.9963, 0.9989, 0.9994, 1.0007, 0.9996, 0.9961,\n",
       "         0.9993, 0.9978, 0.9987, 0.9975, 0.9983, 1.0001, 0.9999, 1.0008]),\n",
       " 'transformer.resblocks.10.ln_1.bias': tensor([-9.9415e-04, -1.9775e-03, -1.3741e-03, -1.5857e-03,  2.6872e-03,\n",
       "         -2.3776e-03,  2.3129e-03,  9.5021e-04,  2.4397e-04, -1.3059e-03,\n",
       "         -4.0490e-04,  1.9555e-03, -8.5591e-04, -2.1160e-03,  2.5304e-04,\n",
       "          1.6384e-04,  1.8117e-03, -5.5078e-04,  4.8822e-03,  4.2876e-04,\n",
       "          4.2839e-04,  1.7427e-03,  6.7393e-04, -2.8038e-03, -1.3539e-03,\n",
       "          2.4423e-03,  7.5139e-04,  1.6033e-03,  8.4112e-04,  1.4960e-03,\n",
       "          1.3520e-03,  2.7450e-03, -1.7678e-03,  6.2237e-04, -1.5436e-03,\n",
       "          1.0279e-03, -1.4448e-03, -1.4090e-03, -6.9912e-04,  1.2784e-03,\n",
       "         -1.2879e-03,  1.0438e-03, -4.3677e-04, -6.3388e-04,  1.3675e-05,\n",
       "         -5.6728e-05, -1.3294e-04,  1.0408e-03, -3.1418e-04, -1.8171e-03,\n",
       "         -2.1187e-03, -3.4902e-03, -2.7068e-03, -5.5977e-04, -3.9761e-05,\n",
       "          1.2750e-03,  1.2388e-03, -4.2194e-04,  1.1657e-03,  1.4493e-03,\n",
       "         -1.2587e-03, -1.3785e-03,  7.0069e-04, -7.1522e-04,  2.5265e-04,\n",
       "          5.1564e-04, -1.5385e-03,  6.4422e-04,  1.4564e-03, -7.0054e-04,\n",
       "         -2.6808e-03,  1.5122e-03, -2.6378e-05,  2.4847e-04,  1.1392e-03,\n",
       "          3.9637e-04,  2.3130e-03, -5.6130e-04,  1.2200e-03, -2.3356e-03,\n",
       "          9.3529e-05, -1.2950e-03,  1.9015e-03, -1.1259e-03, -3.1419e-04,\n",
       "         -8.3963e-06, -3.4697e-04, -3.2302e-04, -1.5475e-03,  1.8451e-03,\n",
       "          2.6801e-03, -2.8239e-03, -1.7194e-03,  2.7544e-03,  1.7746e-03,\n",
       "         -7.0541e-04,  1.3742e-03,  8.7869e-04,  1.8391e-03, -1.8395e-03,\n",
       "          1.8047e-03,  7.1108e-04,  1.3909e-03, -1.6081e-03, -1.2113e-03,\n",
       "         -1.7049e-03,  4.1389e-04, -4.8279e-04,  2.0520e-03,  1.2357e-03,\n",
       "          1.3491e-03,  4.9081e-04, -1.2375e-03, -2.5969e-03, -8.3264e-05,\n",
       "          1.8576e-03,  4.1338e-04,  2.2712e-03, -7.0090e-04,  2.7435e-04,\n",
       "          8.7774e-04,  5.1829e-04,  5.2658e-04,  1.6592e-03,  1.0462e-03,\n",
       "          3.2777e-04, -1.0522e-03, -2.8628e-03,  1.2513e-03,  3.8291e-04,\n",
       "         -2.3823e-04,  2.4332e-03,  1.0271e-03,  1.4177e-04,  2.5962e-03,\n",
       "         -5.2365e-04,  3.5534e-04, -2.0795e-03,  1.1183e-03,  1.1510e-03,\n",
       "         -1.0378e-03, -1.6345e-03,  4.7651e-04,  3.2391e-03, -2.4508e-03,\n",
       "          1.0021e-03,  5.4756e-04, -3.2235e-03, -2.1560e-03, -5.7201e-04,\n",
       "         -1.0279e-03, -1.6737e-03,  2.5303e-04,  8.8441e-04, -2.3402e-03,\n",
       "         -6.4849e-04,  9.0588e-04,  1.1169e-03,  3.2142e-04,  1.2995e-03,\n",
       "         -1.3169e-03, -1.3423e-03, -6.0876e-04, -1.9883e-03,  8.7247e-04,\n",
       "         -1.5361e-03,  3.6370e-04,  2.6841e-03,  3.6943e-04,  8.6904e-04,\n",
       "          1.6430e-03,  5.0749e-04,  1.4537e-03,  3.1283e-03,  4.8850e-04,\n",
       "          2.8254e-03, -2.6608e-04, -3.7150e-04,  1.4948e-03, -3.4710e-04,\n",
       "          3.0418e-04,  1.0722e-03, -5.2602e-04,  2.4023e-04, -1.6077e-03,\n",
       "          1.2393e-03,  1.9385e-03,  6.9724e-04, -2.9769e-03, -1.1280e-03,\n",
       "         -1.4477e-03,  1.3329e-03,  2.8148e-03, -3.1733e-03,  2.5408e-03,\n",
       "          1.5125e-03, -1.6401e-03, -5.6176e-04, -2.4488e-04, -4.7521e-04,\n",
       "          1.6149e-03,  2.1487e-03,  5.2828e-04, -4.1726e-04, -2.4243e-03,\n",
       "          9.1514e-04, -2.8484e-03,  1.7034e-03,  1.4293e-03, -2.2761e-03,\n",
       "          1.4173e-03,  2.0870e-05, -3.3125e-03, -6.6482e-04, -2.6052e-04,\n",
       "         -7.8096e-04,  3.6947e-04, -9.0579e-04,  1.6623e-04, -4.3571e-03,\n",
       "         -2.0377e-03, -8.8964e-04,  1.3552e-03, -2.2947e-03,  1.3682e-03,\n",
       "          5.3546e-04, -4.9484e-05,  1.6371e-03, -1.5888e-03, -9.4008e-04,\n",
       "         -3.2265e-03,  5.0345e-04, -1.8609e-05,  2.7146e-03, -2.5054e-04,\n",
       "          2.5336e-03,  1.2244e-03,  3.4265e-04, -1.1018e-04,  4.9525e-04,\n",
       "          2.9271e-03, -1.0692e-03,  8.9129e-04,  6.5677e-04, -1.0479e-03,\n",
       "          1.1349e-03, -1.5658e-03,  3.9711e-04, -4.6767e-04, -7.0058e-04,\n",
       "          8.1901e-04,  1.4106e-03,  2.2082e-03, -5.5146e-04, -5.1527e-04,\n",
       "         -2.8423e-04, -8.3456e-04, -2.5226e-03, -3.7941e-03, -2.2952e-03,\n",
       "          4.4364e-04,  1.8913e-03,  1.1365e-04,  1.1048e-03,  4.0961e-04,\n",
       "         -1.2124e-03,  1.3495e-03,  4.2716e-03, -2.7358e-03, -1.1690e-04,\n",
       "          6.4994e-04, -8.3854e-04,  1.4822e-03, -4.6280e-04, -4.9893e-05,\n",
       "          1.1245e-03, -1.6820e-03, -5.2633e-04, -1.2794e-03, -1.2401e-03,\n",
       "          1.0029e-03,  3.3251e-03, -3.5965e-05, -2.4525e-03,  2.5564e-03,\n",
       "          5.7483e-04,  8.3643e-04,  2.2845e-04,  1.4234e-03, -1.5449e-03,\n",
       "          1.5445e-03,  7.6540e-05, -6.5042e-04,  1.0662e-03,  1.1134e-03,\n",
       "         -2.8271e-03,  3.8371e-04, -1.0553e-03, -1.7391e-03,  3.3728e-03,\n",
       "         -1.3506e-03,  1.7460e-03, -5.7472e-04, -7.6961e-04,  1.8839e-04,\n",
       "         -8.2674e-04, -1.4044e-03,  1.1560e-04, -1.0098e-03,  3.6846e-03,\n",
       "         -6.4597e-06, -7.6904e-04, -2.3679e-05,  7.9825e-04, -4.1816e-04,\n",
       "          6.3422e-04,  4.9220e-04,  1.0015e-03,  7.5933e-04, -2.0318e-04,\n",
       "          2.2918e-04, -9.5292e-04,  1.2599e-03, -2.2400e-04, -1.8858e-03,\n",
       "          2.2897e-04,  1.1295e-04,  1.3577e-03,  6.4152e-04, -2.2152e-03,\n",
       "         -3.2186e-03,  4.8972e-04, -1.8233e-03,  1.5134e-04,  1.5132e-03,\n",
       "         -2.5683e-03,  1.9214e-04,  7.9505e-04,  6.3837e-04, -1.7069e-03,\n",
       "          1.1924e-03, -4.1760e-04, -1.5223e-03,  4.9876e-04, -1.6455e-03,\n",
       "          7.4388e-04, -1.6612e-03, -1.8480e-03,  8.8139e-06,  7.6613e-04,\n",
       "          1.7329e-04,  6.6968e-04, -2.1502e-03, -1.5917e-03, -2.6370e-03,\n",
       "          2.9090e-03,  1.9269e-03, -3.0020e-04,  1.2811e-03, -8.8765e-04,\n",
       "          6.3140e-05, -1.2638e-03,  2.6087e-04,  2.0719e-03,  1.8609e-03,\n",
       "          8.6630e-04, -4.4889e-04,  4.3447e-04,  9.1817e-04,  1.0425e-03,\n",
       "         -8.1733e-04,  9.3793e-04, -1.2304e-04,  2.4427e-03,  2.8424e-03,\n",
       "          2.7896e-07,  1.0088e-03,  2.0528e-03,  1.3243e-03,  1.7079e-03,\n",
       "          1.6683e-03,  1.3568e-03, -1.8779e-03,  4.8018e-04,  1.7685e-03,\n",
       "          1.3277e-03,  1.4215e-03,  5.0929e-03, -6.6529e-04,  7.6894e-05,\n",
       "         -3.6582e-03, -1.3874e-04, -7.9859e-04,  1.4838e-03,  2.2588e-05,\n",
       "         -1.1876e-03, -6.0438e-05,  2.9264e-03, -1.3854e-03,  1.2643e-03,\n",
       "         -1.0417e-03,  5.6348e-04, -4.5113e-04, -1.1988e-03, -1.0007e-03,\n",
       "         -1.0517e-04,  1.9357e-03,  4.6626e-04,  1.5509e-03, -1.2898e-03,\n",
       "          1.1851e-03, -9.6423e-04, -1.8821e-03, -2.7212e-03,  1.2928e-03,\n",
       "          2.0899e-03,  8.5594e-04,  9.7380e-04,  2.9330e-03,  2.1130e-03,\n",
       "         -2.1141e-03, -2.7620e-04, -2.5958e-03,  1.3840e-03, -2.4986e-03,\n",
       "          5.7699e-04, -2.0751e-03,  2.8778e-03,  7.9816e-04, -6.9356e-04,\n",
       "         -1.6762e-03, -1.2543e-04, -1.6500e-03, -3.0840e-04,  4.0721e-04,\n",
       "         -1.6026e-03,  2.4432e-03, -1.0577e-03, -5.6972e-04, -1.7911e-03,\n",
       "         -1.5218e-03,  1.1592e-03,  7.1482e-04,  2.2315e-03, -1.4828e-03,\n",
       "          5.5001e-04, -1.3691e-03, -2.3283e-03, -3.0337e-03,  7.0268e-04,\n",
       "          2.8750e-03,  3.6475e-03, -1.1605e-03, -2.1559e-03, -6.3032e-04,\n",
       "         -7.3716e-05,  8.1393e-04,  1.1064e-03,  1.7870e-03,  2.4454e-03,\n",
       "          1.2239e-03, -2.6177e-03,  2.0420e-03, -3.5726e-06,  1.1269e-03,\n",
       "          2.8703e-04, -1.7234e-04,  1.6845e-04,  2.7127e-03,  4.4602e-03,\n",
       "          2.3719e-04,  6.7302e-04, -8.3345e-04, -2.6957e-03,  7.6921e-04,\n",
       "          8.2398e-04,  7.3653e-04,  4.2633e-04, -1.6386e-03, -1.5784e-03,\n",
       "          5.3341e-04, -8.3545e-04,  1.5549e-03,  3.0356e-04,  1.9220e-03,\n",
       "         -6.5229e-04,  9.3381e-05, -7.7934e-04,  1.4206e-03,  2.2182e-03,\n",
       "          4.8684e-04,  8.1972e-04,  9.9901e-04, -7.8199e-05, -7.9466e-04,\n",
       "          6.0801e-05, -4.6150e-04, -1.8883e-04, -1.3336e-03,  4.6406e-04,\n",
       "         -3.3025e-04,  2.8354e-04, -9.6466e-04, -2.4269e-03,  2.8595e-04,\n",
       "         -2.3958e-03, -1.2800e-04,  8.3624e-04, -1.3307e-03,  5.2730e-04,\n",
       "          1.6032e-04, -2.3728e-03]),\n",
       " 'transformer.resblocks.10.mlp.c_fc.weight': tensor([[-0.0189, -0.0201,  0.0080,  ..., -0.0131, -0.0101, -0.0397],\n",
       "         [ 0.0005,  0.0228,  0.0195,  ...,  0.0247,  0.0133,  0.0579],\n",
       "         [ 0.0120,  0.0168,  0.0285,  ..., -0.0044, -0.0115, -0.0329],\n",
       "         ...,\n",
       "         [ 0.0029, -0.0386,  0.0193,  ..., -0.0099, -0.0173, -0.0468],\n",
       "         [ 0.0087,  0.0082, -0.0261,  ..., -0.0054, -0.0561, -0.0129],\n",
       "         [ 0.0182,  0.0166, -0.0199,  ..., -0.0101, -0.0162, -0.0016]]),\n",
       " 'transformer.resblocks.10.mlp.c_fc.bias': tensor([ 0.0094,  0.0381,  0.0335,  ...,  0.0020, -0.0267,  0.0232]),\n",
       " 'transformer.resblocks.10.mlp.c_proj.weight': tensor([[-0.0002, -0.0061, -0.0085,  ...,  0.0115, -0.0021,  0.0029],\n",
       "         [-0.0097, -0.0067, -0.0164,  ..., -0.0018,  0.0015,  0.0099],\n",
       "         [-0.0008,  0.0052,  0.0019,  ..., -0.0044,  0.0052, -0.0077],\n",
       "         ...,\n",
       "         [ 0.0037,  0.0059, -0.0098,  ...,  0.0020, -0.0073, -0.0039],\n",
       "         [ 0.0007,  0.0072,  0.0176,  ..., -0.0090,  0.0041,  0.0079],\n",
       "         [ 0.0095, -0.0002,  0.0007,  ..., -0.0159, -0.0143,  0.0213]]),\n",
       " 'transformer.resblocks.10.mlp.c_proj.bias': tensor([-6.7612e-03, -2.1360e-02,  1.1146e-02, -1.4758e-02,  5.0379e-03,\n",
       "         -9.0624e-03, -1.2899e-02, -4.6039e-03, -4.9047e-03, -9.5331e-03,\n",
       "          1.0399e-02,  1.2807e-02, -4.4603e-03, -2.0197e-02,  3.0146e-03,\n",
       "          1.2543e-02,  3.0944e-03, -8.9894e-03,  2.0835e-02, -4.0097e-03,\n",
       "         -1.4596e-02, -3.9653e-03,  1.8883e-03,  2.6451e-03, -6.8659e-03,\n",
       "          1.5906e-03,  1.3912e-02, -3.8458e-03,  3.8153e-03,  1.4426e-02,\n",
       "         -2.1150e-02, -1.9220e-02,  6.1637e-03, -1.6689e-02,  2.5870e-03,\n",
       "         -2.7908e-03, -1.8013e-02,  4.0774e-03, -3.2451e-03,  1.1176e-03,\n",
       "         -2.6235e-03,  2.6236e-03,  1.2458e-02, -1.1332e-02, -1.0844e-02,\n",
       "          1.7084e-03,  9.3346e-03,  1.7856e-02, -2.2023e-02, -5.3503e-03,\n",
       "         -1.0853e-02,  5.1309e-03,  1.8797e-02, -7.7754e-03,  4.3655e-03,\n",
       "          1.1348e-03,  8.6801e-03,  1.3154e-03, -6.9718e-03, -2.2121e-02,\n",
       "         -2.1277e-02, -1.1062e-02,  1.8974e-02,  1.5301e-02,  1.7646e-02,\n",
       "          3.1334e-03,  1.9670e-02,  1.5407e-02, -2.3587e-02,  1.3219e-02,\n",
       "         -1.5758e-03, -1.5078e-02,  1.4359e-02,  6.0737e-03, -1.4258e-02,\n",
       "          2.1047e-02, -2.0280e-02, -1.8711e-02,  5.0648e-03,  8.4968e-03,\n",
       "         -1.0021e-03, -2.3241e-02,  1.9645e-02,  2.3621e-02, -1.7370e-03,\n",
       "          3.9542e-03, -5.9763e-03, -2.1101e-02,  1.7948e-02, -1.7031e-02,\n",
       "          2.1823e-02,  2.0891e-02, -1.5062e-02, -1.1349e-02,  4.0250e-03,\n",
       "         -8.4717e-03,  3.3682e-03, -5.5021e-03,  8.4672e-05,  6.6207e-03,\n",
       "         -3.3984e-04, -1.2764e-02,  1.1790e-02, -1.1971e-02, -1.4008e-03,\n",
       "          9.2700e-03, -1.5405e-02, -2.1692e-02, -2.0357e-02,  1.1410e-02,\n",
       "          3.4115e-03, -7.0914e-03, -1.4491e-02, -1.9380e-02, -2.3472e-02,\n",
       "         -1.5038e-02, -8.1617e-03, -1.7429e-02,  1.0432e-02, -1.8268e-02,\n",
       "         -1.7342e-02,  2.1876e-03,  2.4096e-02, -1.5402e-02, -9.6808e-03,\n",
       "         -1.6566e-03,  2.0159e-02, -1.7281e-03, -1.9793e-02,  1.8789e-02,\n",
       "         -1.4356e-02,  2.4680e-02,  1.7352e-02,  1.0460e-02, -1.0518e-02,\n",
       "          5.8786e-04,  1.3271e-02, -5.4716e-03,  2.0705e-02,  1.3290e-03,\n",
       "          6.2833e-03, -1.5057e-02, -1.5216e-02, -8.3800e-03, -2.5305e-03,\n",
       "         -9.2847e-03, -8.4093e-03, -5.9625e-03,  1.4493e-02,  6.9458e-03,\n",
       "          1.6145e-02, -1.4880e-02, -9.4566e-03, -3.9223e-03,  2.0259e-02,\n",
       "          1.0910e-02, -6.6954e-03, -6.9850e-03, -2.0843e-02,  2.0406e-02,\n",
       "          1.6312e-03, -1.0935e-02,  1.6885e-02,  3.2187e-03, -1.4734e-02,\n",
       "          1.2120e-02,  1.9017e-02,  2.0378e-02,  1.0801e-02,  1.6094e-02,\n",
       "         -7.8860e-04, -6.3625e-03, -6.7676e-03, -6.4482e-03,  3.4907e-05,\n",
       "          2.0766e-02, -1.7021e-02,  1.9176e-02, -8.1118e-03,  7.8025e-03,\n",
       "         -1.1732e-02,  1.1389e-02,  7.0204e-03,  1.4765e-02,  1.1089e-02,\n",
       "          1.5158e-02,  1.9285e-02,  1.7168e-02,  9.2698e-03,  1.8717e-02,\n",
       "         -1.6294e-02,  1.5936e-02, -1.5559e-02, -1.3084e-02,  1.0220e-02,\n",
       "         -1.2020e-02,  1.8017e-02,  1.2824e-02,  8.2793e-03,  8.4648e-03,\n",
       "          2.1320e-03, -1.2150e-02, -7.2262e-03, -1.8774e-03, -1.0501e-02,\n",
       "          6.9164e-03,  2.8926e-03,  1.7234e-02,  4.4027e-03, -2.0061e-02,\n",
       "         -1.5797e-02, -2.0343e-02, -1.7731e-02, -1.0118e-02, -1.2135e-02,\n",
       "          1.2329e-02, -1.1744e-03,  1.2190e-02, -1.7600e-02, -7.6817e-03,\n",
       "          1.5409e-03, -1.6810e-02, -8.6093e-03,  2.2101e-03,  1.5642e-02,\n",
       "         -9.0062e-03, -5.5135e-03,  1.4103e-02, -7.1824e-03, -1.2483e-02,\n",
       "          4.6931e-04,  2.4001e-03,  1.2826e-02, -1.2197e-02, -1.1187e-02,\n",
       "          1.3173e-02,  2.5393e-03, -1.1898e-02, -1.4968e-02,  6.2173e-03,\n",
       "         -2.1410e-02, -2.0219e-03,  2.2009e-02, -1.0282e-02, -5.1792e-03,\n",
       "         -1.3656e-02,  2.0808e-02, -1.4718e-02,  1.0054e-04, -2.2260e-03,\n",
       "          1.0427e-02, -1.2069e-02, -1.9751e-03,  5.5218e-03,  1.4405e-02,\n",
       "         -1.2857e-02,  1.3744e-02,  8.3661e-03,  6.1601e-03, -2.1919e-02,\n",
       "          1.9787e-02, -1.4582e-02, -8.8261e-03,  5.1688e-03, -1.7377e-02,\n",
       "          1.3395e-02, -4.5048e-03,  2.2587e-02,  9.2478e-03,  3.3452e-03,\n",
       "          2.3190e-02,  1.0840e-02, -4.9847e-03,  2.4972e-03,  1.9548e-02,\n",
       "          1.7393e-02,  7.3377e-03, -4.3966e-03, -3.1972e-03,  1.3271e-02,\n",
       "          1.6616e-02,  2.0889e-02,  1.8536e-02, -1.1599e-02, -4.5058e-03,\n",
       "         -1.5489e-02,  4.1109e-03, -1.2684e-02, -1.3598e-02,  1.3119e-02,\n",
       "         -1.0594e-02, -2.6925e-03, -1.7046e-02,  1.9443e-02,  1.2495e-02,\n",
       "          1.7255e-02, -1.1229e-02,  6.9277e-03,  1.1028e-02, -4.7521e-03,\n",
       "         -2.1009e-03, -2.3787e-03,  7.5366e-03, -1.1076e-02, -8.5909e-03,\n",
       "          4.3559e-03, -2.0461e-02,  6.1921e-03,  6.0738e-03, -8.4324e-03,\n",
       "          9.7696e-03, -5.0706e-03,  1.8672e-02,  3.0693e-03,  1.6910e-02,\n",
       "         -1.1190e-02,  1.4843e-02,  1.9388e-02, -2.7777e-03,  2.1054e-02,\n",
       "          7.6350e-03, -1.3121e-02,  2.1302e-02,  9.0722e-03,  8.6436e-03,\n",
       "          1.2687e-02, -8.5548e-03, -1.6205e-03,  4.4523e-03,  1.5204e-02,\n",
       "         -1.9182e-02,  1.1733e-02,  1.4133e-02,  1.2070e-02,  1.9718e-02,\n",
       "         -1.9029e-02,  9.5350e-03, -1.2263e-02,  7.8960e-03,  3.1764e-03,\n",
       "         -1.6900e-02, -1.5983e-02,  7.2940e-03, -1.8634e-02,  1.4923e-02,\n",
       "         -5.7464e-04, -7.1694e-03,  1.5658e-02, -3.4160e-03, -1.1537e-03,\n",
       "         -5.8062e-03,  1.6270e-02, -1.6886e-02,  8.1161e-03, -1.8818e-02,\n",
       "         -1.6568e-02, -1.7117e-02, -1.6869e-02,  1.0791e-02, -9.2023e-03,\n",
       "         -1.5265e-02, -4.2641e-03, -1.5392e-02,  1.8730e-03,  1.8271e-02,\n",
       "          4.4518e-03,  1.2131e-02,  9.5541e-03, -1.0942e-02,  1.7045e-02,\n",
       "          3.1143e-04,  1.4255e-02,  1.3066e-02,  8.6095e-03,  1.9172e-02,\n",
       "         -1.6623e-02, -3.0855e-03,  1.1400e-03,  2.4079e-03, -1.5742e-02,\n",
       "          1.3011e-02,  1.1311e-02,  1.9796e-02, -8.0281e-03, -1.1853e-02,\n",
       "         -2.3682e-03, -1.6970e-02, -8.1477e-03,  6.5933e-03, -2.0462e-02,\n",
       "         -9.7651e-04,  3.8298e-03,  1.5594e-03,  1.8276e-02, -1.5366e-02,\n",
       "         -8.0065e-03,  2.0082e-02,  1.1651e-03,  1.4533e-02,  2.0911e-02,\n",
       "         -2.1675e-02, -2.7179e-04,  1.9043e-02,  4.4656e-07, -1.0346e-02,\n",
       "          2.1373e-02,  5.7397e-03,  4.5241e-04,  1.8111e-02,  7.6155e-04,\n",
       "          1.2172e-03, -2.5152e-02, -3.1773e-03, -1.8472e-02,  9.9405e-03,\n",
       "          1.5825e-02, -1.7535e-02,  1.3172e-02,  1.0870e-02,  2.3914e-02,\n",
       "          1.7794e-02, -1.4588e-02, -1.8739e-03,  2.0715e-03,  1.6479e-03,\n",
       "          1.5389e-02,  2.0549e-02,  1.7625e-02, -3.5832e-03,  7.5174e-03,\n",
       "          6.4270e-03,  1.3880e-02,  1.0675e-02,  4.9342e-03, -1.7928e-02,\n",
       "         -2.2002e-02, -1.0747e-02,  2.1020e-03, -1.0903e-03,  1.3900e-02,\n",
       "         -2.1344e-02,  1.2260e-02, -1.4112e-02,  1.9226e-02, -1.9388e-02,\n",
       "          9.7985e-03, -1.8544e-02, -4.3599e-03, -2.9058e-03, -1.8104e-02,\n",
       "          8.6703e-03, -1.7501e-02, -6.5228e-03, -7.5996e-03, -1.5691e-02,\n",
       "          4.8630e-03, -1.6756e-02,  8.8199e-03, -1.1813e-02, -6.3473e-03,\n",
       "          1.5345e-03,  5.4082e-03,  1.4628e-03,  2.1383e-02,  5.4880e-03,\n",
       "         -5.5453e-03, -1.9356e-02,  9.1005e-03,  2.1309e-02,  6.5287e-03,\n",
       "         -1.4622e-02,  1.6444e-02,  8.9382e-03, -5.5319e-03,  1.8434e-03,\n",
       "         -1.6061e-04,  1.6659e-02, -2.3274e-03, -1.5014e-02, -2.1675e-02,\n",
       "         -2.0695e-02,  2.7593e-03, -7.5221e-03,  5.0526e-03, -2.2990e-02,\n",
       "          1.1529e-02, -1.0792e-02,  4.9466e-03, -8.5439e-03,  1.1151e-02,\n",
       "         -2.6547e-03, -1.8233e-02,  1.9357e-03, -2.3990e-02, -4.5851e-03,\n",
       "          1.1271e-02,  1.0918e-02, -1.9183e-02,  1.7876e-02, -6.0656e-03,\n",
       "          9.7950e-03,  2.1243e-02,  1.1696e-02,  1.2113e-02, -2.1143e-02,\n",
       "         -9.2467e-03,  4.1766e-03,  1.9939e-02, -1.6974e-02, -1.3754e-03,\n",
       "          1.1687e-02, -2.5524e-03]),\n",
       " 'transformer.resblocks.10.ln_2.weight': tensor([0.9971, 0.9989, 0.9992, 1.0023, 1.0022, 0.9975, 1.0025, 0.9992, 0.9986,\n",
       "         1.0002, 0.9979, 0.9992, 0.9997, 1.0008, 0.9939, 0.9977, 1.0005, 1.0017,\n",
       "         0.9995, 0.9999, 1.0020, 0.9993, 1.0036, 1.0009, 0.9973, 1.0000, 0.9994,\n",
       "         0.9987, 1.0008, 1.0003, 1.0022, 1.0025, 0.9972, 0.9969, 1.0010, 0.9991,\n",
       "         1.0008, 1.0018, 1.0024, 0.9985, 0.9987, 0.9999, 1.0004, 0.9994, 0.9960,\n",
       "         1.0003, 0.9965, 1.0005, 1.0007, 0.9998, 0.9994, 1.0031, 1.0022, 0.9985,\n",
       "         0.9952, 1.0001, 1.0030, 1.0036, 1.0017, 1.0020, 0.9982, 1.0014, 1.0027,\n",
       "         1.0009, 1.0028, 0.9983, 0.9990, 1.0017, 0.9984, 1.0027, 1.0037, 0.9997,\n",
       "         0.9959, 1.0006, 1.0004, 0.9980, 0.9971, 0.9989, 0.9983, 1.0007, 0.9976,\n",
       "         1.0012, 1.0021, 0.9979, 1.0000, 1.0008, 1.0006, 1.0002, 1.0029, 1.0006,\n",
       "         1.0015, 1.0030, 1.0023, 1.0006, 1.0021, 0.9957, 1.0041, 0.9991, 1.0003,\n",
       "         0.9993, 0.9995, 1.0007, 0.9987, 0.9975, 1.0035, 0.9967, 0.9950, 1.0011,\n",
       "         0.9985, 1.0004, 0.9978, 0.9998, 0.9979, 1.0017, 0.9989, 0.9969, 0.9995,\n",
       "         1.0000, 0.9942, 0.9995, 0.9998, 0.9992, 0.9999, 0.9983, 1.0007, 0.9974,\n",
       "         0.9991, 1.0023, 1.0007, 1.0027, 0.9996, 0.9965, 1.0021, 0.9990, 0.9980,\n",
       "         1.0010, 1.0002, 1.0002, 0.9989, 1.0003, 0.9989, 0.9966, 1.0015, 0.9999,\n",
       "         0.9943, 0.9990, 0.9990, 1.0039, 1.0034, 1.0030, 1.0015, 0.9936, 0.9947,\n",
       "         0.9989, 1.0031, 1.0062, 1.0011, 0.9998, 0.9993, 1.0019, 1.0014, 0.9976,\n",
       "         0.9998, 1.0014, 0.9985, 0.9998, 0.9996, 0.9998, 1.0020, 0.9973, 1.0013,\n",
       "         0.9942, 1.0020, 1.0002, 0.9955, 1.0008, 1.0019, 1.0023, 1.0021, 0.9988,\n",
       "         1.0008, 1.0023, 1.0010, 0.9945, 0.9960, 1.0017, 0.9965, 0.9979, 0.9981,\n",
       "         0.9990, 1.0044, 0.9979, 0.9976, 0.9966, 0.9973, 1.0012, 0.9992, 0.9977,\n",
       "         0.9985, 0.9988, 1.0033, 0.9978, 0.9990, 0.9997, 1.0063, 1.0000, 1.0033,\n",
       "         1.0031, 0.9992, 0.9968, 0.9987, 1.0008, 0.9983, 1.0007, 0.9958, 1.0000,\n",
       "         0.9969, 0.9986, 0.9999, 1.0025, 1.0007, 1.0000, 1.0003, 1.0012, 1.0007,\n",
       "         1.0009, 0.9996, 0.9963, 1.0003, 1.0011, 0.9988, 1.0015, 0.9988, 1.0034,\n",
       "         0.9998, 1.0016, 1.0016, 0.9974, 1.0001, 1.0001, 1.0008, 0.9981, 0.9970,\n",
       "         0.9998, 1.0011, 1.0009, 1.0022, 1.0003, 0.9995, 0.9996, 1.0003, 0.9996,\n",
       "         1.0005, 0.9987, 1.0017, 1.0005, 1.0004, 0.9985, 0.9990, 0.9959, 1.0007,\n",
       "         1.0022, 1.0031, 0.9976, 0.9992, 1.0017, 1.0016, 0.9980, 0.9997, 0.9990,\n",
       "         1.0007, 0.9986, 0.9980, 0.9971, 0.9998, 0.9990, 0.9964, 1.0022, 1.0000,\n",
       "         0.9984, 1.0011, 0.9969, 0.9976, 1.0014, 0.9980, 0.9982, 0.9967, 0.9991,\n",
       "         1.0013, 0.9991, 0.9987, 0.9970, 0.9928, 0.9965, 1.0032, 1.0015, 0.9965,\n",
       "         1.0024, 1.0015, 1.0011, 0.9992, 1.0004, 0.9991, 0.9951, 1.0020, 0.9988,\n",
       "         1.0016, 1.0026, 1.0016, 0.9961, 1.0013, 0.9975, 1.0038, 1.0007, 1.0021,\n",
       "         0.9998, 0.9992, 0.9992, 1.0027, 1.0009, 0.9972, 0.9988, 1.0027, 0.9979,\n",
       "         1.0010, 0.9988, 0.9972, 0.9996, 0.9969, 0.9979, 1.0040, 1.0019, 1.0008,\n",
       "         1.0017, 1.0021, 1.0007, 1.0031, 0.9969, 0.9979, 0.9965, 1.0003, 0.9991,\n",
       "         0.9995, 1.0007, 1.0003, 0.9989, 0.9968, 0.9995, 1.0005, 1.0019, 0.9997,\n",
       "         0.9967, 1.0018, 1.0016, 1.0064, 0.9997, 0.9998, 0.9997, 0.9979, 0.9970,\n",
       "         0.9991, 0.9988, 0.9960, 0.9971, 1.0009, 0.9997, 1.0003, 0.9959, 0.9997,\n",
       "         1.0018, 1.0024, 1.0007, 1.0002, 0.9989, 0.9990, 0.9972, 1.0004, 0.9990,\n",
       "         0.9996, 1.0000, 0.9991, 0.9983, 0.9988, 0.9998, 1.0000, 0.9981, 0.9992,\n",
       "         0.9982, 0.9938, 1.0030, 1.0004, 1.0018, 1.0008, 1.0002, 0.9995, 0.9982,\n",
       "         1.0010, 0.9981, 0.9983, 0.9999, 1.0007, 0.9994, 1.0013, 0.9983, 1.0017,\n",
       "         0.9995, 0.9984, 0.9989, 0.9996, 1.0024, 1.0005, 0.9977, 0.9989, 0.9986,\n",
       "         1.0000, 0.9984, 0.9997, 1.0001, 0.9977, 0.9990, 1.0010, 1.0029, 0.9989,\n",
       "         0.9989, 1.0000, 1.0013, 1.0031, 1.0035, 0.9986, 1.0008, 0.9996, 1.0018,\n",
       "         1.0029, 0.9978, 0.9975, 1.0028, 0.9968, 0.9942, 1.0012, 1.0021, 0.9979,\n",
       "         0.9992, 1.0021, 1.0006, 0.9972, 0.9964, 1.0010, 0.9990, 1.0006, 1.0019,\n",
       "         1.0023, 0.9983, 1.0009, 0.9951, 1.0022, 0.9991, 1.0020, 0.9999, 0.9978,\n",
       "         1.0001, 1.0024, 1.0000, 0.9970, 1.0016, 1.0021, 0.9985, 1.0009, 0.9977,\n",
       "         1.0026, 0.9986, 0.9964, 1.0017, 0.9962, 1.0022, 0.9997, 1.0013, 0.9975,\n",
       "         0.9969, 1.0008, 1.0020, 1.0001, 1.0014, 1.0025, 1.0013, 1.0002, 1.0001,\n",
       "         1.0000, 0.9961, 0.9968, 0.9983, 0.9984, 1.0001, 1.0004, 0.9998, 1.0067,\n",
       "         1.0003, 0.9988, 0.9970, 1.0001, 0.9969, 1.0000, 0.9999, 0.9955, 0.9964,\n",
       "         1.0011, 1.0041, 0.9986, 0.9995, 0.9993, 1.0034, 0.9988, 0.9992]),\n",
       " 'transformer.resblocks.10.ln_2.bias': tensor([ 9.4319e-04, -1.1011e-03, -4.1306e-04,  9.0315e-04,  5.1397e-04,\n",
       "          1.6224e-03,  1.1033e-03, -2.5706e-03,  2.3567e-03, -1.9270e-03,\n",
       "          1.6880e-03,  1.9130e-03, -8.3972e-04,  2.8846e-04,  3.7829e-03,\n",
       "         -4.7556e-03,  5.1423e-04, -2.3822e-03,  1.6426e-03,  2.5589e-03,\n",
       "          1.0164e-03,  2.9103e-03,  1.1049e-04, -2.5546e-03,  1.7884e-03,\n",
       "          1.9986e-03,  2.3278e-03, -1.3856e-05,  1.6401e-03,  1.0990e-03,\n",
       "         -3.0430e-03,  1.7285e-03, -1.3804e-03,  2.1369e-03,  1.5128e-03,\n",
       "         -1.2321e-03,  6.0889e-04,  2.4306e-04,  9.4199e-04,  1.3706e-03,\n",
       "         -2.0006e-03,  6.9366e-04,  7.5318e-04, -2.6593e-03,  1.4915e-03,\n",
       "         -7.3043e-04,  2.2133e-03,  2.4686e-03, -5.1234e-04, -1.4904e-03,\n",
       "          7.6917e-04, -2.8154e-03,  9.2745e-04,  1.2960e-04, -2.2757e-03,\n",
       "         -8.3793e-04,  2.1103e-03, -1.2578e-03,  2.3910e-03,  1.1606e-03,\n",
       "          1.8085e-03,  5.9870e-04, -2.2929e-03, -8.7712e-05, -4.7409e-04,\n",
       "          2.5759e-03,  2.1514e-04,  7.2536e-04,  1.4384e-03,  3.0130e-03,\n",
       "          7.4082e-04,  2.3501e-03,  2.7633e-03,  6.3077e-04,  6.7286e-04,\n",
       "          9.5403e-05,  1.4537e-03,  3.1300e-03,  1.2908e-03, -2.2608e-03,\n",
       "         -4.8473e-04,  1.7159e-03, -2.2875e-03, -3.9770e-03, -1.5632e-03,\n",
       "          1.4846e-03,  3.1487e-05,  9.3596e-04, -2.4216e-03,  8.4297e-04,\n",
       "         -8.6678e-04, -8.1240e-05,  8.7737e-04,  2.6092e-03, -1.4930e-05,\n",
       "          2.8733e-03, -6.7679e-04,  1.4449e-04, -5.5102e-04, -2.0507e-03,\n",
       "         -1.9133e-03,  1.0345e-03, -3.0641e-03,  2.0598e-03, -1.3047e-03,\n",
       "          2.5320e-03,  2.3534e-03,  9.5969e-04, -1.8242e-03,  9.6357e-04,\n",
       "         -6.1576e-04, -1.3145e-03, -1.5649e-03,  1.1963e-03,  1.2828e-03,\n",
       "          1.6899e-03,  1.2432e-04,  1.2478e-03,  4.2290e-03,  2.3870e-03,\n",
       "         -8.1029e-04,  2.6917e-03, -3.4236e-03,  1.3119e-03,  1.3831e-03,\n",
       "          1.2375e-03,  1.9813e-03, -1.7459e-03, -5.2285e-04,  1.2376e-03,\n",
       "          1.3399e-03, -2.7195e-03,  9.0275e-04, -5.1896e-04,  3.9006e-04,\n",
       "          1.7420e-03,  9.9844e-04, -6.8844e-04, -1.0172e-04, -1.8414e-04,\n",
       "         -2.1149e-03, -4.4232e-03, -2.5098e-03,  7.0122e-04, -3.8272e-03,\n",
       "          2.1732e-04,  2.1587e-03,  1.4058e-04, -1.9675e-03,  2.2764e-03,\n",
       "          2.3104e-03, -4.0913e-03, -5.5931e-03, -8.0706e-04,  1.2348e-03,\n",
       "         -1.5060e-03, -1.3480e-03,  5.7410e-04,  3.1701e-04, -2.4922e-03,\n",
       "          8.4266e-04, -9.6577e-04, -1.6108e-03, -5.2010e-04,  2.6752e-03,\n",
       "          1.4674e-03,  2.2184e-04, -1.6611e-03,  2.0249e-03,  4.3062e-03,\n",
       "          4.6748e-04,  6.5753e-04,  1.3322e-03, -3.8697e-03, -2.9013e-03,\n",
       "         -7.0262e-04,  6.7708e-05, -3.7760e-04, -1.0789e-03,  2.7997e-03,\n",
       "          3.7861e-04, -9.2202e-04, -5.6221e-04,  7.3973e-03, -1.7114e-03,\n",
       "          1.2686e-03, -1.9088e-03, -1.1481e-03, -9.1188e-04, -7.5835e-04,\n",
       "         -1.0263e-03, -1.0020e-03,  2.7631e-04, -2.8430e-03,  1.3588e-03,\n",
       "          1.1316e-03, -1.8161e-03, -1.5721e-03,  4.6034e-04, -3.2353e-04,\n",
       "          7.0729e-04,  3.8722e-03, -2.5803e-03, -9.0362e-04, -3.8067e-04,\n",
       "         -6.5643e-04, -2.3429e-03, -2.8433e-03, -3.8945e-03,  9.1724e-04,\n",
       "         -3.2433e-03,  1.4864e-03,  7.8332e-04,  1.2301e-03,  3.9174e-03,\n",
       "          2.6310e-03,  1.5239e-03, -1.7815e-03,  2.7031e-03, -5.5441e-04,\n",
       "          3.6558e-03, -1.1062e-03, -1.9269e-03,  4.6353e-04, -1.9774e-03,\n",
       "         -3.5088e-03,  9.3537e-04,  4.2008e-03, -1.1224e-03,  5.8956e-04,\n",
       "          2.4905e-03, -7.0181e-04, -1.3637e-03, -1.0644e-03,  2.9768e-04,\n",
       "         -2.5690e-03,  1.9697e-03,  3.2834e-03, -3.7126e-03, -1.7376e-03,\n",
       "          9.3684e-04, -1.3938e-03, -1.1617e-03,  8.3585e-04,  1.3097e-03,\n",
       "          1.8601e-04,  4.2044e-04, -1.4912e-03,  1.5417e-03, -3.8432e-04,\n",
       "         -1.9028e-03,  1.4216e-03, -1.2027e-03,  5.6731e-04,  5.6723e-04,\n",
       "          8.3806e-04,  2.2599e-04,  1.2043e-03, -3.3171e-04,  4.5038e-03,\n",
       "          1.5668e-03,  9.1970e-04,  8.7517e-04,  2.1634e-03,  1.9465e-03,\n",
       "          6.1342e-04, -8.9267e-04, -2.8696e-04,  6.4223e-04,  6.4734e-04,\n",
       "         -6.2647e-05,  2.5913e-03,  1.1431e-03, -1.6633e-03, -1.5234e-03,\n",
       "          8.5165e-04, -3.3718e-03,  1.5065e-03, -6.1128e-05,  2.3113e-03,\n",
       "         -1.0782e-03, -3.7876e-03,  1.6923e-03,  1.1163e-03,  2.1797e-03,\n",
       "         -1.1780e-04, -1.6110e-03, -5.1391e-04, -5.9931e-04,  1.1511e-03,\n",
       "         -1.4444e-03, -4.3732e-04,  4.5947e-03, -2.3169e-03, -1.8526e-03,\n",
       "         -1.7006e-03,  2.1757e-03, -1.3071e-03, -5.6087e-04, -1.5078e-03,\n",
       "          8.7375e-04, -6.4494e-04,  8.9942e-04,  2.4799e-03, -6.7486e-04,\n",
       "          5.7266e-05, -6.3590e-04, -9.8441e-04,  1.3653e-04,  3.5593e-03,\n",
       "         -2.5386e-03, -7.7998e-04, -2.8090e-03,  1.3513e-04, -1.6909e-03,\n",
       "          1.0038e-03, -4.1958e-04, -2.6717e-03,  5.3584e-04, -1.3405e-03,\n",
       "         -1.9035e-05,  1.9492e-03, -2.1654e-03, -3.1816e-03,  6.9859e-04,\n",
       "          1.2455e-03, -2.3734e-03,  2.0021e-03,  3.3205e-03,  7.8482e-04,\n",
       "          1.6673e-03, -1.3600e-03,  2.9760e-03,  1.5648e-03,  2.1863e-03,\n",
       "         -1.5059e-04, -4.3845e-05,  2.9503e-04, -3.2271e-03, -4.5751e-03,\n",
       "         -3.6879e-04, -3.0963e-03, -1.9033e-03, -4.0339e-04,  8.1788e-04,\n",
       "          4.0474e-04, -1.8558e-03,  2.0896e-04,  3.0813e-03, -9.0667e-04,\n",
       "          7.6153e-05,  3.0224e-03, -1.3300e-03, -1.1595e-03,  1.6639e-03,\n",
       "         -3.9266e-04, -1.2197e-03,  1.6682e-04, -1.2233e-03,  3.1094e-03,\n",
       "          1.2583e-03, -1.2810e-04, -1.8465e-03, -1.1580e-03,  2.7607e-03,\n",
       "          5.8715e-04,  9.4030e-05,  2.7139e-03, -8.8838e-04,  1.4289e-03,\n",
       "         -8.5031e-04, -1.7320e-03, -2.6036e-04,  2.5164e-04,  2.5235e-03,\n",
       "         -1.1820e-03,  8.8695e-04, -6.3877e-04,  7.5874e-04,  2.8799e-04,\n",
       "         -1.6275e-03, -7.4891e-04,  1.2983e-04,  5.7566e-04, -3.1519e-04,\n",
       "          3.1056e-03, -9.3240e-04, -2.7687e-03, -5.7927e-03, -1.7165e-04,\n",
       "          4.5648e-04,  1.6804e-03, -1.9442e-03,  2.4028e-03, -3.7522e-03,\n",
       "         -2.5657e-03,  6.1634e-04, -7.5359e-04,  2.3811e-03, -2.9739e-03,\n",
       "         -1.3373e-03,  2.7964e-03,  2.2074e-04,  6.3440e-05, -1.1358e-03,\n",
       "         -5.7543e-04, -1.8763e-03, -1.6960e-03, -5.7034e-04,  1.4205e-03,\n",
       "         -6.5679e-04,  2.4580e-04, -2.8152e-03, -2.8093e-03, -2.4708e-03,\n",
       "          1.2991e-03, -2.6179e-03, -1.8648e-03,  1.6833e-03, -6.0705e-04,\n",
       "          2.8087e-03, -5.3100e-04, -7.5612e-04,  2.9216e-04, -3.1981e-04,\n",
       "          1.0401e-03,  7.3690e-04,  8.6590e-04,  8.3784e-06, -3.3442e-04,\n",
       "         -9.0430e-06, -2.3700e-03,  4.5447e-03, -1.1232e-03,  1.2121e-03,\n",
       "         -1.5161e-04, -3.5837e-03,  5.8128e-03,  6.3078e-04, -8.5300e-04,\n",
       "          1.5564e-03, -3.9702e-04, -1.4768e-03,  1.7114e-03,  2.9502e-03,\n",
       "          3.2885e-03, -7.4400e-04,  1.6480e-03,  1.0266e-03, -1.2263e-04,\n",
       "         -1.1216e-03, -6.0948e-04,  1.2585e-04, -4.4200e-03,  2.1197e-03,\n",
       "          1.4033e-03, -1.6150e-03, -1.3564e-03, -6.1141e-04,  3.7962e-04,\n",
       "          1.1217e-03, -1.4570e-04, -1.6096e-04,  2.2367e-04,  2.1000e-03,\n",
       "          4.6327e-04, -4.9778e-04, -4.9325e-04, -2.9351e-04,  1.2688e-03,\n",
       "          3.0356e-03,  2.3364e-03,  4.7003e-03,  1.7233e-04,  1.5557e-03,\n",
       "          6.8073e-04,  1.7653e-03,  1.7757e-03,  9.7133e-04, -1.3475e-03,\n",
       "         -1.7969e-04, -1.1925e-03, -8.1685e-04,  4.9904e-04, -5.2342e-04,\n",
       "         -1.8908e-03,  1.1778e-03,  3.2716e-03, -1.1979e-03, -2.3539e-03,\n",
       "         -1.1480e-04,  1.7577e-03,  5.4853e-04,  1.8714e-03,  4.4883e-04,\n",
       "          2.1141e-03, -1.7361e-03,  2.8116e-03,  9.1313e-04,  9.7210e-04,\n",
       "          3.1038e-03,  1.6608e-03,  7.6906e-04, -3.7036e-03, -3.0977e-04,\n",
       "         -1.3317e-03,  4.3571e-04,  8.0508e-06, -1.5553e-03, -8.0996e-04,\n",
       "          3.5751e-04, -2.5971e-03]),\n",
       " 'transformer.resblocks.11.attn.in_proj_weight': tensor([[ 0.0498,  0.0044,  0.0030,  ..., -0.0054,  0.0678,  0.0146],\n",
       "         [ 0.0186, -0.0386,  0.0581,  ...,  0.0236,  0.0419,  0.0042],\n",
       "         [-0.0175, -0.0078, -0.0464,  ...,  0.0154, -0.0101,  0.0857],\n",
       "         ...,\n",
       "         [-0.0490, -0.0939, -0.0157,  ...,  0.0141,  0.0372,  0.0527],\n",
       "         [-0.0021, -0.0304, -0.0332,  ..., -0.0220, -0.0558, -0.0110],\n",
       "         [ 0.0596,  0.0105, -0.0014,  ...,  0.0536, -0.0678, -0.1278]]),\n",
       " 'transformer.resblocks.11.attn.in_proj_bias': tensor([-0.0043, -0.0037, -0.0013,  ...,  0.0005, -0.0005, -0.0006]),\n",
       " 'transformer.resblocks.11.attn.out_proj.weight': tensor([[-0.0005,  0.0041,  0.0013,  ..., -0.0054,  0.0123, -0.0028],\n",
       "         [ 0.0017, -0.0057,  0.0052,  ...,  0.0033,  0.0056,  0.0015],\n",
       "         [-0.0030, -0.0152, -0.0075,  ...,  0.0141,  0.0019,  0.0021],\n",
       "         ...,\n",
       "         [-0.0125,  0.0020, -0.0121,  ...,  0.0122, -0.0016, -0.0032],\n",
       "         [ 0.0112, -0.0017,  0.0019,  ..., -0.0062, -0.0071, -0.0130],\n",
       "         [-0.0128, -0.0131, -0.0101,  ..., -0.0016, -0.0034, -0.0016]]),\n",
       " 'transformer.resblocks.11.attn.out_proj.bias': tensor([-6.1307e-04, -3.6574e-03,  9.4274e-05, -5.8925e-04, -8.1612e-04,\n",
       "         -2.9101e-03, -2.1117e-04,  2.3305e-03, -3.3000e-03,  2.0599e-03,\n",
       "         -4.6993e-03, -2.8116e-03, -2.1607e-04, -1.7117e-03, -5.7799e-03,\n",
       "          2.6406e-03, -2.8711e-04,  2.2004e-03, -7.9807e-04, -1.0512e-03,\n",
       "          6.9636e-05,  7.1716e-04, -9.0024e-07, -3.9175e-04, -3.6867e-04,\n",
       "         -1.6626e-03, -1.3128e-03, -1.0045e-03, -9.8053e-04, -1.9204e-03,\n",
       "          8.9550e-04, -2.7436e-04,  2.3420e-03, -3.1536e-03, -4.3493e-04,\n",
       "          7.8072e-04,  9.5668e-04,  4.8882e-06,  9.9848e-04, -2.8403e-03,\n",
       "          1.5520e-03,  1.8349e-03,  1.3358e-03,  1.7437e-03, -4.0047e-03,\n",
       "          1.7167e-03, -3.2732e-03,  3.4141e-04, -2.0531e-04,  1.9773e-03,\n",
       "         -6.8713e-04, -2.0607e-04, -1.0366e-03,  2.6564e-03,  3.1000e-03,\n",
       "          9.8657e-04, -1.7684e-03, -8.9718e-04, -3.4195e-04, -1.0984e-03,\n",
       "         -1.9793e-03,  4.2754e-05,  9.6157e-04,  9.3942e-04, -4.1077e-04,\n",
       "         -1.1305e-03,  2.0169e-03,  2.7213e-04, -1.6228e-03,  1.5825e-03,\n",
       "         -4.7582e-04, -4.2402e-04, -2.3761e-03,  4.3416e-04, -8.1769e-04,\n",
       "          1.7573e-03, -1.7906e-03, -2.0992e-03, -2.5998e-03,  2.0429e-03,\n",
       "          1.5524e-03, -2.1359e-03,  6.4895e-04,  1.8840e-03,  2.1646e-03,\n",
       "          1.9097e-04, -6.1629e-04,  3.1447e-04,  4.9333e-04, -2.8256e-03,\n",
       "          2.4081e-04, -4.7996e-04,  1.7428e-03, -2.3840e-03, -1.9953e-03,\n",
       "         -4.6885e-03, -2.9814e-04, -1.2916e-03, -1.9301e-03,  6.8604e-04,\n",
       "          6.9785e-04, -1.6439e-03,  5.4703e-04, -3.4719e-04, -7.4387e-04,\n",
       "         -2.8522e-03, -3.8702e-03, -9.8405e-04,  7.0981e-04, -7.4797e-04,\n",
       "          2.5283e-03,  8.7343e-04,  3.1796e-03,  1.2968e-03, -1.7139e-03,\n",
       "         -5.1082e-03,  1.4771e-03,  1.5002e-03, -3.2085e-03, -1.1394e-03,\n",
       "          4.3702e-04, -2.5204e-03,  2.0422e-03, -9.1192e-04, -1.2953e-03,\n",
       "          8.0362e-04, -1.4289e-03, -2.1629e-03, -5.7332e-04, -6.6584e-05,\n",
       "          4.4601e-05,  3.6214e-03, -3.8837e-04, -2.3573e-03, -5.7275e-04,\n",
       "         -1.2083e-03, -1.2175e-03,  2.5392e-03, -9.8369e-04,  3.7277e-04,\n",
       "          3.8401e-04,  1.8235e-03,  1.1897e-03,  2.9038e-03,  1.7257e-03,\n",
       "          2.6859e-03, -2.9167e-03,  1.4266e-03,  7.7532e-04,  7.6357e-04,\n",
       "          1.6605e-03,  4.8700e-03,  6.7527e-03,  1.8777e-03, -9.5170e-04,\n",
       "          2.4939e-04,  9.0350e-04, -1.9289e-03,  2.2763e-04,  4.6760e-04,\n",
       "         -2.0536e-03, -9.9389e-04,  3.5421e-03,  7.1679e-04, -1.3649e-03,\n",
       "          1.6041e-03,  2.1514e-03, -2.7405e-04, -5.4420e-04, -1.9262e-03,\n",
       "          8.8213e-04, -2.7904e-04, -2.1465e-03,  3.7782e-04,  2.1732e-03,\n",
       "          8.9260e-04, -1.3981e-03,  6.1911e-04,  2.8448e-04, -1.3456e-03,\n",
       "         -2.1119e-03, -5.1913e-04,  1.1659e-03, -3.5745e-03,  5.0017e-03,\n",
       "         -6.2118e-04,  2.9809e-04, -2.0274e-04,  1.3475e-03, -2.1762e-03,\n",
       "          8.4702e-04,  6.0780e-04, -5.7421e-04,  2.2679e-03, -1.9290e-03,\n",
       "         -1.6629e-03,  1.8586e-03,  2.8865e-03,  4.1193e-03,  3.2702e-03,\n",
       "         -4.9061e-04, -1.5429e-03,  1.7147e-03,  1.7653e-03,  1.1446e-03,\n",
       "          5.2444e-04, -6.6764e-04,  8.6961e-04,  1.5784e-03, -1.0669e-05,\n",
       "          2.7713e-03, -2.7869e-04,  3.5126e-04,  1.6228e-03, -2.0179e-03,\n",
       "         -1.6401e-03, -3.1919e-05, -1.5830e-03, -1.7464e-03,  8.9347e-04,\n",
       "         -7.7297e-04, -4.9292e-04,  1.7415e-03,  2.6150e-03,  1.0812e-03,\n",
       "          8.3922e-04, -1.3055e-03, -3.0227e-03,  1.8230e-03,  1.6116e-03,\n",
       "         -1.6997e-03,  4.8045e-04,  7.4819e-04, -1.5740e-03,  1.4083e-03,\n",
       "          2.4658e-04,  3.0157e-03, -3.6321e-03,  1.8412e-03,  1.7803e-03,\n",
       "         -1.6841e-03,  2.8845e-03,  2.7693e-03,  1.1585e-03, -2.2356e-03,\n",
       "         -1.8962e-04,  1.1696e-03,  4.0823e-04,  1.2095e-04, -1.3910e-03,\n",
       "          1.1776e-03, -2.4523e-03,  6.0162e-05, -1.7733e-03, -6.2943e-04,\n",
       "         -1.0267e-03, -3.3448e-04, -1.5289e-03, -5.7311e-05, -3.3190e-03,\n",
       "          1.2684e-03,  7.9925e-04,  6.4520e-04, -3.4171e-03, -2.0394e-03,\n",
       "          2.6248e-03,  8.8969e-04,  8.5840e-04, -5.5856e-04, -1.4385e-03,\n",
       "          1.4097e-03, -1.0841e-03, -1.8661e-03,  3.6238e-04,  1.2662e-03,\n",
       "         -9.1180e-04,  3.5138e-03, -6.4541e-04,  3.6452e-03, -1.9008e-03,\n",
       "         -3.0750e-04,  3.8222e-03, -2.8715e-03, -1.7870e-03, -2.6644e-03,\n",
       "          1.7335e-03,  2.4592e-04,  1.2406e-03,  1.8864e-03, -5.6365e-05,\n",
       "         -6.1862e-04,  1.3524e-03, -2.0674e-03,  4.5989e-03,  3.3695e-04,\n",
       "          2.2533e-03, -1.6725e-03,  1.2814e-03, -6.5799e-04, -1.1115e-03,\n",
       "         -2.5908e-03,  4.1151e-04,  1.6967e-03, -4.2660e-03,  1.7582e-04,\n",
       "         -2.2874e-03,  1.0142e-03,  2.4880e-04, -9.8649e-04, -2.9773e-03,\n",
       "          2.4090e-03,  3.0693e-03, -7.5058e-04, -1.4768e-03,  3.4140e-04,\n",
       "          2.6190e-03,  2.6788e-03,  1.9331e-03,  7.4008e-04,  9.1520e-04,\n",
       "          3.2885e-03, -9.7735e-04, -4.8932e-04,  3.3945e-04, -2.5895e-03,\n",
       "          1.0235e-04,  3.7346e-03, -2.9834e-04, -3.9280e-03, -1.3054e-03,\n",
       "         -6.8013e-04,  1.6286e-03,  1.6212e-04, -3.6418e-04,  1.3549e-03,\n",
       "          8.9221e-04,  9.3825e-05, -8.1893e-04,  1.1338e-03,  2.1637e-03,\n",
       "         -3.5909e-03,  3.6835e-03,  1.7461e-05, -8.0283e-05, -2.8630e-03,\n",
       "         -2.6147e-04, -2.3816e-03,  5.0801e-05, -1.2357e-03,  1.3713e-03,\n",
       "         -1.9173e-03, -4.1687e-03, -1.1991e-03,  1.2907e-04,  2.6217e-04,\n",
       "          1.9723e-04,  3.2069e-03,  2.6860e-03,  1.0786e-03, -2.5750e-03,\n",
       "          1.7140e-03, -3.9003e-04,  2.9720e-03,  3.0807e-03, -9.0390e-04,\n",
       "          2.4975e-03, -4.7610e-04, -5.2628e-03,  1.7836e-03, -1.0939e-03,\n",
       "         -7.3550e-04,  1.3498e-03,  6.3755e-04, -5.5248e-04, -8.1054e-04,\n",
       "          3.4884e-03, -9.9264e-04,  2.6401e-03, -1.5001e-03, -2.5614e-03,\n",
       "          2.3819e-03,  1.5915e-03, -1.3568e-03, -5.2693e-04, -1.1756e-03,\n",
       "         -1.3128e-03,  2.9693e-03, -1.2633e-03,  4.2119e-03,  1.0854e-03,\n",
       "          7.4133e-04, -2.9262e-04, -5.0562e-04,  6.7299e-04,  3.4650e-03,\n",
       "          2.3092e-03, -1.2751e-03,  2.1182e-03,  3.2620e-04,  7.9269e-04,\n",
       "         -3.6169e-05, -1.9447e-03, -3.3942e-04, -3.3115e-04,  3.8719e-04,\n",
       "          1.5565e-03,  1.0319e-03,  1.3472e-03,  1.0527e-03,  1.9855e-03,\n",
       "         -7.3544e-04, -3.0130e-03,  1.0354e-03,  2.8018e-03,  7.7776e-04,\n",
       "         -2.5087e-03,  2.5314e-03,  2.4331e-03, -6.8949e-04,  2.3592e-03,\n",
       "         -9.0878e-04,  1.3038e-03,  2.2246e-03, -2.2909e-03, -1.5993e-03,\n",
       "          6.6656e-04, -4.4202e-04, -1.0714e-03,  4.6306e-04,  3.8703e-04,\n",
       "         -1.7569e-03,  2.1925e-03,  1.4458e-03, -5.7107e-04, -3.3667e-03,\n",
       "         -8.2389e-04,  2.0416e-03, -3.8814e-03,  1.3675e-03, -1.0583e-03,\n",
       "         -1.3533e-03,  7.9757e-04,  1.0114e-03, -3.5379e-04, -2.1520e-03,\n",
       "         -3.0899e-03, -2.8630e-04,  1.4022e-03, -3.7177e-04,  1.4066e-03,\n",
       "          1.6040e-04,  8.9915e-04, -9.9204e-04,  4.7907e-03,  1.0108e-03,\n",
       "         -2.3068e-03,  3.4026e-04,  2.2319e-03,  1.8390e-03, -3.3234e-05,\n",
       "          6.6459e-04, -9.2274e-04,  1.0256e-03,  3.5714e-03,  5.9844e-05,\n",
       "         -8.2478e-05,  1.2418e-04,  1.5298e-03, -2.4991e-04, -2.2609e-03,\n",
       "         -2.2773e-03,  1.0898e-03, -2.5370e-03, -1.6571e-03, -1.8880e-03,\n",
       "          4.6699e-04, -2.7852e-03, -1.4314e-03, -1.9110e-03, -1.0312e-03,\n",
       "         -2.9944e-03, -2.2199e-03,  9.3676e-04,  4.7950e-04, -1.0095e-03,\n",
       "          1.5182e-03, -1.4212e-03, -2.0956e-03,  2.3985e-03,  1.4522e-03,\n",
       "          8.1667e-04, -1.3526e-03,  6.8264e-04, -2.2825e-03, -1.9343e-03,\n",
       "          3.2226e-04,  1.4901e-03, -8.0225e-04,  5.7267e-04, -3.1164e-03,\n",
       "         -2.2821e-03,  1.1399e-03, -1.4882e-03,  3.8727e-03, -2.9056e-03,\n",
       "          1.4199e-03, -1.6977e-03, -7.9910e-04,  9.9063e-04, -8.0954e-04,\n",
       "         -1.5874e-03, -1.4881e-05]),\n",
       " 'transformer.resblocks.11.ln_1.weight': tensor([0.9967, 0.9996, 0.9994, 0.9988, 0.9988, 0.9989, 0.9999, 0.9984, 0.9961,\n",
       "         0.9982, 0.9997, 0.9957, 0.9996, 0.9968, 0.9984, 0.9990, 0.9980, 0.9986,\n",
       "         1.0017, 1.0002, 0.9977, 1.0013, 1.0028, 0.9979, 0.9998, 0.9983, 0.9999,\n",
       "         1.0004, 1.0000, 0.9978, 0.9972, 1.0000, 1.0000, 1.0012, 1.0004, 1.0018,\n",
       "         1.0014, 0.9973, 1.0006, 0.9981, 0.9994, 1.0016, 0.9998, 0.9968, 0.9999,\n",
       "         1.0010, 1.0017, 1.0002, 0.9965, 0.9995, 1.0000, 0.9983, 1.0006, 0.9972,\n",
       "         0.9987, 1.0008, 0.9970, 1.0006, 0.9996, 1.0013, 0.9983, 0.9997, 0.9991,\n",
       "         0.9984, 0.9995, 0.9997, 0.9983, 1.0013, 0.9973, 1.0008, 0.9985, 1.0017,\n",
       "         0.9987, 1.0000, 1.0009, 0.9994, 0.9969, 1.0013, 0.9982, 1.0007, 0.9982,\n",
       "         1.0010, 0.9981, 1.0026, 1.0001, 0.9994, 0.9991, 1.0001, 0.9995, 0.9983,\n",
       "         1.0017, 0.9990, 0.9983, 0.9997, 1.0001, 0.9968, 1.0020, 0.9996, 0.9990,\n",
       "         0.9990, 1.0000, 0.9980, 0.9978, 0.9993, 0.9992, 0.9987, 0.9965, 1.0008,\n",
       "         1.0002, 0.9962, 1.0007, 1.0007, 0.9988, 0.9982, 1.0007, 0.9982, 0.9997,\n",
       "         0.9975, 0.9982, 0.9974, 1.0003, 0.9998, 1.0007, 0.9999, 1.0008, 1.0001,\n",
       "         0.9987, 0.9973, 1.0005, 0.9989, 0.9974, 0.9988, 0.9948, 1.0011, 0.9990,\n",
       "         0.9988, 1.0001, 0.9998, 0.9990, 1.0003, 1.0004, 0.9987, 0.9996, 0.9999,\n",
       "         1.0013, 0.9960, 0.9959, 0.9990, 0.9967, 0.9998, 0.9980, 1.0012, 0.9979,\n",
       "         0.9982, 1.0008, 0.9994, 0.9982, 1.0019, 1.0004, 1.0001, 0.9982, 0.9974,\n",
       "         0.9969, 0.9966, 0.9994, 0.9970, 0.9983, 0.9970, 0.9977, 0.9983, 0.9997,\n",
       "         0.9981, 1.0015, 0.9980, 0.9992, 1.0021, 1.0016, 0.9993, 1.0018, 0.9980,\n",
       "         1.0001, 0.9977, 1.0000, 0.9975, 0.9996, 0.9995, 0.9991, 0.9996, 1.0005,\n",
       "         1.0013, 1.0057, 0.9985, 0.9957, 0.9991, 1.0018, 0.9991, 0.9992, 0.9970,\n",
       "         0.9991, 0.9986, 1.0009, 1.0025, 0.9968, 0.9993, 0.9997, 0.9969, 1.0003,\n",
       "         1.0003, 1.0001, 0.9998, 0.9983, 0.9969, 1.0014, 0.9999, 0.9981, 0.9997,\n",
       "         0.9983, 0.9998, 0.9970, 0.9980, 0.9992, 1.0007, 0.9996, 1.0001, 0.9962,\n",
       "         1.0005, 0.9994, 0.9989, 0.9967, 0.9990, 0.9964, 0.9990, 0.9993, 0.9980,\n",
       "         1.0025, 0.9971, 0.9979, 1.0009, 1.0008, 0.9990, 0.9983, 0.9992, 1.0006,\n",
       "         0.9988, 0.9982, 1.0020, 0.9982, 0.9995, 1.0014, 0.9983, 0.9987, 1.0007,\n",
       "         0.9982, 0.9996, 0.9985, 1.0006, 0.9988, 0.9976, 1.0007, 0.9998, 1.0016,\n",
       "         0.9976, 1.0019, 0.9988, 0.9990, 0.9988, 0.9972, 1.0032, 1.0026, 1.0001,\n",
       "         1.0000, 0.9990, 0.9999, 0.9985, 1.0002, 0.9989, 0.9977, 1.0012, 0.9995,\n",
       "         0.9988, 0.9985, 1.0015, 0.9959, 0.9974, 0.9990, 0.9974, 0.9985, 0.9975,\n",
       "         0.9996, 0.9990, 0.9980, 1.0004, 0.9980, 0.9982, 1.0018, 1.0013, 0.9987,\n",
       "         0.9986, 0.9996, 1.0005, 0.9994, 1.0007, 0.9997, 0.9965, 1.0023, 1.0003,\n",
       "         0.9967, 1.0003, 0.9982, 0.9983, 0.9988, 0.9975, 1.0014, 1.0011, 1.0012,\n",
       "         0.9980, 0.9990, 0.9967, 0.9981, 0.9999, 0.9975, 0.9994, 0.9976, 0.9980,\n",
       "         0.9961, 0.9996, 1.0000, 0.9962, 0.9975, 1.0021, 1.0008, 0.9962, 0.9991,\n",
       "         1.0023, 0.9988, 0.9993, 0.9964, 0.9997, 0.9990, 1.0000, 0.9957, 0.9982,\n",
       "         0.9997, 1.0014, 1.0004, 0.9979, 0.9993, 1.0014, 0.9992, 0.9997, 0.9991,\n",
       "         0.9986, 0.9994, 1.0020, 0.9994, 0.9989, 0.9979, 0.9976, 1.0023, 0.9978,\n",
       "         0.9996, 0.9996, 1.0022, 0.9973, 1.0007, 0.9990, 1.0020, 0.9987, 0.9983,\n",
       "         0.9997, 0.9992, 0.9994, 0.9995, 0.9963, 1.0010, 1.0003, 1.0018, 1.0003,\n",
       "         0.9997, 0.9982, 0.9986, 0.9994, 0.9999, 0.9992, 0.9987, 0.9973, 0.9980,\n",
       "         0.9981, 0.9982, 0.9996, 0.9986, 0.9961, 1.0013, 0.9971, 0.9976, 0.9984,\n",
       "         0.9983, 0.9997, 0.9988, 1.0041, 0.9962, 0.9984, 1.0011, 0.9972, 0.9989,\n",
       "         0.9996, 0.9967, 0.9991, 1.0004, 0.9963, 1.0016, 1.0001, 0.9972, 1.0030,\n",
       "         0.9991, 1.0021, 1.0019, 0.9975, 1.0002, 0.9978, 0.9978, 0.9958, 1.0000,\n",
       "         0.9998, 0.9995, 1.0012, 0.9993, 0.9973, 1.0010, 0.9991, 0.9992, 0.9988,\n",
       "         1.0011, 0.9990, 1.0004, 0.9977, 1.0011, 0.9972, 1.0005, 1.0011, 1.0008,\n",
       "         0.9978, 1.0015, 1.0001, 0.9994, 0.9978, 0.9960, 0.9995, 1.0009, 0.9993,\n",
       "         0.9992, 0.9993, 1.0019, 1.0004, 0.9984, 0.9986, 1.0011, 0.9994, 1.0007,\n",
       "         0.9996, 0.9992, 0.9996, 0.9986, 0.9995, 0.9984, 0.9982, 0.9979, 0.9965,\n",
       "         0.9973, 1.0006, 0.9969, 0.9994, 0.9992, 1.0011, 0.9983, 1.0004, 0.9990,\n",
       "         1.0015, 0.9962, 0.9981, 0.9994, 0.9987, 1.0011, 0.9979, 0.9992, 1.0012,\n",
       "         1.0021, 0.9990, 0.9989, 0.9986, 0.9970, 0.9989, 0.9993, 0.9989, 0.9979,\n",
       "         0.9990, 0.9996, 0.9976, 0.9991, 1.0014, 0.9974, 0.9980, 0.9985, 1.0002,\n",
       "         1.0007, 0.9992, 0.9990, 1.0012, 0.9974, 1.0007, 0.9970, 0.9988]),\n",
       " 'transformer.resblocks.11.ln_1.bias': tensor([ 8.5350e-04, -4.0553e-04, -2.3098e-03,  1.4118e-03, -1.2114e-03,\n",
       "          6.1052e-04,  1.0598e-03, -5.2449e-04,  3.6446e-03,  7.8534e-04,\n",
       "          2.3566e-04,  9.7907e-04, -1.4788e-03,  1.9354e-03,  1.0784e-03,\n",
       "          7.0782e-05,  2.5819e-04,  6.9128e-05, -1.5798e-03,  8.4279e-04,\n",
       "         -1.9047e-03, -8.7463e-04,  3.4064e-03,  2.0428e-03, -1.2467e-03,\n",
       "          8.4808e-04,  4.3806e-04, -1.2455e-03,  9.1996e-04,  1.4874e-03,\n",
       "         -5.4677e-04,  1.0875e-03,  1.9447e-03, -2.0532e-03,  1.5436e-03,\n",
       "          2.6085e-03,  1.1067e-03, -2.8558e-03,  5.1450e-04,  9.6653e-04,\n",
       "          1.0061e-03,  3.3079e-04,  1.9685e-04, -2.5151e-03, -8.2159e-05,\n",
       "         -4.8368e-05, -1.7576e-03,  9.7286e-04,  7.7674e-04, -6.3242e-04,\n",
       "         -1.8707e-04,  1.2040e-03,  1.9887e-04, -1.3321e-03, -1.1068e-03,\n",
       "          1.2865e-06,  1.1207e-03,  7.9685e-04, -5.4571e-04,  1.4464e-03,\n",
       "          2.0661e-04,  1.5943e-03, -4.9476e-04, -1.9084e-03,  3.9773e-03,\n",
       "         -1.2024e-03,  9.3883e-04, -2.0249e-03,  1.2405e-03,  7.1458e-04,\n",
       "         -9.0703e-04, -1.1533e-03,  6.7367e-04, -1.5869e-03, -7.5450e-04,\n",
       "          2.4092e-03,  1.4089e-03, -8.2965e-04,  2.6894e-03,  1.9521e-03,\n",
       "         -3.1120e-04,  1.6551e-03,  2.7230e-03,  1.9527e-03, -1.0323e-03,\n",
       "          4.3283e-04,  3.1833e-06,  7.9087e-04, -8.6369e-04,  9.2222e-04,\n",
       "          1.0681e-03,  1.7933e-03, -8.9983e-04,  6.0008e-04,  1.3808e-04,\n",
       "          1.8505e-03,  2.4858e-03,  7.8816e-04,  9.1776e-04,  1.3450e-03,\n",
       "          5.6509e-04, -2.0841e-04,  8.5804e-05,  1.4412e-03,  2.8642e-03,\n",
       "          1.4927e-03,  3.3789e-03,  1.6608e-03,  2.1093e-03, -7.9570e-04,\n",
       "          5.8940e-04, -1.1315e-03, -7.3942e-05, -2.8622e-03, -5.2850e-04,\n",
       "          1.9310e-04,  8.2419e-04,  1.3283e-03,  6.3459e-04,  1.5388e-03,\n",
       "          1.3810e-03,  1.5871e-03, -4.7149e-04,  2.8788e-04, -1.5563e-03,\n",
       "          1.1302e-03, -1.0162e-03,  6.8705e-04, -1.4291e-03, -9.0635e-05,\n",
       "         -7.1465e-04, -4.1854e-04, -1.0732e-03,  1.5604e-03, -5.3325e-04,\n",
       "          7.2316e-04, -1.1681e-03,  1.0952e-03, -1.0318e-03,  5.9614e-04,\n",
       "          6.9886e-04,  8.7893e-05,  1.6467e-03,  8.7703e-05,  8.5345e-04,\n",
       "         -3.4485e-03,  2.9051e-03, -3.5393e-03,  3.1243e-04,  1.3796e-03,\n",
       "         -8.9630e-04,  2.2099e-03, -1.5523e-03,  6.4597e-04,  1.8260e-03,\n",
       "         -2.2803e-03, -8.8748e-04,  1.4860e-04,  6.2053e-04,  4.3220e-05,\n",
       "         -1.2215e-04,  2.8351e-05, -2.9477e-03, -2.4923e-03, -3.0766e-03,\n",
       "         -2.2842e-03, -1.2200e-03,  3.5565e-03,  1.3862e-03,  3.9823e-04,\n",
       "          3.9893e-04,  2.1956e-03, -8.7553e-04, -1.6220e-03, -1.5367e-03,\n",
       "          1.4003e-03, -1.4386e-03, -9.7413e-04, -3.0908e-03,  4.9918e-04,\n",
       "          7.0266e-05, -1.1899e-03,  6.6532e-04,  2.0455e-03,  4.2514e-04,\n",
       "          1.3639e-03,  4.5049e-03, -2.4037e-03,  1.7996e-03, -1.7106e-03,\n",
       "         -1.1898e-03, -5.1154e-04, -3.2597e-03, -1.0336e-03, -5.4293e-04,\n",
       "          3.0892e-04, -7.6051e-04, -2.0390e-03, -1.1565e-05, -7.0700e-04,\n",
       "         -8.9150e-04, -2.1264e-03, -3.9420e-03, -8.1553e-04, -2.4629e-04,\n",
       "         -2.5589e-03, -7.1153e-04, -1.4290e-03, -2.1489e-04,  2.5039e-03,\n",
       "         -9.2830e-04,  3.9339e-03,  2.8050e-04,  1.0175e-03, -6.8391e-05,\n",
       "          9.8461e-04,  2.2965e-05, -5.1491e-04,  3.1527e-04, -1.3711e-03,\n",
       "         -4.4604e-04, -3.9419e-04, -5.0820e-04, -2.8997e-04, -2.9449e-03,\n",
       "         -2.0211e-04,  1.0029e-03,  1.3983e-03, -4.9013e-04,  7.5602e-04,\n",
       "          1.6147e-03, -1.0182e-03,  6.2044e-05,  2.2145e-03, -2.8578e-04,\n",
       "          1.3985e-03, -1.0562e-03, -2.0809e-04,  6.0030e-04, -2.5673e-03,\n",
       "          4.7650e-04, -8.3326e-04,  1.7524e-03,  4.0987e-05,  2.3987e-03,\n",
       "          2.1092e-03, -4.2250e-03, -5.3865e-04, -2.1563e-03, -1.4434e-04,\n",
       "          9.5235e-06, -1.9976e-03, -2.0843e-04,  2.1368e-03,  6.0015e-04,\n",
       "          4.3650e-04,  2.1439e-03,  6.3076e-04,  1.2233e-03, -4.9349e-04,\n",
       "          2.8168e-03, -3.2836e-03,  1.5567e-03,  7.9935e-04,  3.1545e-04,\n",
       "         -9.7512e-04,  8.2765e-04,  2.0575e-03,  1.4290e-03, -2.1692e-03,\n",
       "         -1.5188e-03, -1.2204e-03, -1.2461e-03,  2.7568e-03,  8.0376e-04,\n",
       "          1.1905e-03, -7.3522e-04, -1.4594e-03,  2.4971e-04,  1.1067e-03,\n",
       "          2.1823e-03,  5.1672e-04,  1.3004e-03,  7.3124e-05,  1.2255e-03,\n",
       "         -2.1425e-03, -1.6807e-03, -2.1490e-03, -6.6701e-04, -5.4254e-04,\n",
       "          7.7487e-04,  7.7949e-04,  6.3522e-04, -4.7054e-05,  2.2825e-03,\n",
       "         -2.2354e-03,  1.8095e-04,  1.0890e-05,  1.5139e-03,  4.1823e-04,\n",
       "         -1.6198e-04,  9.1554e-04, -2.2295e-03,  2.2842e-03, -8.3136e-04,\n",
       "         -1.6444e-03, -1.7380e-03,  6.9269e-04,  2.5997e-04,  8.4464e-04,\n",
       "         -3.6355e-03, -2.4790e-03, -5.6614e-04, -2.8343e-03,  1.3720e-03,\n",
       "         -2.0064e-03, -1.4523e-04, -2.0559e-03,  1.9553e-04,  7.9707e-04,\n",
       "         -1.6817e-03, -1.4618e-04,  2.3730e-03, -1.2682e-03,  1.3473e-03,\n",
       "          2.3565e-03,  1.0364e-03, -3.8208e-04,  2.1532e-03, -4.6680e-04,\n",
       "         -1.2675e-03, -7.6855e-04,  1.2423e-03, -3.0196e-03, -1.5206e-03,\n",
       "         -2.9065e-03,  7.4112e-04, -7.6306e-04,  1.3342e-03, -5.8006e-05,\n",
       "          3.1593e-03, -2.7390e-04,  2.0336e-03, -1.1344e-03,  1.2272e-04,\n",
       "          2.7946e-03,  5.7808e-04, -1.3402e-03,  1.8120e-04, -3.9004e-04,\n",
       "          5.4774e-04,  1.1216e-03, -1.6380e-03,  1.7656e-03, -5.8055e-04,\n",
       "          1.7187e-05, -7.0764e-04, -9.5021e-04,  1.2897e-03,  1.2219e-03,\n",
       "          9.1709e-05, -1.1839e-03,  3.2251e-03, -1.9613e-03, -1.0290e-04,\n",
       "          9.5640e-05,  1.6570e-03,  3.6368e-04,  1.8418e-04, -6.9857e-04,\n",
       "         -2.4648e-03,  8.0842e-04, -7.7720e-04,  2.8328e-03,  2.6979e-03,\n",
       "          1.4022e-04, -1.5300e-04,  1.9427e-03, -4.9085e-04,  1.7530e-03,\n",
       "         -8.6579e-04, -2.0974e-03, -3.3034e-04,  4.7285e-04,  9.5170e-04,\n",
       "         -9.9818e-04, -6.9971e-04,  2.5193e-03, -2.4274e-03,  2.8432e-04,\n",
       "         -1.1715e-03,  7.3637e-04,  1.6590e-03, -6.7182e-05, -2.5570e-03,\n",
       "          9.7380e-04, -1.3243e-03, -1.3226e-03, -2.5704e-03, -1.3401e-03,\n",
       "         -1.5648e-03,  1.6328e-03,  8.7413e-05,  1.1962e-03, -7.4965e-04,\n",
       "         -5.9334e-04, -1.0422e-03,  9.2931e-04,  7.1702e-04, -3.7558e-03,\n",
       "         -1.4633e-03, -1.2952e-03, -2.6032e-03,  1.4270e-03, -4.1196e-04,\n",
       "         -2.3988e-03,  1.5716e-03, -4.0696e-03,  4.6199e-04, -6.6887e-04,\n",
       "         -8.4726e-04, -4.0843e-04, -1.7597e-03,  6.9180e-04, -1.1771e-03,\n",
       "         -8.6710e-04, -6.5236e-04,  3.4086e-03,  3.5302e-03,  3.8444e-05,\n",
       "          5.4927e-04,  2.1400e-03,  1.1871e-03, -9.3708e-04, -9.1291e-04,\n",
       "          2.7760e-03,  1.0582e-03,  2.6595e-03,  1.5984e-03, -1.7568e-03,\n",
       "         -1.5213e-03, -1.5706e-03, -1.7108e-03, -9.3493e-04, -1.4168e-03,\n",
       "          2.4080e-03, -2.4141e-03, -1.2418e-03,  1.0346e-03,  1.1440e-03,\n",
       "         -7.0461e-04,  1.5788e-03, -1.2386e-03,  1.5952e-03, -2.4293e-03,\n",
       "          3.7233e-04,  4.8609e-04, -1.1014e-04,  2.2198e-04,  2.1560e-04,\n",
       "          1.9886e-04,  6.8531e-04,  6.8138e-04,  1.9850e-05,  1.8829e-04,\n",
       "          8.3638e-04,  5.9747e-04, -3.3410e-03, -2.5439e-03,  7.8688e-04,\n",
       "          2.2627e-03, -6.2571e-04,  1.0042e-03,  1.6240e-03,  1.1841e-03,\n",
       "         -2.8853e-04,  1.5279e-03, -1.6912e-03,  2.9764e-03,  1.1197e-03,\n",
       "         -5.9715e-04,  8.2240e-04,  6.5285e-06,  1.4478e-03, -5.2025e-04,\n",
       "          3.1417e-04, -9.3276e-04,  2.5340e-03,  3.4883e-04, -6.6602e-04,\n",
       "          3.2115e-04,  2.2237e-03,  7.3812e-04,  2.4144e-04,  3.7268e-04,\n",
       "         -4.6308e-04, -5.9365e-04,  1.0747e-04, -2.6376e-03, -2.5713e-03,\n",
       "          1.3161e-03, -2.2740e-03, -6.7332e-04,  7.6425e-04, -1.1303e-03,\n",
       "         -1.1583e-04, -8.0556e-04,  3.3144e-05, -5.5231e-04,  2.8222e-03,\n",
       "          2.8214e-03,  2.2180e-03]),\n",
       " 'transformer.resblocks.11.mlp.c_fc.weight': tensor([[ 0.0310,  0.0396,  0.0546,  ..., -0.0299, -0.0166,  0.0002],\n",
       "         [-0.0375, -0.0682,  0.0015,  ...,  0.0015, -0.0228, -0.0148],\n",
       "         [-0.0208, -0.0211,  0.0334,  ...,  0.0130,  0.0273,  0.0153],\n",
       "         ...,\n",
       "         [ 0.0435,  0.0410,  0.0331,  ...,  0.0350, -0.0156, -0.0174],\n",
       "         [ 0.0466,  0.0565, -0.0074,  ..., -0.0667, -0.0107,  0.0084],\n",
       "         [-0.0271, -0.0015, -0.0162,  ...,  0.0317, -0.0070, -0.0246]]),\n",
       " 'transformer.resblocks.11.mlp.c_fc.bias': tensor([ 0.0024, -0.0001,  0.0009,  ..., -0.0348, -0.0396,  0.0250]),\n",
       " 'transformer.resblocks.11.mlp.c_proj.weight': tensor([[ 2.5465e-03, -3.7949e-04, -2.6675e-03,  ..., -9.9703e-03,\n",
       "           7.6195e-03,  2.7008e-03],\n",
       "         [ 4.8455e-04, -1.3234e-02,  6.6279e-03,  ...,  1.4343e-03,\n",
       "           2.8288e-02,  7.4191e-03],\n",
       "         [ 4.1279e-05, -2.0517e-03,  1.9585e-02,  ..., -4.5030e-03,\n",
       "           9.5562e-04,  1.2278e-02],\n",
       "         ...,\n",
       "         [-6.1257e-04, -7.1217e-04,  1.1816e-02,  ...,  4.3275e-03,\n",
       "          -5.3084e-03,  8.5324e-03],\n",
       "         [-1.2689e-02, -1.0179e-02, -2.6734e-03,  ..., -8.9791e-03,\n",
       "           5.3729e-03, -2.4627e-03],\n",
       "         [-2.1042e-03,  5.1673e-03, -8.5973e-03,  ...,  4.1561e-03,\n",
       "           7.9764e-04, -5.0466e-03]]),\n",
       " 'transformer.resblocks.11.mlp.c_proj.bias': tensor([ 7.8059e-03, -6.7684e-03,  4.8165e-03, -2.1274e-02,  2.4418e-03,\n",
       "          1.2978e-02, -2.0554e-02,  1.8395e-02,  1.7405e-02, -4.2448e-03,\n",
       "         -1.9380e-02, -2.7079e-05,  2.3861e-03,  8.1349e-04,  1.0557e-02,\n",
       "          8.1757e-03, -9.7690e-03,  1.7552e-02, -3.7141e-03, -1.9175e-02,\n",
       "          1.6119e-02, -6.6262e-03,  9.6593e-03,  1.9932e-02, -8.1995e-03,\n",
       "         -2.0746e-02,  1.2763e-02,  1.3391e-02,  6.2244e-04, -4.6520e-03,\n",
       "          1.5078e-02,  2.0767e-02, -1.3260e-02, -1.1176e-02,  1.0006e-02,\n",
       "         -1.9298e-02, -1.0042e-02,  8.6082e-03, -1.4057e-02, -1.9688e-02,\n",
       "         -1.0883e-02, -1.5628e-02,  1.4808e-02,  1.8623e-02,  1.3685e-02,\n",
       "         -4.8580e-03, -1.2165e-02,  2.0312e-02, -1.2727e-02,  2.2169e-02,\n",
       "          3.7957e-03,  1.2975e-02, -1.4678e-02,  1.6062e-02, -1.0194e-02,\n",
       "          8.0011e-04,  4.0542e-03, -1.6990e-02, -1.0290e-02, -3.9652e-03,\n",
       "          1.3348e-02,  1.1630e-02,  1.6878e-02,  5.4103e-03, -7.2393e-03,\n",
       "         -1.1974e-02, -4.5495e-03, -1.9809e-02,  1.4104e-02,  2.1058e-02,\n",
       "          1.5301e-02,  1.7439e-02, -2.9784e-04,  1.7567e-02,  3.7462e-03,\n",
       "         -1.3567e-02, -2.3166e-03,  1.7690e-02, -2.2576e-02, -1.8889e-02,\n",
       "          1.0715e-02,  9.4540e-03, -8.8843e-03, -1.2389e-02, -3.2369e-03,\n",
       "          9.3671e-03, -2.0408e-02,  1.8689e-02, -5.2168e-03, -2.4731e-02,\n",
       "         -1.1929e-02, -1.0335e-02,  1.4147e-02, -1.7585e-02, -5.5298e-03,\n",
       "         -2.4891e-02, -1.9104e-02,  4.3463e-03, -2.0077e-02, -1.0211e-02,\n",
       "          1.3563e-02, -9.7103e-03, -7.2459e-03, -1.6283e-02, -5.6669e-03,\n",
       "          1.2894e-02,  7.7622e-03,  6.3426e-03, -6.3388e-03, -8.2652e-03,\n",
       "         -1.0259e-03, -7.0508e-04,  1.8952e-02,  1.3244e-04,  1.4883e-02,\n",
       "          4.0846e-03,  1.6736e-02, -2.0021e-02,  1.2064e-02, -1.0474e-02,\n",
       "          8.9004e-03, -1.1282e-02, -1.0282e-03, -1.9644e-02, -7.2577e-03,\n",
       "          2.8651e-03,  1.9043e-02, -2.0865e-02, -4.1233e-03,  4.8687e-03,\n",
       "         -1.1221e-02,  1.7094e-02,  1.1620e-02, -1.1246e-02, -1.6870e-02,\n",
       "          1.6602e-02,  6.2921e-03,  4.5725e-03, -1.7709e-02,  6.4209e-03,\n",
       "          1.9247e-04, -1.8386e-02,  1.6373e-03, -1.3577e-02,  9.8031e-03,\n",
       "         -6.5075e-03, -6.4100e-04, -2.0522e-02, -4.1226e-03,  1.1725e-02,\n",
       "         -1.8529e-02, -2.2467e-03, -1.1835e-02,  1.1258e-02, -1.4773e-02,\n",
       "          1.8815e-02, -1.1854e-02,  1.6534e-02,  9.2462e-03, -1.0941e-02,\n",
       "         -2.3188e-02,  1.6616e-02, -7.2992e-03, -1.9656e-03, -2.1585e-02,\n",
       "         -7.4440e-03, -1.5047e-03, -5.8892e-04,  7.9263e-03, -3.0065e-03,\n",
       "          1.6428e-02,  1.5416e-02,  4.3309e-04, -4.8473e-03, -1.5487e-02,\n",
       "          6.6165e-03,  1.1197e-02,  5.6385e-03,  1.3670e-02,  1.8410e-02,\n",
       "          1.2306e-02, -1.6325e-02,  5.5292e-03, -1.9657e-02,  2.2453e-02,\n",
       "         -1.3900e-03,  1.4550e-02, -2.1464e-02, -1.2873e-02,  4.8021e-03,\n",
       "         -1.1066e-02,  1.7732e-02, -9.7927e-03,  2.3905e-02,  5.1433e-03,\n",
       "         -2.1107e-02, -1.6887e-02, -5.3340e-03, -1.4991e-02, -2.9875e-03,\n",
       "         -9.3649e-03,  3.6940e-03, -6.1127e-03, -1.8895e-02, -4.3294e-03,\n",
       "         -1.4832e-02, -1.9515e-02, -9.0503e-03, -2.0132e-02, -1.2808e-02,\n",
       "          2.5475e-03,  1.3028e-02,  6.5088e-03,  1.9616e-02,  9.2075e-03,\n",
       "         -8.2167e-03, -9.3317e-03,  2.0552e-02, -1.7947e-02, -1.4605e-02,\n",
       "          6.5469e-04,  7.7913e-03, -5.7432e-03, -9.1179e-03, -3.8517e-03,\n",
       "          4.3553e-03, -1.8577e-02, -1.7749e-02,  2.2226e-02, -1.4861e-02,\n",
       "          1.5981e-02, -1.7599e-02, -1.3008e-02,  1.7054e-03,  2.0955e-02,\n",
       "          1.0505e-02,  2.1506e-02, -2.0347e-02, -1.4175e-02, -1.9836e-02,\n",
       "         -1.1741e-02,  2.0089e-02,  7.4478e-03,  5.5944e-03,  5.2383e-03,\n",
       "          2.8691e-04, -1.3780e-03, -1.0986e-02, -4.4297e-03, -1.1609e-03,\n",
       "          1.4363e-02,  1.7721e-02,  1.9677e-02,  3.0447e-04, -1.7577e-02,\n",
       "          7.2378e-05,  3.6566e-03,  1.0219e-02,  1.5042e-02, -6.4096e-04,\n",
       "         -1.4273e-03, -2.0681e-02, -1.7413e-02,  1.2005e-02,  3.6465e-03,\n",
       "          2.0629e-02, -6.4656e-03,  3.7906e-03,  1.8074e-02,  5.2258e-03,\n",
       "          1.5188e-02,  1.2143e-02, -1.5547e-02, -1.0441e-02,  1.1958e-02,\n",
       "         -1.6347e-02, -4.9654e-03, -3.4588e-03,  2.1227e-02,  1.1487e-02,\n",
       "         -1.3227e-02,  8.6380e-03, -6.3567e-04,  1.9426e-02,  1.8317e-02,\n",
       "          1.8115e-02,  1.2087e-02, -1.1607e-02,  1.3772e-02, -1.9341e-02,\n",
       "          3.5576e-03,  1.5979e-02, -2.4509e-04,  4.4515e-03,  2.1339e-02,\n",
       "          3.2078e-04, -1.3397e-02,  9.6334e-03,  2.2506e-05,  1.1733e-03,\n",
       "         -1.7951e-02, -1.1148e-02, -1.6718e-02, -2.3859e-02, -3.3470e-03,\n",
       "         -1.7271e-02, -9.1672e-03,  2.0425e-02,  4.0629e-03, -1.8058e-02,\n",
       "         -1.8454e-02,  1.1486e-03, -1.9359e-02, -1.3711e-02,  9.7342e-03,\n",
       "          1.9616e-02,  2.2741e-02, -1.3586e-02,  3.8614e-03,  9.0154e-04,\n",
       "         -4.3203e-03, -1.0144e-02,  1.3845e-02,  7.8192e-03, -2.3472e-02,\n",
       "         -1.7240e-02, -1.3283e-02, -1.5310e-02, -2.2727e-02, -1.1245e-02,\n",
       "          1.3402e-02, -1.9837e-02, -4.0660e-03,  4.1243e-03,  8.1692e-03,\n",
       "          1.2101e-02,  4.7849e-03, -3.5544e-03,  8.7933e-03, -3.2156e-03,\n",
       "          6.5059e-03,  9.2210e-03,  1.7684e-02, -8.1528e-03,  1.2708e-02,\n",
       "          1.7384e-02,  4.0938e-03,  1.7805e-02, -1.5850e-02, -1.0676e-02,\n",
       "         -7.1960e-03, -8.3627e-03,  5.6384e-03,  5.4330e-03, -1.7794e-02,\n",
       "          9.2508e-03,  1.6079e-03,  1.7253e-02, -1.5535e-02,  1.0681e-02,\n",
       "          1.0217e-02,  1.3847e-02,  5.6421e-03,  3.6191e-03,  5.7098e-03,\n",
       "          2.4288e-02, -9.4012e-03, -5.7810e-03, -3.3109e-03, -4.5738e-03,\n",
       "          1.7468e-02,  8.3858e-03,  2.2494e-02,  9.6926e-03,  1.3419e-02,\n",
       "         -1.7063e-02, -1.1732e-02, -1.4067e-02, -1.8558e-02, -8.3969e-03,\n",
       "          1.2212e-02,  2.8085e-03, -1.8921e-02, -8.0657e-03, -1.5252e-02,\n",
       "         -1.1243e-03, -8.4910e-03,  2.3449e-03,  1.8612e-02, -4.1493e-03,\n",
       "          9.7985e-03, -8.8795e-03,  1.1164e-02,  1.2600e-02,  1.0207e-02,\n",
       "          4.5804e-04, -7.7078e-03,  2.6026e-03,  2.2445e-02, -1.6276e-02,\n",
       "          1.9055e-03, -1.7648e-02, -9.7186e-03, -6.0908e-03, -1.0894e-02,\n",
       "          1.8260e-02,  3.3537e-03, -1.5953e-02, -1.2446e-02, -1.9479e-02,\n",
       "         -2.6383e-03,  1.8081e-02,  1.5139e-02,  5.3843e-03,  6.4566e-03,\n",
       "         -1.2655e-02,  1.8952e-02, -1.0880e-02, -2.0677e-02,  9.6777e-03,\n",
       "          1.4595e-03, -1.8374e-02,  1.1515e-02, -9.4409e-03, -8.7682e-03,\n",
       "          1.8678e-02,  4.6089e-03,  8.1795e-03,  1.4457e-02,  1.5770e-02,\n",
       "         -1.5130e-02,  8.8661e-03,  1.3909e-02, -2.2432e-02, -1.3569e-02,\n",
       "         -1.2236e-02,  1.4600e-03, -9.7343e-03, -5.4822e-03, -1.8493e-02,\n",
       "          8.9499e-03,  4.5954e-03,  1.5243e-02,  1.2676e-02,  7.2456e-04,\n",
       "          6.2876e-03,  2.8562e-03,  2.3246e-02, -3.2994e-03, -6.7068e-03,\n",
       "          2.2111e-02,  6.8604e-03,  1.3780e-02,  1.1401e-02, -2.0415e-02,\n",
       "          6.1419e-03,  1.1405e-02,  5.2876e-04, -4.8821e-03, -7.4070e-03,\n",
       "          1.9600e-02,  1.5214e-02,  1.5660e-02, -1.3991e-02,  2.0971e-02,\n",
       "         -1.0660e-02, -7.3005e-03,  1.1792e-02,  3.7189e-03, -1.6910e-02,\n",
       "         -1.9782e-03, -9.8628e-03,  2.3644e-03, -1.4569e-03,  1.8793e-02,\n",
       "          1.9638e-02, -1.5005e-03, -2.0225e-02, -9.2218e-03,  2.6407e-03,\n",
       "         -8.2866e-03, -7.6439e-03, -8.1562e-03, -4.3113e-03, -1.0339e-02,\n",
       "          1.0909e-02, -9.3785e-03, -2.1205e-02,  2.2186e-02,  2.2953e-03,\n",
       "         -1.6553e-02,  1.0278e-02, -1.0113e-02, -8.2417e-03, -2.1735e-02,\n",
       "          1.1317e-02,  1.6405e-02,  4.3104e-03,  1.0270e-02, -9.5406e-03,\n",
       "         -1.9895e-03, -1.3216e-02, -6.6550e-03,  2.0222e-02,  1.2216e-02,\n",
       "         -9.1252e-03,  1.3144e-02, -1.6353e-02, -1.6656e-02,  6.2260e-03,\n",
       "          1.4584e-02,  1.2829e-02]),\n",
       " 'transformer.resblocks.11.ln_2.weight': tensor([0.9997, 0.9974, 1.0010, 1.0010, 1.0021, 0.9986, 0.9993, 0.9968, 0.9977,\n",
       "         0.9995, 0.9962, 1.0004, 1.0004, 0.9996, 0.9944, 0.9961, 1.0028, 1.0000,\n",
       "         1.0011, 1.0001, 1.0014, 1.0008, 1.0035, 1.0032, 0.9992, 0.9979, 1.0017,\n",
       "         0.9980, 0.9992, 0.9975, 0.9993, 0.9993, 0.9981, 0.9973, 0.9998, 1.0020,\n",
       "         0.9999, 1.0021, 1.0017, 0.9999, 0.9984, 0.9972, 0.9989, 1.0005, 0.9965,\n",
       "         0.9997, 0.9961, 0.9992, 0.9986, 1.0008, 1.0008, 1.0041, 1.0009, 0.9974,\n",
       "         0.9963, 0.9998, 0.9973, 1.0018, 1.0018, 1.0020, 0.9981, 1.0025, 0.9985,\n",
       "         1.0020, 1.0020, 0.9998, 0.9979, 1.0010, 1.0003, 0.9972, 1.0004, 1.0009,\n",
       "         0.9982, 0.9997, 1.0006, 0.9985, 0.9977, 0.9970, 0.9969, 0.9987, 0.9989,\n",
       "         1.0008, 0.9991, 0.9989, 0.9997, 1.0019, 0.9990, 1.0002, 1.0020, 0.9985,\n",
       "         0.9994, 1.0036, 0.9991, 0.9978, 0.9975, 0.9971, 0.9959, 0.9981, 1.0015,\n",
       "         0.9996, 1.0011, 1.0019, 0.9979, 0.9991, 1.0001, 1.0000, 0.9979, 1.0008,\n",
       "         0.9961, 1.0019, 0.9980, 0.9988, 0.9979, 0.9976, 1.0010, 0.9951, 0.9979,\n",
       "         0.9981, 0.9943, 0.9979, 1.0015, 0.9984, 0.9988, 1.0013, 0.9990, 0.9993,\n",
       "         0.9990, 1.0008, 1.0005, 1.0027, 0.9990, 0.9958, 1.0027, 0.9969, 1.0007,\n",
       "         1.0022, 0.9980, 0.9991, 1.0028, 0.9990, 0.9952, 0.9991, 1.0033, 0.9990,\n",
       "         0.9979, 0.9995, 0.9956, 1.0014, 0.9996, 1.0011, 0.9991, 0.9924, 0.9966,\n",
       "         1.0028, 1.0003, 1.0046, 1.0002, 1.0002, 1.0044, 1.0004, 1.0005, 0.9948,\n",
       "         0.9950, 1.0016, 0.9971, 0.9982, 0.9991, 0.9983, 1.0016, 0.9970, 0.9999,\n",
       "         0.9961, 0.9972, 0.9986, 0.9972, 0.9999, 0.9983, 1.0017, 0.9984, 0.9972,\n",
       "         1.0004, 1.0006, 0.9988, 0.9984, 0.9957, 0.9996, 0.9955, 0.9976, 0.9998,\n",
       "         0.9963, 1.0030, 0.9982, 0.9979, 0.9958, 0.9984, 0.9997, 1.0001, 0.9987,\n",
       "         0.9982, 0.9988, 1.0012, 1.0001, 1.0002, 0.9978, 1.0026, 0.9988, 0.9998,\n",
       "         1.0009, 0.9980, 0.9993, 0.9930, 1.0004, 1.0002, 0.9952, 0.9970, 0.9980,\n",
       "         0.9998, 0.9984, 0.9976, 1.0013, 0.9985, 1.0010, 0.9986, 0.9994, 0.9991,\n",
       "         1.0008, 1.0008, 0.9999, 1.0014, 1.0006, 0.9983, 1.0006, 1.0012, 0.9964,\n",
       "         0.9985, 0.9981, 0.9985, 0.9982, 0.9997, 0.9993, 0.9974, 1.0011, 0.9956,\n",
       "         1.0031, 0.9980, 1.0034, 1.0031, 0.9964, 1.0016, 1.0034, 0.9973, 1.0017,\n",
       "         1.0020, 0.9998, 0.9998, 1.0023, 1.0006, 0.9961, 0.9975, 0.9952, 1.0013,\n",
       "         1.0005, 1.0019, 0.9993, 0.9982, 0.9992, 0.9998, 1.0007, 0.9963, 1.0016,\n",
       "         0.9998, 0.9987, 0.9994, 1.0003, 0.9991, 1.0015, 0.9952, 1.0043, 0.9974,\n",
       "         0.9998, 1.0011, 0.9978, 0.9975, 1.0018, 0.9989, 1.0001, 0.9995, 0.9987,\n",
       "         0.9985, 1.0022, 1.0042, 0.9991, 0.9977, 0.9960, 1.0024, 0.9993, 0.9971,\n",
       "         1.0022, 1.0010, 1.0009, 0.9977, 0.9974, 0.9995, 0.9953, 0.9980, 0.9973,\n",
       "         1.0004, 1.0030, 0.9985, 0.9991, 1.0010, 0.9955, 1.0022, 1.0012, 1.0026,\n",
       "         0.9988, 0.9988, 0.9995, 1.0035, 0.9993, 0.9994, 0.9997, 1.0011, 1.0012,\n",
       "         0.9979, 0.9984, 0.9963, 1.0009, 0.9942, 0.9970, 1.0015, 1.0008, 0.9996,\n",
       "         1.0028, 1.0004, 1.0008, 1.0048, 1.0014, 0.9977, 0.9974, 0.9985, 0.9989,\n",
       "         0.9979, 1.0021, 0.9975, 1.0015, 0.9991, 1.0033, 0.9990, 0.9996, 0.9974,\n",
       "         0.9975, 0.9990, 1.0010, 1.0014, 1.0041, 0.9981, 0.9979, 1.0033, 0.9977,\n",
       "         1.0022, 0.9979, 0.9988, 0.9991, 0.9980, 0.9968, 1.0000, 1.0000, 0.9960,\n",
       "         1.0010, 0.9994, 0.9984, 1.0008, 1.0012, 0.9993, 0.9995, 0.9993, 0.9994,\n",
       "         0.9976, 0.9992, 0.9992, 1.0009, 0.9984, 0.9986, 0.9950, 0.9989, 0.9993,\n",
       "         0.9980, 0.9980, 1.0009, 0.9990, 0.9998, 0.9995, 0.9974, 0.9975, 1.0013,\n",
       "         0.9999, 0.9989, 1.0022, 1.0013, 1.0008, 0.9952, 1.0027, 0.9999, 1.0029,\n",
       "         1.0018, 0.9981, 0.9987, 0.9984, 1.0001, 0.9983, 0.9992, 1.0011, 0.9970,\n",
       "         0.9964, 0.9959, 0.9978, 0.9950, 0.9984, 0.9971, 1.0031, 1.0005, 1.0007,\n",
       "         0.9997, 0.9993, 0.9996, 0.9994, 1.0029, 0.9975, 0.9969, 0.9997, 0.9972,\n",
       "         1.0018, 1.0014, 0.9967, 0.9999, 0.9945, 0.9960, 1.0000, 1.0007, 0.9983,\n",
       "         1.0018, 0.9999, 0.9998, 0.9987, 0.9974, 1.0002, 1.0012, 0.9991, 1.0012,\n",
       "         1.0011, 0.9953, 1.0013, 0.9952, 1.0006, 0.9984, 0.9994, 0.9994, 0.9993,\n",
       "         0.9973, 0.9990, 0.9997, 1.0005, 0.9959, 1.0021, 0.9956, 1.0014, 0.9978,\n",
       "         1.0015, 0.9973, 1.0011, 0.9993, 0.9974, 1.0009, 0.9983, 1.0008, 1.0016,\n",
       "         1.0009, 0.9955, 1.0011, 1.0003, 1.0020, 1.0041, 1.0018, 1.0002, 0.9996,\n",
       "         1.0020, 0.9975, 0.9990, 0.9971, 0.9989, 0.9958, 0.9991, 0.9988, 1.0045,\n",
       "         0.9994, 0.9984, 0.9971, 1.0023, 0.9979, 0.9976, 0.9985, 0.9978, 0.9941,\n",
       "         0.9977, 0.9991, 1.0009, 1.0015, 0.9995, 1.0023, 0.9966, 0.9993]),\n",
       " 'transformer.resblocks.11.ln_2.bias': tensor([-9.2171e-05,  2.3245e-03, -2.5286e-03,  1.1751e-04,  6.1468e-04,\n",
       "          9.1659e-04,  1.6652e-03, -1.7608e-03,  1.6653e-03, -1.5656e-03,\n",
       "          4.0853e-03,  7.6892e-04,  3.6977e-04,  1.7856e-03,  5.2387e-03,\n",
       "         -3.5321e-03, -2.0611e-03,  6.9932e-04,  2.1912e-04,  1.1983e-03,\n",
       "          9.1557e-04,  1.0814e-03, -8.1391e-04, -3.7289e-03,  2.3849e-04,\n",
       "          2.2580e-03,  1.1371e-03,  6.6655e-04, -7.6684e-04,  2.2305e-03,\n",
       "         -1.8129e-03,  1.8469e-03, -1.3942e-03,  2.7165e-03, -8.3769e-04,\n",
       "          2.3276e-03,  3.1315e-04,  1.0981e-03,  1.4670e-03,  3.9822e-04,\n",
       "         -1.8483e-03, -1.7777e-03, -1.6186e-03, -4.6709e-04,  2.7923e-03,\n",
       "         -3.9020e-03,  1.8355e-03,  2.0013e-03,  1.0375e-03,  1.7968e-03,\n",
       "         -2.1721e-04, -8.2363e-04,  5.0791e-04, -1.1754e-03, -3.0912e-03,\n",
       "          7.6119e-04,  2.2861e-03,  2.5340e-03,  1.0770e-03,  2.1413e-03,\n",
       "          9.4712e-04,  1.2887e-04, -1.2102e-03, -1.7100e-03, -1.2286e-03,\n",
       "          4.4232e-04, -2.1889e-03,  8.9302e-04,  1.3094e-03, -8.6080e-05,\n",
       "         -3.3240e-03, -1.6966e-03,  2.0083e-03,  1.0988e-03,  4.0449e-04,\n",
       "         -9.0735e-04,  2.4216e-03,  9.4698e-04,  1.4057e-03, -1.0857e-03,\n",
       "          5.5574e-04,  2.3167e-03,  5.0983e-04, -1.0315e-03, -2.5100e-03,\n",
       "          1.2075e-03,  7.1479e-05,  4.0756e-04,  1.4235e-03,  1.8173e-03,\n",
       "         -2.8282e-03, -1.9570e-03, -1.6700e-03,  4.0417e-03,  3.3642e-03,\n",
       "          1.1848e-03,  3.4451e-03,  5.1966e-03,  5.0167e-04, -6.8504e-04,\n",
       "         -4.3664e-04, -3.8428e-04,  8.2531e-04,  1.6733e-03, -2.7625e-03,\n",
       "          5.5106e-04,  9.1059e-04,  1.4512e-03, -1.0620e-03,  4.9388e-04,\n",
       "         -3.5434e-03,  1.1701e-03, -1.9972e-03, -1.7702e-03,  2.3132e-03,\n",
       "          3.3218e-03, -7.7897e-04, -5.9661e-04,  4.7489e-03,  7.3759e-04,\n",
       "          5.0462e-04,  3.4107e-03, -1.7017e-03, -5.4685e-04,  4.5933e-04,\n",
       "         -3.8725e-04,  2.5816e-03,  1.2914e-03,  1.5613e-03,  1.0439e-03,\n",
       "          2.7442e-03, -3.4417e-03,  4.9424e-03,  1.1204e-03,  8.4351e-04,\n",
       "          3.3198e-04,  1.6340e-03, -7.2540e-04, -1.9715e-03, -3.1226e-03,\n",
       "          7.5849e-04, -3.3499e-03,  1.0331e-03,  3.4092e-04, -3.1514e-03,\n",
       "         -2.1392e-03,  3.3864e-03, -3.2697e-03, -1.4708e-03,  5.1692e-04,\n",
       "         -2.0173e-04, -5.1683e-03, -1.0817e-03,  2.4919e-03,  3.9161e-04,\n",
       "         -1.0140e-03, -5.7440e-04,  2.3239e-03, -2.5594e-03,  1.0829e-03,\n",
       "         -2.0855e-03,  1.2900e-03, -3.8772e-03, -2.8343e-04,  2.0062e-03,\n",
       "         -2.4824e-04,  2.2458e-03, -2.0727e-03,  7.4631e-04,  1.8631e-03,\n",
       "         -2.1996e-03,  5.7635e-04,  2.1548e-03, -7.2379e-04, -5.2743e-04,\n",
       "         -1.4149e-03,  1.6434e-03, -3.3675e-03,  5.6042e-04,  3.1483e-03,\n",
       "          2.7889e-04,  6.7214e-04, -1.5909e-06,  2.3001e-03, -2.6008e-03,\n",
       "          2.0758e-04, -1.9960e-03,  5.5267e-04, -4.3111e-04,  2.3651e-03,\n",
       "         -2.6400e-03, -1.3039e-03, -9.9156e-04, -1.9391e-03,  5.4435e-04,\n",
       "          2.1446e-03, -2.1112e-03, -1.7523e-03, -2.1472e-03, -1.4686e-03,\n",
       "          3.2168e-03,  1.4686e-03, -1.2903e-03, -1.8539e-03, -5.9825e-05,\n",
       "         -1.3978e-03,  2.2311e-04, -4.0663e-03, -4.1651e-03,  1.5266e-03,\n",
       "         -5.3019e-03,  1.1459e-03, -1.6627e-03, -3.4435e-03,  4.1361e-03,\n",
       "          1.2480e-03, -2.6683e-03, -1.0607e-04,  1.9415e-03, -5.9185e-04,\n",
       "          1.4659e-03, -9.5756e-04, -2.0094e-03, -1.0423e-03, -1.9688e-05,\n",
       "         -2.6813e-03, -9.2601e-05,  2.2010e-03, -3.0188e-04, -8.6796e-04,\n",
       "         -5.3953e-04,  1.2730e-03, -1.2175e-03,  3.5194e-03, -3.3288e-03,\n",
       "          2.5416e-04,  4.3367e-04,  1.9406e-03, -2.0360e-03, -1.1122e-03,\n",
       "          2.9402e-03,  4.4010e-04, -4.2312e-03,  2.3999e-03,  1.4227e-03,\n",
       "         -1.1881e-03, -5.0213e-04, -7.8805e-04,  3.8510e-04,  2.8105e-04,\n",
       "         -7.1787e-04,  1.1158e-03, -7.4329e-04,  3.1336e-05, -1.5552e-03,\n",
       "          1.5907e-03,  1.0762e-03,  3.2088e-03,  1.0918e-03,  1.6801e-03,\n",
       "         -2.7365e-03, -9.4626e-04, -2.6877e-03,  1.7914e-03,  1.6958e-03,\n",
       "          1.3214e-03, -2.8774e-03, -1.2675e-03, -1.9547e-03, -5.7688e-04,\n",
       "         -3.6114e-04,  1.4179e-03,  2.6042e-03, -1.1038e-04,  1.7168e-04,\n",
       "          4.1265e-04, -2.5306e-03,  1.4745e-03, -1.4366e-03, -6.4306e-04,\n",
       "         -1.3299e-03, -1.8163e-03,  1.0569e-04, -8.1596e-06,  2.3360e-03,\n",
       "         -1.0643e-04,  1.2727e-03, -2.7514e-03, -1.7935e-03, -7.9210e-05,\n",
       "         -5.0522e-05, -3.7508e-04,  1.1399e-03, -3.0157e-03,  6.1218e-04,\n",
       "         -1.1488e-04,  1.1813e-03, -7.4506e-04, -3.9212e-04, -4.1452e-05,\n",
       "          3.1212e-03,  8.6914e-04,  3.7392e-04,  5.6951e-03,  2.4014e-03,\n",
       "          1.2022e-04, -6.4385e-04, -2.7745e-03,  1.6233e-03,  2.4405e-03,\n",
       "         -1.9462e-03, -2.1211e-03,  1.0305e-03,  7.2707e-04,  1.8082e-03,\n",
       "         -1.8603e-03, -2.4790e-03, -3.0350e-03, -1.0066e-03, -2.9173e-04,\n",
       "         -1.8943e-03, -2.6156e-04,  1.0180e-04,  3.6826e-04,  1.8906e-03,\n",
       "         -2.2783e-04, -5.2962e-03,  4.0976e-04,  6.2308e-03,  1.8716e-03,\n",
       "          1.6769e-03, -1.3873e-04, -1.3953e-03, -1.1003e-03,  1.2023e-03,\n",
       "         -2.0240e-03, -1.8754e-03,  1.8610e-03, -3.0433e-03, -1.4023e-03,\n",
       "          1.3851e-03, -3.3300e-03, -1.9962e-03,  6.3550e-04,  3.4396e-03,\n",
       "          4.3165e-04, -2.7738e-03, -1.1873e-03, -4.1863e-04, -2.6837e-03,\n",
       "          1.4505e-03,  1.2778e-03,  1.6877e-03, -3.8724e-03, -1.3982e-03,\n",
       "         -2.8292e-05,  4.5798e-04, -1.6804e-03, -1.5673e-03,  3.7500e-03,\n",
       "          1.3212e-03,  2.7515e-03, -2.3027e-03,  5.1456e-04, -7.2435e-04,\n",
       "         -5.1493e-04,  6.5173e-04, -9.4453e-04, -3.1557e-03,  6.6357e-05,\n",
       "          8.9922e-04, -1.0649e-03, -2.1796e-04,  1.6256e-03,  1.5850e-04,\n",
       "         -1.1828e-03, -2.7274e-03, -3.1144e-03,  1.8091e-03,  2.5838e-03,\n",
       "         -7.4063e-04,  1.8139e-03, -2.1774e-04,  1.1401e-03,  1.7254e-03,\n",
       "          1.6392e-03,  1.0405e-03, -1.3267e-03, -2.6610e-03,  3.3199e-04,\n",
       "         -8.3956e-04, -2.5140e-04,  9.4884e-04, -7.0185e-04, -3.9826e-03,\n",
       "         -3.8692e-04,  1.3275e-03, -5.1760e-04, -1.7745e-03, -2.5861e-03,\n",
       "         -7.9282e-04,  4.2725e-03,  2.0328e-03,  3.1885e-04, -5.4196e-03,\n",
       "          9.0832e-04, -9.8965e-04, -7.1237e-04, -7.9394e-04, -4.0242e-04,\n",
       "          1.5157e-03, -1.1693e-03, -9.2504e-04, -2.7676e-03, -2.0621e-03,\n",
       "          3.3709e-03, -1.9068e-03, -5.7099e-03,  1.1957e-03, -1.3727e-03,\n",
       "          5.0074e-04, -7.6585e-04, -4.4034e-04,  1.8018e-03,  6.8863e-04,\n",
       "         -1.1611e-03, -6.6003e-04,  1.9869e-03, -3.3360e-05,  3.6407e-04,\n",
       "          6.8094e-04, -1.3515e-03,  2.3942e-03,  1.4480e-03,  3.0516e-03,\n",
       "         -1.2085e-03, -3.0605e-03,  2.6941e-03, -3.5929e-04, -7.9206e-04,\n",
       "          5.5701e-04,  2.3800e-03, -1.2455e-03,  2.6391e-04, -1.0827e-03,\n",
       "          2.6116e-03,  8.9695e-04,  7.0486e-04,  1.6989e-03, -1.5297e-03,\n",
       "         -2.1714e-03, -3.7061e-03,  4.5507e-04, -3.3481e-03,  1.0737e-03,\n",
       "          2.5644e-03, -1.5410e-03, -1.6700e-03, -2.0569e-03, -2.0672e-03,\n",
       "         -2.6137e-03, -1.3048e-03, -9.7882e-04, -4.3731e-03, -7.6241e-04,\n",
       "         -6.1405e-04,  1.4389e-05,  2.1839e-03,  3.0282e-03,  2.6193e-03,\n",
       "          1.5050e-04, -1.2774e-03,  2.1831e-03,  1.7305e-03,  2.1129e-03,\n",
       "          1.1459e-04, -1.0496e-03,  6.3619e-04,  3.2412e-03,  1.4457e-04,\n",
       "          8.1311e-05,  3.1259e-04,  1.0690e-03,  4.1531e-04,  5.4812e-04,\n",
       "         -1.6570e-03, -7.3482e-04,  3.2951e-03, -1.6994e-03, -1.7189e-03,\n",
       "         -2.3941e-03,  1.5859e-03, -3.8135e-03,  1.4151e-03, -6.1553e-04,\n",
       "         -1.2044e-03,  1.5257e-03,  3.7460e-03,  1.0863e-03,  1.1519e-04,\n",
       "          2.6475e-03,  3.2696e-03, -5.1644e-04, -3.9798e-03,  1.0509e-03,\n",
       "         -1.2800e-03,  1.0257e-03, -1.4032e-03, -5.9605e-04, -5.1611e-04,\n",
       "          3.4505e-03,  3.3926e-03]),\n",
       " 'token_embedding.weight': tensor([[ 0.0029, -0.0087, -0.0036,  ..., -0.0200, -0.0224, -0.0124],\n",
       "         [ 0.0055, -0.0065,  0.0246,  ...,  0.0183, -0.0117, -0.0036],\n",
       "         [-0.0341,  0.0144,  0.0161,  ...,  0.0064,  0.0342,  0.0076],\n",
       "         ...,\n",
       "         [-0.0059, -0.0220,  0.0202,  ...,  0.0036,  0.0180,  0.0201],\n",
       "         [ 0.0003,  0.0123,  0.0106,  ...,  0.0036,  0.0065,  0.0084],\n",
       "         [-0.0155, -0.0130,  0.0088,  ..., -0.0018,  0.0092,  0.0331]]),\n",
       " 'ln_final.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'ln_final.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pretrained_gnn, '/data/LPJ/ICML25/all_checkpoints/graph_tower/pretrain_unified_lr_8e3_gnn_projector_without_lora/clip_gt_arxiv/model_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0741, -0.0778, -0.0239,  ..., -0.0861, -0.1395, -0.0260],\n",
       "        [ 0.0893,  0.1540, -0.1024,  ...,  0.0712,  0.0160, -0.0735],\n",
       "        [ 0.0887,  0.0918, -0.0063,  ..., -0.1260,  0.1193, -0.0394],\n",
       "        ...,\n",
       "        [-0.0344, -0.0132, -0.0895,  ..., -0.0237, -0.0309,  0.0127],\n",
       "        [-0.0968, -0.1065,  0.0725,  ..., -0.1153, -0.0287, -0.1241],\n",
       "        [-0.1292, -0.0773,  0.0175,  ..., -0.1423, -0.0280, -0.1142]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.model.graph_tower.gtLayers[0].qTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['positional_embedding', 'text_projection', 'gnn.W_pos', 'gnn.gtLayers.0.qTrans', 'gnn.gtLayers.0.kTrans', 'gnn.gtLayers.0.vTrans', 'gnn.gtLayers.0.norm.weight', 'gnn.gtLayers.0.norm.bias', 'gnn.gtLayers.1.qTrans', 'gnn.gtLayers.1.kTrans', 'gnn.gtLayers.1.vTrans', 'gnn.gtLayers.1.norm.weight', 'gnn.gtLayers.1.norm.bias', 'gnn.gtLayers.2.qTrans', 'gnn.gtLayers.2.kTrans', 'gnn.gtLayers.2.vTrans', 'gnn.gtLayers.2.norm.weight', 'gnn.gtLayers.2.norm.bias', 'gnn.W_P.weight', 'gnn.W_P.bias', 'gnn.inverW_P.weight', 'gnn.inverW_P.bias', 'transformer.resblocks.0.attn.in_proj_weight', 'transformer.resblocks.0.attn.in_proj_bias', 'transformer.resblocks.0.attn.out_proj.weight', 'transformer.resblocks.0.attn.out_proj.bias', 'transformer.resblocks.0.ln_1.weight', 'transformer.resblocks.0.ln_1.bias', 'transformer.resblocks.0.mlp.c_fc.weight', 'transformer.resblocks.0.mlp.c_fc.bias', 'transformer.resblocks.0.mlp.c_proj.weight', 'transformer.resblocks.0.mlp.c_proj.bias', 'transformer.resblocks.0.ln_2.weight', 'transformer.resblocks.0.ln_2.bias', 'transformer.resblocks.1.attn.in_proj_weight', 'transformer.resblocks.1.attn.in_proj_bias', 'transformer.resblocks.1.attn.out_proj.weight', 'transformer.resblocks.1.attn.out_proj.bias', 'transformer.resblocks.1.ln_1.weight', 'transformer.resblocks.1.ln_1.bias', 'transformer.resblocks.1.mlp.c_fc.weight', 'transformer.resblocks.1.mlp.c_fc.bias', 'transformer.resblocks.1.mlp.c_proj.weight', 'transformer.resblocks.1.mlp.c_proj.bias', 'transformer.resblocks.1.ln_2.weight', 'transformer.resblocks.1.ln_2.bias', 'transformer.resblocks.2.attn.in_proj_weight', 'transformer.resblocks.2.attn.in_proj_bias', 'transformer.resblocks.2.attn.out_proj.weight', 'transformer.resblocks.2.attn.out_proj.bias', 'transformer.resblocks.2.ln_1.weight', 'transformer.resblocks.2.ln_1.bias', 'transformer.resblocks.2.mlp.c_fc.weight', 'transformer.resblocks.2.mlp.c_fc.bias', 'transformer.resblocks.2.mlp.c_proj.weight', 'transformer.resblocks.2.mlp.c_proj.bias', 'transformer.resblocks.2.ln_2.weight', 'transformer.resblocks.2.ln_2.bias', 'transformer.resblocks.3.attn.in_proj_weight', 'transformer.resblocks.3.attn.in_proj_bias', 'transformer.resblocks.3.attn.out_proj.weight', 'transformer.resblocks.3.attn.out_proj.bias', 'transformer.resblocks.3.ln_1.weight', 'transformer.resblocks.3.ln_1.bias', 'transformer.resblocks.3.mlp.c_fc.weight', 'transformer.resblocks.3.mlp.c_fc.bias', 'transformer.resblocks.3.mlp.c_proj.weight', 'transformer.resblocks.3.mlp.c_proj.bias', 'transformer.resblocks.3.ln_2.weight', 'transformer.resblocks.3.ln_2.bias', 'transformer.resblocks.4.attn.in_proj_weight', 'transformer.resblocks.4.attn.in_proj_bias', 'transformer.resblocks.4.attn.out_proj.weight', 'transformer.resblocks.4.attn.out_proj.bias', 'transformer.resblocks.4.ln_1.weight', 'transformer.resblocks.4.ln_1.bias', 'transformer.resblocks.4.mlp.c_fc.weight', 'transformer.resblocks.4.mlp.c_fc.bias', 'transformer.resblocks.4.mlp.c_proj.weight', 'transformer.resblocks.4.mlp.c_proj.bias', 'transformer.resblocks.4.ln_2.weight', 'transformer.resblocks.4.ln_2.bias', 'transformer.resblocks.5.attn.in_proj_weight', 'transformer.resblocks.5.attn.in_proj_bias', 'transformer.resblocks.5.attn.out_proj.weight', 'transformer.resblocks.5.attn.out_proj.bias', 'transformer.resblocks.5.ln_1.weight', 'transformer.resblocks.5.ln_1.bias', 'transformer.resblocks.5.mlp.c_fc.weight', 'transformer.resblocks.5.mlp.c_fc.bias', 'transformer.resblocks.5.mlp.c_proj.weight', 'transformer.resblocks.5.mlp.c_proj.bias', 'transformer.resblocks.5.ln_2.weight', 'transformer.resblocks.5.ln_2.bias', 'transformer.resblocks.6.attn.in_proj_weight', 'transformer.resblocks.6.attn.in_proj_bias', 'transformer.resblocks.6.attn.out_proj.weight', 'transformer.resblocks.6.attn.out_proj.bias', 'transformer.resblocks.6.ln_1.weight', 'transformer.resblocks.6.ln_1.bias', 'transformer.resblocks.6.mlp.c_fc.weight', 'transformer.resblocks.6.mlp.c_fc.bias', 'transformer.resblocks.6.mlp.c_proj.weight', 'transformer.resblocks.6.mlp.c_proj.bias', 'transformer.resblocks.6.ln_2.weight', 'transformer.resblocks.6.ln_2.bias', 'transformer.resblocks.7.attn.in_proj_weight', 'transformer.resblocks.7.attn.in_proj_bias', 'transformer.resblocks.7.attn.out_proj.weight', 'transformer.resblocks.7.attn.out_proj.bias', 'transformer.resblocks.7.ln_1.weight', 'transformer.resblocks.7.ln_1.bias', 'transformer.resblocks.7.mlp.c_fc.weight', 'transformer.resblocks.7.mlp.c_fc.bias', 'transformer.resblocks.7.mlp.c_proj.weight', 'transformer.resblocks.7.mlp.c_proj.bias', 'transformer.resblocks.7.ln_2.weight', 'transformer.resblocks.7.ln_2.bias', 'transformer.resblocks.8.attn.in_proj_weight', 'transformer.resblocks.8.attn.in_proj_bias', 'transformer.resblocks.8.attn.out_proj.weight', 'transformer.resblocks.8.attn.out_proj.bias', 'transformer.resblocks.8.ln_1.weight', 'transformer.resblocks.8.ln_1.bias', 'transformer.resblocks.8.mlp.c_fc.weight', 'transformer.resblocks.8.mlp.c_fc.bias', 'transformer.resblocks.8.mlp.c_proj.weight', 'transformer.resblocks.8.mlp.c_proj.bias', 'transformer.resblocks.8.ln_2.weight', 'transformer.resblocks.8.ln_2.bias', 'transformer.resblocks.9.attn.in_proj_weight', 'transformer.resblocks.9.attn.in_proj_bias', 'transformer.resblocks.9.attn.out_proj.weight', 'transformer.resblocks.9.attn.out_proj.bias', 'transformer.resblocks.9.ln_1.weight', 'transformer.resblocks.9.ln_1.bias', 'transformer.resblocks.9.mlp.c_fc.weight', 'transformer.resblocks.9.mlp.c_fc.bias', 'transformer.resblocks.9.mlp.c_proj.weight', 'transformer.resblocks.9.mlp.c_proj.bias', 'transformer.resblocks.9.ln_2.weight', 'transformer.resblocks.9.ln_2.bias', 'transformer.resblocks.10.attn.in_proj_weight', 'transformer.resblocks.10.attn.in_proj_bias', 'transformer.resblocks.10.attn.out_proj.weight', 'transformer.resblocks.10.attn.out_proj.bias', 'transformer.resblocks.10.ln_1.weight', 'transformer.resblocks.10.ln_1.bias', 'transformer.resblocks.10.mlp.c_fc.weight', 'transformer.resblocks.10.mlp.c_fc.bias', 'transformer.resblocks.10.mlp.c_proj.weight', 'transformer.resblocks.10.mlp.c_proj.bias', 'transformer.resblocks.10.ln_2.weight', 'transformer.resblocks.10.ln_2.bias', 'transformer.resblocks.11.attn.in_proj_weight', 'transformer.resblocks.11.attn.in_proj_bias', 'transformer.resblocks.11.attn.out_proj.weight', 'transformer.resblocks.11.attn.out_proj.bias', 'transformer.resblocks.11.ln_1.weight', 'transformer.resblocks.11.ln_1.bias', 'transformer.resblocks.11.mlp.c_fc.weight', 'transformer.resblocks.11.mlp.c_fc.bias', 'transformer.resblocks.11.mlp.c_proj.weight', 'transformer.resblocks.11.mlp.c_proj.bias', 'transformer.resblocks.11.ln_2.weight', 'transformer.resblocks.11.ln_2.bias', 'token_embedding.weight', 'ln_final.weight', 'ln_final.bias'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/data/LPJ/ICML25/all_checkpoints/pretrain_gnn_with_tuning_projector_without_lora_unified_lr/v0_balanced_lr_8e3_2epoch_batch2/balanced_lr_8e3_2epoch_batch2.ckpt', map_location='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.model.embed_tokens.weight',\n",
       "              tensor([[ 1.1921e-06, -1.7881e-06, -4.2915e-06,  ...,  8.3447e-07,\n",
       "                       -6.4373e-06,  8.9407e-07],\n",
       "                      [ 1.8387e-03, -3.8147e-03,  9.6130e-04,  ..., -9.0332e-03,\n",
       "                        2.6550e-03, -3.7537e-03],\n",
       "                      [ 1.0193e-02,  9.7656e-03, -5.2795e-03,  ...,  2.9297e-03,\n",
       "                        4.0817e-04, -5.0964e-03],\n",
       "                      ...,\n",
       "                      [ 6.1512e-05, -4.4678e-02, -1.8555e-02,  ..., -5.3711e-03,\n",
       "                       -5.4626e-03,  1.4282e-02],\n",
       "                      [-4.8828e-03, -9.1553e-03, -2.7588e-02,  ..., -2.6703e-03,\n",
       "                        5.4016e-03, -2.0385e-05],\n",
       "                      [ 1.6556e-03, -2.1515e-03, -4.9316e-02,  ...,  1.0742e-02,\n",
       "                        1.6357e-02, -2.3193e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0060, -0.0146, -0.0021,  ...,  0.0042,  0.0018, -0.0035],\n",
       "                      [ 0.0142, -0.0043,  0.0032,  ..., -0.0092, -0.0108,  0.0073],\n",
       "                      [-0.0137,  0.0121,  0.0002,  ...,  0.0061,  0.0181, -0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0018,  0.0093, -0.0006,  ...,  0.0092, -0.0289,  0.0085],\n",
       "                      [ 0.0249,  0.0116,  0.0035,  ..., -0.0322, -0.0165, -0.0111],\n",
       "                      [-0.0136, -0.0067,  0.0016,  ...,  0.0176,  0.0175, -0.0083]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0155,  0.0078, -0.0011,  ...,  0.0164, -0.0097, -0.0136],\n",
       "                      [ 0.0182,  0.0012,  0.0034,  ..., -0.0206,  0.0143,  0.0229],\n",
       "                      [-0.0245, -0.0220,  0.0018,  ...,  0.0150, -0.0157, -0.0110],\n",
       "                      ...,\n",
       "                      [ 0.0123, -0.0007, -0.0008,  ...,  0.0002,  0.0029,  0.0081],\n",
       "                      [-0.0050,  0.0171, -0.0031,  ..., -0.0033,  0.0112, -0.0110],\n",
       "                      [ 0.0036, -0.0023,  0.0012,  ...,  0.0073, -0.0114,  0.0095]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.self_attn.v_proj.weight',\n",
       "              tensor([[-3.1471e-05, -2.3346e-03,  2.6550e-03,  ...,  7.5684e-03,\n",
       "                       -9.7656e-04,  9.5215e-03],\n",
       "                      [-7.0190e-03,  6.7711e-05, -6.1035e-03,  ..., -1.1597e-02,\n",
       "                        1.2512e-02,  6.4087e-03],\n",
       "                      [ 7.8583e-04,  1.0315e-02,  1.5335e-03,  ...,  4.8523e-03,\n",
       "                       -1.3489e-02, -1.3550e-02],\n",
       "                      ...,\n",
       "                      [-6.5308e-03, -6.1951e-03,  1.0864e-02,  ...,  3.9368e-03,\n",
       "                        2.2583e-03, -1.6785e-03],\n",
       "                      [ 1.7395e-03,  5.6152e-03, -9.5749e-04,  ...,  6.7444e-03,\n",
       "                        1.5625e-02, -8.8692e-05],\n",
       "                      [-1.9264e-04,  1.3123e-03,  5.3711e-03,  ..., -3.3188e-04,\n",
       "                       -7.5531e-04,  1.4267e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.self_attn.o_proj.weight',\n",
       "              tensor([[ 8.8692e-05, -2.3041e-03,  4.3945e-03,  ...,  5.3406e-03,\n",
       "                        4.2725e-03, -9.8267e-03],\n",
       "                      [ 1.9989e-03,  3.5667e-04,  6.8665e-04,  ...,  4.4632e-04,\n",
       "                        1.3809e-03,  4.9133e-03],\n",
       "                      [ 1.3885e-03,  3.1471e-05,  1.4496e-03,  ..., -5.0735e-04,\n",
       "                       -5.2185e-03, -1.1368e-03],\n",
       "                      ...,\n",
       "                      [ 4.5166e-03, -3.7384e-03,  4.7607e-03,  ...,  1.8387e-03,\n",
       "                        3.3112e-03,  5.2795e-03],\n",
       "                      [-4.3335e-03, -1.7166e-03, -2.1820e-03,  ...,  5.2795e-03,\n",
       "                       -4.5776e-03, -8.7738e-05],\n",
       "                      [ 5.6152e-03, -1.0376e-03,  3.7079e-03,  ...,  3.6621e-03,\n",
       "                       -7.3547e-03, -7.9346e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0173,  0.0172,  0.0287,  ..., -0.0147,  0.0070,  0.0131],\n",
       "                      [-0.0027, -0.0060,  0.0056,  ...,  0.0149, -0.0108,  0.0073],\n",
       "                      [ 0.0056, -0.0209,  0.0205,  ..., -0.0117,  0.0227,  0.0184],\n",
       "                      ...,\n",
       "                      [-0.0007, -0.0330,  0.0061,  ..., -0.0076, -0.0131,  0.0400],\n",
       "                      [ 0.0254,  0.0186, -0.0160,  ..., -0.0048, -0.0063, -0.0192],\n",
       "                      [-0.0014, -0.0112, -0.0217,  ...,  0.0011,  0.0056, -0.0249]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0011, -0.0292,  0.0156,  ..., -0.0193, -0.0250,  0.0057],\n",
       "                      [-0.0119, -0.0320,  0.0134,  ...,  0.0206,  0.0051,  0.0028],\n",
       "                      [-0.0055,  0.0126, -0.0076,  ..., -0.0220,  0.0062,  0.0024],\n",
       "                      ...,\n",
       "                      [-0.0070, -0.0026, -0.0063,  ...,  0.0273, -0.0023,  0.0178],\n",
       "                      [-0.0176,  0.0082,  0.0193,  ..., -0.0004, -0.0145,  0.0020],\n",
       "                      [ 0.0195,  0.0201, -0.0015,  ...,  0.0106, -0.0124, -0.0383]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0050, -0.0131,  0.0096,  ..., -0.0184, -0.0073,  0.0016],\n",
       "                      [ 0.0044, -0.0031,  0.0057,  ...,  0.0156, -0.0148,  0.0344],\n",
       "                      [ 0.0017,  0.0339, -0.0042,  ..., -0.0142,  0.0205,  0.0165],\n",
       "                      ...,\n",
       "                      [-0.0098, -0.0125,  0.0055,  ...,  0.0242, -0.0157,  0.0275],\n",
       "                      [-0.0182,  0.0376,  0.0090,  ..., -0.0068, -0.0126, -0.0215],\n",
       "                      [ 0.0117, -0.0015, -0.0066,  ..., -0.0005, -0.0038, -0.0300]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.input_layernorm.weight',\n",
       "              tensor([0.0298, 0.0140, 0.0031,  ..., 0.0099, 0.0114, 0.0073],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.0.post_attention_layernorm.weight',\n",
       "              tensor([0.0508, 0.0525, 0.0498,  ..., 0.0522, 0.0542, 0.0479],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.self_attn.q_proj.weight',\n",
       "              tensor([[-1.3367e-02,  8.0566e-03, -3.8574e-02,  ..., -1.6403e-03,\n",
       "                       -5.7861e-02,  3.4912e-02],\n",
       "                      [-1.4038e-03, -7.8125e-03,  7.8125e-03,  ..., -9.5215e-03,\n",
       "                       -4.9805e-02,  2.7222e-02],\n",
       "                      [ 3.0151e-02,  3.1738e-02,  1.9897e-02,  ...,  1.1635e-04,\n",
       "                       -7.4707e-02,  2.3193e-02],\n",
       "                      ...,\n",
       "                      [-7.5531e-04,  2.7313e-03,  3.3417e-03,  ..., -8.9722e-03,\n",
       "                       -3.0975e-03, -5.9204e-03],\n",
       "                      [-1.5182e-03, -4.6997e-03, -3.8605e-03,  ...,  9.0942e-03,\n",
       "                        4.3640e-03,  6.1035e-03],\n",
       "                      [-6.8665e-05,  5.6763e-03,  6.2866e-03,  ..., -8.2397e-03,\n",
       "                        2.8014e-06, -1.0864e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.self_attn.k_proj.weight',\n",
       "              tensor([[-2.4780e-02, -3.3875e-03,  3.8818e-02,  ...,  1.7090e-02,\n",
       "                        1.9165e-02, -6.9580e-03],\n",
       "                      [-2.8198e-02,  5.6458e-03, -1.2939e-02,  ..., -1.7334e-02,\n",
       "                        1.1047e-02, -5.7861e-02],\n",
       "                      [-2.4170e-02,  2.8198e-02, -6.5430e-02,  ...,  3.3447e-02,\n",
       "                        9.1553e-03, -5.7373e-02],\n",
       "                      ...,\n",
       "                      [-1.0437e-02,  1.7822e-02, -1.7014e-03,  ...,  9.8877e-03,\n",
       "                       -1.3123e-03,  7.4158e-03],\n",
       "                      [ 7.3242e-03, -1.8066e-02,  4.0894e-03,  ..., -1.1047e-02,\n",
       "                       -2.6226e-05, -7.1411e-03],\n",
       "                      [-7.2632e-03,  1.3733e-02,  5.6076e-04,  ...,  2.9602e-03,\n",
       "                       -3.0365e-03,  6.6223e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0080, -0.0080, -0.0037,  ..., -0.0060, -0.0322, -0.0256],\n",
       "                      [-0.0038,  0.0026, -0.0027,  ..., -0.0006,  0.0105,  0.0332],\n",
       "                      [ 0.0065, -0.0109,  0.0095,  ...,  0.0017, -0.0003, -0.0010],\n",
       "                      ...,\n",
       "                      [-0.0052,  0.0015,  0.0026,  ..., -0.0009, -0.0009,  0.0018],\n",
       "                      [-0.0038, -0.0062,  0.0071,  ..., -0.0079,  0.0025, -0.0008],\n",
       "                      [-0.0007,  0.0010,  0.0033,  ...,  0.0025, -0.0065,  0.0082]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0045, -0.0187,  0.0139,  ..., -0.0051,  0.0001,  0.0011],\n",
       "                      [-0.0023, -0.0054, -0.0129,  ..., -0.0018, -0.0019,  0.0041],\n",
       "                      [ 0.0210,  0.0137, -0.0098,  ...,  0.0048,  0.0017, -0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0040,  0.0062,  0.0045,  ..., -0.0001,  0.0023, -0.0028],\n",
       "                      [ 0.0035, -0.0114,  0.0051,  ..., -0.0024,  0.0012,  0.0029],\n",
       "                      [-0.0053, -0.0188,  0.0055,  ...,  0.0022, -0.0056,  0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0277, -0.0091,  0.0371,  ..., -0.0339, -0.0167, -0.0107],\n",
       "                      [-0.0305, -0.0034, -0.0295,  ...,  0.0148, -0.0204, -0.0396],\n",
       "                      [-0.0070, -0.0258,  0.0036,  ..., -0.0089, -0.0142, -0.0208],\n",
       "                      ...,\n",
       "                      [-0.0378,  0.0162, -0.0033,  ..., -0.0020,  0.0121, -0.0022],\n",
       "                      [-0.0150,  0.0045, -0.0361,  ...,  0.0216,  0.0181, -0.0125],\n",
       "                      [-0.0238,  0.0137,  0.0066,  ...,  0.0354,  0.0095, -0.0325]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0015,  0.0352, -0.0160,  ...,  0.0103,  0.0311, -0.0172],\n",
       "                      [ 0.0085, -0.0031, -0.0139,  ..., -0.0074,  0.0097,  0.0101],\n",
       "                      [ 0.0277, -0.0425,  0.0125,  ...,  0.0173, -0.0442,  0.0294],\n",
       "                      ...,\n",
       "                      [-0.0118, -0.0269,  0.0124,  ...,  0.0261,  0.0124, -0.0244],\n",
       "                      [ 0.0330,  0.0031, -0.0188,  ...,  0.0046, -0.0072, -0.0021],\n",
       "                      [-0.0155, -0.0173, -0.0110,  ..., -0.0245, -0.0082,  0.0204]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0084,  0.0095,  0.0537,  ..., -0.0175,  0.0028, -0.0194],\n",
       "                      [ 0.0100,  0.0044, -0.0359,  ...,  0.0087, -0.0133,  0.0023],\n",
       "                      [-0.0168, -0.0144, -0.0219,  ..., -0.0003, -0.0111,  0.0045],\n",
       "                      ...,\n",
       "                      [-0.0103, -0.0056, -0.0166,  ...,  0.0420, -0.0074,  0.0062],\n",
       "                      [-0.0123,  0.0204,  0.0129,  ..., -0.0303, -0.0193,  0.0219],\n",
       "                      [ 0.0253, -0.0044, -0.0186,  ..., -0.0125, -0.0154,  0.0281]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.input_layernorm.weight',\n",
       "              tensor([0.1123, 0.1094, 0.0986,  ..., 0.0640, 0.0918, 0.0723],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.1.post_attention_layernorm.weight',\n",
       "              tensor([0.0996, 0.0996, 0.0962,  ..., 0.1050, 0.0991, 0.1021],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0250, -0.0087,  0.0071,  ..., -0.0095, -0.0194, -0.0057],\n",
       "                      [-0.0136,  0.0085, -0.0364,  ..., -0.0359, -0.0175,  0.0248],\n",
       "                      [-0.0152,  0.0327, -0.0249,  ..., -0.0030, -0.0212,  0.0219],\n",
       "                      ...,\n",
       "                      [-0.0474,  0.0135, -0.0228,  ..., -0.0070, -0.0304, -0.0369],\n",
       "                      [ 0.0023, -0.0102,  0.0016,  ..., -0.0015, -0.0150,  0.0137],\n",
       "                      [-0.0057, -0.0205,  0.0261,  ...,  0.0630, -0.0457,  0.0019]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0023,  0.0098, -0.0072,  ..., -0.0187, -0.0060,  0.0112],\n",
       "                      [ 0.0164, -0.0104, -0.0047,  ...,  0.0179, -0.0070, -0.0006],\n",
       "                      [ 0.0134,  0.0123,  0.0359,  ..., -0.0139,  0.0106,  0.0075],\n",
       "                      ...,\n",
       "                      [ 0.0322,  0.0483, -0.0281,  ...,  0.0293, -0.0400, -0.0070],\n",
       "                      [-0.0052, -0.0118,  0.0001,  ...,  0.0039, -0.0004, -0.0034],\n",
       "                      [ 0.0071, -0.0140, -0.0035,  ..., -0.0830, -0.0275,  0.0879]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.self_attn.v_proj.weight',\n",
       "              tensor([[-5.0659e-03,  4.3106e-04,  2.1851e-02,  ...,  3.8818e-02,\n",
       "                       -7.8125e-03,  4.6082e-03],\n",
       "                      [ 2.7924e-03,  1.5015e-02, -2.2705e-02,  ..., -9.7046e-03,\n",
       "                        5.6763e-03, -1.5625e-02],\n",
       "                      [ 7.2861e-04,  1.5015e-02, -2.2430e-03,  ..., -1.5869e-02,\n",
       "                       -2.8076e-02, -2.9297e-02],\n",
       "                      ...,\n",
       "                      [ 5.5313e-05, -2.8687e-02, -1.8066e-02,  ..., -1.2634e-02,\n",
       "                       -2.5635e-03,  2.3956e-03],\n",
       "                      [-3.5400e-02, -2.0981e-04,  1.8677e-02,  ..., -7.0190e-03,\n",
       "                       -1.0010e-02, -4.3335e-03],\n",
       "                      [-1.2207e-02,  1.6479e-02,  2.1973e-03,  ..., -5.7678e-03,\n",
       "                        1.5625e-02, -8.1787e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0002,  0.0157,  0.0085,  ...,  0.0184,  0.0027, -0.0275],\n",
       "                      [-0.0007, -0.0176,  0.0064,  ...,  0.0038, -0.0134, -0.0200],\n",
       "                      [-0.0204,  0.0036, -0.0160,  ..., -0.0194, -0.0079,  0.0103],\n",
       "                      ...,\n",
       "                      [-0.0069, -0.0278,  0.0310,  ..., -0.0058, -0.0223,  0.0204],\n",
       "                      [ 0.0197, -0.0159,  0.0349,  ...,  0.0173,  0.0018, -0.0226],\n",
       "                      [-0.0117,  0.0013,  0.0101,  ..., -0.0087,  0.0103, -0.0149]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0073,  0.0238, -0.0116,  ..., -0.0079,  0.0286,  0.0021],\n",
       "                      [ 0.0027,  0.0009, -0.0033,  ...,  0.0102, -0.0138, -0.0088],\n",
       "                      [ 0.0019, -0.0095,  0.0262,  ..., -0.0118,  0.0188,  0.0083],\n",
       "                      ...,\n",
       "                      [ 0.0300, -0.0087, -0.0128,  ..., -0.0332, -0.0410, -0.0111],\n",
       "                      [-0.0117, -0.0161, -0.0026,  ...,  0.0227, -0.0493,  0.0153],\n",
       "                      [-0.0030,  0.0129,  0.0057,  ..., -0.0128, -0.0299,  0.0068]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0023, -0.0021,  0.0139,  ..., -0.0189,  0.0059, -0.0016],\n",
       "                      [ 0.0077, -0.0175, -0.0081,  ...,  0.0028,  0.0110,  0.0087],\n",
       "                      [ 0.0282,  0.0167,  0.0299,  ..., -0.0138, -0.0067,  0.0204],\n",
       "                      ...,\n",
       "                      [-0.0080, -0.0156,  0.0018,  ..., -0.0159, -0.0062,  0.0177],\n",
       "                      [ 0.0148, -0.0038,  0.0138,  ..., -0.0023,  0.0055, -0.0256],\n",
       "                      [-0.0131, -0.0286, -0.0228,  ..., -0.0076, -0.0011, -0.0050]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0083,  0.0135,  0.0334,  ..., -0.0223, -0.0015, -0.0306],\n",
       "                      [ 0.0199, -0.0217, -0.0146,  ...,  0.0117,  0.0012, -0.0228],\n",
       "                      [ 0.0396, -0.0187, -0.0023,  ..., -0.0267,  0.0012, -0.0322],\n",
       "                      ...,\n",
       "                      [-0.0115,  0.0261, -0.0084,  ..., -0.0009, -0.0052, -0.0125],\n",
       "                      [-0.0200,  0.0344,  0.0101,  ...,  0.0115, -0.0157, -0.0112],\n",
       "                      [ 0.0334,  0.0093, -0.0214,  ...,  0.0275, -0.0398, -0.0074]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.input_layernorm.weight',\n",
       "              tensor([0.1719, 0.1758, 0.1729,  ..., 0.1768, 0.1699, 0.1729],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.2.post_attention_layernorm.weight',\n",
       "              tensor([0.1338, 0.1377, 0.1348,  ..., 0.1357, 0.1377, 0.1367],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0068,  0.0133, -0.0003,  ...,  0.0264,  0.0162,  0.0215],\n",
       "                      [-0.0087,  0.0151, -0.0197,  ..., -0.0430,  0.0008, -0.0046],\n",
       "                      [ 0.0175,  0.0060, -0.0197,  ...,  0.0123, -0.0198, -0.0337],\n",
       "                      ...,\n",
       "                      [ 0.0698, -0.0679,  0.0405,  ..., -0.0752, -0.0123,  0.0067],\n",
       "                      [-0.0796,  0.0376, -0.0029,  ...,  0.0332,  0.0723,  0.0195],\n",
       "                      [-0.0150, -0.0513,  0.0703,  ..., -0.0245, -0.0085, -0.0135]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0093, -0.0082, -0.0057,  ..., -0.0052,  0.0022,  0.0447],\n",
       "                      [-0.0107,  0.0090, -0.0228,  ...,  0.0021, -0.0099, -0.0361],\n",
       "                      [ 0.0009, -0.0089, -0.0022,  ...,  0.0064, -0.0065, -0.0315],\n",
       "                      ...,\n",
       "                      [ 0.0898, -0.0840,  0.0051,  ..., -0.0618, -0.0016,  0.0256],\n",
       "                      [-0.0864,  0.0267,  0.0435,  ...,  0.0085,  0.0474,  0.0013],\n",
       "                      [ 0.0008, -0.0510,  0.0903,  ..., -0.0175,  0.0072, -0.0104]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0011,  0.0105, -0.0049,  ..., -0.0126,  0.0035, -0.0114],\n",
       "                      [-0.0109,  0.0264, -0.0159,  ..., -0.0060, -0.0092, -0.0074],\n",
       "                      [ 0.0013, -0.0016,  0.0012,  ..., -0.0383, -0.0125, -0.0273],\n",
       "                      ...,\n",
       "                      [-0.0084, -0.0089,  0.0022,  ...,  0.0024, -0.0023, -0.0051],\n",
       "                      [ 0.0036, -0.0008,  0.0072,  ...,  0.0045, -0.0060,  0.0037],\n",
       "                      [-0.0067, -0.0084,  0.0098,  ..., -0.0012,  0.0016,  0.0098]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0292,  0.0079, -0.0168,  ...,  0.0060,  0.0083, -0.0009],\n",
       "                      [ 0.0288, -0.0283, -0.0085,  ...,  0.0074, -0.0041, -0.0006],\n",
       "                      [-0.0005, -0.0101, -0.0011,  ...,  0.0008,  0.0007,  0.0066],\n",
       "                      ...,\n",
       "                      [ 0.0071,  0.0135,  0.0137,  ...,  0.0003,  0.0037,  0.0110],\n",
       "                      [ 0.0194,  0.0233, -0.0029,  ...,  0.0016, -0.0009, -0.0017],\n",
       "                      [ 0.0099, -0.0042,  0.0052,  ...,  0.0045,  0.0040,  0.0089]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0044, -0.0126, -0.0186,  ..., -0.0304,  0.0437,  0.0109],\n",
       "                      [ 0.0146, -0.0008, -0.0123,  ..., -0.0068, -0.0225, -0.0100],\n",
       "                      [-0.0270,  0.0006, -0.0138,  ...,  0.0089, -0.0051, -0.0334],\n",
       "                      ...,\n",
       "                      [-0.0017,  0.0393,  0.0049,  ...,  0.0167, -0.0118, -0.0039],\n",
       "                      [-0.0052,  0.0061, -0.0013,  ..., -0.0024,  0.0009, -0.0086],\n",
       "                      [-0.0027, -0.0142,  0.0031,  ...,  0.0212, -0.0247,  0.0107]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.mlp.up_proj.weight',\n",
       "              tensor([[-0.0093,  0.0139,  0.0066,  ..., -0.0031,  0.0168, -0.0103],\n",
       "                      [-0.0115, -0.0140,  0.0359,  ...,  0.0183,  0.0165, -0.0066],\n",
       "                      [-0.0036, -0.0015, -0.0160,  ...,  0.0014,  0.0039, -0.0112],\n",
       "                      ...,\n",
       "                      [-0.0085, -0.0060, -0.0029,  ...,  0.0212,  0.0113, -0.0243],\n",
       "                      [ 0.0046, -0.0108, -0.0092,  ...,  0.0103, -0.0201, -0.0087],\n",
       "                      [ 0.0420,  0.0020, -0.0006,  ...,  0.0437,  0.0061,  0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.mlp.down_proj.weight',\n",
       "              tensor([[-1.2894e-03, -6.8665e-03,  6.7749e-03,  ..., -7.1411e-03,\n",
       "                        6.2561e-03,  1.8066e-02],\n",
       "                      [ 2.9053e-02,  4.5471e-03,  1.4709e-02,  ..., -1.5747e-02,\n",
       "                       -1.5747e-02, -9.1553e-03],\n",
       "                      [ 1.2878e-02,  1.1902e-02, -1.9409e-02,  ..., -1.4221e-02,\n",
       "                       -7.5684e-03,  4.8523e-03],\n",
       "                      ...,\n",
       "                      [ 2.3560e-02, -6.6528e-03, -1.5381e-02,  ..., -5.7459e-05,\n",
       "                        1.6113e-02,  1.8311e-02],\n",
       "                      [-2.0020e-02,  1.1230e-02, -1.5564e-02,  ..., -4.6730e-04,\n",
       "                       -1.4771e-02,  1.2634e-02],\n",
       "                      [ 8.5449e-03, -1.5869e-02, -2.6978e-02,  ...,  2.5177e-04,\n",
       "                        6.9275e-03,  1.1292e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.input_layernorm.weight',\n",
       "              tensor([0.2793, 0.2793, 0.2793,  ..., 0.2773, 0.2852, 0.2852],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.3.post_attention_layernorm.weight',\n",
       "              tensor([0.1738, 0.1719, 0.1680,  ..., 0.1719, 0.1699, 0.1719],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.self_attn.q_proj.weight',\n",
       "              tensor([[-1.6724e-02,  3.3264e-03, -1.9226e-03,  ..., -3.4180e-03,\n",
       "                       -1.9989e-03,  1.0559e-02],\n",
       "                      [ 2.2461e-02, -1.3428e-02, -9.2773e-03,  ...,  7.6599e-03,\n",
       "                       -2.2949e-02, -3.7537e-03],\n",
       "                      [-4.7302e-03, -2.9297e-02,  1.5381e-02,  ..., -9.8228e-05,\n",
       "                       -1.3611e-02, -2.8442e-02],\n",
       "                      ...,\n",
       "                      [ 2.1973e-02, -3.1281e-03, -4.0588e-03,  ...,  6.3181e-06,\n",
       "                        2.3438e-02, -6.2012e-02],\n",
       "                      [-2.9175e-02,  1.2329e-02,  2.6001e-02,  ..., -4.9805e-02,\n",
       "                        3.6316e-03,  3.2227e-02],\n",
       "                      [-1.1353e-02, -3.4668e-02,  6.5430e-02,  ...,  5.8838e-02,\n",
       "                        3.9062e-02, -3.8086e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.self_attn.k_proj.weight',\n",
       "              tensor([[-8.9722e-03,  1.3733e-02, -1.2756e-02,  ..., -1.8677e-02,\n",
       "                       -3.5400e-03, -8.1062e-05],\n",
       "                      [-2.7466e-02, -5.9814e-03, -4.1580e-04,  ..., -2.7771e-03,\n",
       "                        1.2207e-02,  5.4626e-03],\n",
       "                      [ 1.5625e-02,  3.1471e-04, -1.4648e-02,  ...,  1.3184e-02,\n",
       "                        1.8311e-02,  3.7842e-02],\n",
       "                      ...,\n",
       "                      [-9.0332e-03,  1.2012e-01,  5.9814e-02,  ...,  2.9297e-02,\n",
       "                       -1.8433e-02, -1.1475e-02],\n",
       "                      [ 3.1982e-02,  5.0049e-02, -1.7578e-02,  ...,  5.4688e-02,\n",
       "                       -3.9307e-02,  6.7871e-02],\n",
       "                      [ 1.0803e-02,  5.9814e-02,  1.5503e-02,  ..., -1.9836e-03,\n",
       "                        4.6387e-02, -1.6235e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0004, -0.0003, -0.0079,  ..., -0.0156, -0.0085,  0.0026],\n",
       "                      [-0.0006,  0.0242,  0.0057,  ...,  0.0102,  0.0012,  0.0093],\n",
       "                      [ 0.0151, -0.0001, -0.0352,  ...,  0.0073, -0.0045, -0.0215],\n",
       "                      ...,\n",
       "                      [-0.0015, -0.0069, -0.0209,  ..., -0.0132, -0.0061,  0.0012],\n",
       "                      [-0.0039, -0.0064, -0.0006,  ...,  0.0022, -0.0064,  0.0091],\n",
       "                      [-0.0028, -0.0195, -0.0016,  ...,  0.0061, -0.0012,  0.0151]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.self_attn.o_proj.weight',\n",
       "              tensor([[ 3.5156e-02,  9.7656e-03, -2.1210e-03,  ..., -1.1215e-03,\n",
       "                       -2.5146e-02, -4.0894e-03],\n",
       "                      [ 5.0354e-03, -7.9155e-05, -5.2490e-03,  ...,  4.2725e-03,\n",
       "                       -2.1729e-02,  3.9978e-03],\n",
       "                      [-1.1597e-03,  7.8125e-03, -1.2756e-02,  ..., -1.1230e-02,\n",
       "                        7.7209e-03, -4.0894e-03],\n",
       "                      ...,\n",
       "                      [-1.0559e-02,  3.0029e-02, -1.4648e-02,  ..., -2.4170e-02,\n",
       "                        1.9989e-03, -1.3672e-02],\n",
       "                      [-2.7924e-03, -4.3945e-03, -1.3000e-02,  ..., -7.9346e-03,\n",
       "                       -1.7395e-03, -6.0120e-03],\n",
       "                      [ 1.2329e-02,  2.3193e-02, -8.2397e-03,  ..., -3.6774e-03,\n",
       "                        1.4709e-02,  1.7700e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0044,  0.0081, -0.0115,  ..., -0.0171,  0.0012, -0.0143],\n",
       "                      [-0.0243,  0.0107,  0.0106,  ...,  0.0078,  0.0131, -0.0076],\n",
       "                      [-0.0085, -0.0201, -0.0040,  ...,  0.0022, -0.0034,  0.0272],\n",
       "                      ...,\n",
       "                      [-0.0109, -0.0041,  0.0097,  ..., -0.0259,  0.0039, -0.0040],\n",
       "                      [-0.0297,  0.0186, -0.0280,  ...,  0.0139,  0.0256, -0.0055],\n",
       "                      [ 0.0008, -0.0097, -0.0096,  ..., -0.0119,  0.0199,  0.0150]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.mlp.up_proj.weight',\n",
       "              tensor([[-0.0150, -0.0080, -0.0413,  ...,  0.0116,  0.0049, -0.0041],\n",
       "                      [-0.0210, -0.0413,  0.0066,  ..., -0.0098,  0.0122, -0.0240],\n",
       "                      [ 0.0002,  0.0304,  0.0063,  ..., -0.0386, -0.0153,  0.0049],\n",
       "                      ...,\n",
       "                      [-0.0065, -0.0305, -0.0176,  ...,  0.0067, -0.0064,  0.0493],\n",
       "                      [ 0.0118, -0.0068, -0.0151,  ..., -0.0427, -0.0077,  0.0101],\n",
       "                      [ 0.0023,  0.0221,  0.0034,  ...,  0.0014, -0.0014, -0.0048]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0261, -0.0036,  0.0245,  ..., -0.0173,  0.0032,  0.0298],\n",
       "                      [-0.0049, -0.0454,  0.0378,  ..., -0.0219, -0.0210, -0.0043],\n",
       "                      [ 0.0012, -0.0173,  0.0240,  ...,  0.0090, -0.0009,  0.0189],\n",
       "                      ...,\n",
       "                      [-0.0273, -0.0043, -0.0167,  ...,  0.0203,  0.0019,  0.0116],\n",
       "                      [-0.0166,  0.0075, -0.0262,  ...,  0.0150, -0.0339, -0.0052],\n",
       "                      [ 0.0172, -0.0282, -0.0156,  ..., -0.0034, -0.0454, -0.0259]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.input_layernorm.weight',\n",
       "              tensor([0.2617, 0.2559, 0.2598,  ..., 0.2539, 0.2617, 0.2695],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.4.post_attention_layernorm.weight',\n",
       "              tensor([0.1875, 0.1836, 0.1807,  ..., 0.1855, 0.1836, 0.1846],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0067, -0.0053, -0.0166,  ..., -0.0106, -0.0212,  0.0113],\n",
       "                      [-0.0232,  0.0157,  0.0396,  ..., -0.0115, -0.0156, -0.0339],\n",
       "                      [ 0.0094,  0.0076, -0.0100,  ...,  0.0253, -0.0081,  0.0085],\n",
       "                      ...,\n",
       "                      [-0.0093,  0.0067, -0.0033,  ...,  0.0476,  0.0040,  0.0322],\n",
       "                      [ 0.0352,  0.0137,  0.0182,  ..., -0.0408, -0.0354, -0.0310],\n",
       "                      [ 0.0110,  0.0219,  0.0339,  ..., -0.0305, -0.0176,  0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0126,  0.0300,  0.0045,  ...,  0.0160, -0.0030, -0.0055],\n",
       "                      [ 0.0022, -0.0015, -0.0297,  ...,  0.0197, -0.0060,  0.0356],\n",
       "                      [-0.0107, -0.0156,  0.0017,  ...,  0.0036, -0.0126,  0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0058,  0.0007, -0.0228,  ...,  0.0369,  0.0067, -0.0306],\n",
       "                      [ 0.0537,  0.0183, -0.0188,  ..., -0.0011, -0.0131, -0.0356],\n",
       "                      [-0.0287, -0.0215,  0.0413,  ..., -0.0184, -0.0339,  0.0075]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0354,  0.0048,  0.0160,  ...,  0.0171,  0.0019,  0.0183],\n",
       "                      [ 0.0070,  0.0148, -0.0247,  ..., -0.0137,  0.0014, -0.0094],\n",
       "                      [ 0.0179,  0.0047,  0.0007,  ..., -0.0189,  0.0271,  0.0035],\n",
       "                      ...,\n",
       "                      [ 0.0237,  0.0125,  0.0037,  ...,  0.0376, -0.0082, -0.0004],\n",
       "                      [-0.0049,  0.0012, -0.0054,  ...,  0.0003, -0.0002, -0.0023],\n",
       "                      [-0.0086, -0.0079,  0.0187,  ..., -0.0070, -0.0050, -0.0223]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0125, -0.0071,  0.0027,  ..., -0.0165, -0.0112,  0.0244],\n",
       "                      [ 0.0014,  0.0009, -0.0135,  ..., -0.0182,  0.0017, -0.0007],\n",
       "                      [-0.0079, -0.0073,  0.0120,  ...,  0.0056,  0.0036,  0.0226],\n",
       "                      ...,\n",
       "                      [ 0.0168,  0.0059, -0.0046,  ...,  0.0166,  0.0347, -0.0225],\n",
       "                      [ 0.0070, -0.0043, -0.0013,  ..., -0.0155, -0.0072, -0.0032],\n",
       "                      [ 0.0055,  0.0032, -0.0131,  ...,  0.0073, -0.0082, -0.0198]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.mlp.gate_proj.weight',\n",
       "              tensor([[ 8.1253e-04,  1.9531e-03, -2.9053e-02,  ..., -8.8120e-04,\n",
       "                       -7.0496e-03, -1.6235e-02],\n",
       "                      [ 2.0142e-02, -1.3977e-02, -1.0559e-02,  ...,  3.0151e-02,\n",
       "                        8.0109e-04, -6.9427e-04],\n",
       "                      [ 1.0132e-02,  4.4556e-03, -1.5625e-02,  ...,  1.8433e-02,\n",
       "                        1.7944e-02, -2.9175e-02],\n",
       "                      ...,\n",
       "                      [ 2.2430e-03,  5.9204e-03,  4.1016e-02,  ..., -6.9336e-02,\n",
       "                        1.3611e-02, -2.1729e-02],\n",
       "                      [-2.9907e-02,  2.7313e-03,  1.3000e-02,  ...,  4.6082e-03,\n",
       "                        1.8188e-02, -6.9580e-03],\n",
       "                      [ 2.8687e-02, -3.4668e-02, -1.2512e-02,  ..., -5.1270e-03,\n",
       "                        8.6784e-05, -4.0527e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.mlp.up_proj.weight',\n",
       "              tensor([[-4.3335e-03, -1.5076e-02, -4.6997e-03,  ..., -1.1841e-02,\n",
       "                       -3.7079e-03, -1.5503e-02],\n",
       "                      [-3.2227e-02, -4.1809e-03, -2.7618e-03,  ...,  7.0190e-03,\n",
       "                        2.2095e-02,  2.1973e-02],\n",
       "                      [-4.1485e-05,  1.1169e-02,  6.7139e-03,  ...,  3.1891e-03,\n",
       "                        1.2146e-02,  1.7334e-02],\n",
       "                      ...,\n",
       "                      [ 7.1335e-04,  2.6855e-02, -5.8594e-03,  ..., -1.3000e-02,\n",
       "                        7.7515e-03,  2.8809e-02],\n",
       "                      [ 4.5654e-02, -2.4902e-02, -2.4780e-02,  ..., -2.3651e-03,\n",
       "                       -1.6479e-02,  2.3682e-02],\n",
       "                      [ 8.8501e-03,  2.4658e-02,  5.1575e-03,  ..., -8.7891e-03,\n",
       "                       -2.2125e-03, -2.0874e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.mlp.down_proj.weight',\n",
       "              tensor([[-0.0057, -0.0281, -0.0032,  ..., -0.0026,  0.0062,  0.0126],\n",
       "                      [-0.0148,  0.0084,  0.0229,  ...,  0.0225, -0.0045, -0.0312],\n",
       "                      [ 0.0166,  0.0227,  0.0120,  ..., -0.0308, -0.0444,  0.0083],\n",
       "                      ...,\n",
       "                      [ 0.0101, -0.0233, -0.0018,  ..., -0.0035, -0.0239, -0.0210],\n",
       "                      [-0.0071, -0.0359,  0.0150,  ...,  0.0153,  0.0071, -0.0513],\n",
       "                      [ 0.0156,  0.0119, -0.0177,  ...,  0.0325,  0.0089, -0.0134]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.input_layernorm.weight',\n",
       "              tensor([0.2617, 0.2617, 0.2578,  ..., 0.2500, 0.2656, 0.2676],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.5.post_attention_layernorm.weight',\n",
       "              tensor([0.2021, 0.1914, 0.1875,  ..., 0.2041, 0.1973, 0.2012],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0049, -0.0038,  0.0046,  ..., -0.0304, -0.0031, -0.0046],\n",
       "                      [-0.0015, -0.0166,  0.0104,  ...,  0.0136, -0.0027, -0.0289],\n",
       "                      [-0.0149,  0.0203, -0.0344,  ...,  0.0266, -0.0410, -0.0057],\n",
       "                      ...,\n",
       "                      [ 0.0164,  0.0105, -0.0344,  ...,  0.0193,  0.0078, -0.0040],\n",
       "                      [ 0.0330,  0.0369,  0.0042,  ..., -0.0117, -0.0898, -0.0209],\n",
       "                      [-0.0630, -0.0527,  0.0012,  ...,  0.0167,  0.0017,  0.0114]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0165, -0.0131,  0.0201,  ..., -0.0089,  0.0206,  0.0554],\n",
       "                      [-0.0166, -0.0069,  0.0457,  ...,  0.0103, -0.0152,  0.0123],\n",
       "                      [-0.0095, -0.0130, -0.0015,  ..., -0.0031, -0.0270, -0.0096],\n",
       "                      ...,\n",
       "                      [ 0.0170, -0.0393, -0.0161,  ...,  0.0588, -0.0006, -0.0457],\n",
       "                      [-0.0200,  0.0069, -0.0300,  ..., -0.0474, -0.0415,  0.0098],\n",
       "                      [-0.0486, -0.0267, -0.0330,  ...,  0.0148, -0.0396, -0.0247]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0107,  0.0339,  0.0042,  ...,  0.0121,  0.0043,  0.0043],\n",
       "                      [ 0.0349, -0.0004, -0.0082,  ...,  0.0037,  0.0020,  0.0013],\n",
       "                      [-0.0094, -0.0062,  0.0095,  ...,  0.0084,  0.0200,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0217, -0.0060,  0.0106,  ..., -0.0025, -0.0154,  0.0057],\n",
       "                      [-0.0220, -0.0107,  0.0029,  ..., -0.0059,  0.0008,  0.0220],\n",
       "                      [ 0.0356,  0.0053,  0.0206,  ..., -0.0216,  0.0010, -0.0029]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0146, -0.0151, -0.0369,  ..., -0.0122,  0.0035,  0.0077],\n",
       "                      [-0.0205,  0.0060,  0.0046,  ..., -0.0171,  0.0047, -0.0048],\n",
       "                      [ 0.0029, -0.0039, -0.0052,  ...,  0.0116,  0.0021,  0.0092],\n",
       "                      ...,\n",
       "                      [-0.0011, -0.0157, -0.0110,  ...,  0.0082, -0.0056,  0.0229],\n",
       "                      [-0.0237,  0.0081,  0.0010,  ...,  0.0007,  0.0058, -0.0270],\n",
       "                      [-0.0177, -0.0159,  0.0073,  ..., -0.0156,  0.0079,  0.0123]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0206,  0.0253, -0.0161,  ..., -0.0065, -0.0080, -0.0090],\n",
       "                      [ 0.0342, -0.0041, -0.0131,  ...,  0.0079,  0.0236,  0.0069],\n",
       "                      [-0.0192,  0.0222, -0.0145,  ..., -0.0234, -0.0016,  0.0112],\n",
       "                      ...,\n",
       "                      [ 0.0079, -0.0198, -0.0033,  ...,  0.0156, -0.0262, -0.0046],\n",
       "                      [ 0.0151, -0.0017, -0.0159,  ..., -0.0239, -0.0050, -0.0017],\n",
       "                      [-0.0320, -0.0498,  0.0225,  ..., -0.0393,  0.0022,  0.0197]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.mlp.up_proj.weight',\n",
       "              tensor([[-0.0085, -0.0089, -0.0150,  ...,  0.0067, -0.0040, -0.0167],\n",
       "                      [ 0.0065, -0.0430,  0.0374,  ...,  0.0041,  0.0005, -0.0089],\n",
       "                      [-0.0255, -0.0214, -0.0187,  ...,  0.0057,  0.0177,  0.0217],\n",
       "                      ...,\n",
       "                      [-0.0269,  0.0054,  0.0201,  ...,  0.0186,  0.0022,  0.0408],\n",
       "                      [ 0.0245,  0.0170, -0.0023,  ..., -0.0277,  0.0168, -0.0047],\n",
       "                      [-0.0215,  0.0203, -0.0211,  ...,  0.0060, -0.0143, -0.0476]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.mlp.down_proj.weight',\n",
       "              tensor([[-0.0090,  0.0164, -0.0029,  ...,  0.0005,  0.0125,  0.0154],\n",
       "                      [-0.0151, -0.0148,  0.0086,  ..., -0.0021, -0.0284, -0.0095],\n",
       "                      [-0.0015,  0.0148, -0.0151,  ...,  0.0255,  0.0233,  0.0151],\n",
       "                      ...,\n",
       "                      [ 0.0050, -0.0060,  0.0237,  ...,  0.0330, -0.0299, -0.0041],\n",
       "                      [ 0.0184, -0.0030,  0.0083,  ...,  0.0101, -0.0144, -0.0103],\n",
       "                      [ 0.0150, -0.0189, -0.0037,  ...,  0.0325, -0.0081,  0.0059]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.input_layernorm.weight',\n",
       "              tensor([0.3145, 0.3516, 0.3262,  ..., 0.3125, 0.3340, 0.3184],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.6.post_attention_layernorm.weight',\n",
       "              tensor([0.2148, 0.2061, 0.2021,  ..., 0.2168, 0.2109, 0.2100],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0072,  0.0035,  0.0007,  ...,  0.0016, -0.0173,  0.0004],\n",
       "                      [-0.0045,  0.0078,  0.0021,  ...,  0.0140, -0.0112, -0.0118],\n",
       "                      [ 0.0002,  0.0061, -0.0186,  ..., -0.0205, -0.0015,  0.0090],\n",
       "                      ...,\n",
       "                      [-0.0006, -0.0354, -0.0299,  ..., -0.0165,  0.0342,  0.0359],\n",
       "                      [ 0.0103,  0.0525, -0.0405,  ..., -0.0184, -0.0674, -0.0515],\n",
       "                      [ 0.0791, -0.0248, -0.0131,  ...,  0.0874,  0.0344, -0.0055]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0040, -0.0080, -0.0152,  ...,  0.0071,  0.0044, -0.0048],\n",
       "                      [-0.0037, -0.0164,  0.0025,  ..., -0.0076,  0.0036,  0.0199],\n",
       "                      [-0.0065, -0.0061,  0.0189,  ..., -0.0086, -0.0098, -0.0154],\n",
       "                      ...,\n",
       "                      [ 0.0425,  0.0092,  0.0221,  ...,  0.0364,  0.0371,  0.0258],\n",
       "                      [ 0.0417,  0.0537, -0.0020,  ..., -0.0306, -0.0332, -0.0018],\n",
       "                      [ 0.0342,  0.0078,  0.0128,  ...,  0.0080,  0.0069, -0.0291]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0152,  0.0024,  0.0152,  ...,  0.0264,  0.0042, -0.0129],\n",
       "                      [ 0.0289, -0.0299, -0.0021,  ...,  0.0190, -0.0190, -0.0219],\n",
       "                      [-0.0067, -0.0041, -0.0052,  ..., -0.0146,  0.0065,  0.0031],\n",
       "                      ...,\n",
       "                      [-0.0003, -0.0408,  0.0034,  ...,  0.0032, -0.0056, -0.0079],\n",
       "                      [-0.0315,  0.0156, -0.0269,  ..., -0.0126,  0.0115, -0.0010],\n",
       "                      [-0.0082, -0.0017, -0.0070,  ...,  0.0233,  0.0079,  0.0126]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0142, -0.0161, -0.0047,  ...,  0.0063, -0.0237,  0.0021],\n",
       "                      [-0.0015,  0.0228,  0.0119,  ..., -0.0410, -0.0128, -0.0049],\n",
       "                      [ 0.0054,  0.0052, -0.0201,  ..., -0.0090, -0.0112, -0.0070],\n",
       "                      ...,\n",
       "                      [-0.0188, -0.0266, -0.0060,  ...,  0.0228, -0.0190, -0.0002],\n",
       "                      [-0.0203,  0.0075,  0.0115,  ...,  0.0074, -0.0120,  0.0143],\n",
       "                      [ 0.0001,  0.0044,  0.0009,  ...,  0.0050, -0.0142,  0.0052]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0053, -0.0019, -0.0151,  ..., -0.0062, -0.0305, -0.0043],\n",
       "                      [-0.0295,  0.0197, -0.0073,  ..., -0.0010, -0.0087, -0.0064],\n",
       "                      [ 0.0170, -0.0320, -0.0082,  ...,  0.0302, -0.0183,  0.0037],\n",
       "                      ...,\n",
       "                      [-0.0053,  0.0051, -0.0112,  ..., -0.0281, -0.0041, -0.0261],\n",
       "                      [-0.0033,  0.0184,  0.0101,  ..., -0.0053,  0.0067, -0.0315],\n",
       "                      [ 0.0203, -0.0156, -0.0035,  ...,  0.0176,  0.0200, -0.0009]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0208, -0.0009, -0.0024,  ...,  0.0154, -0.0352,  0.0067],\n",
       "                      [-0.0121, -0.0234,  0.0317,  ...,  0.0020, -0.0135,  0.0198],\n",
       "                      [ 0.0203, -0.0160,  0.0079,  ..., -0.0110,  0.0192, -0.0308],\n",
       "                      ...,\n",
       "                      [ 0.0021,  0.0013, -0.0168,  ..., -0.0046, -0.0225,  0.0020],\n",
       "                      [ 0.0017,  0.0074,  0.0165,  ..., -0.0117,  0.0204, -0.0012],\n",
       "                      [-0.0242, -0.0177,  0.0075,  ...,  0.0184,  0.0098,  0.0039]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.mlp.down_proj.weight',\n",
       "              tensor([[-0.0085, -0.0131, -0.0001,  ..., -0.0143, -0.0071, -0.0238],\n",
       "                      [-0.0032, -0.0069,  0.0194,  ..., -0.0023,  0.0038,  0.0025],\n",
       "                      [ 0.0110,  0.0157,  0.0327,  ..., -0.0166, -0.0125, -0.0166],\n",
       "                      ...,\n",
       "                      [-0.0057, -0.0010, -0.0195,  ...,  0.0171,  0.0017, -0.0275],\n",
       "                      [ 0.0430, -0.0090,  0.0039,  ..., -0.0210,  0.0005, -0.0140],\n",
       "                      [ 0.0177,  0.0193, -0.0349,  ...,  0.0024, -0.0236,  0.0002]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.input_layernorm.weight',\n",
       "              tensor([0.3203, 0.3613, 0.3359,  ..., 0.3242, 0.3535, 0.3262],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.7.post_attention_layernorm.weight',\n",
       "              tensor([0.2305, 0.2158, 0.2139,  ..., 0.2236, 0.2217, 0.2236],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0090, -0.0110, -0.0260,  ..., -0.0056,  0.0038,  0.0033],\n",
       "                      [-0.0166, -0.0154, -0.0342,  ..., -0.0024, -0.0003, -0.0031],\n",
       "                      [-0.0312, -0.0018, -0.0100,  ...,  0.0042,  0.0176, -0.0201],\n",
       "                      ...,\n",
       "                      [-0.0064,  0.0811,  0.0427,  ..., -0.0273, -0.0255, -0.0427],\n",
       "                      [ 0.0024, -0.0630, -0.0159,  ...,  0.0344,  0.0204,  0.0007],\n",
       "                      [-0.0630, -0.0557, -0.0150,  ..., -0.0469,  0.0283, -0.0155]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.self_attn.k_proj.weight',\n",
       "              tensor([[-7.6904e-03,  3.8147e-03, -5.3406e-03,  ..., -4.3640e-03,\n",
       "                        4.8523e-03,  9.1553e-03],\n",
       "                      [-6.5308e-03,  8.3618e-03, -8.5449e-03,  ...,  2.3438e-02,\n",
       "                       -2.1362e-03,  1.4099e-02],\n",
       "                      [-1.0315e-02,  3.0670e-03,  6.8054e-03,  ...,  1.7166e-03,\n",
       "                       -1.5015e-02, -1.6113e-02],\n",
       "                      ...,\n",
       "                      [ 6.0558e-05, -3.8574e-02, -6.7139e-03,  ..., -3.2715e-02,\n",
       "                       -1.6846e-02,  2.7954e-02],\n",
       "                      [ 2.0905e-03,  2.5513e-02,  2.5269e-02,  ..., -3.6377e-02,\n",
       "                        4.3945e-02, -5.8594e-02],\n",
       "                      [ 1.6479e-02, -6.2561e-03, -3.3691e-02,  ...,  2.2949e-02,\n",
       "                        8.3008e-03, -1.1475e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0242, -0.0143,  0.0044,  ..., -0.0182, -0.0111, -0.0064],\n",
       "                      [-0.0220, -0.0118,  0.0066,  ...,  0.0254,  0.0039,  0.0206],\n",
       "                      [-0.0056,  0.0164,  0.0127,  ..., -0.0037,  0.0068, -0.0204],\n",
       "                      ...,\n",
       "                      [ 0.0305, -0.0156,  0.0038,  ...,  0.0214, -0.0184,  0.0212],\n",
       "                      [ 0.0233, -0.0160,  0.0012,  ...,  0.0081, -0.0084, -0.0003],\n",
       "                      [ 0.0151, -0.0006, -0.0060,  ..., -0.0030,  0.0048, -0.0047]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.1780e-02,  1.3550e-02, -1.2573e-02,  ..., -1.0376e-02,\n",
       "                        5.1260e-05,  1.8188e-02],\n",
       "                      [ 1.1597e-02,  3.0823e-03, -3.1250e-02,  ..., -6.4392e-03,\n",
       "                       -7.9956e-03, -1.2268e-02],\n",
       "                      [ 2.8076e-03,  1.0498e-02, -4.3030e-03,  ...,  2.3651e-03,\n",
       "                        1.5198e-02,  2.1851e-02],\n",
       "                      ...,\n",
       "                      [-1.7212e-02,  2.4170e-02, -7.9956e-03,  ...,  1.3351e-03,\n",
       "                       -3.6163e-03, -4.2114e-03],\n",
       "                      [ 2.3651e-03, -1.6327e-03, -2.9602e-03,  ...,  1.9302e-03,\n",
       "                        7.9956e-03, -2.1729e-02],\n",
       "                      [-2.6855e-02,  4.9744e-03, -6.4392e-03,  ...,  2.0630e-02,\n",
       "                       -9.8267e-03, -2.9144e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0103,  0.0022, -0.0398,  ..., -0.0062, -0.0264, -0.0232],\n",
       "                      [-0.0029, -0.0425,  0.0113,  ..., -0.0009, -0.0471,  0.0228],\n",
       "                      [ 0.0177,  0.0176,  0.0232,  ...,  0.0242,  0.0201, -0.0113],\n",
       "                      ...,\n",
       "                      [-0.0120,  0.0034,  0.0049,  ...,  0.0272,  0.0137,  0.0037],\n",
       "                      [-0.0118, -0.0007,  0.0084,  ...,  0.0007, -0.0233,  0.0234],\n",
       "                      [-0.0248,  0.0052, -0.0020,  ..., -0.0149,  0.0111, -0.0013]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.mlp.up_proj.weight',\n",
       "              tensor([[-0.0092,  0.0176, -0.0155,  ...,  0.0272,  0.0075, -0.0113],\n",
       "                      [-0.0486,  0.0176, -0.0400,  ...,  0.0150, -0.0026, -0.0072],\n",
       "                      [ 0.0084,  0.0051,  0.0074,  ..., -0.0128, -0.0172,  0.0308],\n",
       "                      ...,\n",
       "                      [-0.0141,  0.0400, -0.0184,  ...,  0.0168, -0.0103,  0.0084],\n",
       "                      [ 0.0008, -0.0060, -0.0079,  ..., -0.0166,  0.0223, -0.0157],\n",
       "                      [-0.0140,  0.0166, -0.0130,  ..., -0.0153, -0.0008,  0.0033]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.mlp.down_proj.weight',\n",
       "              tensor([[-0.0108, -0.0055,  0.0195,  ...,  0.0101,  0.0120, -0.0243],\n",
       "                      [ 0.0145,  0.0121, -0.0195,  ..., -0.0058,  0.0086, -0.0229],\n",
       "                      [-0.0008, -0.0226, -0.0269,  ..., -0.0117,  0.0066, -0.0060],\n",
       "                      ...,\n",
       "                      [ 0.0132, -0.0188, -0.0347,  ...,  0.0035,  0.0051, -0.0172],\n",
       "                      [ 0.0151,  0.0030, -0.0064,  ..., -0.0035, -0.0071, -0.0165],\n",
       "                      [-0.0009,  0.0140, -0.0021,  ...,  0.0027,  0.0028,  0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.input_layernorm.weight',\n",
       "              tensor([0.3281, 0.3398, 0.3281,  ..., 0.3164, 0.3398, 0.3184],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.8.post_attention_layernorm.weight',\n",
       "              tensor([0.2354, 0.2207, 0.2129,  ..., 0.2344, 0.2275, 0.2246],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0181,  0.0034,  0.0152,  ..., -0.0068, -0.0110, -0.0148],\n",
       "                      [ 0.0022, -0.0342,  0.0146,  ...,  0.0076, -0.0082,  0.0018],\n",
       "                      [ 0.0022, -0.0027, -0.0137,  ..., -0.0227, -0.0282, -0.0225],\n",
       "                      ...,\n",
       "                      [ 0.0028, -0.0439,  0.0181,  ...,  0.0216,  0.0145, -0.0206],\n",
       "                      [ 0.0138, -0.0211,  0.0117,  ...,  0.0089,  0.0099,  0.0096],\n",
       "                      [-0.0020,  0.0540,  0.0190,  ..., -0.0562,  0.0469, -0.0045]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0347,  0.0110, -0.0147,  ...,  0.0073, -0.0203, -0.0082],\n",
       "                      [-0.0061,  0.0317,  0.0099,  ..., -0.0087,  0.0074,  0.0019],\n",
       "                      [-0.0121, -0.0146,  0.0179,  ..., -0.0045, -0.0129, -0.0039],\n",
       "                      ...,\n",
       "                      [ 0.0106, -0.0157, -0.0046,  ..., -0.0090,  0.0199,  0.0203],\n",
       "                      [ 0.0315, -0.0122,  0.0212,  ...,  0.0447, -0.0107,  0.0500],\n",
       "                      [ 0.0039, -0.0243,  0.0283,  ...,  0.0107,  0.0248, -0.0215]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.self_attn.v_proj.weight',\n",
       "              tensor([[-4.2419e-03,  9.4604e-03, -3.9291e-04,  ...,  7.1716e-03,\n",
       "                       -2.1484e-02, -1.8921e-02],\n",
       "                      [-1.5747e-02, -1.6602e-02, -1.4038e-03,  ...,  2.9053e-02,\n",
       "                       -4.0588e-03, -3.6163e-03],\n",
       "                      [ 1.0986e-03, -2.6489e-02,  2.0020e-02,  ..., -2.4292e-02,\n",
       "                        6.6223e-03, -1.2939e-02],\n",
       "                      ...,\n",
       "                      [-5.4321e-03, -1.4160e-02,  4.6692e-03,  ...,  3.4180e-03,\n",
       "                       -2.1973e-02, -9.1934e-04],\n",
       "                      [-1.5442e-02,  1.2589e-03, -9.0942e-03,  ..., -1.1169e-02,\n",
       "                       -2.6464e-05, -1.1230e-02],\n",
       "                      [ 2.4170e-02,  1.9653e-02, -9.6512e-04,  ..., -1.4832e-02,\n",
       "                       -3.3417e-03, -4.3030e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.self_attn.o_proj.weight',\n",
       "              tensor([[ 9.7046e-03,  1.0986e-02,  4.6387e-03,  ..., -9.0332e-03,\n",
       "                        9.6512e-04, -9.7275e-04],\n",
       "                      [-4.0894e-03, -1.5381e-02, -1.8311e-03,  ...,  3.0899e-04,\n",
       "                        7.6599e-03,  1.4771e-02],\n",
       "                      [-3.4180e-02, -3.0762e-02,  1.0986e-02,  ...,  2.7100e-02,\n",
       "                        7.7820e-04,  1.5320e-02],\n",
       "                      ...,\n",
       "                      [ 1.6846e-02, -1.5381e-02,  1.0803e-02,  ...,  6.8359e-03,\n",
       "                        8.9722e-03, -8.1787e-03],\n",
       "                      [-1.1110e-04,  1.8188e-02,  2.2278e-03,  ...,  1.3123e-02,\n",
       "                       -8.8692e-05, -1.4954e-03],\n",
       "                      [ 1.4099e-02,  4.9072e-02, -2.1973e-02,  ..., -2.5635e-03,\n",
       "                        6.1646e-03,  6.0425e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0070,  0.0273, -0.0054,  ..., -0.0266, -0.0089,  0.0024],\n",
       "                      [-0.0232,  0.0277,  0.0281,  ...,  0.0298,  0.0310, -0.0045],\n",
       "                      [ 0.0359,  0.0286, -0.0236,  ..., -0.0260,  0.0105, -0.0050],\n",
       "                      ...,\n",
       "                      [-0.0134, -0.0135, -0.0261,  ..., -0.0010, -0.0391,  0.0043],\n",
       "                      [-0.0116, -0.0248,  0.0239,  ..., -0.0150,  0.0028,  0.0134],\n",
       "                      [ 0.0134, -0.0173, -0.0099,  ...,  0.0126, -0.0154,  0.0083]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.mlp.up_proj.weight',\n",
       "              tensor([[-0.0126,  0.0100,  0.0123,  ..., -0.0005, -0.0073,  0.0110],\n",
       "                      [ 0.0112, -0.0022,  0.0003,  ...,  0.0152,  0.0167, -0.0198],\n",
       "                      [-0.0205, -0.0077, -0.0187,  ..., -0.0053, -0.0110, -0.0042],\n",
       "                      ...,\n",
       "                      [-0.0038,  0.0114, -0.0152,  ..., -0.0085, -0.0294, -0.0192],\n",
       "                      [ 0.0011,  0.0178, -0.0305,  ..., -0.0292, -0.0091, -0.0160],\n",
       "                      [ 0.0006, -0.0347, -0.0135,  ..., -0.0127,  0.0264,  0.0017]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.mlp.down_proj.weight',\n",
       "              tensor([[-0.0053,  0.0237, -0.0435,  ...,  0.0011, -0.0140,  0.0099],\n",
       "                      [ 0.0231, -0.0236, -0.0173,  ..., -0.0020, -0.0281, -0.0322],\n",
       "                      [-0.0005,  0.0043, -0.0011,  ..., -0.0149,  0.0005, -0.0016],\n",
       "                      ...,\n",
       "                      [-0.0095,  0.0205,  0.0017,  ..., -0.0003, -0.0299,  0.0070],\n",
       "                      [-0.0342, -0.0022,  0.0123,  ..., -0.0243, -0.0006, -0.0303],\n",
       "                      [ 0.0228,  0.0153,  0.0150,  ...,  0.0425,  0.0157,  0.0176]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.input_layernorm.weight',\n",
       "              tensor([0.3496, 0.3535, 0.3203,  ..., 0.3457, 0.3418, 0.3340],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.9.post_attention_layernorm.weight',\n",
       "              tensor([0.2402, 0.2305, 0.2197,  ..., 0.2363, 0.2344, 0.2305],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0012,  0.0065, -0.0008,  ...,  0.0098, -0.0042, -0.0087],\n",
       "                      [-0.0034, -0.0072,  0.0143,  ..., -0.0055,  0.0593,  0.0020],\n",
       "                      [-0.0092, -0.0068, -0.0047,  ...,  0.0190, -0.0148, -0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0317,  0.0635, -0.0159,  ...,  0.0295,  0.0369, -0.0092],\n",
       "                      [-0.0598,  0.0559,  0.0011,  ..., -0.0674, -0.0008, -0.0114],\n",
       "                      [-0.0498, -0.0225, -0.0036,  ..., -0.0078, -0.0302, -0.0175]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0204,  0.0118, -0.0041,  ..., -0.0092,  0.0042, -0.0031],\n",
       "                      [-0.0095,  0.0084, -0.0043,  ...,  0.0115, -0.0099,  0.0041],\n",
       "                      [-0.0042, -0.0011,  0.0107,  ..., -0.0165,  0.0223, -0.0012],\n",
       "                      ...,\n",
       "                      [-0.0449, -0.0591,  0.0066,  ...,  0.0303, -0.0786,  0.0089],\n",
       "                      [-0.0356, -0.0065,  0.0305,  ..., -0.0181, -0.0442, -0.0228],\n",
       "                      [-0.0282,  0.0273, -0.0228,  ...,  0.0146,  0.0471, -0.0101]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0403,  0.0030, -0.0067,  ...,  0.0056,  0.0096,  0.0110],\n",
       "                      [-0.0081,  0.0141,  0.0119,  ..., -0.0079, -0.0374,  0.0190],\n",
       "                      [ 0.0153, -0.0127,  0.0162,  ...,  0.0146,  0.0064, -0.0047],\n",
       "                      ...,\n",
       "                      [ 0.0219,  0.0286,  0.0010,  ...,  0.0281, -0.0060, -0.0151],\n",
       "                      [ 0.0028,  0.0067,  0.0016,  ...,  0.0008,  0.0128,  0.0017],\n",
       "                      [ 0.0133,  0.0079, -0.0141,  ..., -0.0052, -0.0129,  0.0283]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0018,  0.0022, -0.0297,  ..., -0.0012,  0.0082,  0.0076],\n",
       "                      [ 0.0004, -0.0136,  0.0128,  ...,  0.0009,  0.0131,  0.0019],\n",
       "                      [ 0.0141, -0.0356, -0.0193,  ...,  0.0090, -0.0112, -0.0056],\n",
       "                      ...,\n",
       "                      [ 0.0008,  0.0118, -0.0125,  ..., -0.0046,  0.0049,  0.0210],\n",
       "                      [ 0.0064, -0.0107, -0.0226,  ...,  0.0116,  0.0082,  0.0018],\n",
       "                      [ 0.0092, -0.0121, -0.0253,  ...,  0.0116, -0.0010,  0.0028]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0320, -0.0391, -0.0277,  ..., -0.0049, -0.0014, -0.0089],\n",
       "                      [-0.0157, -0.0150, -0.0154,  ...,  0.0016,  0.0153,  0.0165],\n",
       "                      [-0.0136, -0.0206,  0.0089,  ...,  0.0306, -0.0054,  0.0016],\n",
       "                      ...,\n",
       "                      [-0.0447, -0.0010,  0.0115,  ...,  0.0075, -0.0302,  0.0192],\n",
       "                      [ 0.0063,  0.0211,  0.0197,  ...,  0.0054, -0.0107, -0.0027],\n",
       "                      [-0.0065, -0.0143, -0.0205,  ..., -0.0118,  0.0025, -0.0141]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.mlp.up_proj.weight',\n",
       "              tensor([[-0.0425, -0.0060, -0.0383,  ...,  0.0128, -0.0089, -0.0201],\n",
       "                      [-0.0090,  0.0092,  0.0041,  ...,  0.0225,  0.0359, -0.0121],\n",
       "                      [-0.0014, -0.0027,  0.0108,  ...,  0.0232, -0.0300, -0.0024],\n",
       "                      ...,\n",
       "                      [ 0.0212,  0.0193, -0.0001,  ..., -0.0432, -0.0027, -0.0021],\n",
       "                      [-0.0035, -0.0014, -0.0280,  ...,  0.0209, -0.0142,  0.0162],\n",
       "                      [ 0.0005,  0.0138,  0.0023,  ...,  0.0006,  0.0077,  0.0167]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.mlp.down_proj.weight',\n",
       "              tensor([[-0.0105, -0.0276, -0.0043,  ...,  0.0281, -0.0135, -0.0004],\n",
       "                      [-0.0050,  0.0159, -0.0085,  ..., -0.0101,  0.0085,  0.0002],\n",
       "                      [-0.0322, -0.0007,  0.0166,  ..., -0.0234, -0.0245,  0.0086],\n",
       "                      ...,\n",
       "                      [ 0.0127, -0.0046,  0.0009,  ...,  0.0135,  0.0147,  0.0075],\n",
       "                      [-0.0028,  0.0481, -0.0176,  ...,  0.0172,  0.0308, -0.0330],\n",
       "                      [-0.0476,  0.0013, -0.0282,  ..., -0.0189, -0.0144,  0.0083]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.input_layernorm.weight',\n",
       "              tensor([0.3594, 0.3555, 0.3184,  ..., 0.3359, 0.3438, 0.3340],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.10.post_attention_layernorm.weight',\n",
       "              tensor([0.2451, 0.2295, 0.2217,  ..., 0.2402, 0.2383, 0.2354],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0159,  0.0161, -0.0087,  ...,  0.0205, -0.0062,  0.0010],\n",
       "                      [-0.0127,  0.0088,  0.0097,  ...,  0.0135,  0.0150,  0.0007],\n",
       "                      [ 0.0015,  0.0066, -0.0245,  ..., -0.0024, -0.0093,  0.0045],\n",
       "                      ...,\n",
       "                      [-0.0012, -0.0064,  0.0325,  ...,  0.0339,  0.0388, -0.0315],\n",
       "                      [ 0.0432,  0.0178, -0.0115,  ..., -0.0080,  0.0170, -0.0288],\n",
       "                      [ 0.0737, -0.0187, -0.0183,  ..., -0.0208,  0.0461,  0.0557]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0063,  0.0165, -0.0011,  ..., -0.0126,  0.0026, -0.0214],\n",
       "                      [-0.0007,  0.0012, -0.0273,  ..., -0.0064, -0.0034,  0.0248],\n",
       "                      [ 0.0133, -0.0267,  0.0069,  ...,  0.0077,  0.0043,  0.0046],\n",
       "                      ...,\n",
       "                      [-0.0071,  0.0217,  0.0020,  ..., -0.0172, -0.0127,  0.0337],\n",
       "                      [ 0.0625, -0.0107, -0.0112,  ..., -0.0020,  0.0041, -0.0052],\n",
       "                      [ 0.0065,  0.0053,  0.0170,  ..., -0.0058,  0.0596,  0.0233]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0061,  0.0030, -0.0093,  ..., -0.0194,  0.0005,  0.0068],\n",
       "                      [ 0.0016, -0.0080, -0.0102,  ...,  0.0061,  0.0056, -0.0242],\n",
       "                      [ 0.0028,  0.0053, -0.0078,  ...,  0.0146,  0.0046, -0.0220],\n",
       "                      ...,\n",
       "                      [-0.0166,  0.0059, -0.0135,  ...,  0.0068, -0.0171, -0.0040],\n",
       "                      [-0.0068,  0.0148, -0.0094,  ..., -0.0151, -0.0164,  0.0024],\n",
       "                      [ 0.0118, -0.0244,  0.0042,  ..., -0.0400, -0.0053, -0.0125]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0177, -0.0014,  0.0064,  ...,  0.0100,  0.0126,  0.0008],\n",
       "                      [ 0.0015, -0.0260,  0.0160,  ..., -0.0140,  0.0021, -0.0183],\n",
       "                      [-0.0087, -0.0041,  0.0188,  ...,  0.0044, -0.0006,  0.0200],\n",
       "                      ...,\n",
       "                      [ 0.0060, -0.0048,  0.0236,  ...,  0.0015,  0.0026, -0.0074],\n",
       "                      [ 0.0008,  0.0229,  0.0095,  ...,  0.0170,  0.0091, -0.0325],\n",
       "                      [-0.0020, -0.0037,  0.0025,  ..., -0.0053,  0.0005, -0.0142]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.mlp.gate_proj.weight',\n",
       "              tensor([[-4.4922e-02, -9.4604e-03, -3.5524e-05,  ...,  4.2114e-03,\n",
       "                       -7.7820e-03,  3.7598e-02],\n",
       "                      [ 1.2665e-03, -9.3384e-03,  2.2949e-02,  ...,  1.1536e-02,\n",
       "                       -1.6846e-02, -4.3213e-02],\n",
       "                      [-2.1515e-03, -4.3701e-02, -3.7842e-02,  ..., -1.1902e-03,\n",
       "                       -4.6692e-03,  3.5400e-03],\n",
       "                      ...,\n",
       "                      [-1.6724e-02, -3.3691e-02, -1.8188e-02,  ..., -4.6997e-03,\n",
       "                       -2.8442e-02,  1.7853e-03],\n",
       "                      [ 2.3651e-03,  5.2185e-03, -2.5391e-02,  ..., -1.1475e-02,\n",
       "                        1.1108e-02,  4.0245e-04],\n",
       "                      [-8.3618e-03,  9.3994e-03,  1.0864e-02,  ...,  3.3203e-02,\n",
       "                       -2.8687e-03, -1.8311e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.mlp.up_proj.weight',\n",
       "              tensor([[-0.0105, -0.0023, -0.0273,  ...,  0.0272,  0.0030,  0.0292],\n",
       "                      [ 0.0135,  0.0019, -0.0187,  ...,  0.0082, -0.0398,  0.0038],\n",
       "                      [-0.0094, -0.0276, -0.0020,  ..., -0.0089, -0.0178,  0.0153],\n",
       "                      ...,\n",
       "                      [ 0.0425,  0.0106, -0.0056,  ..., -0.0021,  0.0018,  0.0190],\n",
       "                      [-0.0113,  0.0081,  0.0081,  ...,  0.0194,  0.0093,  0.0078],\n",
       "                      [-0.0008, -0.0015, -0.0082,  ..., -0.0115,  0.0055,  0.0258]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.mlp.down_proj.weight',\n",
       "              tensor([[-0.0178,  0.0447,  0.0164,  ...,  0.0208,  0.0136, -0.0053],\n",
       "                      [-0.0145, -0.0060, -0.0261,  ..., -0.0284, -0.0094,  0.0148],\n",
       "                      [-0.0210, -0.0096,  0.0049,  ...,  0.0024, -0.0176, -0.0082],\n",
       "                      ...,\n",
       "                      [ 0.0189,  0.0249, -0.0125,  ...,  0.0042,  0.0018,  0.0081],\n",
       "                      [-0.0151, -0.0308, -0.0320,  ..., -0.0237, -0.0076,  0.0146],\n",
       "                      [ 0.0354,  0.0179,  0.0223,  ...,  0.0043, -0.0128,  0.0403]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.input_layernorm.weight',\n",
       "              tensor([0.3906, 0.3887, 0.3555,  ..., 0.3770, 0.3730, 0.3652],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.11.post_attention_layernorm.weight',\n",
       "              tensor([0.2500, 0.2363, 0.2324,  ..., 0.2480, 0.2471, 0.2441],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0046, -0.0189,  0.0091,  ...,  0.0167, -0.0055,  0.0062],\n",
       "                      [ 0.0029,  0.0035, -0.0010,  ..., -0.0110,  0.0253,  0.0139],\n",
       "                      [ 0.0086,  0.0344, -0.0295,  ...,  0.0008,  0.0155, -0.0312],\n",
       "                      ...,\n",
       "                      [-0.0040,  0.0376, -0.0198,  ..., -0.0293, -0.0104,  0.0135],\n",
       "                      [ 0.0344, -0.0049,  0.0115,  ...,  0.0317,  0.0056, -0.0334],\n",
       "                      [ 0.0344,  0.0136, -0.0393,  ..., -0.0187,  0.0170,  0.0244]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.0559e-02, -4.6997e-03,  3.6621e-04,  ...,  1.2878e-02,\n",
       "                        4.7913e-03, -5.9509e-03],\n",
       "                      [ 4.1809e-03,  1.8188e-02, -8.1787e-03,  ...,  1.0803e-02,\n",
       "                       -1.4893e-02, -2.0020e-02],\n",
       "                      [-8.7280e-03, -1.9897e-02,  7.6599e-03,  ...,  1.3916e-02,\n",
       "                       -9.7046e-03,  1.2634e-02],\n",
       "                      ...,\n",
       "                      [ 1.5991e-02,  2.7344e-02, -1.7578e-02,  ..., -3.8086e-02,\n",
       "                       -1.3611e-02, -1.7700e-02],\n",
       "                      [-3.3447e-02,  4.7607e-03,  2.6733e-02,  ..., -2.4170e-02,\n",
       "                        2.0508e-02,  4.5300e-06],\n",
       "                      [-2.3499e-03,  3.4668e-02,  1.0376e-02,  ...,  1.2817e-02,\n",
       "                       -4.6143e-02, -3.2471e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0058,  0.0045,  0.0053,  ..., -0.0056,  0.0079, -0.0181],\n",
       "                      [-0.0259,  0.0166, -0.0010,  ..., -0.0217, -0.0022,  0.0148],\n",
       "                      [-0.0121, -0.0105,  0.0040,  ..., -0.0023,  0.0439, -0.0129],\n",
       "                      ...,\n",
       "                      [ 0.0018, -0.0025, -0.0167,  ..., -0.0055, -0.0128, -0.0009],\n",
       "                      [ 0.0011, -0.0125, -0.0052,  ...,  0.0021,  0.0110,  0.0047],\n",
       "                      [ 0.0227,  0.0021,  0.0019,  ...,  0.0203,  0.0055, -0.0115]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.self_attn.o_proj.weight',\n",
       "              tensor([[ 2.7954e-02,  7.2327e-03, -1.0681e-03,  ...,  2.6550e-03,\n",
       "                        7.1716e-03, -5.3101e-03],\n",
       "                      [-5.5847e-03, -8.7891e-03, -2.5749e-05,  ..., -1.2756e-02,\n",
       "                        3.9368e-03, -1.7334e-02],\n",
       "                      [ 3.7079e-03, -4.9133e-03, -5.8594e-03,  ...,  5.3406e-03,\n",
       "                       -2.6001e-02,  1.1169e-02],\n",
       "                      ...,\n",
       "                      [ 2.6398e-03, -1.4420e-03,  1.5137e-02,  ..., -5.8289e-03,\n",
       "                       -5.7220e-04,  3.0823e-03],\n",
       "                      [-1.0437e-02, -3.2715e-02, -2.9785e-02,  ..., -4.7302e-03,\n",
       "                        1.0437e-02,  1.7822e-02],\n",
       "                      [ 1.8433e-02,  5.2795e-03,  1.0193e-02,  ...,  2.1606e-02,\n",
       "                       -1.1230e-02, -1.7090e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0079, -0.0249,  0.0092,  ..., -0.0049,  0.0198,  0.0058],\n",
       "                      [-0.0251, -0.0261,  0.0139,  ...,  0.0159,  0.0165,  0.0124],\n",
       "                      [ 0.0045,  0.0204, -0.0042,  ..., -0.0077, -0.0442,  0.0024],\n",
       "                      ...,\n",
       "                      [ 0.0003,  0.0085,  0.0123,  ..., -0.0248, -0.0015,  0.0209],\n",
       "                      [-0.0352, -0.0067,  0.0024,  ...,  0.0073,  0.0118, -0.0038],\n",
       "                      [ 0.0010, -0.0195, -0.0070,  ..., -0.0029, -0.0062, -0.0181]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0031,  0.0190,  0.0067,  ..., -0.0046,  0.0200,  0.0166],\n",
       "                      [-0.0029, -0.0132, -0.0123,  ...,  0.0036, -0.0114,  0.0260],\n",
       "                      [ 0.0088, -0.0036,  0.0135,  ..., -0.0449,  0.0118,  0.0022],\n",
       "                      ...,\n",
       "                      [-0.0132, -0.0347, -0.0066,  ...,  0.0151, -0.0194,  0.0038],\n",
       "                      [ 0.0032, -0.0304, -0.0048,  ...,  0.0092,  0.0092, -0.0127],\n",
       "                      [-0.0110, -0.0184, -0.0052,  ...,  0.0381, -0.0049, -0.0354]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0039, -0.0056, -0.0310,  ..., -0.0125,  0.0036,  0.0066],\n",
       "                      [ 0.0156, -0.0245,  0.0016,  ..., -0.0090, -0.0107, -0.0164],\n",
       "                      [ 0.0136,  0.0014,  0.0178,  ...,  0.0044, -0.0090, -0.0197],\n",
       "                      ...,\n",
       "                      [ 0.0156, -0.0115, -0.0459,  ...,  0.0376, -0.0176, -0.0154],\n",
       "                      [-0.0239,  0.0142,  0.0107,  ..., -0.0057,  0.0364, -0.0214],\n",
       "                      [ 0.0045,  0.0193,  0.0262,  ..., -0.0325,  0.0182, -0.0011]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.input_layernorm.weight',\n",
       "              tensor([0.3984, 0.3926, 0.3613,  ..., 0.3730, 0.3789, 0.3809],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.12.post_attention_layernorm.weight',\n",
       "              tensor([0.2578, 0.2432, 0.2363,  ..., 0.2539, 0.2520, 0.2520],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0029, -0.0096, -0.0145,  ...,  0.0065, -0.0047, -0.0078],\n",
       "                      [-0.0140, -0.0154,  0.0003,  ..., -0.0011,  0.0086, -0.0026],\n",
       "                      [-0.0175,  0.0110,  0.0074,  ..., -0.0085, -0.0058,  0.0045],\n",
       "                      ...,\n",
       "                      [ 0.0420,  0.0045,  0.0013,  ...,  0.0476,  0.0181, -0.0471],\n",
       "                      [ 0.0052,  0.0129, -0.0225,  ...,  0.0122,  0.0161, -0.0081],\n",
       "                      [-0.0071, -0.0199, -0.0162,  ..., -0.0121,  0.0204, -0.0092]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0104,  0.0043, -0.0025,  ..., -0.0210,  0.0227,  0.0154],\n",
       "                      [ 0.0089,  0.0177, -0.0047,  ...,  0.0061,  0.0071,  0.0005],\n",
       "                      [ 0.0010, -0.0093,  0.0374,  ...,  0.0010,  0.0165, -0.0322],\n",
       "                      ...,\n",
       "                      [ 0.0337,  0.0327, -0.0023,  ...,  0.0052, -0.0046, -0.0157],\n",
       "                      [-0.0036,  0.0063,  0.0280,  ..., -0.0430, -0.0154,  0.0332],\n",
       "                      [-0.0226, -0.0496, -0.0118,  ..., -0.0339, -0.0284,  0.0084]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0140,  0.0023, -0.0135,  ..., -0.0038,  0.0120,  0.0150],\n",
       "                      [-0.0090,  0.0150, -0.0045,  ...,  0.0144, -0.0071, -0.0044],\n",
       "                      [ 0.0188,  0.0013, -0.0025,  ..., -0.0229,  0.0366, -0.0117],\n",
       "                      ...,\n",
       "                      [-0.0018,  0.0216, -0.0054,  ..., -0.0073,  0.0250, -0.0188],\n",
       "                      [-0.0130,  0.0104,  0.0135,  ...,  0.0009,  0.0332,  0.0082],\n",
       "                      [-0.0045, -0.0096,  0.0253,  ...,  0.0165,  0.0008,  0.0430]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0023,  0.0005, -0.0051,  ...,  0.0014, -0.0067, -0.0038],\n",
       "                      [ 0.0232,  0.0110,  0.0050,  ..., -0.0121, -0.0078,  0.0027],\n",
       "                      [ 0.0066,  0.0077, -0.0001,  ...,  0.0060, -0.0250, -0.0101],\n",
       "                      ...,\n",
       "                      [-0.0087,  0.0102,  0.0160,  ..., -0.0103, -0.0073, -0.0159],\n",
       "                      [ 0.0032,  0.0017,  0.0295,  ..., -0.0011, -0.0253, -0.0265],\n",
       "                      [ 0.0101,  0.0056,  0.0081,  ...,  0.0128, -0.0078, -0.0223]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0194,  0.0079, -0.0091,  ...,  0.0079,  0.0041, -0.0155],\n",
       "                      [-0.0264,  0.0344,  0.0184,  ...,  0.0364, -0.0028, -0.0227],\n",
       "                      [ 0.0084, -0.0211, -0.0101,  ...,  0.0020, -0.0190, -0.0114],\n",
       "                      ...,\n",
       "                      [ 0.0200, -0.0139,  0.0145,  ..., -0.0066,  0.0187,  0.0145],\n",
       "                      [ 0.0204, -0.0028,  0.0123,  ...,  0.0129,  0.0072, -0.0247],\n",
       "                      [ 0.0042,  0.0366,  0.0212,  ..., -0.0045,  0.0204,  0.0270]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.mlp.up_proj.weight',\n",
       "              tensor([[-0.0342,  0.0228, -0.0064,  ...,  0.0048,  0.0150,  0.0002],\n",
       "                      [ 0.0153, -0.0072,  0.0334,  ...,  0.0396,  0.0286,  0.0070],\n",
       "                      [ 0.0192, -0.0276,  0.0199,  ..., -0.0012,  0.0034, -0.0063],\n",
       "                      ...,\n",
       "                      [-0.0047, -0.0003,  0.0255,  ...,  0.0090, -0.0272,  0.0155],\n",
       "                      [-0.0089, -0.0199,  0.0079,  ...,  0.0138,  0.0537,  0.0062],\n",
       "                      [ 0.0101, -0.0205,  0.0148,  ..., -0.0208, -0.0018,  0.0237]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.mlp.down_proj.weight',\n",
       "              tensor([[-0.0064, -0.0035, -0.0022,  ...,  0.0292, -0.0281, -0.0232],\n",
       "                      [-0.0087,  0.0535, -0.0471,  ..., -0.0053,  0.0032, -0.0073],\n",
       "                      [ 0.0028, -0.0039,  0.0178,  ...,  0.0067, -0.0277, -0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0140, -0.0021,  0.0272,  ...,  0.0081, -0.0186,  0.0339],\n",
       "                      [ 0.0078,  0.0022,  0.0159,  ..., -0.0102,  0.0108, -0.0119],\n",
       "                      [-0.0015, -0.0304,  0.0339,  ...,  0.0040,  0.0051,  0.0356]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.input_layernorm.weight',\n",
       "              tensor([0.4102, 0.3984, 0.3672,  ..., 0.3809, 0.3770, 0.3867],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.13.post_attention_layernorm.weight',\n",
       "              tensor([0.2598, 0.2500, 0.2432,  ..., 0.2598, 0.2637, 0.2559],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0056,  0.0013,  0.0190,  ..., -0.0055,  0.0107,  0.0312],\n",
       "                      [ 0.0046,  0.0124, -0.0100,  ..., -0.0354, -0.0183, -0.0271],\n",
       "                      [-0.0044, -0.0050,  0.0072,  ...,  0.0105,  0.0101, -0.0287],\n",
       "                      ...,\n",
       "                      [-0.0160, -0.0002, -0.0286,  ..., -0.0072,  0.0145, -0.0043],\n",
       "                      [ 0.0219, -0.0344,  0.0496,  ..., -0.0096,  0.0101, -0.0072],\n",
       "                      [-0.0128,  0.0033, -0.0128,  ...,  0.0140,  0.0133, -0.0483]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0105, -0.0030,  0.0097,  ..., -0.0074,  0.0204,  0.0413],\n",
       "                      [ 0.0261,  0.0244, -0.0226,  ..., -0.0063, -0.0190,  0.0089],\n",
       "                      [-0.0208,  0.0007,  0.0204,  ...,  0.0228,  0.0115, -0.0029],\n",
       "                      ...,\n",
       "                      [-0.0320,  0.0115,  0.0270,  ..., -0.0027, -0.0026, -0.0217],\n",
       "                      [-0.0281,  0.0044,  0.0137,  ..., -0.0356, -0.0036,  0.0013],\n",
       "                      [-0.0166,  0.0322,  0.0237,  ...,  0.0684,  0.0161, -0.0312]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0084,  0.0071, -0.0393,  ..., -0.0302, -0.0053,  0.0145],\n",
       "                      [-0.0028, -0.0195,  0.0008,  ...,  0.0045, -0.0026,  0.0006],\n",
       "                      [ 0.0253, -0.0190, -0.0002,  ..., -0.0052,  0.0096,  0.0123],\n",
       "                      ...,\n",
       "                      [ 0.0154, -0.0140,  0.0167,  ...,  0.0210, -0.0105,  0.0054],\n",
       "                      [-0.0139,  0.0249, -0.0332,  ...,  0.0039, -0.0074, -0.0302],\n",
       "                      [ 0.0089,  0.0226, -0.0223,  ..., -0.0056, -0.0066,  0.0189]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0098,  0.0065, -0.0220,  ..., -0.0020,  0.0032, -0.0131],\n",
       "                      [ 0.0011,  0.0288,  0.0065,  ...,  0.0201, -0.0090,  0.0010],\n",
       "                      [ 0.0049,  0.0045, -0.0029,  ...,  0.0020,  0.0206,  0.0116],\n",
       "                      ...,\n",
       "                      [ 0.0222, -0.0042,  0.0031,  ..., -0.0145, -0.0012,  0.0101],\n",
       "                      [-0.0037,  0.0006, -0.0194,  ..., -0.0036,  0.0356,  0.0198],\n",
       "                      [-0.0088, -0.0110, -0.0149,  ..., -0.0046,  0.0058,  0.0042]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0143,  0.0096, -0.0090,  ...,  0.0061, -0.0140, -0.0097],\n",
       "                      [ 0.0212,  0.0075, -0.0405,  ...,  0.0378,  0.0171, -0.0103],\n",
       "                      [ 0.0029,  0.0123,  0.0068,  ..., -0.0111,  0.0038, -0.0132],\n",
       "                      ...,\n",
       "                      [ 0.0061,  0.0156, -0.0195,  ..., -0.0041, -0.0060, -0.0023],\n",
       "                      [-0.0071,  0.0131,  0.0199,  ..., -0.0149,  0.0161,  0.0085],\n",
       "                      [ 0.0172, -0.0016,  0.0225,  ..., -0.0047, -0.0272,  0.0227]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0014,  0.0118,  0.0183,  ..., -0.0255,  0.0071,  0.0086],\n",
       "                      [ 0.0275,  0.0132, -0.0275,  ...,  0.0135,  0.0223,  0.0133],\n",
       "                      [-0.0004,  0.0021,  0.0322,  ..., -0.0042,  0.0175, -0.0045],\n",
       "                      ...,\n",
       "                      [-0.0164,  0.0067, -0.0474,  ..., -0.0078, -0.0075, -0.0071],\n",
       "                      [-0.0277,  0.0242, -0.0060,  ..., -0.0132,  0.0242, -0.0157],\n",
       "                      [ 0.0050,  0.0267,  0.0099,  ...,  0.0100, -0.0112,  0.0074]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0089,  0.0239,  0.0060,  ..., -0.0315, -0.0093, -0.0018],\n",
       "                      [ 0.0187,  0.0237, -0.0151,  ...,  0.0054,  0.0014,  0.0152],\n",
       "                      [ 0.0007, -0.0449,  0.0204,  ...,  0.0010, -0.0271,  0.0109],\n",
       "                      ...,\n",
       "                      [-0.0020,  0.0179, -0.0124,  ..., -0.0043, -0.0190, -0.0183],\n",
       "                      [ 0.0176,  0.0308,  0.0280,  ..., -0.0042, -0.0150,  0.0322],\n",
       "                      [-0.0189,  0.0128, -0.0132,  ..., -0.0292,  0.0121,  0.0178]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.input_layernorm.weight',\n",
       "              tensor([0.4102, 0.4199, 0.3711,  ..., 0.4023, 0.3906, 0.3867],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.14.post_attention_layernorm.weight',\n",
       "              tensor([0.2715, 0.2598, 0.2598,  ..., 0.2754, 0.2734, 0.2676],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0084, -0.0201, -0.0044,  ...,  0.0093, -0.0074,  0.0082],\n",
       "                      [ 0.0243, -0.0220,  0.0089,  ..., -0.0015,  0.0042,  0.0055],\n",
       "                      [ 0.0056, -0.0066,  0.0037,  ...,  0.0154, -0.0034, -0.0128],\n",
       "                      ...,\n",
       "                      [-0.0469, -0.0342,  0.0104,  ..., -0.0138, -0.0096,  0.0305],\n",
       "                      [-0.0171, -0.0549,  0.0192,  ...,  0.0166, -0.0063,  0.0041],\n",
       "                      [ 0.0198,  0.0194,  0.0281,  ..., -0.0008, -0.0198, -0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0201, -0.0067, -0.0099,  ..., -0.0020,  0.0073,  0.0012],\n",
       "                      [ 0.0107, -0.0082, -0.0054,  ...,  0.0022,  0.0037, -0.0155],\n",
       "                      [ 0.0027,  0.0043, -0.0035,  ..., -0.0056,  0.0128, -0.0082],\n",
       "                      ...,\n",
       "                      [-0.0405, -0.0496,  0.0255,  ...,  0.0108,  0.0086, -0.0107],\n",
       "                      [ 0.0121,  0.0045,  0.0125,  ...,  0.0286, -0.0140,  0.0168],\n",
       "                      [-0.0229, -0.0067,  0.0295,  ..., -0.0022, -0.0232, -0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0093,  0.0003,  0.0048,  ...,  0.0104,  0.0128,  0.0151],\n",
       "                      [-0.0098, -0.0082,  0.0150,  ...,  0.0173,  0.0045,  0.0245],\n",
       "                      [ 0.0059, -0.0349, -0.0065,  ..., -0.0327,  0.0049,  0.0046],\n",
       "                      ...,\n",
       "                      [-0.0060, -0.0032,  0.0146,  ...,  0.0100,  0.0210, -0.0141],\n",
       "                      [ 0.0042,  0.0046, -0.0240,  ...,  0.0136,  0.0134, -0.0063],\n",
       "                      [ 0.0239,  0.0013,  0.0166,  ..., -0.0216,  0.0037,  0.0077]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0030,  0.0076, -0.0156,  ...,  0.0140, -0.0083,  0.0232],\n",
       "                      [-0.0063, -0.0051, -0.0236,  ...,  0.0129, -0.0330,  0.0186],\n",
       "                      [ 0.0054, -0.0084, -0.0043,  ...,  0.0035, -0.0008,  0.0053],\n",
       "                      ...,\n",
       "                      [-0.0078, -0.0232,  0.0119,  ..., -0.0071, -0.0251, -0.0109],\n",
       "                      [-0.0051, -0.0063, -0.0030,  ...,  0.0126,  0.0036, -0.0035],\n",
       "                      [-0.0243,  0.0037,  0.0262,  ..., -0.0057, -0.0337, -0.0262]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0091,  0.0282, -0.0156,  ...,  0.0105, -0.0223,  0.0275],\n",
       "                      [ 0.0110, -0.0096,  0.0315,  ..., -0.0149,  0.0101,  0.0083],\n",
       "                      [ 0.0054, -0.0133, -0.0131,  ...,  0.0271, -0.0500, -0.0160],\n",
       "                      ...,\n",
       "                      [-0.0077,  0.0029,  0.0320,  ..., -0.0069,  0.0120,  0.0094],\n",
       "                      [-0.0065, -0.0143,  0.0078,  ..., -0.0085, -0.0142, -0.0364],\n",
       "                      [ 0.0005, -0.0143,  0.0013,  ..., -0.0015, -0.0023,  0.0070]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.mlp.up_proj.weight',\n",
       "              tensor([[-0.0121, -0.0116,  0.0248,  ...,  0.0184,  0.0150, -0.0476],\n",
       "                      [-0.0159,  0.0339,  0.0284,  ...,  0.0045,  0.0044,  0.0029],\n",
       "                      [-0.0437, -0.0006, -0.0088,  ..., -0.0031,  0.0155, -0.0073],\n",
       "                      ...,\n",
       "                      [-0.0126, -0.0177, -0.0068,  ...,  0.0073,  0.0039, -0.0041],\n",
       "                      [ 0.0383, -0.0081,  0.0143,  ..., -0.0063,  0.0165, -0.0014],\n",
       "                      [-0.0056, -0.0221, -0.0198,  ...,  0.0054, -0.0171, -0.0019]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.mlp.down_proj.weight',\n",
       "              tensor([[-0.0069, -0.0149, -0.0280,  ...,  0.0249,  0.0122, -0.0111],\n",
       "                      [ 0.0047, -0.0153, -0.0245,  ..., -0.0123, -0.0204,  0.0179],\n",
       "                      [ 0.0405,  0.0481, -0.0251,  ..., -0.0095,  0.0347,  0.0170],\n",
       "                      ...,\n",
       "                      [-0.0137, -0.0427,  0.0125,  ...,  0.0078, -0.0023,  0.0024],\n",
       "                      [-0.0369, -0.0179, -0.0025,  ..., -0.0277, -0.0173,  0.0036],\n",
       "                      [ 0.0033,  0.0293, -0.0069,  ..., -0.0117, -0.0286, -0.0234]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.input_layernorm.weight',\n",
       "              tensor([0.4023, 0.3965, 0.3730,  ..., 0.3789, 0.3809, 0.3828],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.15.post_attention_layernorm.weight',\n",
       "              tensor([0.2852, 0.2715, 0.2695,  ..., 0.2832, 0.2832, 0.2793],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0109,  0.0077, -0.0206,  ...,  0.0067,  0.0242, -0.0063],\n",
       "                      [ 0.0178, -0.0420,  0.0187,  ...,  0.0162,  0.0068, -0.0204],\n",
       "                      [-0.0107, -0.0007,  0.0060,  ..., -0.0219, -0.0061, -0.0229],\n",
       "                      ...,\n",
       "                      [ 0.0223, -0.0303, -0.0021,  ..., -0.0204,  0.0225, -0.0186],\n",
       "                      [-0.0132, -0.0113,  0.0267,  ...,  0.0013, -0.0208,  0.0129],\n",
       "                      [ 0.0076,  0.0173,  0.0160,  ..., -0.0051, -0.0292, -0.0124]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.self_attn.k_proj.weight',\n",
       "              tensor([[-1.6235e-02,  2.0630e-02,  6.3782e-03,  ...,  5.6152e-03,\n",
       "                        1.7944e-02,  2.1973e-02],\n",
       "                      [ 2.2705e-02, -3.4424e-02,  7.9956e-03,  ..., -3.6001e-05,\n",
       "                       -2.6245e-02, -2.8839e-03],\n",
       "                      [ 2.7222e-02, -4.9133e-03, -1.2512e-02,  ..., -2.0020e-02,\n",
       "                        2.9144e-03, -3.4180e-02],\n",
       "                      ...,\n",
       "                      [ 1.4832e-02,  1.1292e-02, -1.4038e-02,  ..., -2.4658e-02,\n",
       "                       -2.4292e-02, -5.3223e-02],\n",
       "                      [ 1.8799e-02,  9.0942e-03,  1.4954e-02,  ...,  2.3926e-02,\n",
       "                        1.1841e-02,  1.2329e-02],\n",
       "                      [-3.6865e-02,  5.0049e-02,  6.1035e-02,  ..., -7.7248e-05,\n",
       "                       -1.5747e-02,  3.0640e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.self_attn.v_proj.weight',\n",
       "              tensor([[-9.3384e-03, -4.7112e-04, -5.0354e-03,  ...,  6.0425e-03,\n",
       "                       -3.3417e-03, -6.5002e-03],\n",
       "                      [-5.6885e-02, -7.5073e-03, -3.9368e-03,  ...,  6.4087e-03,\n",
       "                        1.5320e-02, -2.6611e-02],\n",
       "                      [-2.7008e-03, -3.4943e-03, -5.7678e-03,  ..., -2.4170e-02,\n",
       "                        4.3701e-02,  1.4526e-02],\n",
       "                      ...,\n",
       "                      [ 9.2773e-03, -2.1667e-03,  1.3123e-02,  ..., -1.7090e-02,\n",
       "                        4.5471e-03,  1.7090e-02],\n",
       "                      [-2.0508e-02,  7.6294e-05,  4.4250e-03,  ...,  3.4912e-02,\n",
       "                       -2.4872e-03,  1.0986e-02],\n",
       "                      [ 1.1719e-02,  1.1108e-02,  7.5378e-03,  ..., -9.3994e-03,\n",
       "                        2.0294e-03, -2.9663e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0437, -0.0002, -0.0192,  ...,  0.0077, -0.0146,  0.0025],\n",
       "                      [-0.0231,  0.0112, -0.0223,  ..., -0.0064, -0.0013, -0.0015],\n",
       "                      [-0.0275, -0.0107, -0.0063,  ...,  0.0013,  0.0270,  0.0139],\n",
       "                      ...,\n",
       "                      [-0.0065, -0.0171,  0.0080,  ...,  0.0046, -0.0215,  0.0016],\n",
       "                      [-0.0073,  0.0251,  0.0327,  ..., -0.0029,  0.0084, -0.0154],\n",
       "                      [-0.0171,  0.0015,  0.0027,  ..., -0.0024, -0.0031, -0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0183, -0.0167, -0.0240,  ...,  0.0058, -0.0028,  0.0134],\n",
       "                      [ 0.0327,  0.0171, -0.0106,  ...,  0.0110,  0.0081,  0.0199],\n",
       "                      [-0.0074,  0.0173, -0.0044,  ...,  0.0076, -0.0029, -0.0243],\n",
       "                      ...,\n",
       "                      [-0.0037,  0.0201,  0.0009,  ...,  0.0233,  0.0234,  0.0052],\n",
       "                      [ 0.0027, -0.0085,  0.0119,  ...,  0.0171,  0.0204,  0.0211],\n",
       "                      [ 0.0320,  0.0206,  0.0352,  ..., -0.0356,  0.0234, -0.0113]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.mlp.up_proj.weight',\n",
       "              tensor([[-0.0347,  0.0107,  0.0026,  ...,  0.0187, -0.0162,  0.0061],\n",
       "                      [ 0.0147, -0.0137,  0.0098,  ...,  0.0184, -0.0240, -0.0081],\n",
       "                      [-0.0092, -0.0205, -0.0074,  ..., -0.0113,  0.0078,  0.0244],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.0262, -0.0079,  ...,  0.0030, -0.0107, -0.0184],\n",
       "                      [-0.0135,  0.0225, -0.0210,  ...,  0.0113, -0.0148,  0.0137],\n",
       "                      [ 0.0045,  0.0043, -0.0022,  ...,  0.0025, -0.0231, -0.0238]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.mlp.down_proj.weight',\n",
       "              tensor([[-0.0173, -0.0026,  0.0178,  ...,  0.0275, -0.0044,  0.0050],\n",
       "                      [-0.0234, -0.0040, -0.0126,  ...,  0.0262,  0.0012, -0.0026],\n",
       "                      [-0.0167,  0.0027, -0.0118,  ...,  0.0236, -0.0073, -0.0065],\n",
       "                      ...,\n",
       "                      [ 0.0134, -0.0237, -0.0240,  ...,  0.0109,  0.0153,  0.0008],\n",
       "                      [ 0.0150,  0.0053, -0.0102,  ..., -0.0132, -0.0123, -0.0074],\n",
       "                      [ 0.0106, -0.0149, -0.0273,  ...,  0.0273,  0.0027, -0.0137]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.input_layernorm.weight',\n",
       "              tensor([0.4062, 0.4102, 0.3828,  ..., 0.3828, 0.3984, 0.3965],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.16.post_attention_layernorm.weight',\n",
       "              tensor([0.3008, 0.2871, 0.2910,  ..., 0.3008, 0.3047, 0.2969],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0172, -0.0142,  0.0140,  ..., -0.0050, -0.0112, -0.0049],\n",
       "                      [ 0.0108, -0.0096,  0.0216,  ..., -0.0012, -0.0209,  0.0028],\n",
       "                      [-0.0131, -0.0025, -0.0114,  ...,  0.0184, -0.0079, -0.0024],\n",
       "                      ...,\n",
       "                      [ 0.0366, -0.0280,  0.0549,  ..., -0.0139, -0.0189,  0.0120],\n",
       "                      [ 0.0227, -0.0208,  0.0486,  ...,  0.0117, -0.0150, -0.0603],\n",
       "                      [ 0.0192, -0.0099,  0.0160,  ...,  0.0281,  0.0034,  0.0591]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0134,  0.0023,  0.0081,  ..., -0.0045,  0.0140,  0.0092],\n",
       "                      [ 0.0063,  0.0082,  0.0066,  ...,  0.0096, -0.0018, -0.0084],\n",
       "                      [ 0.0016, -0.0127,  0.0012,  ...,  0.0204, -0.0104, -0.0126],\n",
       "                      ...,\n",
       "                      [-0.0007, -0.0610, -0.0109,  ..., -0.0135,  0.0510, -0.0679],\n",
       "                      [ 0.0262,  0.0220,  0.0147,  ..., -0.0140, -0.0206, -0.0194],\n",
       "                      [-0.0374, -0.0457, -0.0011,  ...,  0.0327,  0.0247, -0.0057]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0120,  0.0075,  0.0059,  ...,  0.0239, -0.0092, -0.0037],\n",
       "                      [-0.0032, -0.0117, -0.0084,  ...,  0.0087, -0.0245, -0.0166],\n",
       "                      [ 0.0151,  0.0112, -0.0037,  ...,  0.0044, -0.0100, -0.0233],\n",
       "                      ...,\n",
       "                      [ 0.0094, -0.0049,  0.0144,  ...,  0.0095,  0.0067, -0.0128],\n",
       "                      [ 0.0159,  0.0049, -0.0388,  ..., -0.0220,  0.0008, -0.0298],\n",
       "                      [-0.0062, -0.0080,  0.0002,  ...,  0.0078,  0.0092,  0.0065]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0026, -0.0060,  0.0064,  ..., -0.0176,  0.0027, -0.0078],\n",
       "                      [-0.0140, -0.0221,  0.0232,  ...,  0.0273, -0.0064,  0.0197],\n",
       "                      [ 0.0120, -0.0138, -0.0052,  ..., -0.0144, -0.0024, -0.0179],\n",
       "                      ...,\n",
       "                      [-0.0094,  0.0117, -0.0170,  ..., -0.0265, -0.0278, -0.0195],\n",
       "                      [-0.0104,  0.0104, -0.0046,  ..., -0.0217, -0.0018,  0.0066],\n",
       "                      [-0.0171, -0.0096,  0.0136,  ...,  0.0304, -0.0178,  0.0150]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0359,  0.0232,  0.0179,  ...,  0.0091, -0.0060,  0.0065],\n",
       "                      [-0.0017,  0.0320,  0.0170,  ...,  0.0159, -0.0018,  0.0102],\n",
       "                      [ 0.0122,  0.0108, -0.0225,  ...,  0.0066,  0.0208, -0.0045],\n",
       "                      ...,\n",
       "                      [-0.0110, -0.0083, -0.0145,  ..., -0.0046, -0.0137, -0.0076],\n",
       "                      [ 0.0066, -0.0013, -0.0010,  ...,  0.0032,  0.0374, -0.0026],\n",
       "                      [-0.0026,  0.0211,  0.0148,  ..., -0.0132,  0.0078, -0.0022]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.mlp.up_proj.weight',\n",
       "              tensor([[-0.0084, -0.0251, -0.0070,  ...,  0.0067,  0.0167, -0.0132],\n",
       "                      [-0.0210,  0.0160,  0.0149,  ..., -0.0049, -0.0008,  0.0069],\n",
       "                      [ 0.0005, -0.0028, -0.0031,  ...,  0.0110, -0.0208,  0.0107],\n",
       "                      ...,\n",
       "                      [ 0.0117,  0.0156,  0.0065,  ...,  0.0063, -0.0142, -0.0120],\n",
       "                      [-0.0081, -0.0168, -0.0122,  ..., -0.0062, -0.0170,  0.0076],\n",
       "                      [ 0.0032,  0.0065, -0.0039,  ...,  0.0046, -0.0349, -0.0170]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0227, -0.0203,  0.0064,  ...,  0.0041, -0.0070, -0.0088],\n",
       "                      [ 0.0221,  0.0240,  0.0146,  ...,  0.0027, -0.0092,  0.0303],\n",
       "                      [-0.0258, -0.0183, -0.0128,  ...,  0.0106, -0.0116,  0.0048],\n",
       "                      ...,\n",
       "                      [ 0.0045, -0.0167, -0.0060,  ..., -0.0127, -0.0004, -0.0272],\n",
       "                      [ 0.0045,  0.0007, -0.0075,  ..., -0.0198, -0.0221,  0.0212],\n",
       "                      [ 0.0037, -0.0205,  0.0162,  ..., -0.0322, -0.0104,  0.0023]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.input_layernorm.weight',\n",
       "              tensor([0.4180, 0.4238, 0.3984,  ..., 0.4160, 0.4199, 0.3984],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.17.post_attention_layernorm.weight',\n",
       "              tensor([0.3184, 0.3086, 0.3086,  ..., 0.3184, 0.3203, 0.3125],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0089,  0.0058,  0.0020,  ..., -0.0103, -0.0405,  0.0058],\n",
       "                      [ 0.0008, -0.0075,  0.0127,  ..., -0.0056,  0.0190,  0.0425],\n",
       "                      [ 0.0026, -0.0189, -0.0156,  ...,  0.0009, -0.0095,  0.0089],\n",
       "                      ...,\n",
       "                      [-0.0286,  0.0146, -0.0089,  ...,  0.0292,  0.0038,  0.0198],\n",
       "                      [ 0.0640, -0.0175, -0.0266,  ...,  0.0566,  0.0317,  0.0076],\n",
       "                      [ 0.0266,  0.0177, -0.0036,  ...,  0.0222,  0.0430, -0.0466]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0134,  0.0258, -0.0082,  ...,  0.0179, -0.0164, -0.0256],\n",
       "                      [ 0.0082, -0.0160,  0.0096,  ...,  0.0206,  0.0266,  0.0016],\n",
       "                      [ 0.0110,  0.0010,  0.0066,  ...,  0.0356, -0.0168,  0.0227],\n",
       "                      ...,\n",
       "                      [-0.1084, -0.0178, -0.0649,  ...,  0.0244, -0.0032, -0.0442],\n",
       "                      [ 0.0466,  0.0386, -0.0117,  ...,  0.0376,  0.0659, -0.0339],\n",
       "                      [-0.0417, -0.0435, -0.0449,  ...,  0.0310, -0.0135, -0.0288]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0247,  0.0192, -0.0192,  ...,  0.0012,  0.0292,  0.0087],\n",
       "                      [ 0.0121,  0.0034, -0.0029,  ..., -0.0216,  0.0383,  0.0027],\n",
       "                      [-0.0016,  0.0108, -0.0058,  ...,  0.0078, -0.0008, -0.0097],\n",
       "                      ...,\n",
       "                      [ 0.0150,  0.0140, -0.0045,  ..., -0.0052, -0.0194, -0.0064],\n",
       "                      [ 0.0031, -0.0030,  0.0003,  ...,  0.0118,  0.0084,  0.0300],\n",
       "                      [-0.0042,  0.0056,  0.0072,  ..., -0.0057, -0.0093, -0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0178,  0.0115, -0.0108,  ...,  0.0090,  0.0273,  0.0175],\n",
       "                      [-0.0031, -0.0378,  0.0100,  ...,  0.0039, -0.0072, -0.0287],\n",
       "                      [-0.0037, -0.0187,  0.0150,  ...,  0.0255,  0.0194,  0.0298],\n",
       "                      ...,\n",
       "                      [-0.0332, -0.0156, -0.0249,  ...,  0.0159,  0.0116,  0.0024],\n",
       "                      [ 0.0206,  0.0537,  0.0139,  ...,  0.0107, -0.0050, -0.0347],\n",
       "                      [-0.0157, -0.0464, -0.0194,  ..., -0.0127,  0.0093, -0.0152]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0142, -0.0039,  0.0118,  ..., -0.0041,  0.0160,  0.0003],\n",
       "                      [-0.0391,  0.0082, -0.0167,  ..., -0.0035, -0.0141,  0.0004],\n",
       "                      [-0.0203,  0.0113,  0.0403,  ..., -0.0251, -0.0060,  0.0013],\n",
       "                      ...,\n",
       "                      [-0.0364, -0.0276, -0.0317,  ..., -0.0288, -0.0164, -0.0206],\n",
       "                      [ 0.0042,  0.0210,  0.0173,  ..., -0.0117, -0.0002,  0.0098],\n",
       "                      [-0.0004, -0.0061,  0.0095,  ...,  0.0083, -0.0051,  0.0026]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0115,  0.0106,  0.0322,  ...,  0.0036,  0.0206, -0.0100],\n",
       "                      [-0.0147,  0.0287, -0.0308,  ...,  0.0014, -0.0034,  0.0074],\n",
       "                      [-0.0264, -0.0099,  0.0074,  ...,  0.0135,  0.0143,  0.0229],\n",
       "                      ...,\n",
       "                      [-0.0038,  0.0070,  0.0081,  ...,  0.0193, -0.0027,  0.0084],\n",
       "                      [-0.0068, -0.0010, -0.0097,  ...,  0.0194,  0.0221,  0.0374],\n",
       "                      [ 0.0106,  0.0205,  0.0092,  ..., -0.0146,  0.0151,  0.0159]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0254,  0.0072, -0.0308,  ...,  0.0408,  0.0110,  0.0160],\n",
       "                      [ 0.0212,  0.0073,  0.0225,  ..., -0.0031, -0.0008, -0.0034],\n",
       "                      [ 0.0038, -0.0186, -0.0337,  ...,  0.0167, -0.0011,  0.0171],\n",
       "                      ...,\n",
       "                      [-0.0115,  0.0125,  0.0157,  ...,  0.0231, -0.0017, -0.0101],\n",
       "                      [-0.0199,  0.0022,  0.0134,  ...,  0.0098,  0.0359,  0.0011],\n",
       "                      [-0.0003,  0.0208,  0.0184,  ..., -0.0069,  0.0203,  0.0178]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.input_layernorm.weight',\n",
       "              tensor([0.4453, 0.4414, 0.4219,  ..., 0.4258, 0.4375, 0.4219],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.18.post_attention_layernorm.weight',\n",
       "              tensor([0.3379, 0.3262, 0.3262,  ..., 0.3340, 0.3379, 0.3320],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0014,  0.0074, -0.0079,  ..., -0.0157, -0.0041,  0.0118],\n",
       "                      [-0.0045, -0.0081,  0.0010,  ...,  0.0005,  0.0278, -0.0073],\n",
       "                      [ 0.0090,  0.0247,  0.0053,  ...,  0.0087,  0.0188,  0.0016],\n",
       "                      ...,\n",
       "                      [-0.0226, -0.0192, -0.0014,  ..., -0.0425, -0.0002,  0.0272],\n",
       "                      [ 0.0073,  0.0371,  0.0266,  ...,  0.0530,  0.0145, -0.0286],\n",
       "                      [-0.0137,  0.0281,  0.0522,  ..., -0.0033, -0.0405, -0.0124]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0192,  0.0116,  0.0011,  ..., -0.0075, -0.0031, -0.0142],\n",
       "                      [-0.0159,  0.0214, -0.0179,  ...,  0.0148,  0.0034,  0.0153],\n",
       "                      [-0.0197,  0.0079, -0.0187,  ..., -0.0002,  0.0142,  0.0201],\n",
       "                      ...,\n",
       "                      [ 0.0002,  0.0415, -0.0136,  ..., -0.0181,  0.0236, -0.0281],\n",
       "                      [-0.0698, -0.0299, -0.0031,  ...,  0.0215,  0.0131, -0.0107],\n",
       "                      [-0.0181,  0.0085, -0.0214,  ..., -0.0247,  0.0166,  0.0349]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0039, -0.0044, -0.0026,  ...,  0.0072, -0.0033,  0.0077],\n",
       "                      [ 0.0034,  0.0216,  0.0015,  ...,  0.0376, -0.0153, -0.0013],\n",
       "                      [-0.0442, -0.0255, -0.0038,  ..., -0.0056, -0.0053,  0.0145],\n",
       "                      ...,\n",
       "                      [-0.0003, -0.0378, -0.0043,  ...,  0.0008,  0.0066, -0.0182],\n",
       "                      [ 0.0139, -0.0042, -0.0239,  ..., -0.0101, -0.0089, -0.0039],\n",
       "                      [-0.0021,  0.0221, -0.0018,  ..., -0.0258, -0.0134,  0.0134]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0214, -0.0562,  0.0236,  ..., -0.0018,  0.0014, -0.0249],\n",
       "                      [ 0.0244, -0.0067,  0.0047,  ..., -0.0131,  0.0082, -0.0051],\n",
       "                      [-0.0049, -0.0128, -0.0271,  ...,  0.0032,  0.0082, -0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0145, -0.0071, -0.0095,  ...,  0.0107, -0.0181,  0.0121],\n",
       "                      [ 0.0077, -0.0008, -0.0108,  ..., -0.0096, -0.0172,  0.0020],\n",
       "                      [ 0.0099,  0.0194,  0.0096,  ..., -0.0192, -0.0097,  0.0126]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0123,  0.0032,  0.0096,  ...,  0.0032,  0.0033, -0.0015],\n",
       "                      [ 0.0100,  0.0206,  0.0058,  ...,  0.0057,  0.0145, -0.0359],\n",
       "                      [-0.0052, -0.0033, -0.0192,  ..., -0.0383, -0.0068,  0.0161],\n",
       "                      ...,\n",
       "                      [ 0.0013,  0.0026,  0.0043,  ..., -0.0211, -0.0056,  0.0029],\n",
       "                      [-0.0496,  0.0007, -0.0019,  ..., -0.0040, -0.0250,  0.0139],\n",
       "                      [-0.0259, -0.0234,  0.0122,  ..., -0.0021,  0.0096,  0.0022]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0008, -0.0065,  0.0054,  ...,  0.0042,  0.0110,  0.0061],\n",
       "                      [-0.0095, -0.0008, -0.0122,  ...,  0.0249, -0.0220, -0.0121],\n",
       "                      [ 0.0152,  0.0056,  0.0171,  ..., -0.0161, -0.0125,  0.0491],\n",
       "                      ...,\n",
       "                      [ 0.0125,  0.0159, -0.0089,  ...,  0.0199, -0.0129,  0.0359],\n",
       "                      [ 0.0039,  0.0240,  0.0361,  ..., -0.0140,  0.0327, -0.0153],\n",
       "                      [ 0.0025, -0.0190, -0.0107,  ..., -0.0017, -0.0014,  0.0082]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.mlp.down_proj.weight',\n",
       "              tensor([[-0.0050, -0.0172,  0.0054,  ...,  0.0040, -0.0160,  0.0155],\n",
       "                      [-0.0091, -0.0393, -0.0247,  ...,  0.0110,  0.0077, -0.0075],\n",
       "                      [-0.0227, -0.0186,  0.0234,  ...,  0.0053, -0.0045,  0.0067],\n",
       "                      ...,\n",
       "                      [-0.0076, -0.0057,  0.0184,  ...,  0.0164,  0.0178, -0.0020],\n",
       "                      [ 0.0066, -0.0148, -0.0413,  ...,  0.0283, -0.0083, -0.0144],\n",
       "                      [ 0.0081, -0.0060, -0.0266,  ...,  0.0010,  0.0068,  0.0075]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.input_layernorm.weight',\n",
       "              tensor([0.4473, 0.4531, 0.4316,  ..., 0.4219, 0.4277, 0.4316],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.19.post_attention_layernorm.weight',\n",
       "              tensor([0.3516, 0.3379, 0.3398,  ..., 0.3477, 0.3477, 0.3438],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0014, -0.0043,  0.0161,  ...,  0.0114,  0.0020, -0.0228],\n",
       "                      [ 0.0074, -0.0201,  0.0086,  ..., -0.0083, -0.0076,  0.0087],\n",
       "                      [ 0.0017, -0.0041, -0.0079,  ...,  0.0122, -0.0023, -0.0042],\n",
       "                      ...,\n",
       "                      [-0.0223, -0.0405,  0.0114,  ..., -0.0027,  0.0179,  0.0040],\n",
       "                      [-0.0718, -0.0175,  0.0130,  ...,  0.0101,  0.0208,  0.0061],\n",
       "                      [ 0.0167, -0.0005, -0.0065,  ..., -0.0154, -0.0087, -0.0028]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.self_attn.k_proj.weight',\n",
       "              tensor([[-3.0060e-03,  5.8289e-03,  6.1951e-03,  ..., -2.8992e-03,\n",
       "                        3.3722e-03,  4.6997e-03],\n",
       "                      [ 3.5706e-03,  8.2397e-03,  3.8147e-03,  ..., -9.8228e-05,\n",
       "                       -2.0142e-03,  1.1230e-02],\n",
       "                      [ 3.3569e-03,  4.4556e-03, -7.5684e-03,  ..., -5.0964e-03,\n",
       "                        5.5847e-03,  1.7822e-02],\n",
       "                      ...,\n",
       "                      [ 6.5231e-04, -2.5330e-03, -2.1851e-02,  ..., -1.8311e-02,\n",
       "                        2.3193e-02,  2.6733e-02],\n",
       "                      [-1.8188e-02,  1.8799e-02, -2.3041e-03,  ...,  2.2949e-02,\n",
       "                        1.9531e-02,  5.9570e-02],\n",
       "                      [-1.5015e-02, -2.7832e-02, -1.3977e-02,  ...,  2.6001e-02,\n",
       "                       -2.7222e-02,  4.5410e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0036, -0.0106,  0.0136,  ..., -0.0045,  0.0110,  0.0111],\n",
       "                      [-0.0076, -0.0425, -0.0006,  ..., -0.0085,  0.0015,  0.0090],\n",
       "                      [-0.0081, -0.0137,  0.0010,  ..., -0.0203, -0.0005,  0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0073, -0.0082, -0.0041,  ...,  0.0195, -0.0038,  0.0075],\n",
       "                      [ 0.0176,  0.0087, -0.0077,  ...,  0.0019,  0.0129, -0.0089],\n",
       "                      [ 0.0004, -0.0046,  0.0260,  ..., -0.0082, -0.0112, -0.0393]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0068,  0.0036,  0.0297,  ..., -0.0024,  0.0175, -0.0092],\n",
       "                      [ 0.0100, -0.0126, -0.0095,  ..., -0.0008, -0.0096,  0.0225],\n",
       "                      [ 0.0156, -0.0027, -0.0115,  ..., -0.0266,  0.0019,  0.0101],\n",
       "                      ...,\n",
       "                      [-0.0133,  0.0010,  0.0055,  ...,  0.0138, -0.0150,  0.0112],\n",
       "                      [ 0.0054,  0.0151, -0.0148,  ...,  0.0123, -0.0269,  0.0030],\n",
       "                      [ 0.0188, -0.0237, -0.0051,  ...,  0.0156, -0.0211,  0.0117]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0005, -0.0031, -0.0215,  ...,  0.0221, -0.0096,  0.0132],\n",
       "                      [-0.0171,  0.0062, -0.0081,  ..., -0.0045, -0.0104,  0.0162],\n",
       "                      [-0.0128, -0.0071,  0.0569,  ..., -0.0198,  0.0223, -0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0012, -0.0066,  0.0258,  ..., -0.0233,  0.0056, -0.0036],\n",
       "                      [ 0.0121,  0.0031,  0.0107,  ...,  0.0199,  0.0002,  0.0140],\n",
       "                      [-0.0073, -0.0231, -0.0145,  ...,  0.0029, -0.0286, -0.0047]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.mlp.up_proj.weight',\n",
       "              tensor([[-0.0581, -0.0013,  0.0004,  ...,  0.0138, -0.0107, -0.0104],\n",
       "                      [-0.0107,  0.0062, -0.0101,  ..., -0.0439, -0.0157,  0.0376],\n",
       "                      [ 0.0106,  0.0327,  0.0188,  ...,  0.0089, -0.0198,  0.0076],\n",
       "                      ...,\n",
       "                      [-0.0142,  0.0251, -0.0096,  ..., -0.0177, -0.0046,  0.0007],\n",
       "                      [ 0.0157, -0.0255, -0.0162,  ...,  0.0029, -0.0214, -0.0354],\n",
       "                      [ 0.0144, -0.0236,  0.0278,  ..., -0.0116, -0.0197, -0.0086]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.mlp.down_proj.weight',\n",
       "              tensor([[-0.0028,  0.0251, -0.0037,  ..., -0.0085, -0.0093, -0.0298],\n",
       "                      [-0.0010,  0.0173, -0.0082,  ...,  0.0090, -0.0190,  0.0231],\n",
       "                      [-0.0094,  0.0116, -0.0225,  ..., -0.0067,  0.0239, -0.0155],\n",
       "                      ...,\n",
       "                      [-0.0315,  0.0304,  0.0065,  ...,  0.0129, -0.0008, -0.0005],\n",
       "                      [-0.0108,  0.0051, -0.0150,  ..., -0.0125,  0.0149, -0.0111],\n",
       "                      [ 0.0118,  0.0025, -0.0005,  ..., -0.0007, -0.0081, -0.0056]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.input_layernorm.weight',\n",
       "              tensor([0.4473, 0.4609, 0.4316,  ..., 0.4297, 0.4316, 0.4434],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.20.post_attention_layernorm.weight',\n",
       "              tensor([0.3613, 0.3535, 0.3496,  ..., 0.3633, 0.3574, 0.3535],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.self_attn.q_proj.weight',\n",
       "              tensor([[-1.6479e-02, -4.6997e-03,  1.7090e-02,  ..., -5.5542e-03,\n",
       "                        6.2561e-03, -5.8899e-03],\n",
       "                      [ 1.4893e-02, -2.1935e-05, -5.4626e-03,  ...,  7.8125e-03,\n",
       "                       -2.4796e-05,  1.8311e-02],\n",
       "                      [-1.0864e-02, -6.1646e-03, -4.4823e-05,  ...,  2.8442e-02,\n",
       "                       -7.0801e-03,  1.0223e-03],\n",
       "                      ...,\n",
       "                      [ 1.3275e-03, -6.3171e-03,  2.8687e-02,  ..., -1.1841e-02,\n",
       "                       -5.1270e-02,  8.5449e-03],\n",
       "                      [-1.2451e-02,  1.5991e-02,  1.9409e-02,  ...,  2.4658e-02,\n",
       "                        3.5889e-02, -5.5237e-03],\n",
       "                      [-7.3730e-02, -1.3123e-02, -8.7280e-03,  ..., -8.4839e-03,\n",
       "                        1.4343e-02, -4.2236e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0067,  0.0115,  0.0045,  ...,  0.0127, -0.0012,  0.0039],\n",
       "                      [-0.0019,  0.0079,  0.0135,  ...,  0.0061,  0.0071,  0.0148],\n",
       "                      [ 0.0095,  0.0030,  0.0334,  ...,  0.0146, -0.0004,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0165,  0.0178,  0.0059,  ...,  0.0240, -0.0371,  0.0461],\n",
       "                      [ 0.0005,  0.0039, -0.0119,  ..., -0.0147,  0.0278, -0.0073],\n",
       "                      [-0.0229,  0.0053, -0.0188,  ..., -0.0004, -0.0187,  0.0625]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0010,  0.0084, -0.0036,  ..., -0.0043,  0.0195, -0.0237],\n",
       "                      [-0.0265,  0.0262, -0.0071,  ...,  0.0210, -0.0211,  0.0265],\n",
       "                      [ 0.0220,  0.0029, -0.0049,  ..., -0.0209,  0.0214, -0.0082],\n",
       "                      ...,\n",
       "                      [-0.0073, -0.0232,  0.0325,  ...,  0.0014, -0.0002, -0.0170],\n",
       "                      [-0.0161, -0.0125,  0.0137,  ...,  0.0079, -0.0018,  0.0189],\n",
       "                      [-0.0095,  0.0188, -0.0142,  ...,  0.0052, -0.0146,  0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0133, -0.0006,  0.0116,  ..., -0.0107,  0.0251, -0.0027],\n",
       "                      [-0.0050,  0.0238, -0.0112,  ..., -0.0281,  0.0091, -0.0052],\n",
       "                      [-0.0175, -0.0126,  0.0063,  ...,  0.0255,  0.0297,  0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0088, -0.0085, -0.0128,  ...,  0.0215,  0.0079,  0.0190],\n",
       "                      [ 0.0223, -0.0075,  0.0131,  ..., -0.0100,  0.0086, -0.0164],\n",
       "                      [-0.0206,  0.0154, -0.0133,  ...,  0.0327, -0.0193,  0.0027]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.mlp.gate_proj.weight',\n",
       "              tensor([[-2.2736e-03, -2.0874e-02,  1.2085e-02,  ..., -3.2715e-02,\n",
       "                        4.0039e-02,  1.6357e-02],\n",
       "                      [ 1.2939e-02,  9.9487e-03,  9.3384e-03,  ..., -3.6865e-02,\n",
       "                        5.1758e-02,  6.5308e-03],\n",
       "                      [ 1.2451e-02,  6.1035e-03,  2.1667e-03,  ..., -4.1016e-02,\n",
       "                       -3.9673e-03, -1.9409e-02],\n",
       "                      ...,\n",
       "                      [ 1.1475e-02, -2.6367e-02, -2.8076e-02,  ...,  3.3936e-02,\n",
       "                       -2.5024e-02,  1.3885e-03],\n",
       "                      [-3.2196e-03,  2.9663e-02,  8.1177e-03,  ...,  9.3384e-03,\n",
       "                       -2.1851e-02,  5.9509e-03],\n",
       "                      [ 1.8082e-03, -9.2773e-03, -2.0752e-03,  ..., -1.6235e-02,\n",
       "                        2.5630e-05,  6.8054e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0240, -0.0297, -0.0168,  ...,  0.0223,  0.0369, -0.0137],\n",
       "                      [-0.0234, -0.0147,  0.0002,  ...,  0.0352,  0.0432,  0.0041],\n",
       "                      [ 0.0160, -0.0045, -0.0009,  ...,  0.0026, -0.0115, -0.0075],\n",
       "                      ...,\n",
       "                      [-0.0449, -0.0031,  0.0170,  ...,  0.0139, -0.0081, -0.0090],\n",
       "                      [-0.0017, -0.0114,  0.0066,  ..., -0.0234, -0.0055,  0.0031],\n",
       "                      [-0.0076, -0.0254, -0.0173,  ..., -0.0076, -0.0066, -0.0327]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0111,  0.0160, -0.0037,  ...,  0.0160,  0.0203,  0.0082],\n",
       "                      [ 0.0164, -0.0044, -0.0139,  ...,  0.0023, -0.0128, -0.0135],\n",
       "                      [-0.0081, -0.0016,  0.0096,  ..., -0.0012,  0.0339,  0.0209],\n",
       "                      ...,\n",
       "                      [-0.0116,  0.0378,  0.0229,  ..., -0.0025,  0.0010,  0.0060],\n",
       "                      [ 0.0160, -0.0010, -0.0031,  ..., -0.0491, -0.0147,  0.0107],\n",
       "                      [ 0.0071,  0.0040,  0.0217,  ..., -0.0135,  0.0188, -0.0212]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.input_layernorm.weight',\n",
       "              tensor([0.4727, 0.4805, 0.4629,  ..., 0.4512, 0.4668, 0.4727],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.21.post_attention_layernorm.weight',\n",
       "              tensor([0.3711, 0.3652, 0.3613,  ..., 0.3770, 0.3691, 0.3711],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0214, -0.0143, -0.0102,  ...,  0.0435, -0.0374,  0.0295],\n",
       "                      [-0.0398, -0.0155, -0.0018,  ..., -0.0405,  0.0123, -0.0413],\n",
       "                      [-0.0303,  0.0233, -0.0140,  ...,  0.0225,  0.0123, -0.0223],\n",
       "                      ...,\n",
       "                      [ 0.0123,  0.0141,  0.0093,  ...,  0.0128,  0.0547, -0.0050],\n",
       "                      [-0.0261, -0.0042, -0.0261,  ...,  0.0466,  0.0040, -0.0211],\n",
       "                      [ 0.0295, -0.0128,  0.0090,  ..., -0.0515,  0.0128,  0.0262]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.self_attn.k_proj.weight',\n",
       "              tensor([[-1.8921e-02, -9.9182e-04, -7.4463e-03,  ...,  1.4526e-02,\n",
       "                       -1.3733e-02, -1.7456e-02],\n",
       "                      [ 5.4016e-03, -1.3672e-02,  2.9663e-02,  ..., -2.3315e-02,\n",
       "                        3.5156e-02, -1.6602e-02],\n",
       "                      [-1.8311e-02,  3.3203e-02, -4.2725e-02,  ...,  5.8105e-02,\n",
       "                       -8.1787e-03, -8.5449e-03],\n",
       "                      ...,\n",
       "                      [ 1.9775e-02,  7.8735e-03, -2.6489e-02,  ..., -2.1606e-02,\n",
       "                       -1.3672e-02,  1.7700e-02],\n",
       "                      [-1.1963e-02, -4.5898e-02, -8.1177e-03,  ..., -3.5048e-05,\n",
       "                       -6.8054e-03,  1.7834e-04],\n",
       "                      [ 2.7466e-02,  5.6458e-03,  3.1982e-02,  ...,  3.0151e-02,\n",
       "                       -3.4912e-02,  1.5198e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0195,  0.0051, -0.0026,  ..., -0.0449, -0.0248,  0.0223],\n",
       "                      [-0.0309, -0.0143,  0.0104,  ..., -0.0239, -0.0349, -0.0092],\n",
       "                      [ 0.0260, -0.0240, -0.0082,  ...,  0.0483,  0.0015, -0.0147],\n",
       "                      ...,\n",
       "                      [-0.0081, -0.0442, -0.0258,  ...,  0.0146, -0.0051,  0.0153],\n",
       "                      [-0.0359, -0.0160,  0.0250,  ..., -0.0152,  0.0079, -0.0091],\n",
       "                      [-0.0022,  0.0210, -0.0381,  ...,  0.0056,  0.0003,  0.0231]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0233,  0.0162, -0.0201,  ..., -0.0030, -0.0273,  0.0067],\n",
       "                      [ 0.0339,  0.0137,  0.0048,  ...,  0.0035, -0.0126, -0.0183],\n",
       "                      [ 0.0151,  0.0049, -0.0087,  ..., -0.0283,  0.0083, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0005, -0.0138, -0.0107,  ..., -0.0156, -0.0184,  0.0021],\n",
       "                      [ 0.0217, -0.0089, -0.0198,  ..., -0.0173, -0.0096,  0.0127],\n",
       "                      [ 0.0022, -0.0236,  0.0253,  ...,  0.0098,  0.0159,  0.0168]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0186, -0.0153,  0.0258,  ...,  0.0128,  0.0229, -0.0339],\n",
       "                      [ 0.0089, -0.0042,  0.0021,  ...,  0.0011,  0.0026,  0.0179],\n",
       "                      [ 0.0075, -0.0130,  0.0063,  ...,  0.0120,  0.0100, -0.0058],\n",
       "                      ...,\n",
       "                      [-0.0040, -0.0015,  0.0038,  ..., -0.0132,  0.0057, -0.0041],\n",
       "                      [-0.0008,  0.0237, -0.0262,  ..., -0.0012,  0.0151, -0.0215],\n",
       "                      [-0.0096,  0.0061,  0.0031,  ..., -0.0093,  0.0210,  0.0161]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0303,  0.0204,  0.0036,  ...,  0.0103,  0.0315, -0.0038],\n",
       "                      [ 0.0088,  0.0074,  0.0176,  ..., -0.0139, -0.0058,  0.0020],\n",
       "                      [ 0.0050, -0.0302, -0.0405,  ...,  0.0237,  0.0079,  0.0055],\n",
       "                      ...,\n",
       "                      [-0.0084, -0.0006, -0.0205,  ..., -0.0018,  0.0061,  0.0046],\n",
       "                      [ 0.0167, -0.0129, -0.0209,  ..., -0.0317,  0.0088, -0.0164],\n",
       "                      [-0.0034,  0.0011, -0.0040,  ..., -0.0420, -0.0557,  0.0144]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0145, -0.0117,  0.0126,  ..., -0.0142,  0.0052, -0.0415],\n",
       "                      [ 0.0030,  0.0201, -0.0093,  ..., -0.0104,  0.0118, -0.0264],\n",
       "                      [-0.0060, -0.0082,  0.0076,  ...,  0.0167,  0.0145,  0.0209],\n",
       "                      ...,\n",
       "                      [ 0.0258, -0.0057, -0.0469,  ..., -0.0067, -0.0171, -0.0208],\n",
       "                      [-0.0134, -0.0471,  0.0103,  ...,  0.0188, -0.0253,  0.0327],\n",
       "                      [ 0.0094, -0.0005, -0.0344,  ..., -0.0042, -0.0162,  0.0090]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.input_layernorm.weight',\n",
       "              tensor([0.4805, 0.4805, 0.4707,  ..., 0.4629, 0.4824, 0.4824],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.22.post_attention_layernorm.weight',\n",
       "              tensor([0.3809, 0.3789, 0.3809,  ..., 0.3887, 0.3828, 0.3867],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0025, -0.0149,  0.0007,  ..., -0.0171, -0.0005,  0.0012],\n",
       "                      [ 0.0090, -0.0020,  0.0101,  ..., -0.0171, -0.0034,  0.0089],\n",
       "                      [-0.0079,  0.0039,  0.0194,  ..., -0.0024, -0.0145,  0.0077],\n",
       "                      ...,\n",
       "                      [-0.0199,  0.0608,  0.0124,  ..., -0.0378, -0.0598, -0.0280],\n",
       "                      [-0.0352, -0.0337,  0.0227,  ..., -0.0166, -0.0039,  0.0095],\n",
       "                      [ 0.0106, -0.0583, -0.0038,  ...,  0.0139,  0.0391, -0.0093]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0014,  0.0048,  0.0023,  ..., -0.0006, -0.0172,  0.0027],\n",
       "                      [-0.0175, -0.0011, -0.0131,  ...,  0.0164,  0.0142, -0.0032],\n",
       "                      [ 0.0091,  0.0060, -0.0140,  ...,  0.0176,  0.0113, -0.0081],\n",
       "                      ...,\n",
       "                      [ 0.0065,  0.0309,  0.0525,  ..., -0.0293, -0.0136, -0.0190],\n",
       "                      [-0.0004, -0.0306, -0.0114,  ..., -0.0204, -0.0220,  0.0143],\n",
       "                      [-0.0164, -0.0081, -0.0232,  ...,  0.0049,  0.0027, -0.0142]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0041,  0.0349, -0.0042,  ..., -0.0118, -0.0093, -0.0161],\n",
       "                      [-0.0005, -0.0120, -0.0177,  ...,  0.0047, -0.0110,  0.0037],\n",
       "                      [ 0.0077, -0.0145, -0.0258,  ...,  0.0137, -0.0457, -0.0291],\n",
       "                      ...,\n",
       "                      [ 0.0098,  0.0082,  0.0070,  ..., -0.0010,  0.0299,  0.0120],\n",
       "                      [ 0.0203,  0.0273,  0.0073,  ..., -0.0010, -0.0089, -0.0126],\n",
       "                      [-0.0184, -0.0128, -0.0554,  ...,  0.0092,  0.0095, -0.0315]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0062, -0.0050, -0.0194,  ..., -0.0177,  0.0135, -0.0073],\n",
       "                      [-0.0103, -0.0165, -0.0022,  ..., -0.0024,  0.0119, -0.0028],\n",
       "                      [ 0.0164,  0.0036,  0.0310,  ..., -0.0142,  0.0060, -0.0215],\n",
       "                      ...,\n",
       "                      [ 0.0352, -0.0063,  0.0364,  ..., -0.0198,  0.0097,  0.0014],\n",
       "                      [-0.0069,  0.0223, -0.0135,  ..., -0.0039, -0.0141, -0.0052],\n",
       "                      [ 0.0117, -0.0060, -0.0115,  ..., -0.0066, -0.0024, -0.0293]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.mlp.gate_proj.weight',\n",
       "              tensor([[-8.7280e-03,  2.7466e-02, -3.8338e-04,  ...,  7.4387e-04,\n",
       "                        3.5889e-02,  1.2573e-02],\n",
       "                      [-9.3384e-03,  4.5654e-02, -1.2329e-02,  ...,  5.0659e-03,\n",
       "                       -2.0142e-02,  1.7944e-02],\n",
       "                      [-3.1128e-02,  1.9897e-02,  4.3945e-03,  ...,  1.9409e-02,\n",
       "                       -5.8350e-02,  2.2583e-02],\n",
       "                      ...,\n",
       "                      [-6.9580e-03,  8.8501e-03, -1.2875e-04,  ..., -9.4604e-03,\n",
       "                        1.0742e-02,  5.2795e-03],\n",
       "                      [-2.4658e-02,  3.2715e-02, -1.1444e-03,  ...,  1.5793e-03,\n",
       "                        1.4771e-02, -2.5146e-02],\n",
       "                      [ 8.7619e-06,  1.0742e-02,  1.3184e-02,  ..., -6.6223e-03,\n",
       "                        4.8218e-03, -1.3855e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0294,  0.0124,  0.0320,  ...,  0.0054, -0.0278, -0.0103],\n",
       "                      [ 0.0014,  0.0044, -0.0144,  ..., -0.0038, -0.0147,  0.0109],\n",
       "                      [-0.0166,  0.0042, -0.0251,  ...,  0.0172,  0.0103, -0.0056],\n",
       "                      ...,\n",
       "                      [-0.0105, -0.0150,  0.0164,  ...,  0.0070,  0.0400,  0.0070],\n",
       "                      [-0.0024,  0.0116, -0.0393,  ...,  0.0142,  0.0045,  0.0152],\n",
       "                      [ 0.0062,  0.0112, -0.0036,  ..., -0.0017, -0.0193,  0.0205]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.mlp.down_proj.weight',\n",
       "              tensor([[ 1.9789e-05,  8.9722e-03, -1.9409e-02,  ..., -3.3936e-02,\n",
       "                       -1.4099e-02,  3.0518e-02],\n",
       "                      [ 9.7656e-03,  1.8188e-02, -5.9814e-03,  ...,  2.4780e-02,\n",
       "                        2.7100e-02,  1.1597e-03],\n",
       "                      [-3.7231e-03, -1.3062e-02, -2.0264e-02,  ...,  6.8359e-03,\n",
       "                        3.5156e-02, -1.2451e-02],\n",
       "                      ...,\n",
       "                      [ 7.9956e-03, -5.4016e-03,  1.0254e-02,  ...,  1.5869e-02,\n",
       "                       -2.8419e-04, -1.7822e-02],\n",
       "                      [ 2.8076e-02, -8.1177e-03, -6.9809e-04,  ..., -4.7913e-03,\n",
       "                        5.9204e-03,  1.8597e-04],\n",
       "                      [-2.6703e-03, -3.1494e-02, -1.4648e-02,  ..., -9.1553e-03,\n",
       "                        1.1108e-02, -8.3618e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.input_layernorm.weight',\n",
       "              tensor([0.5039, 0.5156, 0.5039,  ..., 0.5000, 0.5156, 0.5195],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.23.post_attention_layernorm.weight',\n",
       "              tensor([0.3984, 0.3906, 0.3906,  ..., 0.3945, 0.3984, 0.4023],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0201, -0.0143,  0.0117,  ...,  0.0378, -0.0184,  0.0008],\n",
       "                      [ 0.0229,  0.0110,  0.0056,  ..., -0.0112,  0.0146, -0.0063],\n",
       "                      [-0.0043, -0.0070, -0.0219,  ...,  0.0082, -0.0104,  0.0172],\n",
       "                      ...,\n",
       "                      [-0.0062,  0.0021, -0.0201,  ...,  0.0121, -0.0015, -0.0061],\n",
       "                      [-0.0211,  0.0107,  0.0075,  ...,  0.0210, -0.0381,  0.0022],\n",
       "                      [-0.0209, -0.0371, -0.0181,  ..., -0.0096,  0.0264,  0.0654]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0226,  0.0020, -0.0091,  ...,  0.0101, -0.0054,  0.0025],\n",
       "                      [ 0.0109, -0.0198,  0.0093,  ..., -0.0254,  0.0200,  0.0090],\n",
       "                      [ 0.0090,  0.0058, -0.0444,  ...,  0.0159,  0.0155, -0.0139],\n",
       "                      ...,\n",
       "                      [ 0.0289,  0.0058,  0.0393,  ...,  0.0116,  0.0195, -0.0447],\n",
       "                      [-0.0045,  0.0221,  0.0199,  ...,  0.0344, -0.0170, -0.0003],\n",
       "                      [-0.0229, -0.0175, -0.0050,  ...,  0.0042, -0.0055,  0.0598]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.self_attn.v_proj.weight',\n",
       "              tensor([[ 2.6489e-02, -1.9653e-02, -4.2114e-03,  ..., -3.5248e-03,\n",
       "                       -2.5787e-03, -3.7079e-03],\n",
       "                      [ 2.8442e-02, -2.2095e-02, -1.1658e-02,  ..., -6.2012e-02,\n",
       "                       -4.0771e-02, -1.7212e-02],\n",
       "                      [-5.2002e-02, -1.8921e-02, -1.5625e-02,  ..., -1.0010e-02,\n",
       "                       -1.4160e-02,  8.2016e-04],\n",
       "                      ...,\n",
       "                      [-7.1049e-05, -1.7944e-02,  1.3855e-02,  ...,  2.4292e-02,\n",
       "                       -3.9673e-03,  2.5024e-02],\n",
       "                      [-4.9133e-03, -1.2085e-02, -1.4587e-02,  ...,  3.0518e-02,\n",
       "                       -4.8523e-03,  8.1787e-03],\n",
       "                      [ 1.2756e-02,  1.9287e-02,  2.3438e-02,  ...,  2.0752e-02,\n",
       "                       -1.5442e-02, -9.1553e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0107,  0.0166,  0.0262,  ...,  0.0064,  0.0017, -0.0177],\n",
       "                      [-0.0122,  0.0287,  0.0125,  ...,  0.0162,  0.0267, -0.0272],\n",
       "                      [-0.0095, -0.0022,  0.0108,  ..., -0.0245,  0.0014, -0.0082],\n",
       "                      ...,\n",
       "                      [-0.0027,  0.0170,  0.0154,  ..., -0.0064, -0.0071,  0.0019],\n",
       "                      [ 0.0518,  0.0043, -0.0071,  ...,  0.0016, -0.0190, -0.0061],\n",
       "                      [ 0.0052,  0.0153,  0.0090,  ...,  0.0056, -0.0206,  0.0144]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0299, -0.0137,  0.0119,  ..., -0.0106, -0.0018, -0.0009],\n",
       "                      [-0.0005, -0.0128, -0.0253,  ...,  0.0109,  0.0003, -0.0159],\n",
       "                      [ 0.0004, -0.0344,  0.0242,  ..., -0.0085, -0.0457, -0.0186],\n",
       "                      ...,\n",
       "                      [-0.0020, -0.0282,  0.0056,  ..., -0.0031,  0.0129, -0.0061],\n",
       "                      [ 0.0172,  0.0165,  0.0061,  ...,  0.0075, -0.0004, -0.0031],\n",
       "                      [-0.0347,  0.0291, -0.0006,  ...,  0.0332, -0.0186, -0.0452]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0023, -0.0025,  0.0216,  ...,  0.0249, -0.0019,  0.0275],\n",
       "                      [-0.0035,  0.0152, -0.0262,  ..., -0.0098,  0.0012, -0.0204],\n",
       "                      [ 0.0136, -0.0474,  0.0157,  ...,  0.0410,  0.0181, -0.0327],\n",
       "                      ...,\n",
       "                      [ 0.0157, -0.0291, -0.0073,  ..., -0.0014,  0.0161,  0.0136],\n",
       "                      [ 0.0339, -0.0080,  0.0146,  ...,  0.0047,  0.0128, -0.0088],\n",
       "                      [-0.0012, -0.0160,  0.0239,  ...,  0.0253,  0.0349,  0.0197]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.mlp.down_proj.weight',\n",
       "              tensor([[-4.8218e-03, -1.2695e-02, -3.2715e-02,  ..., -7.2956e-05,\n",
       "                        4.2114e-03, -1.9653e-02],\n",
       "                      [ 2.7588e-02, -6.9885e-03,  1.6357e-02,  ..., -3.1433e-03,\n",
       "                        7.9956e-03, -4.6875e-02],\n",
       "                      [ 2.0264e-02,  1.7700e-02, -3.1738e-02,  ...,  1.3580e-03,\n",
       "                        2.1973e-02,  6.5002e-03],\n",
       "                      ...,\n",
       "                      [-7.1411e-03,  9.2773e-03, -1.8799e-02,  ..., -3.3447e-02,\n",
       "                       -1.2573e-02, -2.3193e-02],\n",
       "                      [ 1.0376e-02,  2.5513e-02,  6.1035e-03,  ...,  2.2949e-02,\n",
       "                       -2.6123e-02,  1.8066e-02],\n",
       "                      [-2.1851e-02, -7.3853e-03,  5.4321e-03,  ...,  1.2268e-02,\n",
       "                       -6.9885e-03, -3.1494e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.input_layernorm.weight',\n",
       "              tensor([0.4902, 0.5156, 0.5039,  ..., 0.4824, 0.5078, 0.5000],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.24.post_attention_layernorm.weight',\n",
       "              tensor([0.4082, 0.4043, 0.4043,  ..., 0.4082, 0.4121, 0.4043],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0049, -0.0138, -0.0195,  ...,  0.0048, -0.0156,  0.0018],\n",
       "                      [-0.0118,  0.0021,  0.0127,  ..., -0.0148,  0.0001, -0.0073],\n",
       "                      [-0.0023,  0.0069,  0.0012,  ..., -0.0045, -0.0178, -0.0182],\n",
       "                      ...,\n",
       "                      [ 0.0444,  0.0469,  0.0049,  ...,  0.0019,  0.0146, -0.0265],\n",
       "                      [-0.0554, -0.0564, -0.0068,  ..., -0.0156, -0.0248,  0.0047],\n",
       "                      [ 0.0393, -0.0408, -0.0219,  ...,  0.0026,  0.0181, -0.0317]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0110, -0.0142, -0.0035,  ...,  0.0006,  0.0033, -0.0029],\n",
       "                      [-0.0179, -0.0090,  0.0065,  ..., -0.0041, -0.0225,  0.0088],\n",
       "                      [-0.0101,  0.0015, -0.0010,  ...,  0.0065, -0.0089,  0.0075],\n",
       "                      ...,\n",
       "                      [ 0.0055,  0.0513, -0.0130,  ..., -0.0571, -0.0542,  0.0327],\n",
       "                      [ 0.0366, -0.0381,  0.0011,  ..., -0.0025,  0.0074,  0.0149],\n",
       "                      [-0.0086, -0.0208,  0.0021,  ...,  0.0364,  0.0095, -0.0205]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0107,  0.0105,  0.0046,  ...,  0.0243, -0.0175, -0.0120],\n",
       "                      [-0.0048, -0.0071,  0.0042,  ...,  0.0070, -0.0327, -0.0084],\n",
       "                      [-0.0144,  0.0087, -0.0172,  ..., -0.0096, -0.0048, -0.0046],\n",
       "                      ...,\n",
       "                      [-0.0166, -0.0293, -0.0082,  ...,  0.0146,  0.0064,  0.0106],\n",
       "                      [-0.0137,  0.0011,  0.0192,  ..., -0.0033,  0.0084, -0.0208],\n",
       "                      [-0.0330, -0.0137, -0.0217,  ...,  0.0026,  0.0187,  0.0229]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0154,  0.0156,  0.0028,  ...,  0.0143,  0.0029,  0.0129],\n",
       "                      [-0.0240, -0.0189, -0.0187,  ...,  0.0250,  0.0037, -0.0045],\n",
       "                      [-0.0059, -0.0105,  0.0182,  ..., -0.0118, -0.0153,  0.0005],\n",
       "                      ...,\n",
       "                      [-0.0170,  0.0124, -0.0042,  ...,  0.0141,  0.0124,  0.0261],\n",
       "                      [-0.0060, -0.0240, -0.0278,  ..., -0.0212, -0.0106,  0.0081],\n",
       "                      [-0.0160, -0.0073, -0.0142,  ..., -0.0413, -0.0240, -0.0145]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0026, -0.0371,  0.0403,  ..., -0.0048, -0.0099,  0.0125],\n",
       "                      [-0.0206, -0.0014, -0.0087,  ...,  0.0222,  0.0359,  0.0148],\n",
       "                      [-0.0098,  0.0242,  0.0074,  ...,  0.0132,  0.0079,  0.0258],\n",
       "                      ...,\n",
       "                      [-0.0425,  0.0030,  0.0116,  ..., -0.0023, -0.0205,  0.0064],\n",
       "                      [ 0.0204, -0.0034,  0.0574,  ...,  0.0249,  0.0219,  0.0066],\n",
       "                      [-0.0101, -0.0027, -0.0117,  ...,  0.0466,  0.0028, -0.0058]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.mlp.up_proj.weight',\n",
       "              tensor([[-0.0325, -0.0061,  0.0166,  ...,  0.0075,  0.0121, -0.0184],\n",
       "                      [-0.0176,  0.0018,  0.0110,  ..., -0.0049,  0.0128, -0.0004],\n",
       "                      [ 0.0272,  0.0007,  0.0315,  ..., -0.0159, -0.0220, -0.0046],\n",
       "                      ...,\n",
       "                      [ 0.0076,  0.0056,  0.0405,  ...,  0.0066, -0.0283, -0.0199],\n",
       "                      [-0.0294,  0.0126, -0.0325,  ..., -0.0413, -0.0266,  0.0019],\n",
       "                      [-0.0148, -0.0028,  0.0283,  ...,  0.0219,  0.0077,  0.0219]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0291, -0.0089,  0.0131,  ...,  0.0176, -0.0037,  0.0309],\n",
       "                      [-0.0181,  0.0018,  0.0004,  ...,  0.0147, -0.0029,  0.0239],\n",
       "                      [ 0.0159,  0.0109, -0.0017,  ..., -0.0184,  0.0076, -0.0164],\n",
       "                      ...,\n",
       "                      [ 0.0131, -0.0194, -0.0266,  ..., -0.0447,  0.0025,  0.0234],\n",
       "                      [ 0.0030, -0.0052,  0.0048,  ...,  0.0045,  0.0038,  0.0003],\n",
       "                      [-0.0085, -0.0083,  0.0112,  ..., -0.0339, -0.0036, -0.0065]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.input_layernorm.weight',\n",
       "              tensor([0.5391, 0.5508, 0.5391,  ..., 0.5508, 0.5547, 0.5430],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.25.post_attention_layernorm.weight',\n",
       "              tensor([0.4121, 0.4121, 0.4141,  ..., 0.4219, 0.4219, 0.4199],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0339, -0.0047, -0.0074,  ..., -0.0064,  0.0374, -0.0008],\n",
       "                      [-0.0085, -0.0145,  0.0103,  ..., -0.0023, -0.0226,  0.0092],\n",
       "                      [-0.0017,  0.0048,  0.0100,  ...,  0.0033, -0.0400, -0.0170],\n",
       "                      ...,\n",
       "                      [-0.0015, -0.0099, -0.0067,  ...,  0.0137,  0.0200,  0.0026],\n",
       "                      [ 0.0027, -0.0228, -0.0282,  ...,  0.0183, -0.0073, -0.0092],\n",
       "                      [-0.0145,  0.0535,  0.0264,  ..., -0.0099, -0.0591, -0.0457]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0251,  0.0182, -0.0116,  ..., -0.0222,  0.0349, -0.0003],\n",
       "                      [-0.0254,  0.0103,  0.0140,  ..., -0.0022,  0.0208,  0.0298],\n",
       "                      [ 0.0135,  0.0267, -0.0052,  ..., -0.0244, -0.0608, -0.0055],\n",
       "                      ...,\n",
       "                      [ 0.0236,  0.0203,  0.0021,  ..., -0.0435, -0.0520,  0.0243],\n",
       "                      [-0.0317, -0.0334, -0.0094,  ...,  0.0454,  0.0237, -0.0033],\n",
       "                      [-0.0138,  0.0206, -0.0139,  ..., -0.0007, -0.0204, -0.0097]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0574, -0.0232, -0.0258,  ..., -0.0135,  0.0075, -0.0142],\n",
       "                      [-0.0540, -0.0210,  0.0250,  ...,  0.0050, -0.0083,  0.0062],\n",
       "                      [ 0.0024,  0.0043, -0.0265,  ..., -0.0058, -0.0046, -0.0297],\n",
       "                      ...,\n",
       "                      [-0.0220, -0.0166,  0.0034,  ...,  0.0459, -0.0039,  0.0084],\n",
       "                      [ 0.0068, -0.0339, -0.0037,  ..., -0.0167, -0.0186, -0.0002],\n",
       "                      [ 0.0062,  0.0226, -0.0112,  ...,  0.0096, -0.0003,  0.0187]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0120,  0.0027, -0.0297,  ..., -0.0023,  0.0132,  0.0298],\n",
       "                      [ 0.0087,  0.0500,  0.0075,  ..., -0.0056,  0.0471, -0.0068],\n",
       "                      [-0.0073,  0.0143, -0.0226,  ..., -0.0164,  0.0053, -0.0024],\n",
       "                      ...,\n",
       "                      [ 0.0299,  0.0030,  0.0120,  ...,  0.0007,  0.0120,  0.0038],\n",
       "                      [-0.0133,  0.0095, -0.0089,  ...,  0.0123, -0.0045, -0.0452],\n",
       "                      [ 0.0157,  0.0012,  0.0054,  ..., -0.0222,  0.0212, -0.0195]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.mlp.gate_proj.weight',\n",
       "              tensor([[ 1.2329e-02, -5.7678e-03,  1.4587e-02,  ...,  2.6733e-02,\n",
       "                        2.7344e-02, -3.9062e-02],\n",
       "                      [-8.6670e-03,  1.0803e-02, -1.8433e-02,  ..., -2.7710e-02,\n",
       "                       -2.5146e-02,  1.9409e-02],\n",
       "                      [ 4.4556e-03,  8.8501e-04, -1.0610e-05,  ...,  6.7139e-03,\n",
       "                       -9.7046e-03, -1.9287e-02],\n",
       "                      ...,\n",
       "                      [-2.8839e-03,  3.6865e-02, -9.8267e-03,  ..., -6.7139e-04,\n",
       "                       -3.4912e-02, -1.9043e-02],\n",
       "                      [-1.7944e-02,  1.1597e-02,  2.4658e-02,  ...,  1.8188e-02,\n",
       "                        1.6724e-02,  5.7068e-03],\n",
       "                      [ 1.7090e-02,  1.1635e-04, -6.4392e-03,  ..., -2.4902e-02,\n",
       "                       -2.6093e-03, -4.6082e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.mlp.up_proj.weight',\n",
       "              tensor([[-0.0145,  0.0408, -0.0031,  ...,  0.0129, -0.0079, -0.0068],\n",
       "                      [-0.0356, -0.0134,  0.0024,  ...,  0.0289, -0.0011,  0.0107],\n",
       "                      [ 0.0040, -0.0121, -0.0276,  ...,  0.0139, -0.0157, -0.0077],\n",
       "                      ...,\n",
       "                      [ 0.0072, -0.0264,  0.0048,  ...,  0.0107,  0.0354, -0.0110],\n",
       "                      [-0.0430, -0.0012,  0.0544,  ...,  0.0281,  0.0014,  0.0045],\n",
       "                      [ 0.0028, -0.0173,  0.0182,  ..., -0.0131,  0.0094, -0.0061]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.mlp.down_proj.weight',\n",
       "              tensor([[-0.0098, -0.0116, -0.0132,  ...,  0.0289,  0.0142,  0.0039],\n",
       "                      [ 0.0139,  0.0107,  0.0043,  ...,  0.0574,  0.0193, -0.0260],\n",
       "                      [-0.0192,  0.0121, -0.0249,  ..., -0.0120,  0.0203,  0.0109],\n",
       "                      ...,\n",
       "                      [-0.0146, -0.0077, -0.0137,  ...,  0.0095,  0.0381,  0.0010],\n",
       "                      [-0.0160, -0.0150, -0.0245,  ...,  0.0101,  0.0089,  0.0181],\n",
       "                      [ 0.0066, -0.0057, -0.0084,  ...,  0.0137,  0.0166,  0.0108]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.input_layernorm.weight',\n",
       "              tensor([0.5117, 0.5312, 0.5273,  ..., 0.5117, 0.5352, 0.5273],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.26.post_attention_layernorm.weight',\n",
       "              tensor([0.4316, 0.4258, 0.4297,  ..., 0.4395, 0.4375, 0.4355],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0306,  0.0199,  0.0045,  ..., -0.0018,  0.0130,  0.0388],\n",
       "                      [-0.0193,  0.0408,  0.0229,  ..., -0.0199,  0.0029,  0.0050],\n",
       "                      [-0.0140, -0.0248,  0.0064,  ...,  0.0030, -0.0245,  0.0349],\n",
       "                      ...,\n",
       "                      [-0.0547,  0.0297, -0.0075,  ...,  0.0208, -0.0178, -0.0199],\n",
       "                      [ 0.0019,  0.0317,  0.0157,  ...,  0.0079,  0.0026,  0.0233],\n",
       "                      [-0.0232, -0.0132,  0.0084,  ...,  0.0195,  0.0153, -0.0064]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0043, -0.0048,  0.0073,  ..., -0.0033, -0.0160,  0.0112],\n",
       "                      [-0.0017,  0.0056,  0.0172,  ...,  0.0432, -0.0093, -0.0087],\n",
       "                      [ 0.0084, -0.0171,  0.0050,  ...,  0.0006, -0.0055, -0.0040],\n",
       "                      ...,\n",
       "                      [-0.0060, -0.0137,  0.0349,  ...,  0.0085, -0.0251, -0.0002],\n",
       "                      [ 0.0270, -0.0193, -0.0430,  ...,  0.0608, -0.0099,  0.0474],\n",
       "                      [ 0.0303,  0.0156, -0.0049,  ..., -0.0315, -0.0376,  0.0103]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0212, -0.0068, -0.0364,  ...,  0.0320,  0.0356, -0.0123],\n",
       "                      [ 0.0188,  0.0161,  0.0095,  ...,  0.0028, -0.0177,  0.0156],\n",
       "                      [-0.0275,  0.0029,  0.0219,  ..., -0.0128,  0.0476, -0.0271],\n",
       "                      ...,\n",
       "                      [ 0.0066, -0.0190, -0.0221,  ...,  0.0222, -0.0131, -0.0110],\n",
       "                      [ 0.0002, -0.0217, -0.0271,  ..., -0.0133, -0.0092, -0.0156],\n",
       "                      [ 0.0205, -0.0253, -0.0197,  ..., -0.0273, -0.0293,  0.0195]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0041, -0.0076,  0.0048,  ...,  0.0315,  0.0102, -0.0266],\n",
       "                      [ 0.0041,  0.0334, -0.0199,  ..., -0.0105, -0.0109,  0.0009],\n",
       "                      [-0.0042, -0.0151, -0.0176,  ...,  0.0170,  0.0070, -0.0048],\n",
       "                      ...,\n",
       "                      [ 0.0238, -0.0078, -0.0101,  ...,  0.0135,  0.0464, -0.0012],\n",
       "                      [ 0.0332, -0.0222, -0.0101,  ..., -0.0017, -0.0045,  0.0238],\n",
       "                      [ 0.0410,  0.0176,  0.0270,  ..., -0.0123,  0.0017, -0.0195]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0369,  0.0153, -0.0136,  ...,  0.0128,  0.0029, -0.0166],\n",
       "                      [-0.0212,  0.0062, -0.0182,  ...,  0.0304,  0.0315,  0.0327],\n",
       "                      [ 0.0106,  0.0143, -0.0084,  ..., -0.0160, -0.0247,  0.0281],\n",
       "                      ...,\n",
       "                      [ 0.0286,  0.0173,  0.0029,  ..., -0.0077,  0.0176, -0.0078],\n",
       "                      [-0.0187,  0.0305, -0.0258,  ..., -0.0019, -0.0128,  0.0091],\n",
       "                      [ 0.0072, -0.0198, -0.0109,  ..., -0.0063, -0.0173, -0.0030]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.mlp.up_proj.weight',\n",
       "              tensor([[-0.0057,  0.0167,  0.0286,  ..., -0.0012,  0.0116, -0.0371],\n",
       "                      [-0.0165,  0.0029, -0.0253,  ...,  0.0172, -0.0154, -0.0229],\n",
       "                      [ 0.0271,  0.0094,  0.0420,  ...,  0.0294, -0.0056, -0.0148],\n",
       "                      ...,\n",
       "                      [-0.0079,  0.0266,  0.0284,  ..., -0.0183,  0.0187, -0.0371],\n",
       "                      [ 0.0135,  0.0294,  0.0110,  ...,  0.0153, -0.0159, -0.0132],\n",
       "                      [-0.0045, -0.0096, -0.0092,  ..., -0.0064, -0.0194,  0.0010]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0082, -0.0211,  0.0320,  ..., -0.0178,  0.0157, -0.0003],\n",
       "                      [-0.0049,  0.0106, -0.0143,  ..., -0.0184, -0.0249,  0.0057],\n",
       "                      [ 0.0315,  0.0106,  0.0069,  ..., -0.0309,  0.0046,  0.0281],\n",
       "                      ...,\n",
       "                      [-0.0052,  0.0118,  0.0118,  ...,  0.0121, -0.0493,  0.0306],\n",
       "                      [-0.0217, -0.0059, -0.0132,  ...,  0.0175, -0.0016,  0.0228],\n",
       "                      [ 0.0056,  0.0150, -0.0042,  ...,  0.0259, -0.0067,  0.0040]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.input_layernorm.weight',\n",
       "              tensor([0.5391, 0.5430, 0.5430,  ..., 0.5430, 0.5469, 0.5508],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.27.post_attention_layernorm.weight',\n",
       "              tensor([0.4492, 0.4414, 0.4395,  ..., 0.4473, 0.4531, 0.4434],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0010, -0.0183,  0.0080,  ..., -0.0103, -0.0117, -0.0040],\n",
       "                      [ 0.0147, -0.0040, -0.0064,  ...,  0.0123,  0.0021, -0.0011],\n",
       "                      [ 0.0130,  0.0046, -0.0061,  ..., -0.0013, -0.0217,  0.0203],\n",
       "                      ...,\n",
       "                      [-0.0031,  0.0298, -0.0522,  ...,  0.0104,  0.0002, -0.0430],\n",
       "                      [ 0.0161, -0.0325, -0.0162,  ...,  0.0253,  0.0413, -0.0610],\n",
       "                      [ 0.0087,  0.0140, -0.0483,  ...,  0.0471,  0.0226, -0.0332]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0042,  0.0009,  0.0131,  ..., -0.0037,  0.0095, -0.0217],\n",
       "                      [-0.0137,  0.0043, -0.0217,  ..., -0.0129,  0.0027,  0.0086],\n",
       "                      [ 0.0047,  0.0071, -0.0175,  ..., -0.0179, -0.0077, -0.0031],\n",
       "                      ...,\n",
       "                      [-0.0320, -0.0120,  0.0024,  ..., -0.0095, -0.0476, -0.0019],\n",
       "                      [ 0.0432, -0.0216,  0.0145,  ...,  0.0142,  0.0386,  0.0322],\n",
       "                      [ 0.0175, -0.0118,  0.0042,  ..., -0.0033, -0.0209, -0.0127]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0256,  0.0089,  0.0261,  ..., -0.0190, -0.0269,  0.0184],\n",
       "                      [ 0.0200, -0.0123, -0.0099,  ..., -0.0471,  0.0322, -0.0121],\n",
       "                      [-0.0047,  0.0182, -0.0043,  ...,  0.0046, -0.0420,  0.0128],\n",
       "                      ...,\n",
       "                      [ 0.0276,  0.0069, -0.0056,  ...,  0.0156, -0.0023, -0.0231],\n",
       "                      [ 0.0439,  0.0095,  0.0430,  ...,  0.0072,  0.0117, -0.0048],\n",
       "                      [ 0.0134,  0.0151, -0.0461,  ...,  0.0291,  0.0032,  0.0023]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0087,  0.0244, -0.0464,  ..., -0.0437,  0.0198,  0.0030],\n",
       "                      [-0.0170, -0.0220,  0.0016,  ..., -0.0115,  0.0211,  0.0327],\n",
       "                      [ 0.0282,  0.0128, -0.0562,  ...,  0.0253,  0.0264, -0.0164],\n",
       "                      ...,\n",
       "                      [ 0.0003, -0.0099,  0.0317,  ..., -0.0131, -0.0413, -0.0055],\n",
       "                      [-0.0098, -0.0547, -0.0157,  ...,  0.0108, -0.0254, -0.0291],\n",
       "                      [-0.0481, -0.0049, -0.0123,  ..., -0.0228, -0.0025,  0.0049]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0023,  0.0046, -0.0031,  ...,  0.0289, -0.0104,  0.0347],\n",
       "                      [-0.0232,  0.0203, -0.0121,  ...,  0.0092,  0.0071,  0.0266],\n",
       "                      [ 0.0156,  0.0317,  0.0226,  ...,  0.0221,  0.0014,  0.0091],\n",
       "                      ...,\n",
       "                      [-0.0107,  0.0299,  0.0096,  ...,  0.0166,  0.0130, -0.0077],\n",
       "                      [ 0.0101, -0.0317, -0.0146,  ..., -0.0160, -0.0125,  0.0038],\n",
       "                      [-0.0303,  0.0171,  0.0128,  ..., -0.0080, -0.0138,  0.0184]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0210,  0.0062,  0.0242,  ...,  0.0289,  0.0193,  0.0167],\n",
       "                      [-0.0137, -0.0229, -0.0028,  ..., -0.0157, -0.0303, -0.0243],\n",
       "                      [ 0.0131,  0.0148, -0.0050,  ...,  0.0083,  0.0022, -0.0256],\n",
       "                      ...,\n",
       "                      [-0.0147, -0.0043,  0.0269,  ..., -0.0042,  0.0027,  0.0026],\n",
       "                      [-0.0248,  0.0040, -0.0006,  ...,  0.0114, -0.0045, -0.0486],\n",
       "                      [-0.0096, -0.0250, -0.0036,  ..., -0.0077,  0.0053, -0.0061]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.mlp.down_proj.weight',\n",
       "              tensor([[ 8.3618e-03,  3.8086e-02, -1.0620e-02,  ...,  7.9346e-03,\n",
       "                       -9.8877e-03, -2.4536e-02],\n",
       "                      [ 9.5367e-04, -3.0518e-02,  1.8066e-02,  ..., -3.4180e-02,\n",
       "                       -5.7373e-03, -2.6123e-02],\n",
       "                      [-1.5198e-02,  2.5513e-02, -8.4229e-03,  ...,  5.7678e-03,\n",
       "                        1.5259e-03,  4.0054e-05],\n",
       "                      ...,\n",
       "                      [ 6.8970e-03, -1.3123e-02,  2.3956e-03,  ..., -2.2461e-02,\n",
       "                       -2.7466e-02,  1.1902e-02],\n",
       "                      [-1.1841e-02, -2.0386e-02,  5.5176e-02,  ..., -1.7456e-02,\n",
       "                       -1.5717e-03,  8.3618e-03],\n",
       "                      [-2.5757e-02,  9.8267e-03, -9.8267e-03,  ...,  2.2949e-02,\n",
       "                        3.4424e-02, -1.6602e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.input_layernorm.weight',\n",
       "              tensor([0.5547, 0.5625, 0.5547,  ..., 0.5391, 0.5664, 0.5547],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.28.post_attention_layernorm.weight',\n",
       "              tensor([0.4570, 0.4590, 0.4512,  ..., 0.4648, 0.4570, 0.4531],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.self_attn.q_proj.weight',\n",
       "              tensor([[ 4.8828e-03,  7.7209e-03, -7.7209e-03,  ...,  2.9297e-03,\n",
       "                       -3.7231e-03,  8.1787e-03],\n",
       "                      [-2.3804e-02, -2.0386e-02, -2.5635e-02,  ..., -1.4038e-02,\n",
       "                        3.9816e-05, -2.5787e-03],\n",
       "                      [-2.0630e-02,  3.3936e-02, -2.3746e-04,  ..., -6.4850e-04,\n",
       "                       -9.3384e-03,  1.4893e-02],\n",
       "                      ...,\n",
       "                      [-6.2256e-03,  6.0303e-02, -3.2959e-02,  ...,  8.8501e-03,\n",
       "                       -1.0681e-02, -3.4180e-02],\n",
       "                      [-2.2705e-02, -2.2339e-02,  1.7822e-02,  ..., -7.8125e-03,\n",
       "                       -2.4292e-02,  2.3926e-02],\n",
       "                      [-8.6594e-04, -3.8330e-02, -1.3550e-02,  ...,  1.2146e-02,\n",
       "                        8.0566e-03, -3.6865e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0378, -0.0086, -0.0173,  ..., -0.0011,  0.0049,  0.0052],\n",
       "                      [-0.0161,  0.0216, -0.0016,  ..., -0.0208,  0.0092, -0.0255],\n",
       "                      [ 0.0186,  0.0086,  0.0197,  ...,  0.0183,  0.0239,  0.0045],\n",
       "                      ...,\n",
       "                      [-0.0334,  0.0366,  0.0055,  ..., -0.0072, -0.0027, -0.0099],\n",
       "                      [ 0.0034, -0.0160, -0.0669,  ..., -0.0117, -0.0117, -0.0415],\n",
       "                      [ 0.0151, -0.0099, -0.0557,  ..., -0.0598, -0.0223,  0.0055]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.self_attn.v_proj.weight',\n",
       "              tensor([[ 1.2695e-02,  2.7008e-03,  3.1738e-02,  ...,  2.6001e-02,\n",
       "                        3.2654e-03,  1.8433e-02],\n",
       "                      [-3.3264e-03,  2.3804e-02,  3.8818e-02,  ..., -2.0996e-02,\n",
       "                        6.8054e-03, -5.4932e-03],\n",
       "                      [-1.2939e-02,  5.0964e-03, -5.0049e-03,  ..., -6.6757e-04,\n",
       "                        2.8687e-02,  1.3611e-02],\n",
       "                      ...,\n",
       "                      [ 9.8877e-03,  2.1729e-02,  1.0681e-02,  ..., -2.5024e-02,\n",
       "                       -7.8735e-03, -1.3916e-02],\n",
       "                      [ 1.4191e-03,  3.5889e-02, -2.4292e-02,  ...,  1.7578e-02,\n",
       "                        3.1494e-02, -1.1353e-02],\n",
       "                      [ 5.6744e-05, -1.5869e-02,  6.8970e-03,  ...,  3.0060e-03,\n",
       "                       -1.5320e-02,  2.5635e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.self_attn.o_proj.weight',\n",
       "              tensor([[-5.2185e-03,  3.3691e-02,  3.2959e-02,  ...,  3.7994e-03,\n",
       "                       -1.7334e-02, -8.4839e-03],\n",
       "                      [-2.1118e-02, -2.3438e-02, -8.8501e-03,  ...,  1.2512e-02,\n",
       "                       -2.8076e-02,  1.2756e-02],\n",
       "                      [ 2.4536e-02,  2.5024e-02, -9.2163e-03,  ..., -3.5156e-02,\n",
       "                        1.5991e-02, -1.8066e-02],\n",
       "                      ...,\n",
       "                      [-3.0273e-02, -1.4526e-02,  2.1240e-02,  ..., -8.3618e-03,\n",
       "                       -8.2397e-03, -1.8066e-02],\n",
       "                      [-1.1841e-02, -7.8735e-03,  3.4180e-02,  ..., -7.3910e-05,\n",
       "                       -1.3733e-02,  6.1951e-03],\n",
       "                      [ 1.2268e-02,  1.1414e-02,  6.5308e-03,  ...,  6.1035e-03,\n",
       "                        1.7700e-02, -2.7954e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0093,  0.0262, -0.0104,  ..., -0.0064, -0.0287,  0.0322],\n",
       "                      [ 0.0075,  0.0201, -0.0100,  ..., -0.0243,  0.0112,  0.0045],\n",
       "                      [ 0.0381,  0.0028, -0.0098,  ..., -0.0099,  0.0247,  0.0371],\n",
       "                      ...,\n",
       "                      [-0.0142,  0.0071, -0.0146,  ..., -0.0052,  0.0085,  0.0075],\n",
       "                      [ 0.0149,  0.0142, -0.0322,  ..., -0.0178, -0.0076,  0.0173],\n",
       "                      [ 0.0034,  0.0120,  0.0013,  ...,  0.0206, -0.0195, -0.0134]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.mlp.up_proj.weight',\n",
       "              tensor([[-0.0013, -0.0006,  0.0052,  ...,  0.0165,  0.0052,  0.0094],\n",
       "                      [-0.0064,  0.0071,  0.0317,  ...,  0.0047, -0.0236,  0.0352],\n",
       "                      [-0.0146, -0.0254, -0.0033,  ..., -0.0459,  0.0011, -0.0359],\n",
       "                      ...,\n",
       "                      [ 0.0327,  0.0128,  0.0127,  ...,  0.0101,  0.0273,  0.0386],\n",
       "                      [-0.0103, -0.0160, -0.0050,  ...,  0.0151,  0.0170, -0.0048],\n",
       "                      [ 0.0110, -0.0023,  0.0135,  ..., -0.0043,  0.0164,  0.0050]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0464,  0.0082, -0.0012,  ...,  0.0143, -0.0190, -0.0142],\n",
       "                      [ 0.0197,  0.0106,  0.0258,  ...,  0.0293,  0.0081,  0.0007],\n",
       "                      [ 0.0024, -0.0172, -0.0130,  ...,  0.0062,  0.0047, -0.0267],\n",
       "                      ...,\n",
       "                      [ 0.0124, -0.0117,  0.0095,  ..., -0.0155, -0.0065, -0.0206],\n",
       "                      [ 0.0289,  0.0212,  0.0317,  ...,  0.0093, -0.0400,  0.0058],\n",
       "                      [ 0.0112, -0.0190, -0.0075,  ..., -0.0133, -0.0081,  0.0016]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.input_layernorm.weight',\n",
       "              tensor([0.5234, 0.5352, 0.5234,  ..., 0.5195, 0.5312, 0.5469],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.29.post_attention_layernorm.weight',\n",
       "              tensor([0.4668, 0.4668, 0.4629,  ..., 0.4688, 0.4688, 0.4668],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0005, -0.0166, -0.0011,  ..., -0.0016, -0.0005, -0.0011],\n",
       "                      [ 0.0156,  0.0111,  0.0103,  ..., -0.0013, -0.0098, -0.0201],\n",
       "                      [-0.0060, -0.0204,  0.0454,  ...,  0.0154,  0.0039, -0.0303],\n",
       "                      ...,\n",
       "                      [-0.0087, -0.0023, -0.0026,  ...,  0.0131, -0.0090,  0.0041],\n",
       "                      [-0.0013, -0.0192, -0.0139,  ...,  0.0199,  0.0107, -0.0320],\n",
       "                      [-0.0693, -0.0140,  0.0400,  ..., -0.0068, -0.0016, -0.0055]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0242, -0.0236,  0.0015,  ..., -0.0072,  0.0129,  0.0101],\n",
       "                      [ 0.0374,  0.0152,  0.0039,  ..., -0.0045, -0.0063,  0.0005],\n",
       "                      [-0.0050, -0.0123,  0.0220,  ..., -0.0016, -0.0093, -0.0018],\n",
       "                      ...,\n",
       "                      [ 0.0080,  0.0251, -0.0009,  ...,  0.0535,  0.0063, -0.0024],\n",
       "                      [ 0.0103, -0.0640,  0.0347,  ...,  0.0059, -0.0098, -0.0208],\n",
       "                      [-0.0400, -0.0125,  0.0172,  ...,  0.0028,  0.0118, -0.0209]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0042, -0.0352,  0.0276,  ..., -0.0278, -0.0347,  0.0302],\n",
       "                      [ 0.0130,  0.0240,  0.0293,  ..., -0.0107,  0.0364,  0.0054],\n",
       "                      [-0.0182,  0.0120, -0.0156,  ...,  0.0053, -0.0258, -0.0135],\n",
       "                      ...,\n",
       "                      [ 0.0167, -0.0039,  0.0003,  ..., -0.0248, -0.0025,  0.0388],\n",
       "                      [-0.0031, -0.0452,  0.0212,  ...,  0.0300, -0.0045,  0.0262],\n",
       "                      [-0.0026,  0.0060,  0.0161,  ..., -0.0103, -0.0559, -0.0248]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0081,  0.0109, -0.0075,  ...,  0.0069, -0.0181, -0.0240],\n",
       "                      [-0.0201, -0.0192, -0.0011,  ...,  0.0160, -0.0232, -0.0211],\n",
       "                      [-0.0203, -0.0039,  0.0179,  ..., -0.0513, -0.0151, -0.0123],\n",
       "                      ...,\n",
       "                      [ 0.0043,  0.0033,  0.0026,  ..., -0.0275,  0.0059,  0.0143],\n",
       "                      [ 0.0276, -0.0106,  0.0103,  ...,  0.0284, -0.0022,  0.0075],\n",
       "                      [-0.0074,  0.0114,  0.0026,  ..., -0.0018,  0.0150, -0.0156]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0352, -0.0315, -0.0070,  ...,  0.0125, -0.0087,  0.0061],\n",
       "                      [-0.0184, -0.0034,  0.0302,  ..., -0.0048, -0.0002,  0.0277],\n",
       "                      [-0.0221, -0.0101, -0.0288,  ...,  0.0132,  0.0090, -0.0055],\n",
       "                      ...,\n",
       "                      [-0.0232, -0.0090,  0.0179,  ..., -0.0135, -0.0244, -0.0166],\n",
       "                      [ 0.0322,  0.0227, -0.0137,  ...,  0.0126, -0.0388,  0.0033],\n",
       "                      [-0.0089, -0.0242, -0.0194,  ..., -0.0020,  0.0093, -0.0199]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.mlp.up_proj.weight',\n",
       "              tensor([[-0.0008,  0.0080,  0.0131,  ...,  0.0167,  0.0197,  0.0032],\n",
       "                      [-0.0272, -0.0303,  0.0356,  ...,  0.0013, -0.0093, -0.0164],\n",
       "                      [-0.0342, -0.0212, -0.0371,  ..., -0.0181, -0.0062,  0.0170],\n",
       "                      ...,\n",
       "                      [-0.0208, -0.0123,  0.0137,  ...,  0.0177, -0.0137, -0.0225],\n",
       "                      [-0.0075,  0.0026, -0.0125,  ...,  0.0090, -0.0143,  0.0165],\n",
       "                      [-0.0056, -0.0234,  0.0019,  ..., -0.0141,  0.0027,  0.0190]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0190, -0.0143, -0.0311,  ...,  0.0295, -0.0052,  0.0061],\n",
       "                      [ 0.0243,  0.0013,  0.0435,  ...,  0.0148,  0.0047, -0.0079],\n",
       "                      [ 0.0021, -0.0081, -0.0204,  ..., -0.0017,  0.0116,  0.0175],\n",
       "                      ...,\n",
       "                      [-0.0312,  0.0050, -0.0231,  ...,  0.0201, -0.0216, -0.0147],\n",
       "                      [-0.0011, -0.0312,  0.0030,  ...,  0.0215,  0.0027,  0.0034],\n",
       "                      [-0.0070, -0.0144, -0.0239,  ...,  0.0121, -0.0039, -0.0177]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.input_layernorm.weight',\n",
       "              tensor([0.5664, 0.5742, 0.5625,  ..., 0.5469, 0.5547, 0.5781],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.30.post_attention_layernorm.weight',\n",
       "              tensor([0.4746, 0.4824, 0.4707,  ..., 0.4746, 0.4766, 0.4707],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0322,  0.0122,  0.0178,  ...,  0.0159, -0.0021, -0.0199],\n",
       "                      [-0.0077, -0.0200, -0.0317,  ...,  0.0012, -0.0026, -0.0062],\n",
       "                      [-0.0131,  0.0210,  0.0187,  ...,  0.0002,  0.0053,  0.0055],\n",
       "                      ...,\n",
       "                      [ 0.0152, -0.0077, -0.0211,  ..., -0.0090,  0.0229, -0.0240],\n",
       "                      [-0.0212,  0.0063, -0.0100,  ..., -0.0030, -0.0004,  0.0032],\n",
       "                      [-0.0549, -0.0087,  0.0315,  ...,  0.0405,  0.0111,  0.0288]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0110, -0.0225,  0.0166,  ...,  0.0087, -0.0033, -0.0212],\n",
       "                      [ 0.0248, -0.0127,  0.0129,  ...,  0.0075,  0.0010, -0.0080],\n",
       "                      [ 0.0082,  0.0016, -0.0100,  ..., -0.0019,  0.0121, -0.0288],\n",
       "                      ...,\n",
       "                      [ 0.0225, -0.0337, -0.0251,  ..., -0.0115, -0.0145, -0.0483],\n",
       "                      [ 0.0153,  0.0177,  0.0181,  ..., -0.0231, -0.0211,  0.0173],\n",
       "                      [-0.0737, -0.0306,  0.0116,  ..., -0.0094,  0.0005,  0.0038]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0112, -0.0271, -0.0045,  ...,  0.0079,  0.0004,  0.0118],\n",
       "                      [ 0.0215,  0.0089,  0.0187,  ...,  0.0061,  0.0361,  0.0123],\n",
       "                      [ 0.0107, -0.0052, -0.0156,  ...,  0.0137,  0.0008,  0.0325],\n",
       "                      ...,\n",
       "                      [-0.0064, -0.0079, -0.0137,  ..., -0.0181, -0.0017,  0.0267],\n",
       "                      [ 0.0103, -0.0253,  0.0162,  ..., -0.0102, -0.0302,  0.0089],\n",
       "                      [-0.0112, -0.0254,  0.0271,  ...,  0.0204, -0.0060,  0.0403]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0058,  0.0282, -0.0030,  ..., -0.0053, -0.0129, -0.0488],\n",
       "                      [-0.0081,  0.0045, -0.0249,  ...,  0.0266, -0.0181, -0.0137],\n",
       "                      [ 0.0090, -0.0004,  0.0035,  ..., -0.0136,  0.0248,  0.0091],\n",
       "                      ...,\n",
       "                      [ 0.0074, -0.0172,  0.0281,  ..., -0.0183, -0.0103, -0.0164],\n",
       "                      [ 0.0010,  0.0189, -0.0206,  ..., -0.0210, -0.0131, -0.0171],\n",
       "                      [ 0.0078,  0.0167, -0.0109,  ...,  0.0337, -0.0187,  0.0042]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0029, -0.0096,  0.0297,  ...,  0.0143,  0.0177,  0.0041],\n",
       "                      [-0.0737, -0.0249,  0.0028,  ...,  0.0078, -0.0189,  0.0173],\n",
       "                      [ 0.0182,  0.0025, -0.0138,  ..., -0.0036, -0.0162, -0.0106],\n",
       "                      ...,\n",
       "                      [-0.0147, -0.0242,  0.0101,  ...,  0.0041,  0.0201,  0.0308],\n",
       "                      [ 0.0332,  0.0184, -0.0320,  ...,  0.0369, -0.0129, -0.0108],\n",
       "                      [ 0.0233, -0.0320, -0.0315,  ..., -0.0142,  0.0135,  0.0065]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.mlp.up_proj.weight',\n",
       "              tensor([[-4.1016e-02, -1.5869e-02,  2.9541e-02,  ...,  7.9346e-03,\n",
       "                       -1.1597e-02,  4.0894e-03],\n",
       "                      [ 2.1973e-02,  3.1128e-02,  8.7280e-03,  ..., -1.7700e-02,\n",
       "                        2.4796e-04, -1.1673e-03],\n",
       "                      [-5.2185e-03,  7.1411e-03, -2.0020e-02,  ...,  1.0132e-02,\n",
       "                        9.8419e-04, -1.2024e-02],\n",
       "                      ...,\n",
       "                      [ 1.1902e-02, -9.4604e-03,  4.1008e-05,  ...,  1.8616e-03,\n",
       "                        3.3203e-02, -1.9043e-02],\n",
       "                      [ 3.8757e-03,  7.9727e-04, -2.2339e-02,  ...,  2.6611e-02,\n",
       "                        1.1597e-02,  1.0223e-03],\n",
       "                      [ 4.3701e-02, -3.6621e-02, -3.6011e-03,  ..., -5.8594e-03,\n",
       "                        3.9978e-03,  2.4536e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.mlp.down_proj.weight',\n",
       "              tensor([[-0.0022, -0.0255, -0.0085,  ..., -0.0349, -0.0154,  0.0232],\n",
       "                      [ 0.0415,  0.0025, -0.0031,  ..., -0.0028, -0.0156,  0.0137],\n",
       "                      [-0.0136, -0.0101,  0.0239,  ...,  0.0304, -0.0019,  0.0238],\n",
       "                      ...,\n",
       "                      [ 0.0040,  0.0322, -0.0021,  ...,  0.0102,  0.0062,  0.0210],\n",
       "                      [-0.0006,  0.0483, -0.0078,  ..., -0.0070,  0.0210,  0.0219],\n",
       "                      [ 0.0258, -0.0342,  0.0166,  ...,  0.0068,  0.0016,  0.0269]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.input_layernorm.weight',\n",
       "              tensor([0.4824, 0.4805, 0.4336,  ..., 0.4258, 0.4531, 0.4785],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.layers.31.post_attention_layernorm.weight',\n",
       "              tensor([0.4297, 0.4297, 0.4355,  ..., 0.4180, 0.4043, 0.4238],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.norm.weight',\n",
       "              tensor([1.8594, 1.8516, 1.7969,  ..., 1.7109, 1.8125, 1.5938],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.W_pos',\n",
       "              tensor([[ 5.9814e-03,  1.3916e-02, -1.9646e-04, -1.7944e-02,  9.3384e-03,\n",
       "                       -3.8910e-03, -2.8229e-03,  7.7248e-05,  1.6235e-02,  1.1444e-03,\n",
       "                       -7.6294e-03,  1.0681e-02, -1.3489e-02,  5.1880e-03,  1.4832e-02,\n",
       "                       -4.4250e-03, -4.6997e-03, -3.6926e-03, -1.5991e-02,  4.8828e-03,\n",
       "                       -9.7046e-03, -1.2939e-02, -1.2756e-02,  9.9487e-03, -1.5991e-02,\n",
       "                        8.7280e-03,  6.8970e-03,  1.2207e-03,  5.7678e-03, -3.0060e-03,\n",
       "                       -1.0132e-02,  9.7656e-03,  1.2329e-02, -1.4191e-03, -7.6294e-03,\n",
       "                       -1.1169e-02,  1.8433e-02,  8.1787e-03, -8.2397e-04,  7.7515e-03,\n",
       "                       -7.3547e-03, -1.9653e-02, -5.3711e-03,  1.9165e-02,  5.9509e-03,\n",
       "                       -1.6968e-02, -2.8992e-03, -1.0437e-02,  1.3000e-02,  4.8828e-03,\n",
       "                       -1.4343e-03, -1.4526e-02,  4.3945e-03, -4.6082e-03,  5.9307e-06,\n",
       "                       -5.9814e-03, -1.3428e-02, -7.8125e-03, -2.1667e-03,  1.1658e-02,\n",
       "                       -1.1658e-02, -4.0588e-03, -1.1108e-02, -1.6251e-03, -3.1128e-03,\n",
       "                        1.4709e-02,  3.4790e-03, -1.0315e-02, -1.0620e-02, -3.6163e-03,\n",
       "                       -1.3489e-02, -9.7046e-03,  1.5869e-02, -1.2451e-02, -4.7913e-03,\n",
       "                        1.0254e-02, -1.7166e-03, -5.1880e-04, -7.5378e-03, -4.6387e-03,\n",
       "                       -1.0437e-02,  1.1475e-02,  5.9509e-03,  1.1536e-02,  1.0757e-03,\n",
       "                       -1.0315e-02, -6.5613e-03,  1.1780e-02,  1.8799e-02,  1.6113e-02,\n",
       "                       -1.9531e-02, -1.9775e-02, -2.4872e-03, -1.0223e-03,  1.8921e-02,\n",
       "                       -8.5449e-03,  1.2573e-02,  3.6774e-03,  1.9043e-02,  6.2561e-04,\n",
       "                        9.8267e-03, -4.2725e-03,  5.9814e-03,  3.2349e-03, -8.6670e-03,\n",
       "                       -1.6357e-02,  4.4556e-03,  7.8201e-04,  6.4392e-03,  3.1090e-04,\n",
       "                        1.5625e-02, -2.8839e-03, -1.7944e-02,  5.5237e-03,  1.8311e-02,\n",
       "                        5.3406e-03,  1.9775e-02, -1.0071e-02, -1.1963e-02,  8.1177e-03,\n",
       "                        8.6060e-03, -8.4229e-03, -5.0545e-05, -4.0283e-03, -1.6846e-02,\n",
       "                       -3.1891e-03, -2.0020e-02, -3.3722e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.0.qTrans',\n",
       "              tensor([[ 0.0767, -0.0869, -0.0223,  ..., -0.0840, -0.1426, -0.0306],\n",
       "                      [ 0.0903,  0.1572, -0.0986,  ...,  0.0811, -0.0152, -0.0786],\n",
       "                      [ 0.1118,  0.0830,  0.0219,  ..., -0.1279,  0.1494, -0.0664],\n",
       "                      ...,\n",
       "                      [-0.0459, -0.0012, -0.0952,  ...,  0.0071, -0.0859,  0.0084],\n",
       "                      [-0.1084, -0.0864,  0.0613,  ..., -0.0859, -0.0981, -0.1289],\n",
       "                      [-0.1289, -0.0942,  0.0171,  ..., -0.1406, -0.0085, -0.1143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.0.kTrans',\n",
       "              tensor([[-0.0250, -0.0835, -0.1406,  ...,  0.1152,  0.0938,  0.0435],\n",
       "                      [-0.0437,  0.2080, -0.0669,  ..., -0.0859,  0.1582, -0.1030],\n",
       "                      [ 0.0981, -0.1250, -0.0275,  ..., -0.0684, -0.1885, -0.0339],\n",
       "                      ...,\n",
       "                      [-0.0030,  0.1807, -0.1338,  ...,  0.1787, -0.2539,  0.2344],\n",
       "                      [-0.0688, -0.1074,  0.1182,  ...,  0.0325,  0.1250, -0.2656],\n",
       "                      [ 0.0522, -0.0197,  0.1240,  ...,  0.0525, -0.1211,  0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.0.vTrans',\n",
       "              tensor([[ 0.0352,  0.0771, -0.0435,  ...,  0.0221,  0.0757,  0.0688],\n",
       "                      [-0.1260, -0.1084,  0.0781,  ..., -0.0835,  0.0413, -0.0447],\n",
       "                      [ 0.0615,  0.0156,  0.0048,  ...,  0.1348,  0.1348,  0.0781],\n",
       "                      ...,\n",
       "                      [-0.0991,  0.1367,  0.1387,  ..., -0.0369,  0.0300, -0.1133],\n",
       "                      [ 0.0332,  0.1064, -0.0212,  ..., -0.1035, -0.0703, -0.1250],\n",
       "                      [ 0.1318,  0.0767, -0.1084,  ...,  0.0518,  0.0425, -0.0381]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.0.norm.weight',\n",
       "              tensor([1.0000, 0.9922, 1.0000, 0.9961, 0.9922, 0.9609, 1.0078, 1.0000, 1.0078,\n",
       "                      0.9922, 0.9922, 1.0000, 0.9961, 1.0078, 1.0000, 0.9961, 0.9219, 1.0000,\n",
       "                      0.9922, 0.9961, 1.0078, 0.9961, 0.9805, 1.0000, 0.9688, 0.9883, 0.9922,\n",
       "                      1.0000, 0.9961, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 0.9883, 0.9922,\n",
       "                      0.9766, 1.0000, 1.0000, 1.0000, 0.9922, 0.9922, 0.9961, 1.0000, 0.9922,\n",
       "                      0.9688, 0.9883, 0.9961, 1.0000, 1.0000, 0.9961, 0.9844, 0.9648, 1.0000,\n",
       "                      0.9961, 1.0000, 1.0000, 0.9922, 0.9844, 1.0000, 1.0000, 0.9961, 1.0000,\n",
       "                      0.9922, 0.9922, 1.0000, 0.9883, 1.0000, 1.0078, 0.9883, 0.9961, 0.9961,\n",
       "                      1.0000, 1.0000, 0.9727, 0.9961, 1.0000, 0.9727, 1.0078, 1.0000, 0.9922,\n",
       "                      0.9844, 1.0000, 0.9883, 0.9961, 0.9883, 0.9961, 0.9922, 0.9922, 1.0000,\n",
       "                      1.0078, 1.0000, 1.0000, 0.9922, 1.0000, 1.0000, 0.9453, 1.0000, 1.0078,\n",
       "                      0.9922, 1.0078, 0.9844, 0.9961, 1.0078, 0.9531, 0.9922, 0.9922, 1.0078,\n",
       "                      1.0000, 1.0000, 1.0078, 0.9922, 0.9766, 0.9961, 1.0000, 0.9961, 1.0078,\n",
       "                      0.9961, 0.9922, 0.9648, 0.9961, 0.9883, 1.0078, 0.9961, 0.9961, 0.9883,\n",
       "                      0.9922, 1.0000], dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.0.norm.bias',\n",
       "              tensor([ 0.0089,  0.0515, -0.0405, -0.0010,  0.0276, -0.0150,  0.0688,  0.0452,\n",
       "                       0.0052, -0.0294, -0.0175, -0.0532, -0.0034,  0.0337, -0.0026,  0.0508,\n",
       "                       0.0273, -0.0408, -0.0043,  0.0007, -0.0457,  0.0099, -0.0518, -0.0198,\n",
       "                      -0.0356,  0.0417,  0.0312,  0.0176,  0.0239,  0.0133, -0.0077,  0.0254,\n",
       "                      -0.0112,  0.0015, -0.0229, -0.0280,  0.0364,  0.0222, -0.0165, -0.0400,\n",
       "                       0.0167, -0.0212, -0.0273, -0.0076, -0.0310,  0.0292,  0.0198,  0.0132,\n",
       "                       0.0137,  0.0011,  0.0099, -0.0228, -0.0522,  0.0164, -0.0415, -0.0088,\n",
       "                      -0.0298,  0.0233, -0.0018, -0.0302, -0.0200,  0.0410, -0.0038,  0.0160,\n",
       "                       0.0378,  0.0087,  0.0425,  0.0615, -0.0352,  0.0077,  0.0258, -0.0576,\n",
       "                      -0.0013,  0.0405,  0.0437, -0.0206, -0.0122,  0.0413,  0.0327, -0.0114,\n",
       "                      -0.0229,  0.0003,  0.0532, -0.0008, -0.0167, -0.0005,  0.0010,  0.0050,\n",
       "                      -0.0101, -0.0693, -0.0062,  0.1006,  0.0168, -0.0164,  0.0479, -0.0059,\n",
       "                       0.0231, -0.0356,  0.0052, -0.0150, -0.0693, -0.0104,  0.0276, -0.0288,\n",
       "                       0.0072,  0.0374,  0.0183,  0.0356, -0.0283,  0.0378, -0.0171, -0.0223,\n",
       "                       0.0728,  0.0003, -0.0442,  0.0140, -0.0796,  0.0172, -0.0220,  0.0369,\n",
       "                      -0.0060, -0.0111, -0.0520,  0.0415,  0.0216,  0.0322,  0.0221,  0.0187],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.1.qTrans',\n",
       "              tensor([[ 0.0649, -0.0277, -0.1367,  ..., -0.1484,  0.0481, -0.0605],\n",
       "                      [ 0.1172,  0.0094,  0.1250,  ...,  0.0295, -0.1201,  0.1157],\n",
       "                      [-0.0020, -0.1338,  0.0515,  ..., -0.0581, -0.1426,  0.0515],\n",
       "                      ...,\n",
       "                      [-0.0986, -0.0452,  0.1157,  ...,  0.1406,  0.0410,  0.0608],\n",
       "                      [-0.0089, -0.0664,  0.0275,  ..., -0.0398,  0.0024,  0.0435],\n",
       "                      [ 0.1260, -0.1465,  0.0464,  ...,  0.0292, -0.0452,  0.1055]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.1.kTrans',\n",
       "              tensor([[-1.2256e-01, -5.9082e-02, -1.2988e-01,  ..., -1.0742e-01,\n",
       "                       -8.2520e-02,  2.7313e-03],\n",
       "                      [-4.7302e-03, -3.9551e-02, -8.7402e-02,  ...,  4.0771e-02,\n",
       "                       -1.0205e-01, -8.3008e-02],\n",
       "                      [ 1.0205e-01, -1.3086e-01, -1.1035e-01,  ...,  6.1035e-05,\n",
       "                       -2.5024e-02, -8.7402e-02],\n",
       "                      ...,\n",
       "                      [ 9.6191e-02, -8.5938e-02,  4.2236e-02,  ..., -1.2793e-01,\n",
       "                       -7.1289e-02,  4.8828e-02],\n",
       "                      [-1.3574e-01,  1.2598e-01, -6.9580e-03,  ..., -1.0938e-01,\n",
       "                        5.6396e-02, -3.8086e-02],\n",
       "                      [-5.2734e-02,  1.3965e-01, -1.0596e-01,  ...,  5.1025e-02,\n",
       "                       -1.1475e-01, -4.4189e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.1.vTrans',\n",
       "              tensor([[-0.0520,  0.1328,  0.0262,  ..., -0.0057,  0.0820,  0.1143],\n",
       "                      [ 0.1416, -0.0359, -0.1309,  ...,  0.0198, -0.1006,  0.0053],\n",
       "                      [ 0.1084, -0.0118, -0.0806,  ...,  0.0117, -0.1504, -0.0481],\n",
       "                      ...,\n",
       "                      [-0.0571,  0.0219, -0.0298,  ..., -0.1611, -0.0061,  0.0879],\n",
       "                      [ 0.1226,  0.0369, -0.0503,  ..., -0.1138,  0.0069,  0.0182],\n",
       "                      [-0.0232,  0.1089, -0.0549,  ..., -0.1348, -0.0894, -0.0649]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.1.norm.weight',\n",
       "              tensor([1.0000, 0.9961, 0.9883, 1.0000, 0.9961, 0.9844, 0.9922, 0.9922, 1.0078,\n",
       "                      1.0000, 1.0078, 1.0078, 0.9883, 0.9609, 1.0000, 1.0078, 0.9961, 1.0000,\n",
       "                      0.9922, 0.9844, 1.0000, 0.9844, 0.9961, 0.9922, 0.9453, 1.0000, 0.9961,\n",
       "                      0.9883, 0.9961, 0.9570, 1.0000, 0.9922, 0.9922, 0.9883, 0.9766, 1.0000,\n",
       "                      0.9648, 1.0000, 1.0156, 1.0000, 1.0000, 0.9883, 0.9961, 0.9922, 0.9883,\n",
       "                      1.0078, 0.9805, 0.9805, 1.0000, 1.0078, 0.9805, 0.9883, 0.9727, 1.0078,\n",
       "                      0.9961, 0.9961, 1.0156, 0.9883, 1.0078, 1.0156, 1.0000, 1.0078, 1.0000,\n",
       "                      0.9961, 1.0000, 1.0000, 0.9883, 0.9805, 1.0000, 0.9844, 0.9922, 1.0000,\n",
       "                      1.0000, 1.0000, 0.9805, 1.0000, 0.9961, 0.9844, 1.0000, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 0.9883, 0.9961, 1.0078, 1.0078, 0.9805, 1.0000, 1.0000,\n",
       "                      1.0000, 0.9961, 1.0000, 1.0078, 1.0000, 1.0000, 0.9844, 0.9922, 1.0000,\n",
       "                      0.9922, 0.9844, 0.9883, 0.9961, 1.0000, 0.9961, 0.9766, 0.9922, 0.9102,\n",
       "                      1.0000, 0.9961, 0.9922, 0.9961, 0.9883, 1.0000, 1.0078, 1.0000, 1.0078,\n",
       "                      1.0000, 0.9922, 1.0078, 0.9961, 0.9922, 0.9883, 0.9922, 0.9922, 1.0078,\n",
       "                      0.9961, 0.9961], dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.1.norm.bias',\n",
       "              tensor([ 0.0212,  0.0679, -0.0344,  0.0515,  0.0156, -0.0129,  0.0588,  0.0576,\n",
       "                      -0.0074, -0.0277,  0.0048, -0.0038, -0.0142,  0.0070,  0.0012,  0.0496,\n",
       "                       0.0234, -0.0391, -0.0295,  0.0019, -0.0393,  0.0256, -0.0498,  0.0004,\n",
       "                      -0.0204,  0.0005,  0.0461, -0.0118,  0.0315, -0.0176, -0.0079,  0.0058,\n",
       "                      -0.0187,  0.0011, -0.0273, -0.0435,  0.0210,  0.0349, -0.0157, -0.0352,\n",
       "                       0.0269,  0.0081, -0.0325,  0.0020, -0.0281,  0.0128,  0.0376,  0.0103,\n",
       "                       0.0107,  0.0023, -0.0049, -0.0114, -0.0449,  0.0009, -0.0454,  0.0293,\n",
       "                      -0.0300,  0.0129, -0.0364, -0.0255,  0.0159,  0.0040, -0.0199,  0.0243,\n",
       "                       0.0403,  0.0059,  0.0405,  0.0405, -0.0234, -0.0003,  0.0356, -0.0225,\n",
       "                      -0.0258,  0.0133,  0.0238, -0.0452,  0.0009,  0.0286,  0.0334, -0.0115,\n",
       "                      -0.0161, -0.0234,  0.0776, -0.0129, -0.0121,  0.0413, -0.0159, -0.0048,\n",
       "                      -0.0254, -0.0625, -0.0388,  0.1040,  0.0260, -0.0217,  0.0486,  0.0081,\n",
       "                       0.0175, -0.0026, -0.0165, -0.0042, -0.0781, -0.0212,  0.0474, -0.0228,\n",
       "                       0.0110, -0.0039, -0.0024, -0.0020, -0.0017,  0.0364, -0.0299, -0.0253,\n",
       "                       0.0718, -0.0126, -0.0253,  0.0051, -0.0747,  0.0276, -0.0005,  0.0432,\n",
       "                      -0.0007, -0.0048, -0.0454,  0.0251,  0.0161,  0.0557,  0.0089,  0.0088],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.2.qTrans',\n",
       "              tensor([[ 9.9487e-03, -4.9316e-02, -2.7344e-02,  ..., -1.5076e-02,\n",
       "                        1.6724e-02,  2.5635e-02],\n",
       "                      [-6.9824e-02, -1.2598e-01, -3.6377e-02,  ...,  6.4941e-02,\n",
       "                       -1.3379e-01, -1.3184e-01],\n",
       "                      [-9.7656e-02,  6.4453e-02,  6.4453e-02,  ..., -2.0386e-02,\n",
       "                       -7.1289e-02, -4.1992e-02],\n",
       "                      ...,\n",
       "                      [-1.0071e-02, -1.5039e-01, -2.7954e-02,  ..., -2.9907e-02,\n",
       "                        8.2520e-02, -1.0059e-01],\n",
       "                      [ 6.6406e-02, -8.8379e-02, -1.7090e-01,  ...,  5.2979e-02,\n",
       "                       -5.8350e-02,  6.0059e-02],\n",
       "                      [ 2.4048e-02, -3.4668e-02,  1.4355e-01,  ..., -4.9316e-02,\n",
       "                       -2.9325e-05,  9.2773e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.2.kTrans',\n",
       "              tensor([[-0.1562,  0.1001,  0.0688,  ..., -0.0092, -0.0845,  0.0309],\n",
       "                      [-0.0864,  0.0625, -0.0186,  ..., -0.1025, -0.0894, -0.0359],\n",
       "                      [ 0.0820, -0.0050,  0.0571,  ..., -0.0737, -0.0216, -0.0312],\n",
       "                      ...,\n",
       "                      [-0.0806, -0.1338,  0.1309,  ...,  0.0559, -0.0244,  0.0349],\n",
       "                      [-0.1582, -0.1582, -0.0908,  ...,  0.1035, -0.0366,  0.0532],\n",
       "                      [-0.0289,  0.1074,  0.0476,  ...,  0.0344,  0.0461, -0.1553]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.2.vTrans',\n",
       "              tensor([[-0.0479,  0.1455,  0.0752,  ...,  0.0708, -0.0762, -0.1045],\n",
       "                      [ 0.0220,  0.0454, -0.0354,  ...,  0.0223, -0.0101,  0.0688],\n",
       "                      [-0.0603,  0.0664,  0.0094,  ...,  0.1562,  0.0952, -0.0383],\n",
       "                      ...,\n",
       "                      [-0.1060, -0.0413, -0.1338,  ..., -0.0160,  0.0192, -0.1396],\n",
       "                      [-0.0913, -0.0312, -0.0091,  ...,  0.0962, -0.0135, -0.0820],\n",
       "                      [-0.1602, -0.0564,  0.1030,  ..., -0.0167,  0.0952,  0.0206]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.2.norm.weight',\n",
       "              tensor([1.0000, 1.0000, 0.9883, 1.0000, 0.9922, 0.9648, 1.0078, 1.0000, 0.9648,\n",
       "                      1.0000, 1.0000, 0.9922, 0.9531, 0.9609, 1.0000, 1.0000, 0.9961, 0.9961,\n",
       "                      1.0000, 0.9922, 1.0000, 0.9961, 1.0000, 0.9883, 0.9453, 0.9922, 0.9961,\n",
       "                      0.9961, 0.9922, 1.0078, 0.9961, 0.9922, 1.0078, 1.0000, 0.9883, 0.9961,\n",
       "                      0.9648, 0.9961, 1.0000, 0.9961, 1.0000, 1.0078, 1.0000, 0.9961, 0.9922,\n",
       "                      0.9648, 0.9844, 1.0078, 0.9688, 1.0000, 0.9844, 1.0000, 0.9453, 1.0000,\n",
       "                      0.9922, 0.9844, 0.9961, 0.9961, 1.0078, 1.0078, 0.9727, 1.0000, 0.9961,\n",
       "                      0.9922, 1.0000, 1.0000, 1.0078, 0.9727, 1.0000, 0.9492, 1.0000, 0.9961,\n",
       "                      0.9961, 0.9961, 0.9844, 0.9766, 1.0078, 0.9922, 1.0000, 1.0000, 1.0000,\n",
       "                      0.9883, 1.0000, 0.9805, 0.9844, 1.0000, 0.9141, 1.0078, 1.0000, 0.9922,\n",
       "                      1.0000, 1.0000, 1.0000, 1.0078, 1.0000, 0.9844, 0.9883, 1.0000, 0.9961,\n",
       "                      0.9961, 0.9805, 0.9844, 1.0000, 0.9961, 1.0078, 0.9883, 1.0078, 0.9531,\n",
       "                      0.9961, 1.0000, 0.9883, 0.9961, 1.0000, 1.0000, 0.9961, 1.0000, 1.0078,\n",
       "                      0.9922, 0.9961, 1.0078, 0.9883, 1.0078, 0.9844, 1.0000, 0.9883, 0.9648,\n",
       "                      1.0000, 1.0000], dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.gtLayers.2.norm.bias',\n",
       "              tensor([ 0.0457,  0.0457, -0.0179,  0.0659, -0.0201, -0.0156,  0.0177,  0.0312,\n",
       "                      -0.0004, -0.0518,  0.0371,  0.0327,  0.0299,  0.0092,  0.0420, -0.0304,\n",
       "                      -0.0195,  0.0532,  0.0201,  0.0112, -0.0405,  0.0245, -0.0215,  0.0204,\n",
       "                       0.0025, -0.0439, -0.0386,  0.0271,  0.0308,  0.0092,  0.0469, -0.0225,\n",
       "                       0.0184, -0.0090, -0.0010,  0.0100, -0.0347,  0.0208,  0.0254, -0.0085,\n",
       "                      -0.0312, -0.0018, -0.0197,  0.0080, -0.0015,  0.0464, -0.0161, -0.0236,\n",
       "                       0.0630, -0.0269, -0.0330, -0.0187, -0.0413,  0.0053,  0.0035,  0.0139,\n",
       "                      -0.0278, -0.0219, -0.0364, -0.0410,  0.0240,  0.0276, -0.0452,  0.0491,\n",
       "                       0.0067,  0.0151,  0.0723, -0.0309,  0.0035, -0.0226,  0.0435, -0.0095,\n",
       "                      -0.0096, -0.0483,  0.0439,  0.0132, -0.0403, -0.0003, -0.0106, -0.0265,\n",
       "                       0.0300, -0.0525, -0.0112, -0.0530,  0.0198,  0.0214,  0.0260,  0.0273,\n",
       "                      -0.0471,  0.0356,  0.0156,  0.0022,  0.0124, -0.0292,  0.0228,  0.0012,\n",
       "                       0.0001,  0.0049,  0.0417, -0.0055,  0.0005, -0.0107,  0.0123,  0.0166,\n",
       "                       0.0408, -0.0330, -0.0339, -0.0498,  0.0635, -0.0201, -0.0073, -0.0559,\n",
       "                       0.0182, -0.0310,  0.0165,  0.0142, -0.0605,  0.0481, -0.0347,  0.0010,\n",
       "                      -0.0259, -0.0099,  0.0201,  0.0057, -0.0039,  0.0371, -0.0114, -0.0096],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.W_P.weight',\n",
       "              tensor([[-0.0913, -0.0762, -0.0145,  ..., -0.0299,  0.0957,  0.0183],\n",
       "                      [-0.0796,  0.0101,  0.0693,  ..., -0.0165, -0.0889, -0.0255],\n",
       "                      [-0.0330,  0.0623,  0.0266,  ...,  0.0771,  0.0625, -0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0055, -0.0344, -0.0613,  ...,  0.0728, -0.0378,  0.0684],\n",
       "                      [-0.0100, -0.0079,  0.0457,  ...,  0.0097,  0.0430, -0.0062],\n",
       "                      [-0.0674,  0.0757, -0.0505,  ..., -0.0129,  0.0620,  0.0786]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.W_P.bias',\n",
       "              tensor([-0.0942,  0.0757, -0.0037, -0.0603,  0.1035, -0.0898,  0.1187, -0.0510,\n",
       "                       0.0645, -0.0479,  0.0102, -0.0308, -0.0070,  0.0854, -0.0157, -0.0447,\n",
       "                      -0.0120, -0.1172, -0.0898,  0.0078, -0.1260, -0.0605, -0.1001,  0.0981,\n",
       "                      -0.0249,  0.0156,  0.0253,  0.0767, -0.0103, -0.0525, -0.0049, -0.0295,\n",
       "                       0.0469,  0.0371, -0.0708, -0.0537, -0.0659,  0.0654, -0.0659, -0.1001,\n",
       "                       0.0664, -0.0364, -0.1182,  0.0371, -0.0732,  0.0532, -0.0491,  0.0118,\n",
       "                       0.0471, -0.0408, -0.0664,  0.0874, -0.0309,  0.0957, -0.0752, -0.0251,\n",
       "                      -0.0605, -0.0549,  0.0084,  0.0879, -0.1006,  0.0261, -0.0376, -0.0620,\n",
       "                       0.0071, -0.0522,  0.0306,  0.0625, -0.1118, -0.0718,  0.0281, -0.0240,\n",
       "                      -0.0057,  0.0674,  0.0859,  0.0449,  0.0165, -0.0034, -0.0371, -0.0559,\n",
       "                      -0.0747,  0.0481,  0.0092,  0.0713, -0.0605,  0.0486, -0.0435,  0.0654,\n",
       "                       0.0515, -0.1040, -0.1030,  0.1250,  0.0175, -0.0630, -0.0757, -0.0874,\n",
       "                      -0.0184,  0.0601,  0.1147, -0.0303, -0.0312, -0.0442,  0.1016, -0.0229,\n",
       "                      -0.0007,  0.0498, -0.0092, -0.0223,  0.0718,  0.0277, -0.0942, -0.0591,\n",
       "                       0.0623, -0.0757, -0.0698,  0.0228, -0.0586, -0.0298, -0.0053,  0.0698,\n",
       "                       0.0503,  0.0991, -0.1152, -0.0141, -0.0008,  0.0055, -0.1118,  0.1245],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.inverW_P.weight',\n",
       "              tensor([[ 0.0173, -0.0840,  0.0197,  ...,  0.0491, -0.0500, -0.0152],\n",
       "                      [ 0.0620, -0.0781,  0.0315,  ...,  0.0154,  0.0156, -0.0515],\n",
       "                      [-0.0134,  0.0618,  0.0571,  ..., -0.0464, -0.0767,  0.0674],\n",
       "                      ...,\n",
       "                      [-0.0251, -0.0664,  0.0786,  ...,  0.0574,  0.0171,  0.0187],\n",
       "                      [ 0.0461,  0.0055,  0.0383,  ...,  0.0347, -0.0752, -0.0223],\n",
       "                      [ 0.0537, -0.0544, -0.0320,  ..., -0.0459, -0.0688, -0.0383]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_tower.inverW_P.bias',\n",
       "              tensor([-0.1338,  0.0251, -0.0542, -0.0698,  0.1089,  0.0547, -0.0280,  0.0101,\n",
       "                       0.0006, -0.0361,  0.0713, -0.0332, -0.0178, -0.0037,  0.0791,  0.0500,\n",
       "                      -0.1069,  0.0903,  0.0771, -0.0981,  0.0442,  0.0684, -0.0830,  0.0654,\n",
       "                       0.0630, -0.0267,  0.0649,  0.0037,  0.0991, -0.0574,  0.0135,  0.0547,\n",
       "                      -0.0356, -0.0209,  0.0044,  0.0535,  0.0581, -0.1338,  0.0011,  0.0698,\n",
       "                      -0.1133,  0.0747, -0.0227, -0.0121,  0.0630, -0.0486,  0.0728,  0.0723,\n",
       "                       0.0260,  0.0427,  0.0311, -0.0698, -0.0586,  0.0996,  0.0327,  0.0688,\n",
       "                      -0.0339,  0.0194, -0.1299,  0.0474, -0.0908, -0.0786, -0.0825, -0.0483,\n",
       "                       0.0374, -0.0146,  0.0674,  0.0160,  0.0289, -0.0041, -0.0825,  0.0898,\n",
       "                      -0.0898, -0.0288, -0.0938, -0.0728, -0.0040, -0.0957, -0.0356,  0.0508,\n",
       "                      -0.0596,  0.1069, -0.0981, -0.0398,  0.0194, -0.0325,  0.0520, -0.0344,\n",
       "                      -0.0189, -0.1235,  0.0747, -0.0420, -0.0186,  0.0070,  0.0918, -0.0708,\n",
       "                       0.1030,  0.0515, -0.0737, -0.0378,  0.0471,  0.0073, -0.0153, -0.0258,\n",
       "                      -0.0145, -0.0086, -0.0557, -0.1001, -0.0194, -0.1157,  0.0747,  0.0845,\n",
       "                      -0.1035,  0.0549, -0.0630, -0.0193, -0.0190,  0.1167,  0.0854,  0.0220,\n",
       "                       0.0540, -0.0942,  0.1167, -0.0388,  0.1050, -0.0229,  0.0457, -0.0679],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_projector.weight',\n",
       "              tensor([[ 0.0488,  0.0113,  0.0503,  ..., -0.0255, -0.0659,  0.1211],\n",
       "                      [ 0.0232, -0.1016, -0.0422,  ..., -0.0825,  0.0554, -0.0874],\n",
       "                      [-0.0923, -0.0159,  0.0193,  ...,  0.0649,  0.0312, -0.0299],\n",
       "                      ...,\n",
       "                      [ 0.1406,  0.0535,  0.1309,  ..., -0.1279, -0.1079,  0.0559],\n",
       "                      [ 0.0243, -0.0540,  0.0957,  ...,  0.0288, -0.0078,  0.0898],\n",
       "                      [-0.0369,  0.0635, -0.0298,  ...,  0.0635, -0.0437,  0.0332]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.model.graph_projector.bias',\n",
       "              tensor([ 0.0209, -0.0713, -0.0347,  ...,  0.0011,  0.0894,  0.0339],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.lm_head.weight',\n",
       "              tensor([[-0.0036,  0.0027, -0.0074,  ...,  0.0039, -0.0084,  0.0065],\n",
       "                      [-0.0311,  0.0449, -0.0029,  ..., -0.0228,  0.0147,  0.0320],\n",
       "                      [-0.0125,  0.0014,  0.0188,  ..., -0.0264,  0.0156, -0.0073],\n",
       "                      ...,\n",
       "                      [ 0.0146,  0.0143, -0.0156,  ..., -0.0004, -0.0461,  0.0047],\n",
       "                      [-0.0008,  0.0007, -0.0013,  ...,  0.0008, -0.0041,  0.0028],\n",
       "                      [-0.0008,  0.0007, -0.0013,  ...,  0.0008, -0.0041,  0.0028]],\n",
       "                     dtype=torch.bfloat16))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0021, -0.0146, -0.0131,  ...,  0.0009,  0.0067,  0.0068],\n",
       "        [-0.0099, -0.0052, -0.0150,  ...,  0.0017, -0.0013, -0.0135],\n",
       "        [ 0.0150,  0.0039, -0.0144,  ...,  0.0133,  0.0093,  0.0130],\n",
       "        ...,\n",
       "        [-0.0093,  0.0047, -0.0042,  ..., -0.0111,  0.0081, -0.0135],\n",
       "        [-0.0043, -0.0063,  0.0148,  ...,  0.0151,  0.0016,  0.0107],\n",
       "        [-0.0070,  0.0057, -0.0030,  ...,  0.0100, -0.0084,  0.0148]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight.shape)\n",
    "model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8735e-03,  1.1658e-02, -1.5564e-02,  ..., -7.5684e-03,\n",
       "         -1.3428e-02, -9.1553e-03],\n",
       "        [ 8.0566e-03,  1.3855e-02,  2.3041e-03,  ...,  6.4087e-04,\n",
       "         -3.7231e-03, -1.6479e-03],\n",
       "        [-9.4604e-03, -2.8038e-04,  6.0120e-03,  ...,  6.9885e-03,\n",
       "         -1.3611e-02, -1.2939e-02],\n",
       "        ...,\n",
       "        [-1.1841e-02, -1.4160e-02,  2.2697e-04,  ...,  5.4016e-03,\n",
       "         -7.4863e-05, -2.8419e-04],\n",
       "        [ 2.9297e-03,  1.1902e-02, -1.4465e-02,  ...,  8.5449e-03,\n",
       "          1.2451e-02, -1.2390e-02],\n",
       "        [ 1.1780e-02, -1.3855e-02, -1.0010e-02,  ..., -9.2163e-03,\n",
       "          3.3722e-03,  1.2451e-02]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ckpt['state_dict']['model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight'].shape)\n",
    "ckpt['state_dict']['model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0318, -0.0859, -0.0363,  ...,  0.0433,  0.0841,  0.0707],\n",
       "        [ 0.0416,  0.0004, -0.0445,  ...,  0.0639, -0.0693, -0.0577],\n",
       "        [-0.0066,  0.0691,  0.0675,  ...,  0.0540, -0.0401,  0.0155],\n",
       "        ...,\n",
       "        [-0.0603,  0.0843,  0.0803,  ..., -0.0173,  0.0747, -0.0626],\n",
       "        [-0.0117,  0.0249,  0.0586,  ...,  0.0700, -0.0607,  0.0595],\n",
       "        [ 0.0437, -0.0550, -0.0018,  ...,  0.0012, -0.0693, -0.0251]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.base_model.model.model.graph_projector.weight\n",
    "# model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.Size([128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.3853e-03, -5.8105e-02,  5.9814e-02,  ...,  4.9072e-02,\n",
       "         -6.9336e-02,  1.1902e-02],\n",
       "        [ 6.3477e-02, -8.1055e-02,  3.7354e-02,  ..., -8.1787e-03,\n",
       "         -3.3112e-03, -6.7871e-02],\n",
       "        [-5.7220e-05,  6.9824e-02,  7.9590e-02,  ..., -3.2715e-02,\n",
       "         -8.5938e-02,  8.4961e-02],\n",
       "        ...,\n",
       "        [-2.4902e-02, -6.7383e-02,  8.1543e-02,  ...,  6.3477e-02,\n",
       "          1.2390e-02,  3.0273e-02],\n",
       "        [ 4.8584e-02, -1.2024e-02,  4.3335e-03,  ...,  6.5430e-02,\n",
       "         -6.1279e-02, -1.8921e-02],\n",
       "        [ 5.8838e-02, -5.8838e-02, -3.0884e-02,  ..., -5.8105e-02,\n",
       "         -6.7871e-02, -3.1494e-02]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = model.model.base_model.model.model.graph_projector.weight.to(torch.bfloat16)\n",
    "# x = model.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight.to(torch.bfloat16)\n",
    "x = model.model.base_model.model.model.graph_tower.inverW_P.weight.to(torch.bfloat16)\n",
    "# x = model.model.base_model.model.model.graph_tower.gtLayers[0].norm.weight.to(torch.bfloat16)\n",
    "print(x.dtype)\n",
    "print(x.shape)\n",
    "# x[0][:2]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0854, -0.0825, -0.0147, -0.0786,  0.0286, -0.0297, -0.0728, -0.0554,\n",
       "        -0.0195,  0.0017], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.model.base_model.model.model.graph_tower.W_P.weight[0][:10].to(torch.bfloat16)\n",
    "print(y.dtype)\n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1494, -0.1177,  0.0466,  ...,  0.0349,  0.1533,  0.0796],\n",
       "        [-0.0522,  0.1074,  0.0361,  ..., -0.0410, -0.1187, -0.0496],\n",
       "        [-0.0075,  0.0588, -0.0008,  ...,  0.0708,  0.0625, -0.0312],\n",
       "        ...,\n",
       "        [-0.0327, -0.0214, -0.0359,  ...,  0.0859, -0.0061,  0.0815],\n",
       "        [-0.0503,  0.0243,  0.0806,  ...,  0.0483,  0.0698,  0.0315],\n",
       "        [-0.1045,  0.0461, -0.0229,  ...,  0.0139,  0.0986,  0.1162]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['state_dict']['model.base_model.model.model.graph_tower.W_P.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1074, -0.0786,  0.0369,  ...,  0.0437, -0.0011, -0.0098],\n",
       "        [ 0.0403, -0.0398,  0.0618,  ..., -0.0425, -0.0114, -0.0442],\n",
       "        [-0.0139,  0.1001,  0.1025,  ..., -0.0481, -0.0806,  0.1162],\n",
       "        ...,\n",
       "        [-0.0503, -0.0742,  0.0542,  ...,  0.0938, -0.0444,  0.0417],\n",
       "        [ 0.0830, -0.0304, -0.0012,  ...,  0.0918, -0.0067, -0.0376],\n",
       "        [ 0.0635, -0.0262,  0.0050,  ..., -0.0825, -0.0532, -0.0026]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckpt['state_dict']['model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight'][0][:2]\n",
    "# ckpt['state_dict']['model.base_model.model.model.graph_tower.gtLayers.0.norm.weight']\n",
    "# ckpt['state_dict']['model.base_model.model.model.graph_projector.weight']\n",
    "print(ckpt['state_dict']['model.base_model.model.model.graph_tower.inverW_P.weight'].shape)\n",
    "ckpt['state_dict']['model.base_model.model.model.graph_tower.inverW_P.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type GraphLlama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:02<00:00, 61.28s/it]\n",
      "WARNING:root:Adding LoRA adapters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph pre train model\n",
      "CLIP(\n",
      "  (gnn): graph_transformer(\n",
      "    (gtLayers): Sequential(\n",
      "      (0): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (W_P): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (inverW_P): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "************************** parameters: # 131678208\n",
      "['base_model.model.model.embed_tokens.weight', 'base_model.model.model.graph_projector.weight', 'base_model.model.model.graph_projector.bias']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fptrained_model = GraphGPT_pl.load_from_checkpoint('/data/LPJ/ICML25/all_checkpoints/train_unfreeze_gnn_with_eval_dataset/with_module_head/v2_lr3e1_70epoch_batch2/lr3e1_70epoch_batch2_unfreeze_gnn.ckpt', training_args=train_args, model_args=model_args, data_args=data_args, tokenizer=tokenizer, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph_transformer(\n",
       "  (gtLayers): Sequential(\n",
       "    (0): GTLayer(\n",
       "      (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): GTLayer(\n",
       "      (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (2): GTLayer(\n",
       "      (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (W_P): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (inverW_P): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fptrained_model.model.base_model.model.model.graph_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fptrained_model.model.base_model.model.model.graph_tower.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainedConfig {\n",
       "  \"graph_end_token\": 0,\n",
       "  \"graph_patch_token\": 32016,\n",
       "  \"graph_start_token\": 32017,\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_graph_start_end\": true\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.graph_end_token = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainedConfig {\n",
       "  \"graph_end_token\": 0,\n",
       "  \"graph_patch_token\": 32016,\n",
       "  \"graph_start_token\": 32017,\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_graph_start_end\": true\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fptrained_model.model.base_model.model.model.graph_tower.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义一个网络\n",
    "class net(nn.Module):\n",
    "    def __init__(self, num_class=10):\n",
    "        super(net, self).__init__()\n",
    "        self.pool1 = nn.AvgPool1d(2)\n",
    "        self.bn1 = nn.BatchNorm1d(3)\n",
    "        self.fc1 = nn.Linear(12, 4)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "tensor([ 0.1875, -0.8615,  0.3708,  0.3033])\n"
     ]
    }
   ],
   "source": [
    "# 定义网络\n",
    "model = net()\n",
    "\n",
    "# 定义loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "# 定义训练数据\n",
    "x = torch.randn((3, 3, 8))\n",
    "\n",
    "model.fc1.weight.requires_grad = False  # fc1.weight不计算梯度\n",
    "print(model.fc1.weight.grad)\n",
    "print(model.fc1.bias.grad)  # fc1.bias计算梯度\n",
    "\n",
    "output = model(x)\n",
    "target = torch.tensor([1, 1, 1])\n",
    "loss = loss_fn(output, target)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(model.fc1.weight.grad)\n",
    "print(model.fc1.bias.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type GraphLlama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/yiyao_yang/anaconda3/envs/graphgpt/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Adding LoRA adapters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph pre train model\n",
      "CLIP(\n",
      "  (gnn): graph_transformer(\n",
      "    (gtLayers): Sequential(\n",
      "      (0): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (W_P): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (inverW_P): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "************************** parameters: # 131678208\n",
      "['base_model.model.model.embed_tokens.weight', 'base_model.model.model.graph_projector.weight', 'base_model.model.model.graph_projector.bias']\n"
     ]
    }
   ],
   "source": [
    "# model = GraphGPT_pl.load_from_checkpoint(checkpoint_path='/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2/last.ckpt', strict=False)\n",
    "model = GraphGPT_pl.load_from_checkpoint(checkpoint_path='/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2/last.ckpt', map_location='cpu', training_args=train_args, model_args=model_args, data_args=data_args, tokenizer=tokenizer, strict=False)\n",
    "# model = GraphGPT_pl.load_from_checkpoint(checkpoint_path='/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2/last.ckpt', training_args=None, model_args=None, data_args=None, tokenizer=None, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphGPT_pl(\n",
       "  (model): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): GraphLlamaForCausalLM(\n",
       "        (model): GraphLlamaModel(\n",
       "          (embed_tokens): Embedding(32019, 4096)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): Linear(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (k_proj): Linear(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (v_proj): Linear(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (o_proj): Linear(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear(\n",
       "                  in_features=4096, out_features=11008, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (up_proj): Linear(\n",
       "                  in_features=4096, out_features=11008, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (down_proj): Linear(\n",
       "                  in_features=11008, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (graph_tower): graph_transformer(\n",
       "            (gtLayers): Sequential(\n",
       "              (0): GTLayer(\n",
       "                (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (1): GTLayer(\n",
       "                (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GTLayer(\n",
       "                (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (W_P): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (inverW_P): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (graph_projector): Linear(in_features=128, out_features=4096, bias=True)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=32019, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.half()\n",
    "model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.70 GiB of which 44.81 MiB is free. Including non-PyTorch memory, this process has 23.65 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 4.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mergered_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_and_unload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphgpt/lib/python3.9/site-packages/transformers/modeling_utils.py:2918\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2914\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2915\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2916\u001b[0m         )\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 23.70 GiB of which 44.81 MiB is free. Including non-PyTorch memory, this process has 23.65 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 4.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "mergered_model = model.model.merge_and_unload().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for positional_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for text_projection: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.W_pos: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.0.qTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.0.kTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.0.vTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.1.qTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.1.kTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.1.vTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.2.qTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.2.kTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.2.vTrans: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.2.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.gtLayers.2.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.W_P.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.W_P.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.inverW_P.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for gnn.inverW_P.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.0.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.1.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.2.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.3.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.4.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.5.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.6.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.7.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.8.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.9.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.10.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.attn.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.attn.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.ln_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.mlp.c_fc.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.mlp.c_fc.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.mlp.c_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.mlp.c_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for transformer.resblocks.11.ln_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for token_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for ln_final.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2025: UserWarning: for ln_final.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph pre train model\n",
      "CLIP(\n",
      "  (gnn): graph_transformer(\n",
      "    (gtLayers): Sequential(\n",
      "      (0): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): GTLayer(\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (W_P): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (inverW_P): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): Sequential(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): QuickGELU()\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [05:29<00:00, 109.79s/it]\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yiyao_yang/anaconda3/envs/graphgpt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gra_model = GraphLlamaForCausalLM.from_pretrained('/data/LPJ/GraphGPT/checkpoints/GraphGPT-7B-mix-all', torch_dtype=torch.bfloat16, use_cache=True, low_cpu_mem_usage=True).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GraphLlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32019, 4096]) from checkpoint, the shape in current model is torch.Size([32003, 4096]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32019, 4096]) from checkpoint, the shape in current model is torch.Size([32003, 4096]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgra_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmergered_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(ckpt['state_dict'])\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphgpt/lib/python3.9/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GraphLlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32019, 4096]) from checkpoint, the shape in current model is torch.Size([32003, 4096]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32019, 4096]) from checkpoint, the shape in current model is torch.Size([32003, 4096])."
     ]
    }
   ],
   "source": [
    "gra_model.load_state_dict(mergered_model.state_dict())\n",
    "# model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2/last.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.base_model.model.model.embed_tokens.weight',\n",
       "              tensor([[-0.0027,  0.0359,  0.0177,  ..., -0.0029, -0.0096,  0.0086],\n",
       "                      [ 0.0069,  0.0031, -0.0013,  ...,  0.0003, -0.0031, -0.0026],\n",
       "                      [ 0.0258,  0.0088, -0.0028,  ..., -0.0051,  0.0060, -0.0001],\n",
       "                      ...,\n",
       "                      [-0.0073, -0.0049, -0.0114,  ...,  0.0157, -0.0325,  0.0228],\n",
       "                      [-0.0020,  0.0020, -0.0039,  ..., -0.0039,  0.0019,  0.0039],\n",
       "                      [-0.0021,  0.0039, -0.0039,  ..., -0.0039,  0.0016,  0.0039]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.q_proj.weight',\n",
       "              tensor([[ 2.0294e-03,  1.3657e-03, -5.2795e-03,  ...,  5.3711e-03,\n",
       "                        5.2490e-03,  4.0531e-05],\n",
       "                      [ 7.5684e-03, -5.7068e-03,  4.6997e-03,  ..., -7.3853e-03,\n",
       "                       -7.0801e-03, -9.7275e-04],\n",
       "                      [-1.4404e-02,  5.6458e-03,  9.3842e-04,  ...,  6.1951e-03,\n",
       "                        1.0315e-02,  1.0605e-03],\n",
       "                      ...,\n",
       "                      [ 1.9653e-02, -4.9744e-03,  1.1597e-02,  ...,  1.5991e-02,\n",
       "                       -1.3062e-02,  1.8406e-04],\n",
       "                      [-1.2817e-02,  1.2054e-03, -6.7139e-03,  ..., -2.0752e-02,\n",
       "                       -2.0508e-02, -6.5918e-03],\n",
       "                      [ 5.9814e-03,  3.4180e-03,  5.6458e-03,  ...,  9.0942e-03,\n",
       "                        7.4463e-03, -4.0894e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0072,  0.0008, -0.0019,  ..., -0.0134, -0.0146, -0.0074],\n",
       "                      [-0.0005,  0.0035,  0.0124,  ...,  0.0114, -0.0136, -0.0016],\n",
       "                      [-0.0025,  0.0059,  0.0081,  ..., -0.0148,  0.0132,  0.0043],\n",
       "                      ...,\n",
       "                      [ 0.0029, -0.0067, -0.0068,  ..., -0.0116,  0.0017, -0.0054],\n",
       "                      [-0.0132, -0.0091,  0.0009,  ...,  0.0128, -0.0015,  0.0035],\n",
       "                      [-0.0037,  0.0033,  0.0083,  ...,  0.0154,  0.0084, -0.0120]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.4282e-02,  1.4160e-02, -2.6550e-03,  ...,  4.6692e-03,\n",
       "                       -1.1658e-02, -9.7046e-03],\n",
       "                      [-3.0396e-02, -4.2114e-03,  4.6387e-03,  ..., -8.6060e-03,\n",
       "                        1.2451e-02,  1.2512e-02],\n",
       "                      [ 5.9128e-04, -1.6113e-02,  1.5793e-03,  ...,  1.0559e-02,\n",
       "                        5.1117e-04, -8.6670e-03],\n",
       "                      ...,\n",
       "                      [ 5.7373e-03,  4.2725e-03,  9.9182e-04,  ..., -2.8687e-03,\n",
       "                        4.0588e-03,  5.0659e-03],\n",
       "                      [-3.6011e-03,  1.9531e-03, -5.7373e-03,  ..., -2.0504e-05,\n",
       "                        4.5471e-03, -1.1047e-02],\n",
       "                      [ 2.7008e-03,  3.6774e-03,  1.5182e-03,  ...,  2.7466e-03,\n",
       "                       -9.8877e-03,  6.0730e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0077, -0.0114,  0.0063,  ..., -0.0105,  0.0150,  0.0151],\n",
       "                      [-0.0071, -0.0074,  0.0054,  ...,  0.0079, -0.0005,  0.0067],\n",
       "                      [-0.0098, -0.0075,  0.0137,  ..., -0.0021, -0.0028, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0081,  0.0120, -0.0065,  ..., -0.0058,  0.0055,  0.0153],\n",
       "                      [-0.0107, -0.0034,  0.0007,  ..., -0.0001,  0.0130,  0.0131],\n",
       "                      [ 0.0015, -0.0038, -0.0142,  ..., -0.0091, -0.0115,  0.0015]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.v_proj.weight',\n",
       "              tensor([[-2.1973e-02, -9.5367e-05, -2.6855e-03,  ...,  6.0425e-03,\n",
       "                        6.8665e-03,  6.8283e-04],\n",
       "                      [-2.8687e-03,  5.9509e-03,  2.5482e-03,  ..., -2.9325e-05,\n",
       "                       -1.1139e-03, -5.4932e-03],\n",
       "                      [ 3.8757e-03, -1.0071e-02, -2.2697e-04,  ...,  3.3112e-03,\n",
       "                       -2.0508e-02, -3.6430e-04],\n",
       "                      ...,\n",
       "                      [ 2.2583e-03, -1.3000e-02,  2.0752e-03,  ..., -6.1951e-03,\n",
       "                       -5.8594e-03,  1.1749e-03],\n",
       "                      [-2.4567e-03,  4.5166e-03, -4.0283e-03,  ..., -6.9885e-03,\n",
       "                        2.0885e-04, -4.5471e-03],\n",
       "                      [-1.5625e-02,  3.9673e-03, -9.7046e-03,  ..., -9.7275e-04,\n",
       "                       -1.3977e-02,  3.5706e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-1.2695e-02, -1.3550e-02,  5.9814e-03,  ..., -1.5564e-03,\n",
       "                       -1.7738e-04,  1.3184e-02],\n",
       "                      [-5.2490e-03, -7.3242e-03, -6.3477e-03,  ..., -2.4033e-04,\n",
       "                       -9.5215e-03,  1.1902e-02],\n",
       "                      [-6.9046e-04, -3.2806e-03,  4.2200e-05,  ..., -1.1658e-02,\n",
       "                       -4.2725e-03, -1.6499e-04],\n",
       "                      ...,\n",
       "                      [-5.9509e-04, -1.4282e-02,  5.2795e-03,  ..., -1.0986e-03,\n",
       "                       -1.4404e-02,  6.9275e-03],\n",
       "                      [ 1.8997e-03, -1.6251e-03, -5.1270e-03,  ...,  9.4604e-03,\n",
       "                        5.3711e-03, -8.1787e-03],\n",
       "                      [ 8.9111e-03,  2.3041e-03,  1.1841e-02,  ...,  3.0975e-03,\n",
       "                        2.6703e-04, -1.0925e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0013,  0.0053,  0.0056,  ..., -0.0023, -0.0038, -0.0135],\n",
       "                      [ 0.0109, -0.0003,  0.0078,  ..., -0.0046,  0.0060, -0.0091],\n",
       "                      [-0.0020,  0.0067,  0.0024,  ...,  0.0022,  0.0001, -0.0011],\n",
       "                      ...,\n",
       "                      [ 0.0107, -0.0078,  0.0003,  ...,  0.0032, -0.0029, -0.0061],\n",
       "                      [-0.0016,  0.0043, -0.0054,  ...,  0.0035,  0.0039, -0.0007],\n",
       "                      [ 0.0026, -0.0118,  0.0005,  ...,  0.0006,  0.0021, -0.0024]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0055, -0.0121,  0.0027,  ..., -0.0097, -0.0072,  0.0065],\n",
       "                      [ 0.0154, -0.0013, -0.0007,  ...,  0.0079,  0.0004, -0.0005],\n",
       "                      [-0.0056,  0.0120,  0.0139,  ..., -0.0122, -0.0135, -0.0017],\n",
       "                      ...,\n",
       "                      [ 0.0093,  0.0112, -0.0017,  ..., -0.0143, -0.0098,  0.0138],\n",
       "                      [ 0.0037, -0.0106, -0.0023,  ..., -0.0146, -0.0150,  0.0112],\n",
       "                      [-0.0106,  0.0150,  0.0156,  ...,  0.0149, -0.0150,  0.0151]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0100,  0.0187, -0.0075,  ..., -0.0148, -0.0161,  0.0258],\n",
       "                      [ 0.0030, -0.0378,  0.0135,  ..., -0.0006, -0.0067,  0.0209],\n",
       "                      [ 0.0089, -0.0171, -0.0168,  ..., -0.0244,  0.0050,  0.0068],\n",
       "                      ...,\n",
       "                      [ 0.0161, -0.0139,  0.0077,  ..., -0.0146, -0.0098, -0.0027],\n",
       "                      [ 0.0128,  0.0042, -0.0208,  ..., -0.0130,  0.0205, -0.0168],\n",
       "                      [-0.0101,  0.0112, -0.0237,  ...,  0.0194,  0.0223, -0.0297]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0020, -0.0092,  0.0006,  ...,  0.0066, -0.0022,  0.0097],\n",
       "                      [-0.0106,  0.0136, -0.0101,  ..., -0.0135, -0.0071, -0.0063],\n",
       "                      [ 0.0036, -0.0010,  0.0050,  ...,  0.0139,  0.0071, -0.0118],\n",
       "                      ...,\n",
       "                      [-0.0038, -0.0039,  0.0104,  ...,  0.0131, -0.0085,  0.0147],\n",
       "                      [ 0.0085,  0.0070,  0.0012,  ..., -0.0141, -0.0018, -0.0062],\n",
       "                      [ 0.0077,  0.0056, -0.0007,  ..., -0.0077,  0.0020,  0.0099]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.up_proj.weight',\n",
       "              tensor([[-0.0117,  0.0090,  0.0420,  ..., -0.0125, -0.0432,  0.0332],\n",
       "                      [-0.0243, -0.0006,  0.0220,  ...,  0.0045,  0.0100, -0.0022],\n",
       "                      [-0.0182,  0.0266, -0.0020,  ...,  0.0232,  0.0310,  0.0011],\n",
       "                      ...,\n",
       "                      [ 0.0116,  0.0378,  0.0099,  ...,  0.0281,  0.0190, -0.0091],\n",
       "                      [-0.0141,  0.0168, -0.0238,  ..., -0.0264, -0.0359, -0.0265],\n",
       "                      [ 0.0177,  0.0277,  0.0188,  ...,  0.0381, -0.0182, -0.0159]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0035, -0.0041, -0.0099,  ...,  0.0080, -0.0144, -0.0065],\n",
       "                      [-0.0089,  0.0020,  0.0024,  ...,  0.0151, -0.0016, -0.0096],\n",
       "                      [-0.0156, -0.0042, -0.0084,  ...,  0.0101, -0.0033, -0.0093],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.0095,  0.0056,  ..., -0.0089,  0.0045,  0.0038],\n",
       "                      [-0.0010, -0.0040,  0.0101,  ...,  0.0036, -0.0012, -0.0012],\n",
       "                      [-0.0004, -0.0095,  0.0109,  ..., -0.0119, -0.0013, -0.0119]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0081, -0.0347, -0.0091,  ..., -0.0096,  0.0153, -0.0134],\n",
       "                      [ 0.0126, -0.0108,  0.0422,  ..., -0.0007,  0.0177,  0.0549],\n",
       "                      [ 0.0459,  0.0403,  0.0231,  ..., -0.0162,  0.0261,  0.0010],\n",
       "                      ...,\n",
       "                      [-0.0064,  0.0033,  0.0029,  ...,  0.0151, -0.0304,  0.0183],\n",
       "                      [ 0.0032,  0.0242,  0.0291,  ..., -0.0261,  0.0151, -0.0013],\n",
       "                      [ 0.0386, -0.0078, -0.0317,  ...,  0.0027, -0.0315, -0.0203]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0067,  0.0025, -0.0061,  ..., -0.0055,  0.0027,  0.0089],\n",
       "                      [ 0.0023,  0.0082, -0.0060,  ...,  0.0072, -0.0013, -0.0018],\n",
       "                      [ 0.0003, -0.0064, -0.0008,  ...,  0.0094, -0.0035,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0023,  0.0066,  0.0056,  ..., -0.0091, -0.0073,  0.0095],\n",
       "                      [ 0.0095, -0.0092,  0.0049,  ...,  0.0019, -0.0003,  0.0071],\n",
       "                      [ 0.0078, -0.0020, -0.0039,  ...,  0.0048,  0.0019, -0.0014]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.input_layernorm.weight',\n",
       "              tensor([0.0208, 0.0061, 0.0020,  ..., 0.0029, 0.0103, 0.0006],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.0.post_attention_layernorm.weight',\n",
       "              tensor([0.0371, 0.0361, 0.0344,  ..., 0.0378, 0.0359, 0.0352],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0017, -0.0510, -0.0315,  ..., -0.0127, -0.0208,  0.0239],\n",
       "                      [ 0.0028, -0.0117,  0.0376,  ..., -0.0034, -0.0334,  0.0393],\n",
       "                      [ 0.0223, -0.0072,  0.0311,  ..., -0.0026, -0.0273,  0.0165],\n",
       "                      ...,\n",
       "                      [-0.0062,  0.0096,  0.0125,  ...,  0.0022, -0.0044, -0.0110],\n",
       "                      [ 0.0058, -0.0106, -0.0128,  ..., -0.0019,  0.0050,  0.0110],\n",
       "                      [-0.0040,  0.0139,  0.0160,  ..., -0.0004, -0.0040, -0.0132]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-1.0559e-02,  6.4087e-03,  2.6093e-03,  ...,  5.7220e-04,\n",
       "                       -6.3782e-03, -1.6403e-03],\n",
       "                      [ 1.0803e-02,  1.1414e-02,  1.1719e-02,  ..., -2.3041e-03,\n",
       "                        6.2256e-03, -8.4839e-03],\n",
       "                      [ 1.5259e-02, -1.1902e-02, -2.4567e-03,  ...,  1.3550e-02,\n",
       "                       -5.8289e-03,  1.4210e-04],\n",
       "                      ...,\n",
       "                      [ 9.3384e-03,  2.1057e-03, -1.4709e-02,  ..., -1.4954e-02,\n",
       "                       -1.1719e-02, -1.5015e-02],\n",
       "                      [ 1.0254e-02,  1.1108e-02,  5.3101e-03,  ..., -1.4832e-02,\n",
       "                        2.5630e-05, -1.3123e-02],\n",
       "                      [-1.4954e-02, -1.1902e-02, -6.8665e-03,  ...,  5.7983e-03,\n",
       "                        8.6670e-03, -1.2589e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.k_proj.weight',\n",
       "              tensor([[-2.5513e-02, -1.2512e-02,  2.3926e-02,  ...,  2.1240e-02,\n",
       "                        1.3062e-02, -3.3691e-02],\n",
       "                      [-3.0273e-02,  1.4465e-02, -2.0630e-02,  ..., -1.0193e-02,\n",
       "                        1.7090e-02, -1.1169e-02],\n",
       "                      [-1.8555e-02,  4.8340e-02,  1.2817e-03,  ...,  5.0049e-03,\n",
       "                       -3.0273e-02,  2.8687e-03],\n",
       "                      ...,\n",
       "                      [-1.2939e-02,  1.2756e-02, -6.5613e-03,  ...,  1.6724e-02,\n",
       "                        3.7231e-03,  1.2695e-02],\n",
       "                      [ 1.1902e-02, -1.2207e-02,  6.3782e-03,  ..., -1.6479e-02,\n",
       "                       -3.9673e-03, -1.2085e-02],\n",
       "                      [-9.2773e-03,  6.0730e-03,  4.7874e-04,  ...,  9.3384e-03,\n",
       "                       -8.0585e-05,  9.1553e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-1.4648e-02, -1.7776e-03, -1.1978e-03,  ..., -9.7656e-03,\n",
       "                        1.2268e-02, -8.9722e-03],\n",
       "                      [-1.2878e-02, -1.0193e-02, -9.5825e-03,  ...,  1.5015e-02,\n",
       "                        7.3242e-03, -2.0752e-03],\n",
       "                      [ 3.6001e-05, -2.0294e-03, -5.2490e-03,  ..., -1.4404e-02,\n",
       "                       -5.3101e-03, -3.0975e-03],\n",
       "                      ...,\n",
       "                      [ 1.0620e-02,  1.1108e-02,  7.4463e-03,  ..., -9.9182e-04,\n",
       "                       -2.1973e-03,  1.5381e-02],\n",
       "                      [-4.8218e-03,  1.5381e-02,  1.0010e-02,  ..., -7.9346e-03,\n",
       "                       -7.4463e-03,  6.1646e-03],\n",
       "                      [ 1.2756e-02, -1.2939e-02, -1.2146e-02,  ..., -2.9297e-03,\n",
       "                        6.6833e-03,  9.8705e-05]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0015, -0.0214,  0.0021,  ..., -0.0498, -0.0381, -0.0175],\n",
       "                      [-0.0162,  0.0153, -0.0098,  ...,  0.0388, -0.0159,  0.0222],\n",
       "                      [ 0.0166, -0.0146,  0.0193,  ..., -0.0378,  0.0260,  0.0064],\n",
       "                      ...,\n",
       "                      [-0.0050,  0.0023,  0.0084,  ...,  0.0076,  0.0022, -0.0116],\n",
       "                      [ 0.0020, -0.0087,  0.0063,  ..., -0.0002, -0.0052, -0.0022],\n",
       "                      [ 0.0094, -0.0043, -0.0016,  ..., -0.0227,  0.0046,  0.0146]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0143, -0.0013, -0.0087,  ...,  0.0077,  0.0121, -0.0105],\n",
       "                      [-0.0099, -0.0094,  0.0050,  ...,  0.0016, -0.0112, -0.0084],\n",
       "                      [-0.0024, -0.0039, -0.0120,  ...,  0.0149,  0.0008, -0.0044],\n",
       "                      ...,\n",
       "                      [-0.0113, -0.0123, -0.0146,  ...,  0.0104,  0.0112, -0.0093],\n",
       "                      [ 0.0142,  0.0121, -0.0113,  ..., -0.0026, -0.0102,  0.0089],\n",
       "                      [ 0.0072, -0.0072, -0.0120,  ..., -0.0030,  0.0128,  0.0152]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0041, -0.0182, -0.0233,  ...,  0.0013,  0.0033,  0.0034],\n",
       "                      [ 0.0223, -0.0075,  0.0070,  ...,  0.0078,  0.0115, -0.0082],\n",
       "                      [ 0.0208,  0.0120, -0.0129,  ...,  0.0140, -0.0111,  0.0084],\n",
       "                      ...,\n",
       "                      [ 0.0006, -0.0276,  0.0068,  ..., -0.0033,  0.0023,  0.0120],\n",
       "                      [ 0.0085, -0.0182,  0.0095,  ...,  0.0099, -0.0040, -0.0072],\n",
       "                      [-0.0105, -0.0294, -0.0073,  ...,  0.0045, -0.0062, -0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0139, -0.0151, -0.0077,  ...,  0.0016, -0.0073, -0.0011],\n",
       "                      [ 0.0088,  0.0090, -0.0037,  ..., -0.0145, -0.0118,  0.0033],\n",
       "                      [-0.0049, -0.0066, -0.0127,  ..., -0.0017,  0.0121,  0.0135],\n",
       "                      ...,\n",
       "                      [-0.0018, -0.0030,  0.0134,  ..., -0.0139, -0.0074, -0.0023],\n",
       "                      [ 0.0129, -0.0008, -0.0145,  ..., -0.0140, -0.0129,  0.0146],\n",
       "                      [-0.0034, -0.0044, -0.0005,  ..., -0.0038, -0.0042,  0.0122]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0598,  0.0006,  0.0083,  ..., -0.0215, -0.0461, -0.0481],\n",
       "                      [-0.0189, -0.0178, -0.0304,  ...,  0.0439, -0.0371, -0.0315],\n",
       "                      [-0.0175, -0.0243,  0.0193,  ..., -0.0060, -0.0334, -0.0182],\n",
       "                      ...,\n",
       "                      [ 0.0036,  0.0330, -0.0167,  ..., -0.0728,  0.0026,  0.0120],\n",
       "                      [-0.0057,  0.0233, -0.0376,  ...,  0.0459,  0.0067, -0.0095],\n",
       "                      [-0.0067,  0.0076,  0.0231,  ...,  0.0432,  0.0172, -0.0194]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0061,  0.0151,  0.0106,  ..., -0.0041, -0.0045,  0.0020],\n",
       "                      [ 0.0020, -0.0141,  0.0085,  ..., -0.0155,  0.0067, -0.0023],\n",
       "                      [ 0.0054, -0.0100, -0.0045,  ...,  0.0156,  0.0087,  0.0042],\n",
       "                      ...,\n",
       "                      [-0.0032, -0.0066,  0.0081,  ..., -0.0095, -0.0113, -0.0138],\n",
       "                      [ 0.0023,  0.0035, -0.0071,  ..., -0.0068, -0.0098,  0.0096],\n",
       "                      [ 0.0036, -0.0073, -0.0023,  ..., -0.0125, -0.0137,  0.0150]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.up_proj.weight',\n",
       "              tensor([[-0.0062,  0.0442, -0.0332,  ..., -0.0012,  0.0287, -0.0396],\n",
       "                      [-0.0197, -0.0063,  0.0266,  ...,  0.0150,  0.0103,  0.0090],\n",
       "                      [ 0.0099, -0.0273,  0.0195,  ..., -0.0280, -0.0459,  0.0277],\n",
       "                      ...,\n",
       "                      [ 0.0109, -0.0248, -0.0047,  ..., -0.0056,  0.0371, -0.0278],\n",
       "                      [ 0.0094, -0.0006, -0.0231,  ..., -0.0236, -0.0023,  0.0079],\n",
       "                      [ 0.0142, -0.0327, -0.0267,  ..., -0.0674,  0.0034,  0.0234]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 1.0437e-02,  7.6904e-03,  1.8082e-03,  ..., -6.5613e-03,\n",
       "                       -5.6763e-03, -1.1169e-02],\n",
       "                      [ 5.1270e-03, -1.1902e-02, -1.3489e-02,  ...,  8.4839e-03,\n",
       "                        2.3346e-03,  1.9073e-03],\n",
       "                      [-1.0315e-02, -9.2773e-03, -1.0986e-02,  ..., -4.8218e-03,\n",
       "                       -1.1292e-03,  3.0670e-03],\n",
       "                      ...,\n",
       "                      [ 1.1841e-02,  7.1716e-03, -4.1246e-05,  ..., -1.2695e-02,\n",
       "                       -1.5182e-03, -1.0925e-02],\n",
       "                      [-5.9509e-03, -1.3550e-02,  1.4221e-02,  ..., -4.3945e-03,\n",
       "                        1.5259e-02, -6.3782e-03],\n",
       "                      [ 2.9297e-03,  1.2573e-02, -6.8665e-03,  ..., -6.0654e-04,\n",
       "                       -1.0071e-02, -5.6076e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.down_proj.weight',\n",
       "              tensor([[-0.0062, -0.0166,  0.0596,  ..., -0.0189,  0.0205, -0.0092],\n",
       "                      [ 0.0312, -0.0107, -0.0208,  ...,  0.0195, -0.0135, -0.0020],\n",
       "                      [-0.0264, -0.0011, -0.0320,  ...,  0.0173, -0.0044, -0.0211],\n",
       "                      ...,\n",
       "                      [ 0.0229, -0.0081,  0.0060,  ...,  0.0317,  0.0242,  0.0131],\n",
       "                      [-0.0208, -0.0031, -0.0237,  ..., -0.0061, -0.0349,  0.0176],\n",
       "                      [ 0.0004,  0.0013, -0.0243,  ..., -0.0208,  0.0048, -0.0162]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-4.5166e-03,  2.9449e-03,  3.9673e-03,  ...,  1.1978e-03,\n",
       "                       -4.1809e-03,  6.9885e-03],\n",
       "                      [ 6.1646e-03, -4.6997e-03,  7.8735e-03,  ...,  7.4768e-03,\n",
       "                        3.6926e-03,  2.6093e-03],\n",
       "                      [ 6.2256e-03,  1.6251e-03, -1.2741e-03,  ..., -7.6904e-03,\n",
       "                        3.2043e-03, -1.6632e-03],\n",
       "                      ...,\n",
       "                      [ 8.9722e-03,  8.1177e-03,  6.8054e-03,  ..., -7.5684e-03,\n",
       "                       -8.5449e-03,  7.7248e-05],\n",
       "                      [ 2.7466e-03, -2.5482e-03, -2.4796e-05,  ..., -6.5613e-03,\n",
       "                        5.1880e-03,  8.4839e-03],\n",
       "                      [ 5.6152e-03, -1.4343e-03, -9.1171e-04,  ..., -9.3384e-03,\n",
       "                       -9.4604e-03, -3.9673e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.input_layernorm.weight',\n",
       "              tensor([0.0625, 0.0762, 0.0674,  ..., 0.0376, 0.0618, 0.0466],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.1.post_attention_layernorm.weight',\n",
       "              tensor([0.0742, 0.0703, 0.0693,  ..., 0.0786, 0.0713, 0.0728],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0249, -0.0128,  0.0057,  ...,  0.0050,  0.0119,  0.0007],\n",
       "                      [ 0.0022, -0.0405, -0.0376,  ..., -0.0442, -0.0100,  0.0281],\n",
       "                      [-0.0210,  0.0366,  0.0015,  ...,  0.0206, -0.0327,  0.0081],\n",
       "                      ...,\n",
       "                      [-0.0366,  0.0016, -0.0498,  ..., -0.0413, -0.0708, -0.0396],\n",
       "                      [-0.0009, -0.0140,  0.0024,  ...,  0.0040, -0.0089,  0.0028],\n",
       "                      [-0.0289, -0.0300,  0.0098,  ...,  0.0933, -0.0928, -0.0109]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0054, -0.0023,  0.0073,  ..., -0.0154, -0.0095,  0.0073],\n",
       "                      [-0.0021, -0.0144, -0.0017,  ..., -0.0146,  0.0109,  0.0024],\n",
       "                      [ 0.0110, -0.0095,  0.0059,  ...,  0.0047, -0.0032, -0.0040],\n",
       "                      ...,\n",
       "                      [-0.0016,  0.0061, -0.0018,  ..., -0.0070,  0.0021, -0.0137],\n",
       "                      [ 0.0064,  0.0098, -0.0011,  ..., -0.0017, -0.0002,  0.0054],\n",
       "                      [-0.0124, -0.0014,  0.0134,  ...,  0.0095,  0.0098,  0.0098]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.k_proj.weight',\n",
       "              tensor([[-2.3041e-03,  1.8188e-02, -4.8828e-03,  ..., -1.3184e-02,\n",
       "                       -1.0254e-02,  2.5635e-03],\n",
       "                      [ 3.4424e-02, -1.5991e-02, -4.2969e-02,  ..., -1.1841e-02,\n",
       "                       -1.7456e-02,  7.6904e-03],\n",
       "                      [ 8.3160e-04,  3.0212e-03,  6.4453e-02,  ...,  1.1597e-02,\n",
       "                        1.2970e-03,  1.7090e-02],\n",
       "                      ...,\n",
       "                      [ 5.9326e-02, -5.2795e-03, -3.4668e-02,  ...,  7.9956e-03,\n",
       "                       -1.1182e-01, -3.9795e-02],\n",
       "                      [-2.5940e-03, -1.4893e-02,  3.5400e-03,  ...,  9.7656e-03,\n",
       "                       -4.3631e-05, -5.5237e-03],\n",
       "                      [ 3.3691e-02, -4.9805e-02, -5.0537e-02,  ..., -9.7656e-02,\n",
       "                       -7.7637e-02,  6.1035e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0076, -0.0118, -0.0051,  ...,  0.0132, -0.0153, -0.0156],\n",
       "                      [-0.0021, -0.0057,  0.0035,  ...,  0.0070,  0.0079,  0.0122],\n",
       "                      [ 0.0141, -0.0071,  0.0004,  ...,  0.0120, -0.0134, -0.0151],\n",
       "                      ...,\n",
       "                      [-0.0113, -0.0148, -0.0017,  ...,  0.0156, -0.0118,  0.0089],\n",
       "                      [-0.0068,  0.0106,  0.0030,  ...,  0.0085, -0.0051,  0.0017],\n",
       "                      [ 0.0142, -0.0136,  0.0015,  ...,  0.0071,  0.0107,  0.0148]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0055, -0.0049, -0.0026,  ...,  0.0267, -0.0156, -0.0095],\n",
       "                      [ 0.0214,  0.0244,  0.0044,  ..., -0.0121,  0.0075, -0.0140],\n",
       "                      [ 0.0116,  0.0134, -0.0182,  ..., -0.0547, -0.0024, -0.0254],\n",
       "                      ...,\n",
       "                      [-0.0018, -0.0325, -0.0167,  ..., -0.0096, -0.0334, -0.0072],\n",
       "                      [-0.0272,  0.0070,  0.0291,  ..., -0.0003,  0.0018,  0.0165],\n",
       "                      [ 0.0076, -0.0057, -0.0120,  ...,  0.0081, -0.0205, -0.0060]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0153, -0.0143,  0.0093,  ...,  0.0010, -0.0036, -0.0100],\n",
       "                      [-0.0001, -0.0075,  0.0081,  ...,  0.0151, -0.0009,  0.0072],\n",
       "                      [ 0.0125,  0.0009, -0.0072,  ...,  0.0123,  0.0120,  0.0068],\n",
       "                      ...,\n",
       "                      [-0.0118, -0.0114,  0.0067,  ...,  0.0047,  0.0131, -0.0004],\n",
       "                      [ 0.0064,  0.0039,  0.0079,  ..., -0.0130,  0.0088,  0.0049],\n",
       "                      [-0.0082,  0.0092,  0.0008,  ..., -0.0055, -0.0095, -0.0110]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0017, -0.0040,  0.0011,  ...,  0.0175, -0.0206, -0.0177],\n",
       "                      [ 0.0270, -0.0273,  0.0031,  ..., -0.0084, -0.0267, -0.0413],\n",
       "                      [-0.0244,  0.0226,  0.0074,  ..., -0.0062, -0.0208, -0.0089],\n",
       "                      ...,\n",
       "                      [-0.0171, -0.0298,  0.0250,  ...,  0.0023, -0.0203,  0.0280],\n",
       "                      [ 0.0090, -0.0002,  0.0320,  ...,  0.0105, -0.0132, -0.0219],\n",
       "                      [-0.0112, -0.0245, -0.0203,  ...,  0.0089,  0.0119,  0.0018]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-6.3477e-03, -1.2207e-02, -1.5564e-03,  ...,  1.3245e-02,\n",
       "                       -2.6093e-03, -1.2817e-02],\n",
       "                      [ 1.3184e-02, -4.0770e-05, -7.5684e-03,  ..., -1.5076e-02,\n",
       "                        1.0803e-02,  5.9509e-03],\n",
       "                      [ 5.2185e-03, -1.8311e-03, -1.7014e-03,  ..., -4.8828e-03,\n",
       "                       -1.8768e-03, -1.3794e-02],\n",
       "                      ...,\n",
       "                      [-1.2390e-02,  7.7209e-03,  3.1281e-03,  ..., -9.2163e-03,\n",
       "                       -8.6212e-04, -4.1809e-03],\n",
       "                      [ 3.9101e-04,  6.5613e-03,  1.3367e-02,  ..., -5.6076e-04,\n",
       "                        1.3123e-02,  7.9346e-03],\n",
       "                      [-6.7749e-03,  1.2817e-02, -9.9487e-03,  ..., -1.3550e-02,\n",
       "                       -7.9956e-03,  5.2795e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.gate_proj.weight',\n",
       "              tensor([[-2.9541e-02,  7.4768e-03, -1.0193e-02,  ..., -2.5391e-02,\n",
       "                        3.2715e-02,  1.3306e-02],\n",
       "                      [ 1.0498e-02,  2.4109e-03, -2.3438e-02,  ...,  1.7944e-02,\n",
       "                       -3.3691e-02,  1.0864e-02],\n",
       "                      [ 9.6436e-03,  2.2827e-02,  1.9379e-03,  ..., -1.6846e-02,\n",
       "                       -2.1362e-03, -3.9368e-03],\n",
       "                      ...,\n",
       "                      [ 4.5410e-02, -3.8910e-03,  2.8687e-03,  ..., -6.2012e-02,\n",
       "                       -9.8877e-03,  7.0801e-03],\n",
       "                      [ 5.8711e-06, -4.3213e-02, -2.4780e-02,  ...,  1.4038e-02,\n",
       "                       -5.4199e-02, -1.1475e-02],\n",
       "                      [ 3.0396e-02,  3.7109e-02,  8.4305e-04,  ..., -2.2339e-02,\n",
       "                       -3.5645e-02,  1.5564e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0067, -0.0070,  0.0119,  ..., -0.0049, -0.0066,  0.0112],\n",
       "                      [ 0.0041,  0.0140,  0.0113,  ..., -0.0058, -0.0097,  0.0023],\n",
       "                      [-0.0004,  0.0085,  0.0128,  ...,  0.0101,  0.0027, -0.0065],\n",
       "                      ...,\n",
       "                      [ 0.0022,  0.0125, -0.0047,  ...,  0.0038, -0.0126, -0.0110],\n",
       "                      [-0.0152, -0.0091,  0.0123,  ...,  0.0086, -0.0109,  0.0057],\n",
       "                      [ 0.0128,  0.0148, -0.0103,  ..., -0.0150, -0.0064, -0.0034]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.up_proj.weight',\n",
       "              tensor([[-0.0007,  0.0194, -0.0059,  ..., -0.0287, -0.0092,  0.0496],\n",
       "                      [-0.0026, -0.0282, -0.0136,  ...,  0.0259,  0.0168, -0.0258],\n",
       "                      [ 0.0288,  0.0182,  0.0410,  ...,  0.0075, -0.0145,  0.0123],\n",
       "                      ...,\n",
       "                      [-0.0299, -0.0298, -0.0189,  ..., -0.0096,  0.0226,  0.0190],\n",
       "                      [ 0.0027, -0.0122, -0.0026,  ..., -0.0260,  0.0364, -0.0435],\n",
       "                      [-0.0371, -0.0238, -0.0236,  ..., -0.0120,  0.0454, -0.0120]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0004,  0.0114,  0.0042,  ...,  0.0093,  0.0019, -0.0063],\n",
       "                      [-0.0116,  0.0016,  0.0039,  ..., -0.0034, -0.0011,  0.0104],\n",
       "                      [-0.0053,  0.0154, -0.0003,  ..., -0.0012, -0.0063,  0.0020],\n",
       "                      ...,\n",
       "                      [ 0.0146,  0.0093,  0.0110,  ..., -0.0128, -0.0048,  0.0129],\n",
       "                      [-0.0015,  0.0028, -0.0007,  ..., -0.0143, -0.0123,  0.0004],\n",
       "                      [ 0.0002,  0.0134,  0.0082,  ...,  0.0062,  0.0089, -0.0037]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0094, -0.0126,  0.0256,  ..., -0.0366, -0.0137, -0.0593],\n",
       "                      [-0.0004, -0.0569, -0.0167,  ..., -0.0330, -0.0181, -0.0493],\n",
       "                      [-0.0057, -0.0554,  0.0243,  ..., -0.0513, -0.0053, -0.0791],\n",
       "                      ...,\n",
       "                      [-0.0231,  0.0081, -0.0205,  ...,  0.0160, -0.0244, -0.0110],\n",
       "                      [-0.0099,  0.0281,  0.0002,  ...,  0.0061,  0.0232,  0.0098],\n",
       "                      [ 0.0190,  0.0072, -0.0294,  ..., -0.0036, -0.0160, -0.0197]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0087,  0.0079,  0.0051,  ...,  0.0049, -0.0070, -0.0070],\n",
       "                      [ 0.0057, -0.0064, -0.0036,  ...,  0.0007, -0.0028,  0.0066],\n",
       "                      [ 0.0083, -0.0082,  0.0027,  ..., -0.0067,  0.0025,  0.0068],\n",
       "                      ...,\n",
       "                      [-0.0068, -0.0028, -0.0027,  ..., -0.0085,  0.0078, -0.0029],\n",
       "                      [-0.0060, -0.0061,  0.0060,  ...,  0.0087,  0.0052,  0.0016],\n",
       "                      [ 0.0024, -0.0063,  0.0040,  ..., -0.0005,  0.0087, -0.0012]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.input_layernorm.weight',\n",
       "              tensor([0.1318, 0.1357, 0.1318,  ..., 0.1299, 0.1260, 0.1221],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.2.post_attention_layernorm.weight',\n",
       "              tensor([0.0991, 0.0972, 0.0991,  ..., 0.1050, 0.1025, 0.1035],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0055,  0.0107,  0.0060,  ..., -0.0064, -0.0056,  0.0229],\n",
       "                      [-0.0276,  0.0065,  0.0251,  ..., -0.0267, -0.0102,  0.0081],\n",
       "                      [ 0.0304,  0.0055, -0.0194,  ...,  0.0112, -0.0193, -0.0415],\n",
       "                      ...,\n",
       "                      [ 0.0903, -0.0776,  0.0500,  ..., -0.0732, -0.0255,  0.0459],\n",
       "                      [-0.0786,  0.0669, -0.0315,  ...,  0.0554,  0.0894,  0.0498],\n",
       "                      [ 0.0101, -0.0359,  0.0610,  ..., -0.0027, -0.0211,  0.0045]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0008, -0.0076,  0.0073,  ...,  0.0110,  0.0036,  0.0039],\n",
       "                      [-0.0050, -0.0126, -0.0006,  ...,  0.0089, -0.0095, -0.0012],\n",
       "                      [-0.0126, -0.0091,  0.0114,  ..., -0.0136, -0.0126, -0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0080, -0.0084, -0.0110,  ...,  0.0128,  0.0088,  0.0088],\n",
       "                      [ 0.0038,  0.0030, -0.0151,  ..., -0.0079,  0.0051, -0.0008],\n",
       "                      [ 0.0049, -0.0052, -0.0068,  ...,  0.0114, -0.0017, -0.0135]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0043,  0.0060, -0.0036,  ..., -0.0018,  0.0032,  0.0128],\n",
       "                      [-0.0352,  0.0413, -0.0503,  ..., -0.0058, -0.0189,  0.0105],\n",
       "                      [ 0.0192,  0.0113,  0.0056,  ...,  0.0034, -0.0356, -0.0425],\n",
       "                      ...,\n",
       "                      [ 0.1108, -0.1006, -0.0040,  ..., -0.0391, -0.0201,  0.0481],\n",
       "                      [-0.1099,  0.0396,  0.0236,  ...,  0.0256,  0.0825,  0.0122],\n",
       "                      [-0.0057, -0.0437,  0.1235,  ..., -0.0110, -0.0093, -0.0110]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0098,  0.0140,  0.0055,  ...,  0.0104,  0.0104, -0.0076],\n",
       "                      [-0.0028,  0.0073,  0.0056,  ...,  0.0124,  0.0129,  0.0141],\n",
       "                      [ 0.0053,  0.0011,  0.0063,  ...,  0.0036, -0.0014,  0.0121],\n",
       "                      ...,\n",
       "                      [ 0.0133, -0.0032,  0.0076,  ...,  0.0080,  0.0013,  0.0079],\n",
       "                      [-0.0054, -0.0045,  0.0110,  ..., -0.0143,  0.0092,  0.0154],\n",
       "                      [ 0.0056, -0.0023,  0.0127,  ...,  0.0042, -0.0122,  0.0109]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0302, -0.0031,  0.0371,  ..., -0.0022,  0.0083, -0.0143],\n",
       "                      [ 0.0093,  0.0396,  0.0029,  ..., -0.0112, -0.0189, -0.0354],\n",
       "                      [ 0.0103,  0.0119,  0.0084,  ..., -0.0175,  0.0059, -0.0223],\n",
       "                      ...,\n",
       "                      [ 0.0065, -0.0017, -0.0027,  ..., -0.0035, -0.0006,  0.0003],\n",
       "                      [-0.0021,  0.0029,  0.0071,  ...,  0.0005, -0.0022,  0.0033],\n",
       "                      [-0.0041, -0.0103, -0.0042,  ...,  0.0107, -0.0057, -0.0017]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0047,  0.0002, -0.0009,  ..., -0.0043, -0.0003, -0.0045],\n",
       "                      [-0.0034, -0.0055, -0.0068,  ..., -0.0139,  0.0101, -0.0126],\n",
       "                      [-0.0104,  0.0141,  0.0095,  ...,  0.0052, -0.0082, -0.0140],\n",
       "                      ...,\n",
       "                      [ 0.0081, -0.0114,  0.0005,  ...,  0.0103, -0.0098,  0.0106],\n",
       "                      [-0.0009,  0.0096, -0.0065,  ..., -0.0086, -0.0113, -0.0126],\n",
       "                      [ 0.0115, -0.0025,  0.0056,  ...,  0.0003,  0.0109,  0.0080]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0148,  0.0073, -0.0061,  ...,  0.0048, -0.0148, -0.0019],\n",
       "                      [-0.0172, -0.0117, -0.0045,  ...,  0.0013, -0.0019,  0.0118],\n",
       "                      [-0.0283,  0.0084,  0.0070,  ...,  0.0019, -0.0129,  0.0155],\n",
       "                      ...,\n",
       "                      [ 0.0210, -0.0047,  0.0242,  ..., -0.0121, -0.0030, -0.0086],\n",
       "                      [ 0.0288,  0.0289,  0.0159,  ...,  0.0033, -0.0066,  0.0074],\n",
       "                      [ 0.0452, -0.0134,  0.0193,  ...,  0.0100, -0.0002, -0.0036]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0084,  0.0057,  0.0134,  ...,  0.0103, -0.0111, -0.0005],\n",
       "                      [ 0.0077, -0.0130,  0.0022,  ..., -0.0013,  0.0112,  0.0113],\n",
       "                      [-0.0104,  0.0111, -0.0054,  ...,  0.0082,  0.0070, -0.0099],\n",
       "                      ...,\n",
       "                      [ 0.0026,  0.0061, -0.0009,  ...,  0.0137,  0.0067,  0.0040],\n",
       "                      [ 0.0020, -0.0089,  0.0119,  ..., -0.0153,  0.0044, -0.0035],\n",
       "                      [-0.0004, -0.0085,  0.0035,  ..., -0.0021,  0.0049, -0.0009]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0045,  0.0027,  0.0110,  ...,  0.0139,  0.0366, -0.0139],\n",
       "                      [ 0.0182,  0.0046, -0.0155,  ...,  0.0332, -0.0435, -0.0210],\n",
       "                      [-0.0654, -0.0143, -0.0035,  ..., -0.0053, -0.0302, -0.0625],\n",
       "                      ...,\n",
       "                      [ 0.0156,  0.0510, -0.0101,  ...,  0.0334, -0.0249, -0.0046],\n",
       "                      [-0.0034,  0.0061,  0.0101,  ...,  0.0007,  0.0026, -0.0100],\n",
       "                      [ 0.0240,  0.0023,  0.0194,  ..., -0.0223, -0.0131, -0.0176]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0082, -0.0092,  0.0049,  ...,  0.0130,  0.0034, -0.0095],\n",
       "                      [-0.0092, -0.0047, -0.0074,  ..., -0.0142, -0.0070, -0.0097],\n",
       "                      [ 0.0154, -0.0126, -0.0142,  ..., -0.0068, -0.0082,  0.0128],\n",
       "                      ...,\n",
       "                      [ 0.0104,  0.0020,  0.0105,  ...,  0.0145,  0.0104,  0.0073],\n",
       "                      [-0.0024, -0.0145,  0.0015,  ..., -0.0153,  0.0093,  0.0064],\n",
       "                      [ 0.0041,  0.0076,  0.0041,  ...,  0.0016,  0.0102, -0.0019]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0042, -0.0095,  0.0349,  ...,  0.0118,  0.0120, -0.0141],\n",
       "                      [-0.0130,  0.0033,  0.0569,  ...,  0.0145,  0.0442, -0.0160],\n",
       "                      [-0.0122, -0.0342, -0.0238,  ...,  0.0179, -0.0466,  0.0010],\n",
       "                      ...,\n",
       "                      [-0.0029,  0.0309,  0.0205,  ...,  0.0148,  0.0049,  0.0267],\n",
       "                      [ 0.0114,  0.0042, -0.0273,  ...,  0.0096, -0.0089, -0.0153],\n",
       "                      [-0.0067,  0.0134, -0.0023,  ...,  0.0140, -0.0204, -0.0025]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0008,  0.0049, -0.0096,  ..., -0.0082, -0.0153,  0.0017],\n",
       "                      [ 0.0026,  0.0089,  0.0075,  ..., -0.0054, -0.0026,  0.0096],\n",
       "                      [-0.0075,  0.0079,  0.0129,  ...,  0.0004,  0.0031, -0.0128],\n",
       "                      ...,\n",
       "                      [-0.0063,  0.0156,  0.0075,  ...,  0.0072, -0.0098,  0.0142],\n",
       "                      [ 0.0148,  0.0116, -0.0057,  ..., -0.0082, -0.0037,  0.0092],\n",
       "                      [-0.0029,  0.0109, -0.0067,  ...,  0.0113,  0.0146,  0.0024]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.down_proj.weight',\n",
       "              tensor([[-0.0422, -0.0106, -0.0115,  ...,  0.0012,  0.0088,  0.0147],\n",
       "                      [ 0.0309,  0.0150, -0.0167,  ..., -0.0171, -0.0039, -0.0094],\n",
       "                      [ 0.0070,  0.0141, -0.0222,  ...,  0.0004, -0.0061, -0.0192],\n",
       "                      ...,\n",
       "                      [-0.0126,  0.0048,  0.0178,  ...,  0.0089,  0.0140,  0.0330],\n",
       "                      [ 0.0077,  0.0036, -0.0466,  ...,  0.0179, -0.0089, -0.0065],\n",
       "                      [ 0.0122, -0.0114, -0.0398,  ..., -0.0243,  0.0198,  0.0177]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-4.6082e-03, -8.7280e-03,  4.4861e-03,  ..., -7.3547e-03,\n",
       "                        8.9111e-03, -2.4109e-03],\n",
       "                      [ 8.9722e-03, -4.8218e-03, -1.8005e-03,  ..., -6.9580e-03,\n",
       "                       -5.9128e-04, -5.4626e-03],\n",
       "                      [-5.6458e-03,  8.9111e-03, -4.1809e-03,  ...,  5.5847e-03,\n",
       "                       -7.4158e-03,  5.5237e-03],\n",
       "                      ...,\n",
       "                      [ 2.9907e-03,  8.8501e-03,  8.0566e-03,  ...,  7.4158e-03,\n",
       "                        1.8463e-03, -3.0365e-03],\n",
       "                      [-8.2016e-05,  5.0049e-03, -9.0942e-03,  ...,  4.7302e-03,\n",
       "                       -2.4261e-03, -1.3275e-03],\n",
       "                      [ 5.5847e-03,  3.7384e-03, -6.2866e-03,  ...,  8.8501e-03,\n",
       "                       -2.3804e-03, -1.0757e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.input_layernorm.weight',\n",
       "              tensor([0.1914, 0.1914, 0.1934,  ..., 0.1855, 0.2012, 0.1973],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.3.post_attention_layernorm.weight',\n",
       "              tensor([0.1289, 0.1270, 0.1221,  ..., 0.1309, 0.1250, 0.1235],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0065, -0.0144, -0.0033,  ...,  0.0106,  0.0132, -0.0120],\n",
       "                      [-0.0105,  0.0060,  0.0026,  ...,  0.0388,  0.0120, -0.0121],\n",
       "                      [ 0.0413, -0.0284,  0.0386,  ...,  0.0255,  0.0086, -0.0864],\n",
       "                      ...,\n",
       "                      [-0.0236,  0.0591, -0.0564,  ...,  0.0083, -0.0059, -0.1118],\n",
       "                      [-0.0898, -0.0260,  0.0049,  ...,  0.0115, -0.0148, -0.0078],\n",
       "                      [-0.0238, -0.0148,  0.0354,  ...,  0.0566,  0.0688, -0.0850]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0016, -0.0052, -0.0085,  ...,  0.0036, -0.0154, -0.0015],\n",
       "                      [-0.0085, -0.0051,  0.0049,  ..., -0.0116,  0.0143,  0.0069],\n",
       "                      [-0.0033,  0.0072, -0.0112,  ...,  0.0032, -0.0074,  0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0150,  0.0073, -0.0118,  ...,  0.0125, -0.0038,  0.0055],\n",
       "                      [ 0.0003, -0.0052, -0.0042,  ...,  0.0019,  0.0011,  0.0125],\n",
       "                      [-0.0072, -0.0022, -0.0015,  ...,  0.0046,  0.0096, -0.0151]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0134,  0.0138, -0.0038,  ..., -0.0010, -0.0178,  0.0059],\n",
       "                      [-0.0708, -0.0013, -0.0041,  ...,  0.0077,  0.0161,  0.0144],\n",
       "                      [-0.0306,  0.0065, -0.0248,  ...,  0.0305,  0.0309,  0.0596],\n",
       "                      ...,\n",
       "                      [ 0.0515,  0.0649,  0.0688,  ...,  0.0075,  0.0031, -0.0486],\n",
       "                      [ 0.0713,  0.0583, -0.0102,  ...,  0.1016, -0.0413,  0.0225],\n",
       "                      [-0.0292,  0.1157,  0.0625,  ...,  0.0408,  0.0259, -0.0280]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0117, -0.0067,  0.0023,  ...,  0.0084,  0.0121, -0.0047],\n",
       "                      [-0.0103, -0.0098,  0.0143,  ...,  0.0052,  0.0051, -0.0018],\n",
       "                      [ 0.0003,  0.0031, -0.0093,  ..., -0.0014,  0.0065,  0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0041, -0.0024, -0.0117,  ...,  0.0095, -0.0123,  0.0148],\n",
       "                      [-0.0131,  0.0036, -0.0117,  ..., -0.0103,  0.0099, -0.0038],\n",
       "                      [-0.0017,  0.0139, -0.0122,  ...,  0.0068,  0.0055, -0.0068]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0179, -0.0071, -0.0197,  ..., -0.0317,  0.0364,  0.0142],\n",
       "                      [ 0.0015,  0.0330,  0.0223,  ...,  0.0079,  0.0078, -0.0270],\n",
       "                      [ 0.0276, -0.0162, -0.0145,  ...,  0.0312,  0.0171, -0.0165],\n",
       "                      ...,\n",
       "                      [-0.0278, -0.0154, -0.0182,  ...,  0.0410, -0.0145,  0.0092],\n",
       "                      [-0.0081, -0.0053, -0.0107,  ..., -0.0227,  0.0078, -0.0060],\n",
       "                      [-0.0098, -0.0023, -0.0089,  ..., -0.0065, -0.0178,  0.0303]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0110,  0.0090,  0.0083,  ..., -0.0117,  0.0107, -0.0094],\n",
       "                      [ 0.0043,  0.0095, -0.0006,  ..., -0.0084,  0.0030,  0.0119],\n",
       "                      [-0.0128,  0.0043, -0.0008,  ..., -0.0121,  0.0060, -0.0155],\n",
       "                      ...,\n",
       "                      [-0.0110, -0.0059,  0.0140,  ..., -0.0041, -0.0105, -0.0106],\n",
       "                      [-0.0062,  0.0088,  0.0061,  ...,  0.0114, -0.0018,  0.0039],\n",
       "                      [-0.0153, -0.0087, -0.0032,  ...,  0.0151,  0.0011, -0.0024]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0277,  0.0076, -0.0079,  ...,  0.0104,  0.0044,  0.0356],\n",
       "                      [ 0.0097, -0.0157,  0.0050,  ...,  0.0026, -0.0167,  0.0151],\n",
       "                      [-0.0010,  0.0192, -0.0015,  ..., -0.0083,  0.0415,  0.0291],\n",
       "                      ...,\n",
       "                      [-0.0147,  0.0227, -0.0396,  ..., -0.0080,  0.0371,  0.0099],\n",
       "                      [ 0.0289, -0.0034,  0.0062,  ..., -0.0144, -0.0009,  0.0134],\n",
       "                      [ 0.0347,  0.0466,  0.0079,  ..., -0.0100,  0.0199, -0.0150]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0107, -0.0069, -0.0121,  ..., -0.0073, -0.0024, -0.0056],\n",
       "                      [ 0.0043, -0.0099, -0.0148,  ...,  0.0042, -0.0105, -0.0076],\n",
       "                      [ 0.0058,  0.0143, -0.0087,  ..., -0.0128,  0.0036, -0.0036],\n",
       "                      ...,\n",
       "                      [ 0.0085, -0.0028, -0.0080,  ...,  0.0084, -0.0065, -0.0053],\n",
       "                      [ 0.0035,  0.0036, -0.0079,  ...,  0.0109, -0.0039,  0.0091],\n",
       "                      [ 0.0142,  0.0058, -0.0071,  ..., -0.0141,  0.0060,  0.0031]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0111, -0.0077,  0.0089,  ..., -0.0344, -0.0069, -0.0073],\n",
       "                      [ 0.0220, -0.0065,  0.0167,  ...,  0.0055,  0.0315,  0.0311],\n",
       "                      [-0.0099, -0.0381, -0.0105,  ..., -0.0403, -0.0417,  0.0035],\n",
       "                      ...,\n",
       "                      [-0.0277,  0.0093,  0.0106,  ..., -0.0579,  0.0422, -0.0170],\n",
       "                      [ 0.0031,  0.0140,  0.0056,  ...,  0.0212,  0.0396, -0.0222],\n",
       "                      [-0.0796,  0.0359, -0.0476,  ..., -0.0282, -0.0017,  0.0200]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0030, -0.0126,  0.0155,  ..., -0.0004,  0.0099,  0.0074],\n",
       "                      [-0.0017,  0.0061, -0.0003,  ...,  0.0111, -0.0075,  0.0106],\n",
       "                      [ 0.0087,  0.0098,  0.0026,  ..., -0.0146,  0.0091, -0.0002],\n",
       "                      ...,\n",
       "                      [ 0.0140, -0.0117, -0.0112,  ..., -0.0035,  0.0093,  0.0045],\n",
       "                      [ 0.0061,  0.0033,  0.0114,  ..., -0.0072,  0.0020, -0.0063],\n",
       "                      [ 0.0089, -0.0153,  0.0110,  ..., -0.0011, -0.0110, -0.0128]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0288, -0.0247, -0.0444,  ..., -0.0098,  0.0201, -0.0190],\n",
       "                      [-0.0165, -0.0369, -0.0086,  ..., -0.0079,  0.0070, -0.0025],\n",
       "                      [ 0.0466,  0.0503, -0.0298,  ...,  0.0116, -0.0359, -0.0160],\n",
       "                      ...,\n",
       "                      [-0.0165, -0.0396, -0.0112,  ..., -0.0075, -0.0176,  0.0635],\n",
       "                      [ 0.0122, -0.0109,  0.0187,  ..., -0.0640,  0.0049, -0.0032],\n",
       "                      [ 0.0237,  0.0374,  0.0142,  ...,  0.0139, -0.0168, -0.0094]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0033,  0.0115, -0.0133,  ..., -0.0137,  0.0150, -0.0106],\n",
       "                      [-0.0150,  0.0068, -0.0053,  ..., -0.0073,  0.0091,  0.0148],\n",
       "                      [ 0.0150, -0.0026, -0.0031,  ...,  0.0039,  0.0012,  0.0133],\n",
       "                      ...,\n",
       "                      [-0.0070,  0.0069, -0.0021,  ...,  0.0080,  0.0035,  0.0101],\n",
       "                      [-0.0065, -0.0134, -0.0011,  ..., -0.0156, -0.0094,  0.0090],\n",
       "                      [-0.0116,  0.0118,  0.0049,  ...,  0.0142, -0.0023,  0.0101]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0518,  0.0070,  0.0466,  ..., -0.0107,  0.0024,  0.0276],\n",
       "                      [ 0.0128, -0.0420,  0.0344,  ...,  0.0013, -0.0564,  0.0239],\n",
       "                      [-0.0306, -0.0344,  0.0220,  ...,  0.0143,  0.0088,  0.0327],\n",
       "                      ...,\n",
       "                      [-0.0278,  0.0435,  0.0119,  ..., -0.0145, -0.0140,  0.0376],\n",
       "                      [-0.0294, -0.0057, -0.0221,  ...,  0.0034, -0.0437,  0.0154],\n",
       "                      [ 0.0131, -0.0330, -0.0537,  ...,  0.0280, -0.0459,  0.0195]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0017, -0.0067, -0.0013,  ..., -0.0072,  0.0069, -0.0054],\n",
       "                      [-0.0023, -0.0015,  0.0088,  ..., -0.0045,  0.0074, -0.0010],\n",
       "                      [-0.0014, -0.0046,  0.0076,  ...,  0.0030, -0.0021,  0.0095],\n",
       "                      ...,\n",
       "                      [-0.0022, -0.0031, -0.0002,  ...,  0.0015, -0.0003,  0.0066],\n",
       "                      [ 0.0069,  0.0032, -0.0033,  ..., -0.0049, -0.0001,  0.0046],\n",
       "                      [-0.0038, -0.0011, -0.0032,  ..., -0.0061,  0.0018, -0.0094]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.input_layernorm.weight',\n",
       "              tensor([0.1885, 0.1865, 0.1904,  ..., 0.1816, 0.1875, 0.1943],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.4.post_attention_layernorm.weight',\n",
       "              tensor([0.1426, 0.1357, 0.1318,  ..., 0.1406, 0.1367, 0.1396],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0179,  0.0081, -0.0111,  ..., -0.0229, -0.0016,  0.0140],\n",
       "                      [-0.0381, -0.0197,  0.0308,  ..., -0.0194, -0.0176, -0.0144],\n",
       "                      [-0.0369,  0.0033,  0.0217,  ...,  0.0050, -0.0552,  0.0093],\n",
       "                      ...,\n",
       "                      [-0.0369, -0.0118, -0.0120,  ...,  0.0344,  0.0194,  0.0081],\n",
       "                      [ 0.0114,  0.0151,  0.0024,  ..., -0.0304, -0.0518, -0.0532],\n",
       "                      [ 0.0120,  0.0021,  0.0410,  ..., -0.0364, -0.0156, -0.0052]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0143, -0.0085, -0.0017,  ...,  0.0098, -0.0109,  0.0021],\n",
       "                      [ 0.0014, -0.0143, -0.0126,  ..., -0.0081, -0.0028,  0.0007],\n",
       "                      [ 0.0142, -0.0008,  0.0065,  ..., -0.0072,  0.0069, -0.0049],\n",
       "                      ...,\n",
       "                      [ 0.0107,  0.0011, -0.0110,  ..., -0.0040,  0.0135,  0.0051],\n",
       "                      [ 0.0009,  0.0112, -0.0070,  ..., -0.0015, -0.0118,  0.0140],\n",
       "                      [ 0.0099,  0.0047,  0.0141,  ..., -0.0042,  0.0132, -0.0011]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0084,  0.0327,  0.0069,  ...,  0.0132, -0.0125, -0.0149],\n",
       "                      [ 0.0400,  0.0075, -0.0039,  ..., -0.0016,  0.0101,  0.0234],\n",
       "                      [ 0.0104, -0.0044,  0.0266,  ...,  0.0095,  0.0124,  0.0073],\n",
       "                      ...,\n",
       "                      [ 0.0153, -0.0003, -0.0312,  ...,  0.0359,  0.0133, -0.0259],\n",
       "                      [ 0.0151, -0.0549, -0.0449,  ...,  0.0493, -0.0096, -0.0243],\n",
       "                      [-0.0498, -0.0161,  0.0723,  ..., -0.0396, -0.0640,  0.0320]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0010,  0.0101,  0.0091,  ...,  0.0048, -0.0074, -0.0049],\n",
       "                      [ 0.0138, -0.0123, -0.0030,  ..., -0.0036, -0.0103, -0.0139],\n",
       "                      [ 0.0010, -0.0127,  0.0123,  ...,  0.0078, -0.0121, -0.0128],\n",
       "                      ...,\n",
       "                      [ 0.0010, -0.0148,  0.0074,  ..., -0.0093,  0.0032, -0.0065],\n",
       "                      [ 0.0122,  0.0042,  0.0065,  ..., -0.0096, -0.0012, -0.0053],\n",
       "                      [-0.0032,  0.0123,  0.0010,  ..., -0.0098, -0.0140,  0.0061]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.v_proj.weight',\n",
       "              tensor([[-4.1260e-02, -8.9722e-03,  2.4658e-02,  ...,  5.2246e-02,\n",
       "                       -6.1035e-03,  6.2866e-03],\n",
       "                      [-9.3384e-03, -1.0010e-02, -3.0756e-05,  ..., -9.7046e-03,\n",
       "                        2.8442e-02, -2.5024e-02],\n",
       "                      [ 1.5991e-02,  3.0151e-02,  2.4567e-03,  ..., -3.3203e-02,\n",
       "                        4.6692e-03, -1.1292e-02],\n",
       "                      ...,\n",
       "                      [-7.5684e-03, -1.6724e-02,  1.6113e-02,  ...,  9.7656e-03,\n",
       "                       -2.7222e-02,  1.3672e-02],\n",
       "                      [-1.5320e-02, -3.7354e-02, -7.3242e-03,  ...,  2.4902e-02,\n",
       "                       -3.0396e-02,  1.1475e-02],\n",
       "                      [ 2.9144e-03, -2.4414e-02,  4.3457e-02,  ...,  1.8677e-02,\n",
       "                       -5.7068e-03,  2.6245e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0032,  0.0133,  0.0016,  ..., -0.0049, -0.0079,  0.0022],\n",
       "                      [-0.0079,  0.0077,  0.0141,  ...,  0.0121,  0.0063,  0.0142],\n",
       "                      [ 0.0007,  0.0025, -0.0114,  ..., -0.0068,  0.0045, -0.0093],\n",
       "                      ...,\n",
       "                      [ 0.0104,  0.0056,  0.0012,  ..., -0.0049,  0.0132, -0.0125],\n",
       "                      [-0.0128, -0.0061, -0.0105,  ...,  0.0004,  0.0063, -0.0051],\n",
       "                      [ 0.0076,  0.0052,  0.0131,  ...,  0.0085,  0.0061,  0.0118]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0195, -0.0059, -0.0179,  ..., -0.0071, -0.0386,  0.0364],\n",
       "                      [-0.0036, -0.0004, -0.0182,  ..., -0.0222, -0.0159,  0.0134],\n",
       "                      [-0.0068, -0.0664,  0.0145,  ...,  0.0150, -0.0094,  0.0206],\n",
       "                      ...,\n",
       "                      [ 0.0312,  0.0164, -0.0172,  ...,  0.0115,  0.0156, -0.0096],\n",
       "                      [-0.0125, -0.0047, -0.0145,  ..., -0.0179, -0.0225,  0.0019],\n",
       "                      [ 0.0145, -0.0047, -0.0022,  ..., -0.0194, -0.0124, -0.0287]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0143,  0.0028,  0.0026,  ..., -0.0031, -0.0043,  0.0132],\n",
       "                      [-0.0039,  0.0067,  0.0063,  ..., -0.0093,  0.0073, -0.0058],\n",
       "                      [-0.0088, -0.0025,  0.0026,  ...,  0.0036, -0.0119, -0.0114],\n",
       "                      ...,\n",
       "                      [ 0.0050,  0.0125, -0.0075,  ..., -0.0020, -0.0004,  0.0078],\n",
       "                      [-0.0059,  0.0082,  0.0048,  ..., -0.0018,  0.0105,  0.0105],\n",
       "                      [ 0.0013,  0.0124,  0.0031,  ...,  0.0058, -0.0046,  0.0067]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0116,  0.0073, -0.0217,  ..., -0.0092,  0.0172, -0.0176],\n",
       "                      [ 0.0361, -0.0006, -0.0249,  ...,  0.0420, -0.0284, -0.0094],\n",
       "                      [ 0.0457,  0.0007,  0.0131,  ..., -0.0006, -0.0151, -0.0386],\n",
       "                      ...,\n",
       "                      [-0.0315,  0.0108,  0.0259,  ..., -0.0811,  0.0388, -0.0474],\n",
       "                      [-0.0430,  0.0277,  0.0088,  ...,  0.0141,  0.0359, -0.0199],\n",
       "                      [ 0.0571, -0.0092,  0.0459,  ...,  0.0177, -0.0033, -0.0190]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0091, -0.0089,  0.0087,  ...,  0.0027, -0.0059, -0.0049],\n",
       "                      [-0.0005,  0.0049,  0.0052,  ..., -0.0029,  0.0071,  0.0030],\n",
       "                      [ 0.0086,  0.0045, -0.0145,  ..., -0.0096,  0.0033,  0.0070],\n",
       "                      ...,\n",
       "                      [-0.0034, -0.0063, -0.0036,  ...,  0.0030,  0.0076, -0.0035],\n",
       "                      [ 0.0140, -0.0033,  0.0126,  ...,  0.0010, -0.0140, -0.0008],\n",
       "                      [ 0.0123,  0.0031,  0.0031,  ..., -0.0046, -0.0116, -0.0038]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.up_proj.weight',\n",
       "              tensor([[-0.0035, -0.0020,  0.0074,  ..., -0.0068,  0.0080, -0.0408],\n",
       "                      [-0.0076,  0.0542,  0.0101,  ...,  0.0510, -0.0074, -0.0159],\n",
       "                      [-0.0057,  0.0437,  0.0461,  ...,  0.0055, -0.0198,  0.0172],\n",
       "                      ...,\n",
       "                      [-0.0184,  0.0209, -0.0330,  ..., -0.0276, -0.0197,  0.0259],\n",
       "                      [ 0.0276, -0.0427, -0.0405,  ..., -0.0118, -0.0015, -0.0075],\n",
       "                      [-0.0056,  0.0520,  0.0371,  ...,  0.0076, -0.0242, -0.0415]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0114, -0.0135, -0.0092,  ..., -0.0132, -0.0024,  0.0089],\n",
       "                      [ 0.0020,  0.0031,  0.0062,  ...,  0.0003, -0.0042,  0.0058],\n",
       "                      [ 0.0048, -0.0045,  0.0093,  ...,  0.0089,  0.0128, -0.0034],\n",
       "                      ...,\n",
       "                      [-0.0051,  0.0004, -0.0018,  ...,  0.0148, -0.0140, -0.0070],\n",
       "                      [ 0.0037,  0.0014, -0.0117,  ...,  0.0074,  0.0008, -0.0077],\n",
       "                      [ 0.0024,  0.0101, -0.0011,  ...,  0.0106,  0.0039, -0.0014]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0113, -0.0425,  0.0090,  ..., -0.0115, -0.0055,  0.0251],\n",
       "                      [-0.0498,  0.0249, -0.0034,  ...,  0.0094, -0.0154,  0.0053],\n",
       "                      [ 0.0090,  0.0026,  0.0544,  ...,  0.0010, -0.0562,  0.0386],\n",
       "                      ...,\n",
       "                      [-0.0215, -0.0110,  0.0038,  ...,  0.0129,  0.0081,  0.0112],\n",
       "                      [ 0.0417, -0.0248, -0.0052,  ...,  0.0092,  0.0161, -0.0364],\n",
       "                      [ 0.0251,  0.0209, -0.0361,  ...,  0.0522,  0.0271, -0.0101]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0039, -0.0056,  0.0019,  ..., -0.0034,  0.0024,  0.0046],\n",
       "                      [ 0.0090,  0.0054, -0.0007,  ..., -0.0068,  0.0053,  0.0063],\n",
       "                      [-0.0078, -0.0057, -0.0012,  ...,  0.0042,  0.0078, -0.0056],\n",
       "                      ...,\n",
       "                      [-0.0055,  0.0055, -0.0069,  ...,  0.0092, -0.0085, -0.0049],\n",
       "                      [ 0.0004, -0.0012,  0.0039,  ...,  0.0065,  0.0068,  0.0044],\n",
       "                      [-0.0077, -0.0079,  0.0095,  ..., -0.0031, -0.0041,  0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.input_layernorm.weight',\n",
       "              tensor([0.1768, 0.1729, 0.1787,  ..., 0.1650, 0.1768, 0.1816],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.5.post_attention_layernorm.weight',\n",
       "              tensor([0.1582, 0.1416, 0.1367,  ..., 0.1553, 0.1465, 0.1484],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0354, -0.0045,  0.0483,  ...,  0.0179, -0.0182, -0.0089],\n",
       "                      [-0.0171, -0.0070,  0.0142,  ..., -0.0064, -0.0271, -0.0713],\n",
       "                      [-0.0061,  0.0078,  0.0048,  ...,  0.0376,  0.0037, -0.0070],\n",
       "                      ...,\n",
       "                      [ 0.0253,  0.0757, -0.0820,  ...,  0.0425,  0.0239,  0.0554],\n",
       "                      [ 0.0598,  0.0425,  0.0034,  ..., -0.0140, -0.1060, -0.0510],\n",
       "                      [-0.0002, -0.0547,  0.0092,  ...,  0.0447, -0.0060, -0.0479]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0130,  0.0064, -0.0136,  ...,  0.0025,  0.0044, -0.0011],\n",
       "                      [-0.0115,  0.0010, -0.0056,  ..., -0.0034, -0.0066, -0.0135],\n",
       "                      [-0.0153,  0.0026,  0.0141,  ...,  0.0112,  0.0132,  0.0006],\n",
       "                      ...,\n",
       "                      [ 0.0038, -0.0031,  0.0155,  ..., -0.0118,  0.0151,  0.0117],\n",
       "                      [ 0.0110, -0.0030,  0.0063,  ..., -0.0101, -0.0089,  0.0106],\n",
       "                      [ 0.0059,  0.0137,  0.0037,  ...,  0.0047, -0.0079,  0.0156]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0264, -0.0159,  0.0118,  ..., -0.0138,  0.0157,  0.0159],\n",
       "                      [-0.0271, -0.0435,  0.0205,  ...,  0.0302,  0.0046,  0.0004],\n",
       "                      [-0.0194,  0.0020,  0.0215,  ...,  0.0082,  0.0127,  0.0039],\n",
       "                      ...,\n",
       "                      [-0.0312, -0.0253,  0.0114,  ...,  0.0762,  0.0021, -0.0311],\n",
       "                      [-0.0219,  0.0256, -0.0569,  ..., -0.0332, -0.0806,  0.0028],\n",
       "                      [-0.0284, -0.0250, -0.0352,  ...,  0.0240, -0.0449, -0.0452]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-5.5847e-03, -1.2695e-02,  3.8300e-03,  ..., -3.8605e-03,\n",
       "                       -9.9659e-05,  1.3809e-03],\n",
       "                      [ 1.4832e-02, -5.4626e-03, -1.1108e-02,  ...,  6.0272e-04,\n",
       "                        1.4160e-02,  5.1575e-03],\n",
       "                      [ 1.2665e-03, -6.9275e-03, -3.6011e-03,  ..., -5.7373e-03,\n",
       "                       -1.4954e-02,  4.5776e-03],\n",
       "                      ...,\n",
       "                      [-4.0817e-04,  4.7913e-03, -1.2024e-02,  ...,  7.2632e-03,\n",
       "                        5.1575e-03, -4.3640e-03],\n",
       "                      [-1.5076e-02, -3.2196e-03,  6.2866e-03,  ...,  1.4648e-02,\n",
       "                        1.4954e-02, -7.3547e-03],\n",
       "                      [-1.0395e-04,  3.5858e-03, -1.5198e-02,  ...,  8.4839e-03,\n",
       "                        1.4526e-02, -1.4648e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0469,  0.0133,  0.0245,  ...,  0.0291,  0.0201, -0.0101],\n",
       "                      [ 0.0238, -0.0212, -0.0027,  ..., -0.0060, -0.0069, -0.0100],\n",
       "                      [-0.0211,  0.0157,  0.0437,  ...,  0.0223,  0.0069, -0.0479],\n",
       "                      ...,\n",
       "                      [-0.0236, -0.0248,  0.0166,  ..., -0.0022, -0.0089, -0.0006],\n",
       "                      [-0.0302, -0.0226, -0.0076,  ...,  0.0157, -0.0029,  0.0119],\n",
       "                      [ 0.0366,  0.0197,  0.0287,  ..., -0.0464, -0.0259,  0.0099]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0045,  0.0008, -0.0036,  ...,  0.0131,  0.0035,  0.0027],\n",
       "                      [ 0.0131,  0.0150,  0.0125,  ..., -0.0146, -0.0120, -0.0132],\n",
       "                      [ 0.0005,  0.0010,  0.0131,  ..., -0.0083, -0.0028, -0.0014],\n",
       "                      ...,\n",
       "                      [-0.0083,  0.0038, -0.0134,  ..., -0.0045,  0.0059, -0.0143],\n",
       "                      [-0.0077,  0.0103, -0.0014,  ...,  0.0118,  0.0115, -0.0127],\n",
       "                      [-0.0022,  0.0029,  0.0018,  ..., -0.0100, -0.0120, -0.0066]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0082, -0.0276, -0.0170,  ..., -0.0312,  0.0103, -0.0065],\n",
       "                      [ 0.0201, -0.0026,  0.0126,  ..., -0.0297,  0.0060, -0.0058],\n",
       "                      [ 0.0081,  0.0117, -0.0288,  ...,  0.0056, -0.0327,  0.0309],\n",
       "                      ...,\n",
       "                      [-0.0062, -0.0133,  0.0239,  ...,  0.0221, -0.0236,  0.0518],\n",
       "                      [-0.0077, -0.0105, -0.0007,  ...,  0.0017, -0.0078, -0.0233],\n",
       "                      [-0.0226, -0.0221,  0.0317,  ..., -0.0354,  0.0557,  0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0146,  0.0114, -0.0123,  ..., -0.0046,  0.0083,  0.0155],\n",
       "                      [-0.0148, -0.0112, -0.0121,  ...,  0.0039, -0.0045, -0.0097],\n",
       "                      [ 0.0010, -0.0149, -0.0020,  ...,  0.0140, -0.0140, -0.0108],\n",
       "                      ...,\n",
       "                      [-0.0142, -0.0036,  0.0106,  ...,  0.0094, -0.0117, -0.0138],\n",
       "                      [ 0.0091, -0.0119,  0.0154,  ..., -0.0065, -0.0143, -0.0126],\n",
       "                      [-0.0069,  0.0068,  0.0063,  ...,  0.0012,  0.0023,  0.0060]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0217,  0.0422,  0.0278,  ...,  0.0027, -0.0068, -0.0122],\n",
       "                      [ 0.0413, -0.0015, -0.0112,  ...,  0.0347,  0.0150,  0.0098],\n",
       "                      [-0.0027,  0.0552, -0.0295,  ..., -0.0337, -0.0088,  0.0306],\n",
       "                      ...,\n",
       "                      [ 0.0452, -0.0562,  0.0253,  ...,  0.0437, -0.0040, -0.0227],\n",
       "                      [ 0.0332, -0.0102, -0.0135,  ..., -0.0356,  0.0292, -0.0193],\n",
       "                      [-0.0129, -0.0122,  0.0072,  ...,  0.0082, -0.0147, -0.0178]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0025,  0.0002,  0.0092,  ...,  0.0085, -0.0013,  0.0020],\n",
       "                      [-0.0007, -0.0054, -0.0010,  ...,  0.0032,  0.0050, -0.0076],\n",
       "                      [-0.0103,  0.0004,  0.0087,  ...,  0.0004, -0.0043,  0.0082],\n",
       "                      ...,\n",
       "                      [ 0.0101,  0.0011, -0.0154,  ..., -0.0039, -0.0070,  0.0014],\n",
       "                      [-0.0046,  0.0130, -0.0026,  ..., -0.0080, -0.0028, -0.0022],\n",
       "                      [ 0.0153, -0.0020, -0.0060,  ...,  0.0114,  0.0137,  0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0311, -0.0212, -0.0120,  ...,  0.0236,  0.0099, -0.0121],\n",
       "                      [ 0.0132, -0.0491,  0.0447,  ...,  0.0245,  0.0398, -0.0034],\n",
       "                      [-0.0248, -0.0166, -0.0393,  ...,  0.0284,  0.0327, -0.0071],\n",
       "                      ...,\n",
       "                      [ 0.0493,  0.0277,  0.0046,  ...,  0.0322, -0.0063,  0.0311],\n",
       "                      [-0.0024, -0.0048, -0.0059,  ..., -0.0437, -0.0150, -0.0066],\n",
       "                      [-0.0108,  0.0238,  0.0016,  ...,  0.0491, -0.0056, -0.0496]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0118, -0.0004, -0.0069,  ..., -0.0087, -0.0044, -0.0037],\n",
       "                      [ 0.0107,  0.0098,  0.0059,  ...,  0.0093,  0.0028, -0.0075],\n",
       "                      [ 0.0043,  0.0067, -0.0079,  ..., -0.0060,  0.0115,  0.0123],\n",
       "                      ...,\n",
       "                      [-0.0025,  0.0002, -0.0027,  ...,  0.0062,  0.0062,  0.0114],\n",
       "                      [-0.0070, -0.0074,  0.0147,  ...,  0.0099,  0.0065, -0.0070],\n",
       "                      [ 0.0097,  0.0107,  0.0103,  ...,  0.0073, -0.0102,  0.0136]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.down_proj.weight',\n",
       "              tensor([[-0.0016,  0.0201, -0.0302,  ...,  0.0469,  0.0330,  0.0173],\n",
       "                      [-0.0228, -0.0452,  0.0007,  ..., -0.0134, -0.0265,  0.0045],\n",
       "                      [-0.0061,  0.0188,  0.0170,  ...,  0.0284, -0.0258,  0.0058],\n",
       "                      ...,\n",
       "                      [ 0.0017, -0.0011,  0.0598,  ...,  0.0574, -0.0352,  0.0089],\n",
       "                      [-0.0170, -0.0289,  0.0172,  ...,  0.0491, -0.0107, -0.0327],\n",
       "                      [-0.0126,  0.0183, -0.0322,  ...,  0.0371, -0.0168, -0.0053]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0013, -0.0016,  0.0048,  ..., -0.0089,  0.0074, -0.0070],\n",
       "                      [ 0.0011, -0.0019, -0.0036,  ..., -0.0095, -0.0018,  0.0017],\n",
       "                      [-0.0048, -0.0025, -0.0074,  ...,  0.0061, -0.0085,  0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0083, -0.0027,  0.0091,  ...,  0.0079,  0.0009, -0.0005],\n",
       "                      [-0.0006,  0.0036, -0.0085,  ..., -0.0008,  0.0060,  0.0061],\n",
       "                      [-0.0030, -0.0026,  0.0015,  ..., -0.0030,  0.0068,  0.0031]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.input_layernorm.weight',\n",
       "              tensor([0.2207, 0.2695, 0.2461,  ..., 0.2236, 0.2432, 0.2412],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.6.post_attention_layernorm.weight',\n",
       "              tensor([0.1680, 0.1572, 0.1514,  ..., 0.1719, 0.1592, 0.1631],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0084, -0.0178,  0.0286,  ..., -0.0087, -0.0245,  0.0041],\n",
       "                      [-0.0166, -0.0237, -0.0383,  ...,  0.0061, -0.0027, -0.0302],\n",
       "                      [ 0.0138, -0.0143,  0.0047,  ..., -0.0312, -0.0322,  0.0054],\n",
       "                      ...,\n",
       "                      [ 0.0233, -0.0366,  0.0110,  ..., -0.0093,  0.0270,  0.0481],\n",
       "                      [-0.0068,  0.1064, -0.0608,  ...,  0.0425, -0.0669, -0.0150],\n",
       "                      [ 0.0957, -0.0420, -0.0540,  ...,  0.0815,  0.0315, -0.0303]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 3.7994e-03,  9.2697e-04,  1.2939e-02,  ...,  1.5442e-02,\n",
       "                       -1.4526e-02, -2.7657e-05],\n",
       "                      [ 1.2085e-02,  4.5776e-03, -1.1841e-02,  ...,  1.4526e-02,\n",
       "                        1.1902e-02,  6.0120e-03],\n",
       "                      [ 1.3489e-02, -1.1108e-02,  1.1902e-02,  ...,  8.6670e-03,\n",
       "                       -5.4321e-03, -7.5684e-03],\n",
       "                      ...,\n",
       "                      [-1.0559e-02,  1.3428e-03, -7.6599e-03,  ...,  1.8005e-03,\n",
       "                        1.0010e-02,  1.2634e-02],\n",
       "                      [ 4.8828e-03, -4.6997e-03, -8.9722e-03,  ..., -7.8125e-03,\n",
       "                       -4.6082e-03, -1.2085e-02],\n",
       "                      [ 1.1536e-02,  9.6436e-03,  1.3367e-02,  ..., -1.1597e-02,\n",
       "                        1.1292e-02,  1.4591e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0117,  0.0111, -0.0283,  ...,  0.0250,  0.0027, -0.0115],\n",
       "                      [ 0.0053,  0.0237,  0.0135,  ..., -0.0064,  0.0198,  0.0237],\n",
       "                      [-0.0162, -0.0065, -0.0189,  ...,  0.0188, -0.0236,  0.0081],\n",
       "                      ...,\n",
       "                      [ 0.0381,  0.0199,  0.0486,  ...,  0.0496,  0.0101,  0.0371],\n",
       "                      [ 0.0474,  0.0967, -0.0243,  ..., -0.0137, -0.0364, -0.0209],\n",
       "                      [ 0.0542,  0.0337,  0.0206,  ...,  0.0094,  0.0422, -0.0126]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0142,  0.0083,  0.0096,  ...,  0.0061, -0.0132, -0.0015],\n",
       "                      [ 0.0127,  0.0120, -0.0131,  ..., -0.0081,  0.0089,  0.0022],\n",
       "                      [ 0.0042, -0.0117,  0.0056,  ...,  0.0062,  0.0045, -0.0096],\n",
       "                      ...,\n",
       "                      [ 0.0147,  0.0049, -0.0125,  ...,  0.0109,  0.0128, -0.0098],\n",
       "                      [-0.0006, -0.0011,  0.0038,  ...,  0.0092,  0.0025,  0.0007],\n",
       "                      [-0.0067,  0.0086, -0.0003,  ..., -0.0020,  0.0114,  0.0048]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0146,  0.0214,  0.0025,  ...,  0.0398, -0.0036, -0.0398],\n",
       "                      [ 0.0342,  0.0009, -0.0142,  ...,  0.0383,  0.0383, -0.0038],\n",
       "                      [-0.0212, -0.0254,  0.0178,  ..., -0.0361,  0.0117, -0.0114],\n",
       "                      ...,\n",
       "                      [ 0.0034, -0.0503, -0.0139,  ..., -0.0203,  0.0194, -0.0217],\n",
       "                      [-0.0212, -0.0085, -0.0119,  ...,  0.0046,  0.0076,  0.0189],\n",
       "                      [-0.0107, -0.0187, -0.0253,  ...,  0.0415,  0.0035,  0.0025]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0103,  0.0081, -0.0089,  ..., -0.0079,  0.0124, -0.0143],\n",
       "                      [ 0.0046, -0.0015, -0.0141,  ...,  0.0040, -0.0078, -0.0031],\n",
       "                      [ 0.0079,  0.0123, -0.0099,  ...,  0.0142,  0.0098, -0.0095],\n",
       "                      ...,\n",
       "                      [ 0.0033, -0.0156, -0.0072,  ..., -0.0065,  0.0129, -0.0154],\n",
       "                      [ 0.0082, -0.0009,  0.0133,  ..., -0.0156,  0.0057,  0.0069],\n",
       "                      [-0.0012, -0.0057,  0.0017,  ...,  0.0102,  0.0145, -0.0065]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0052, -0.0247,  0.0009,  ...,  0.0234, -0.0444, -0.0187],\n",
       "                      [-0.0156,  0.0150, -0.0085,  ..., -0.0703, -0.0195,  0.0101],\n",
       "                      [-0.0085,  0.0192, -0.0315,  ...,  0.0008, -0.0094, -0.0215],\n",
       "                      ...,\n",
       "                      [-0.0075, -0.0461,  0.0129,  ...,  0.0135, -0.0096,  0.0233],\n",
       "                      [-0.0027, -0.0195,  0.0173,  ..., -0.0234,  0.0025,  0.0476],\n",
       "                      [ 0.0117,  0.0166,  0.0236,  ..., -0.0063,  0.0072,  0.0114]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0155,  0.0089, -0.0013,  ...,  0.0012, -0.0063,  0.0145],\n",
       "                      [ 0.0064, -0.0007, -0.0133,  ..., -0.0118, -0.0126, -0.0047],\n",
       "                      [-0.0099,  0.0100, -0.0139,  ..., -0.0117, -0.0085, -0.0032],\n",
       "                      ...,\n",
       "                      [ 0.0129, -0.0129,  0.0107,  ...,  0.0135, -0.0067, -0.0031],\n",
       "                      [ 0.0120, -0.0079,  0.0038,  ...,  0.0135, -0.0046,  0.0137],\n",
       "                      [ 0.0135,  0.0042,  0.0039,  ..., -0.0139,  0.0018, -0.0068]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0078, -0.0240, -0.0249,  ...,  0.0067, -0.0102, -0.0085],\n",
       "                      [-0.0140, -0.0154,  0.0003,  ...,  0.0292, -0.0535, -0.0209],\n",
       "                      [ 0.0284, -0.0236,  0.0231,  ...,  0.0320, -0.0299,  0.0396],\n",
       "                      ...,\n",
       "                      [ 0.0262, -0.0459,  0.0171,  ..., -0.0371,  0.0095, -0.0097],\n",
       "                      [-0.0195,  0.0168,  0.0070,  ..., -0.0182,  0.0117, -0.0216],\n",
       "                      [ 0.0194,  0.0027, -0.0204,  ...,  0.0161,  0.0278, -0.0063]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0050, -0.0018, -0.0136,  ...,  0.0081,  0.0037,  0.0032],\n",
       "                      [-0.0070,  0.0063, -0.0036,  ..., -0.0067,  0.0008,  0.0101],\n",
       "                      [-0.0019, -0.0089,  0.0015,  ..., -0.0121, -0.0068, -0.0114],\n",
       "                      ...,\n",
       "                      [-0.0115, -0.0150,  0.0092,  ...,  0.0063,  0.0127,  0.0127],\n",
       "                      [ 0.0114,  0.0138,  0.0135,  ..., -0.0010, -0.0030, -0.0098],\n",
       "                      [-0.0069, -0.0083,  0.0126,  ..., -0.0059,  0.0111, -0.0062]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0157,  0.0022,  0.0251,  ..., -0.0304, -0.0182, -0.0283],\n",
       "                      [-0.0248, -0.0162,  0.0288,  ..., -0.0068, -0.0354,  0.0525],\n",
       "                      [ 0.0271, -0.0066,  0.0201,  ..., -0.0114,  0.0019, -0.0131],\n",
       "                      ...,\n",
       "                      [-0.0060, -0.0148,  0.0128,  ..., -0.0032,  0.0011, -0.0015],\n",
       "                      [-0.0103, -0.0081,  0.0518,  ..., -0.0179, -0.0046, -0.0173],\n",
       "                      [ 0.0031, -0.0544,  0.0128,  ...,  0.0325, -0.0114,  0.0178]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0073,  0.0052, -0.0089,  ...,  0.0081, -0.0122,  0.0088],\n",
       "                      [-0.0023,  0.0055,  0.0112,  ...,  0.0143,  0.0053, -0.0054],\n",
       "                      [ 0.0027,  0.0070, -0.0078,  ...,  0.0153, -0.0095,  0.0139],\n",
       "                      ...,\n",
       "                      [ 0.0145, -0.0086,  0.0072,  ..., -0.0062,  0.0081, -0.0042],\n",
       "                      [-0.0045, -0.0030, -0.0059,  ..., -0.0057, -0.0043,  0.0009],\n",
       "                      [-0.0094,  0.0112,  0.0040,  ..., -0.0131, -0.0084, -0.0014]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0137,  0.0068, -0.0310,  ..., -0.0085, -0.0023, -0.0220],\n",
       "                      [ 0.0143, -0.0116,  0.0320,  ...,  0.0028,  0.0048,  0.0151],\n",
       "                      [ 0.0147,  0.0099,  0.0160,  ...,  0.0037, -0.0413, -0.0304],\n",
       "                      ...,\n",
       "                      [-0.0091,  0.0121, -0.0461,  ...,  0.0217,  0.0181, -0.0410],\n",
       "                      [ 0.0562, -0.0342,  0.0311,  ..., -0.0461,  0.0085, -0.0060],\n",
       "                      [ 0.0396,  0.0330, -0.0179,  ..., -0.0203, -0.0115, -0.0334]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0032, -0.0059,  0.0010,  ...,  0.0026, -0.0012, -0.0059],\n",
       "                      [-0.0077, -0.0090, -0.0067,  ...,  0.0038,  0.0061, -0.0023],\n",
       "                      [ 0.0078, -0.0048, -0.0035,  ...,  0.0025, -0.0029,  0.0067],\n",
       "                      ...,\n",
       "                      [-0.0063, -0.0062, -0.0007,  ...,  0.0067,  0.0026, -0.0014],\n",
       "                      [-0.0088,  0.0029,  0.0087,  ..., -0.0009,  0.0068,  0.0041],\n",
       "                      [-0.0034,  0.0023, -0.0084,  ..., -0.0011, -0.0024, -0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.input_layernorm.weight',\n",
       "              tensor([0.2412, 0.2715, 0.2539,  ..., 0.2363, 0.2637, 0.2393],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.7.post_attention_layernorm.weight',\n",
       "              tensor([0.1855, 0.1680, 0.1621,  ..., 0.1846, 0.1748, 0.1748],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0044, -0.0042, -0.0065,  ..., -0.0317,  0.0231,  0.0186],\n",
       "                      [-0.0254,  0.0197, -0.0192,  ...,  0.0427, -0.0403, -0.0195],\n",
       "                      [-0.0144, -0.0234, -0.0376,  ...,  0.0344,  0.0110,  0.0081],\n",
       "                      ...,\n",
       "                      [-0.0219,  0.0422,  0.0400,  ..., -0.0287, -0.0254, -0.0571],\n",
       "                      [ 0.0178, -0.0391, -0.0415,  ...,  0.0344,  0.0223,  0.0366],\n",
       "                      [-0.0498, -0.0449, -0.0757,  ..., -0.0449, -0.0126,  0.0210]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0142, -0.0009,  0.0024,  ...,  0.0132, -0.0079,  0.0043],\n",
       "                      [ 0.0112, -0.0098, -0.0083,  ..., -0.0126,  0.0046,  0.0131],\n",
       "                      [ 0.0145,  0.0118,  0.0133,  ...,  0.0081, -0.0137,  0.0146],\n",
       "                      ...,\n",
       "                      [ 0.0121, -0.0020,  0.0146,  ...,  0.0056,  0.0068, -0.0019],\n",
       "                      [-0.0087,  0.0076, -0.0103,  ...,  0.0115, -0.0134, -0.0094],\n",
       "                      [ 0.0011, -0.0030,  0.0117,  ..., -0.0085, -0.0052,  0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0126, -0.0033,  0.0125,  ...,  0.0092, -0.0111,  0.0023],\n",
       "                      [ 0.0255, -0.0227, -0.0190,  ...,  0.0002, -0.0139,  0.0123],\n",
       "                      [-0.0008,  0.0052,  0.0099,  ..., -0.0034, -0.0124, -0.0087],\n",
       "                      ...,\n",
       "                      [ 0.0027, -0.0292, -0.0079,  ..., -0.0132, -0.0046,  0.0344],\n",
       "                      [ 0.0156,  0.0400,  0.0010,  ..., -0.0270,  0.0596, -0.0425],\n",
       "                      [-0.0090,  0.0153, -0.0157,  ...,  0.0369,  0.0221, -0.0066]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-2.3270e-04,  8.8501e-03,  1.1475e-02,  ..., -1.1841e-02,\n",
       "                       -9.7656e-03,  4.4861e-03],\n",
       "                      [ 2.1210e-03,  7.8125e-03, -1.3062e-02,  ...,  7.5073e-03,\n",
       "                        3.7994e-03,  7.3547e-03],\n",
       "                      [-6.6528e-03,  3.8300e-03, -1.0452e-03,  ..., -1.2756e-02,\n",
       "                        5.7068e-03, -8.7261e-05],\n",
       "                      ...,\n",
       "                      [ 2.4109e-03,  9.1553e-03,  1.4526e-02,  ...,  8.0490e-04,\n",
       "                        2.0752e-03,  1.2695e-02],\n",
       "                      [ 1.3916e-02, -1.2207e-02, -1.1292e-02,  ..., -1.2329e-02,\n",
       "                       -5.7068e-03,  9.4604e-03],\n",
       "                      [ 2.8381e-03, -1.1780e-02, -6.2256e-03,  ..., -1.5137e-02,\n",
       "                       -3.5553e-03, -1.0254e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0276, -0.0217,  0.0231,  ..., -0.0004,  0.0299, -0.0108],\n",
       "                      [-0.0167, -0.0045, -0.0033,  ...,  0.0209,  0.0525, -0.0112],\n",
       "                      [-0.0099,  0.0114, -0.0026,  ...,  0.0014, -0.0107, -0.0199],\n",
       "                      ...,\n",
       "                      [ 0.0212, -0.0405,  0.0067,  ...,  0.0210, -0.0498,  0.0294],\n",
       "                      [ 0.0352, -0.0139, -0.0188,  ..., -0.0111, -0.0058,  0.0449],\n",
       "                      [ 0.0260, -0.0135,  0.0325,  ...,  0.0095,  0.0031, -0.0003]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-4.0588e-03, -6.3896e-05, -1.7262e-04,  ...,  1.5442e-02,\n",
       "                       -1.2512e-02, -1.9379e-03],\n",
       "                      [-1.2207e-03, -1.1292e-02, -1.4648e-02,  ..., -3.0365e-03,\n",
       "                        1.0010e-02,  6.5002e-03],\n",
       "                      [ 1.1536e-02,  1.0742e-02, -3.5553e-03,  ..., -4.5166e-03,\n",
       "                        5.4016e-03, -5.8594e-03],\n",
       "                      ...,\n",
       "                      [ 8.8882e-04, -1.3672e-02,  1.5381e-02,  ...,  1.3306e-02,\n",
       "                        1.1902e-03,  1.3489e-02],\n",
       "                      [-5.3711e-03, -9.3994e-03, -9.7656e-03,  ..., -1.4465e-02,\n",
       "                        2.9445e-05, -5.4932e-03],\n",
       "                      [-4.4250e-03, -6.7749e-03,  8.0566e-03,  ...,  4.5471e-03,\n",
       "                        4.3335e-03,  6.2256e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0106,  0.0259, -0.0179,  ..., -0.0132,  0.0124,  0.0092],\n",
       "                      [ 0.0121, -0.0267, -0.0103,  ..., -0.0084, -0.0135, -0.0132],\n",
       "                      [ 0.0095,  0.0133, -0.0052,  ...,  0.0203,  0.0376,  0.0522],\n",
       "                      ...,\n",
       "                      [-0.0270,  0.0557,  0.0231,  ..., -0.0041, -0.0044, -0.0216],\n",
       "                      [-0.0354, -0.0046,  0.0161,  ..., -0.0022,  0.0289, -0.0095],\n",
       "                      [ 0.0056, -0.0107, -0.0304,  ...,  0.0250, -0.0308, -0.0201]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0020, -0.0051, -0.0100,  ...,  0.0095,  0.0074, -0.0130],\n",
       "                      [-0.0114, -0.0086,  0.0089,  ..., -0.0077, -0.0005,  0.0037],\n",
       "                      [-0.0042,  0.0110, -0.0133,  ..., -0.0065,  0.0078,  0.0101],\n",
       "                      ...,\n",
       "                      [ 0.0128, -0.0108,  0.0140,  ...,  0.0016, -0.0074, -0.0133],\n",
       "                      [ 0.0042, -0.0075, -0.0029,  ...,  0.0118,  0.0140, -0.0106],\n",
       "                      [ 0.0071,  0.0065, -0.0078,  ..., -0.0135,  0.0089, -0.0088]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0337, -0.0121, -0.0302,  ...,  0.0074, -0.0371, -0.0164],\n",
       "                      [-0.0281, -0.0176, -0.0033,  ...,  0.0192,  0.0025, -0.0381],\n",
       "                      [-0.0140, -0.0089,  0.0459,  ...,  0.0033,  0.0136,  0.0143],\n",
       "                      ...,\n",
       "                      [ 0.0072,  0.0226,  0.0272,  ...,  0.0114,  0.0221,  0.0070],\n",
       "                      [-0.0065,  0.0147, -0.0064,  ..., -0.0025, -0.0442, -0.0168],\n",
       "                      [-0.0398,  0.0095,  0.0063,  ...,  0.0105,  0.0270,  0.0219]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0022, -0.0082,  0.0042,  ...,  0.0124,  0.0023,  0.0125],\n",
       "                      [ 0.0095,  0.0064, -0.0024,  ..., -0.0117, -0.0049,  0.0140],\n",
       "                      [ 0.0114, -0.0098, -0.0022,  ...,  0.0137,  0.0012,  0.0141],\n",
       "                      ...,\n",
       "                      [ 0.0013, -0.0036,  0.0126,  ..., -0.0133, -0.0146,  0.0151],\n",
       "                      [-0.0010,  0.0060,  0.0145,  ...,  0.0103, -0.0050,  0.0119],\n",
       "                      [ 0.0019,  0.0112,  0.0042,  ..., -0.0013,  0.0121, -0.0153]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.up_proj.weight',\n",
       "              tensor([[-0.0168,  0.0173, -0.0139,  ...,  0.0260, -0.0039, -0.0256],\n",
       "                      [-0.0256,  0.0435, -0.0004,  ...,  0.0229,  0.0085, -0.0107],\n",
       "                      [ 0.0503, -0.0437, -0.0093,  ...,  0.0123, -0.0217,  0.0349],\n",
       "                      ...,\n",
       "                      [-0.0130,  0.0435,  0.0063,  ..., -0.0155, -0.0542, -0.0249],\n",
       "                      [ 0.0035,  0.0085,  0.0070,  ...,  0.0021,  0.0137,  0.0042],\n",
       "                      [ 0.0096,  0.0266, -0.0032,  ..., -0.0356,  0.0042,  0.0012]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0059,  0.0132, -0.0031,  ..., -0.0003, -0.0105,  0.0044],\n",
       "                      [ 0.0156, -0.0027,  0.0034,  ..., -0.0134,  0.0093, -0.0109],\n",
       "                      [-0.0056,  0.0075,  0.0121,  ..., -0.0134, -0.0018, -0.0131],\n",
       "                      ...,\n",
       "                      [-0.0124, -0.0008, -0.0064,  ..., -0.0061, -0.0027,  0.0061],\n",
       "                      [ 0.0051, -0.0034, -0.0066,  ..., -0.0049,  0.0156, -0.0058],\n",
       "                      [ 0.0123, -0.0015, -0.0131,  ..., -0.0050, -0.0080, -0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.down_proj.weight',\n",
       "              tensor([[-0.0067,  0.0124,  0.0192,  ...,  0.0199, -0.0038, -0.0260],\n",
       "                      [ 0.0288,  0.0222, -0.0121,  ..., -0.0596, -0.0068, -0.0330],\n",
       "                      [ 0.0104, -0.0041, -0.0588,  ...,  0.0117,  0.0515,  0.0064],\n",
       "                      ...,\n",
       "                      [ 0.0113, -0.0205, -0.0112,  ..., -0.0093,  0.0122, -0.0250],\n",
       "                      [ 0.0186, -0.0035, -0.0086,  ...,  0.0298, -0.0165, -0.0021],\n",
       "                      [-0.0339,  0.0303, -0.0027,  ..., -0.0076, -0.0006, -0.0206]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0081, -0.0062, -0.0009,  ..., -0.0040, -0.0023, -0.0025],\n",
       "                      [ 0.0014, -0.0035,  0.0003,  ..., -0.0056, -0.0068, -0.0059],\n",
       "                      [-0.0014,  0.0003, -0.0034,  ...,  0.0069,  0.0054,  0.0073],\n",
       "                      ...,\n",
       "                      [ 0.0083, -0.0049,  0.0009,  ..., -0.0032, -0.0023, -0.0012],\n",
       "                      [ 0.0033, -0.0047, -0.0071,  ...,  0.0044, -0.0044,  0.0066],\n",
       "                      [ 0.0010, -0.0009,  0.0013,  ..., -0.0058,  0.0092,  0.0017]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.input_layernorm.weight',\n",
       "              tensor([0.2490, 0.2695, 0.2412,  ..., 0.2451, 0.2695, 0.2354],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.8.post_attention_layernorm.weight',\n",
       "              tensor([0.1904, 0.1748, 0.1670,  ..., 0.1885, 0.1816, 0.1787],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.q_proj.weight',\n",
       "              tensor([[ 2.0142e-02, -1.2390e-02, -1.9165e-02,  ...,  1.8616e-03,\n",
       "                       -7.9346e-03, -8.2397e-03],\n",
       "                      [-1.2634e-02, -2.4414e-03,  1.9409e-02,  ...,  1.3062e-02,\n",
       "                       -2.0996e-02, -3.7689e-03],\n",
       "                      [ 2.6855e-02, -1.5747e-02,  5.1975e-05,  ...,  1.0742e-02,\n",
       "                        7.2098e-04, -3.3691e-02],\n",
       "                      ...,\n",
       "                      [ 1.9409e-02, -7.5684e-02, -4.3945e-03,  ...,  2.2736e-03,\n",
       "                        6.8970e-03, -2.0386e-02],\n",
       "                      [-2.8931e-02,  3.6133e-02,  4.0039e-02,  ..., -3.9551e-02,\n",
       "                       -1.9287e-02,  6.0303e-02],\n",
       "                      [ 8.1787e-03,  4.0039e-02,  9.2773e-03,  ..., -1.8677e-02,\n",
       "                       -1.9287e-02,  2.9663e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0029, -0.0030, -0.0098,  ..., -0.0136,  0.0020,  0.0128],\n",
       "                      [ 0.0066,  0.0114, -0.0123,  ..., -0.0125, -0.0029, -0.0117],\n",
       "                      [-0.0013, -0.0067, -0.0004,  ..., -0.0040,  0.0090, -0.0045],\n",
       "                      ...,\n",
       "                      [ 0.0045,  0.0056,  0.0021,  ...,  0.0125,  0.0107,  0.0024],\n",
       "                      [ 0.0006,  0.0042, -0.0016,  ..., -0.0110, -0.0150, -0.0150],\n",
       "                      [-0.0027,  0.0113, -0.0069,  ..., -0.0038, -0.0152, -0.0124]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0217,  0.0069, -0.0003,  ..., -0.0173, -0.0055, -0.0364],\n",
       "                      [-0.0039,  0.0186,  0.0223,  ...,  0.0154, -0.0093, -0.0352],\n",
       "                      [ 0.0055,  0.0067, -0.0086,  ...,  0.0231, -0.0151,  0.0432],\n",
       "                      ...,\n",
       "                      [-0.0031, -0.0544, -0.0134,  ..., -0.0081,  0.0074, -0.0028],\n",
       "                      [ 0.0094, -0.0078,  0.0608,  ...,  0.0344, -0.0271,  0.0199],\n",
       "                      [-0.0070, -0.0713, -0.0039,  ...,  0.0093, -0.0152,  0.0238]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0134,  0.0135,  0.0101,  ..., -0.0082, -0.0105,  0.0028],\n",
       "                      [-0.0067, -0.0049, -0.0106,  ...,  0.0008, -0.0114,  0.0085],\n",
       "                      [ 0.0034,  0.0034,  0.0095,  ..., -0.0076, -0.0104,  0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0013, -0.0139, -0.0085,  ...,  0.0143, -0.0101, -0.0041],\n",
       "                      [-0.0116, -0.0071, -0.0103,  ..., -0.0021, -0.0127,  0.0070],\n",
       "                      [ 0.0066, -0.0085, -0.0009,  ..., -0.0125,  0.0014, -0.0075]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.v_proj.weight',\n",
       "              tensor([[ 2.4902e-02,  5.0537e-02, -1.0437e-02,  ...,  1.3489e-02,\n",
       "                       -5.5664e-02, -4.4250e-03],\n",
       "                      [-2.0874e-02,  2.3438e-02, -1.0559e-02,  ...,  2.4536e-02,\n",
       "                       -1.5320e-02,  2.3071e-02],\n",
       "                      [-5.7373e-03,  2.1729e-02, -1.6357e-02,  ..., -2.8564e-02,\n",
       "                        1.5076e-02, -3.3203e-02],\n",
       "                      ...,\n",
       "                      [-2.1118e-02, -3.3936e-02,  1.3046e-03,  ...,  1.5320e-02,\n",
       "                       -4.6387e-02,  7.0496e-03],\n",
       "                      [-2.7588e-02,  3.7432e-05, -2.0874e-02,  ..., -3.2959e-02,\n",
       "                       -2.4048e-02, -3.4637e-03],\n",
       "                      [ 5.8350e-02,  1.9165e-02, -1.6968e-02,  ...,  1.4893e-02,\n",
       "                       -9.1934e-04,  3.0060e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0120, -0.0134,  0.0086,  ...,  0.0059, -0.0114,  0.0042],\n",
       "                      [ 0.0139,  0.0089,  0.0082,  ..., -0.0079,  0.0082,  0.0050],\n",
       "                      [-0.0038,  0.0027, -0.0042,  ...,  0.0022,  0.0122,  0.0155],\n",
       "                      ...,\n",
       "                      [-0.0067,  0.0012, -0.0134,  ...,  0.0022, -0.0017,  0.0122],\n",
       "                      [-0.0019,  0.0038, -0.0148,  ..., -0.0027, -0.0136, -0.0018],\n",
       "                      [ 0.0011,  0.0145, -0.0127,  ..., -0.0047,  0.0137,  0.0112]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0024, -0.0051,  0.0320,  ...,  0.0106,  0.0444, -0.0442],\n",
       "                      [-0.0060,  0.0032,  0.0083,  ...,  0.0048, -0.0030,  0.0051],\n",
       "                      [ 0.0009, -0.0278, -0.0250,  ...,  0.0273,  0.0233,  0.0190],\n",
       "                      ...,\n",
       "                      [ 0.0177,  0.0227,  0.0075,  ...,  0.0300, -0.0212, -0.0273],\n",
       "                      [ 0.0282,  0.0204,  0.0066,  ...,  0.0002, -0.0047, -0.0112],\n",
       "                      [ 0.0269,  0.0464, -0.0233,  ..., -0.0084,  0.0084, -0.0152]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0046, -0.0150, -0.0069,  ...,  0.0074, -0.0011, -0.0090],\n",
       "                      [ 0.0135,  0.0133, -0.0056,  ..., -0.0082,  0.0115, -0.0128],\n",
       "                      [ 0.0039,  0.0106,  0.0128,  ...,  0.0120, -0.0067,  0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0014, -0.0145, -0.0005,  ..., -0.0036, -0.0107,  0.0124],\n",
       "                      [ 0.0036,  0.0040,  0.0048,  ...,  0.0098, -0.0099,  0.0007],\n",
       "                      [ 0.0135, -0.0025,  0.0033,  ..., -0.0016,  0.0099,  0.0123]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0079,  0.0240, -0.0299,  ..., -0.0498, -0.0154,  0.0168],\n",
       "                      [-0.0413,  0.0280,  0.0579,  ..., -0.0182,  0.0067, -0.0177],\n",
       "                      [ 0.0366,  0.0244, -0.0393,  ..., -0.0215, -0.0042, -0.0099],\n",
       "                      ...,\n",
       "                      [ 0.0130, -0.0240, -0.0364,  ...,  0.0233, -0.0168,  0.0236],\n",
       "                      [-0.0194, -0.0344,  0.0349,  ..., -0.0369, -0.0079,  0.0188],\n",
       "                      [-0.0251,  0.0234, -0.0050,  ...,  0.0381, -0.0148, -0.0305]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0083, -0.0132, -0.0129,  ...,  0.0070, -0.0103,  0.0022],\n",
       "                      [ 0.0107, -0.0139,  0.0130,  ..., -0.0103, -0.0033, -0.0138],\n",
       "                      [-0.0094,  0.0107,  0.0151,  ..., -0.0030, -0.0095,  0.0058],\n",
       "                      ...,\n",
       "                      [ 0.0112, -0.0015,  0.0065,  ..., -0.0135, -0.0039,  0.0018],\n",
       "                      [-0.0025, -0.0040,  0.0096,  ..., -0.0016, -0.0018,  0.0006],\n",
       "                      [ 0.0154, -0.0034,  0.0038,  ...,  0.0055,  0.0057,  0.0023]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.up_proj.weight',\n",
       "              tensor([[-0.0070, -0.0018,  0.0286,  ..., -0.0352,  0.0140,  0.0311],\n",
       "                      [ 0.0081, -0.0039, -0.0242,  ...,  0.0684,  0.0030,  0.0090],\n",
       "                      [-0.0017, -0.0216, -0.0376,  ..., -0.0327,  0.0021, -0.0023],\n",
       "                      ...,\n",
       "                      [-0.0227, -0.0050,  0.0133,  ..., -0.0013, -0.0042,  0.0089],\n",
       "                      [ 0.0100, -0.0253, -0.0488,  ..., -0.0315, -0.0281,  0.0080],\n",
       "                      [-0.0090,  0.0369, -0.0603,  ..., -0.0132,  0.0347,  0.0032]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0048,  0.0115, -0.0061,  ..., -0.0070, -0.0047,  0.0074],\n",
       "                      [-0.0063,  0.0133, -0.0052,  ..., -0.0146, -0.0016, -0.0079],\n",
       "                      [-0.0015, -0.0096,  0.0138,  ...,  0.0137, -0.0084,  0.0125],\n",
       "                      ...,\n",
       "                      [-0.0071, -0.0003,  0.0072,  ..., -0.0123,  0.0109, -0.0017],\n",
       "                      [-0.0109,  0.0029, -0.0002,  ...,  0.0101, -0.0091, -0.0011],\n",
       "                      [-0.0128,  0.0090, -0.0023,  ...,  0.0114,  0.0139, -0.0145]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0068,  0.0189, -0.0347,  ...,  0.0209, -0.0464, -0.0114],\n",
       "                      [ 0.0041, -0.0238, -0.0046,  ..., -0.0259, -0.0432,  0.0540],\n",
       "                      [-0.0073, -0.0195,  0.0330,  ..., -0.0150,  0.0069,  0.0037],\n",
       "                      ...,\n",
       "                      [-0.0579,  0.0330,  0.0454,  ...,  0.0136, -0.0513,  0.0322],\n",
       "                      [-0.0400,  0.0091,  0.0111,  ..., -0.0017,  0.0203, -0.0254],\n",
       "                      [ 0.0127,  0.0178,  0.0087,  ...,  0.0464,  0.0075, -0.0057]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-2.6398e-03, -8.7280e-03,  6.5918e-03,  ..., -1.8005e-03,\n",
       "                        1.4725e-03,  6.6833e-03],\n",
       "                      [-6.8665e-03,  6.1646e-03, -4.2419e-03,  ...,  7.4387e-04,\n",
       "                        1.8463e-03,  6.7444e-03],\n",
       "                      [-5.9814e-03,  5.6152e-03,  3.3112e-03,  ...,  7.3853e-03,\n",
       "                       -9.0942e-03, -1.7776e-03],\n",
       "                      ...,\n",
       "                      [-7.6599e-03,  4.3030e-03, -6.6757e-04,  ..., -6.8188e-05,\n",
       "                       -5.7983e-03, -3.8605e-03],\n",
       "                      [ 2.1820e-03, -2.5787e-03, -7.7820e-03,  ..., -8.8501e-03,\n",
       "                       -1.4191e-03,  1.6479e-03],\n",
       "                      [-8.4839e-03,  2.3041e-03,  9.0942e-03,  ...,  4.7913e-03,\n",
       "                        2.1973e-03,  6.7444e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.input_layernorm.weight',\n",
       "              tensor([0.2715, 0.2676, 0.2393,  ..., 0.2695, 0.2578, 0.2656],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.9.post_attention_layernorm.weight',\n",
       "              tensor([0.1973, 0.1826, 0.1729,  ..., 0.1943, 0.1914, 0.1855],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0128, -0.0067,  0.0140,  ...,  0.0138,  0.0199, -0.0217],\n",
       "                      [ 0.0242, -0.0039,  0.0195,  ..., -0.0154,  0.0461,  0.0145],\n",
       "                      [ 0.0129,  0.0090, -0.0090,  ..., -0.0005,  0.0107,  0.0011],\n",
       "                      ...,\n",
       "                      [ 0.0615,  0.0483,  0.0045,  ...,  0.0243,  0.0269,  0.0194],\n",
       "                      [-0.0601,  0.0264, -0.0067,  ..., -0.1089, -0.0104, -0.0286],\n",
       "                      [-0.0288, -0.0093, -0.0309,  ..., -0.0378, -0.0044, -0.0327]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0025,  0.0015,  0.0053,  ..., -0.0011, -0.0052,  0.0126],\n",
       "                      [-0.0106,  0.0085,  0.0115,  ...,  0.0014, -0.0058,  0.0142],\n",
       "                      [-0.0075,  0.0075,  0.0082,  ...,  0.0154, -0.0117,  0.0060],\n",
       "                      ...,\n",
       "                      [ 0.0052,  0.0101,  0.0084,  ...,  0.0099, -0.0066, -0.0004],\n",
       "                      [-0.0101,  0.0121, -0.0003,  ..., -0.0010, -0.0072, -0.0067],\n",
       "                      [ 0.0130, -0.0081, -0.0049,  ..., -0.0015,  0.0029, -0.0111]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0072,  0.0232, -0.0044,  ..., -0.0050, -0.0036,  0.0199],\n",
       "                      [ 0.0126, -0.0042, -0.0070,  ...,  0.0071, -0.0057, -0.0030],\n",
       "                      [-0.0100,  0.0253,  0.0189,  ...,  0.0138, -0.0042,  0.0469],\n",
       "                      ...,\n",
       "                      [-0.0557, -0.0386,  0.0229,  ...,  0.0518, -0.0493, -0.0225],\n",
       "                      [-0.0515,  0.0483,  0.0452,  ..., -0.0031, -0.0408,  0.0092],\n",
       "                      [-0.0054,  0.0913,  0.0082,  ..., -0.0135,  0.0294, -0.0167]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0007, -0.0143,  0.0056,  ..., -0.0142, -0.0001, -0.0118],\n",
       "                      [-0.0045,  0.0088, -0.0139,  ...,  0.0058,  0.0155,  0.0081],\n",
       "                      [-0.0140,  0.0086, -0.0107,  ...,  0.0146, -0.0014,  0.0140],\n",
       "                      ...,\n",
       "                      [-0.0025,  0.0089,  0.0148,  ...,  0.0110, -0.0058, -0.0049],\n",
       "                      [-0.0067,  0.0028,  0.0123,  ..., -0.0075,  0.0118,  0.0128],\n",
       "                      [-0.0083, -0.0142, -0.0081,  ...,  0.0110,  0.0058, -0.0090]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.v_proj.weight',\n",
       "              tensor([[-4.8096e-02,  5.5664e-02, -2.3651e-03,  ..., -2.9755e-04,\n",
       "                       -5.7983e-03,  1.2146e-02],\n",
       "                      [ 1.4954e-02,  5.9204e-03,  4.0283e-02,  ..., -5.0354e-03,\n",
       "                       -1.9287e-02,  5.4199e-02],\n",
       "                      [-1.6403e-03, -3.9307e-02,  1.2085e-02,  ..., -3.6926e-03,\n",
       "                        3.1738e-02, -2.2461e-02],\n",
       "                      ...,\n",
       "                      [-1.0223e-03,  3.3936e-02, -2.1118e-02,  ...,  2.1973e-02,\n",
       "                       -2.3315e-02, -5.8105e-02],\n",
       "                      [-7.5073e-03, -1.0681e-03,  4.0527e-02,  ...,  1.6724e-02,\n",
       "                        1.8433e-02,  8.5831e-05],\n",
       "                      [-2.3651e-03,  3.9551e-02, -4.0283e-02,  ..., -9.7046e-03,\n",
       "                       -3.4424e-02, -1.4587e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0116,  0.0006,  0.0045,  ..., -0.0123,  0.0062,  0.0031],\n",
       "                      [ 0.0092,  0.0082, -0.0055,  ..., -0.0026, -0.0055, -0.0101],\n",
       "                      [ 0.0015, -0.0031,  0.0135,  ..., -0.0057, -0.0032,  0.0015],\n",
       "                      ...,\n",
       "                      [-0.0129,  0.0053,  0.0014,  ..., -0.0154, -0.0026, -0.0003],\n",
       "                      [ 0.0134,  0.0009,  0.0082,  ..., -0.0009,  0.0044, -0.0059],\n",
       "                      [-0.0110, -0.0056,  0.0119,  ..., -0.0009, -0.0016,  0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0101, -0.0028, -0.0073,  ..., -0.0068,  0.0234,  0.0071],\n",
       "                      [-0.0522, -0.0840,  0.0315,  ..., -0.0101,  0.0078,  0.0027],\n",
       "                      [ 0.0035, -0.0378, -0.0297,  ...,  0.0248,  0.0034,  0.0058],\n",
       "                      ...,\n",
       "                      [ 0.0018,  0.0029, -0.0613,  ...,  0.0098, -0.0052, -0.0073],\n",
       "                      [-0.0217, -0.0177, -0.0009,  ..., -0.0103,  0.0203,  0.0106],\n",
       "                      [ 0.0260, -0.0124, -0.0001,  ..., -0.0194,  0.0293, -0.0018]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0004, -0.0012,  0.0019,  ..., -0.0098, -0.0107, -0.0019],\n",
       "                      [-0.0041, -0.0003,  0.0035,  ..., -0.0066, -0.0024, -0.0002],\n",
       "                      [-0.0015, -0.0107,  0.0123,  ...,  0.0143, -0.0020, -0.0150],\n",
       "                      ...,\n",
       "                      [-0.0124,  0.0034,  0.0031,  ...,  0.0130, -0.0128, -0.0037],\n",
       "                      [ 0.0148, -0.0154,  0.0152,  ..., -0.0048,  0.0133,  0.0104],\n",
       "                      [ 0.0131, -0.0019,  0.0006,  ..., -0.0121, -0.0021,  0.0048]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0417, -0.0305, -0.0103,  ...,  0.0086,  0.0016, -0.0026],\n",
       "                      [-0.0292,  0.0039,  0.0075,  ...,  0.0017,  0.0258,  0.0237],\n",
       "                      [-0.0027,  0.0007,  0.0178,  ...,  0.0593,  0.0247, -0.0110],\n",
       "                      ...,\n",
       "                      [-0.0559,  0.0037,  0.0170,  ...,  0.0293, -0.0303,  0.0195],\n",
       "                      [-0.0113, -0.0033,  0.0371,  ...,  0.0320, -0.0128,  0.0074],\n",
       "                      [ 0.0220, -0.0325,  0.0106,  ..., -0.0183, -0.0059, -0.0277]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-5.9814e-03, -9.2773e-03, -1.3306e-02,  ..., -1.4404e-02,\n",
       "                        3.4027e-03, -8.6060e-03],\n",
       "                      [-1.3657e-03, -4.5967e-04,  6.3782e-03,  ..., -1.3489e-02,\n",
       "                       -6.7749e-03, -1.6022e-03],\n",
       "                      [-1.3916e-02,  4.2419e-03, -1.3245e-02,  ..., -8.9111e-03,\n",
       "                       -1.2085e-02,  1.3672e-02],\n",
       "                      ...,\n",
       "                      [ 7.2937e-03, -5.0964e-03, -3.9368e-03,  ..., -3.3569e-03,\n",
       "                        1.3199e-03, -1.0437e-02],\n",
       "                      [-1.3916e-02, -6.0797e-05,  8.3618e-03,  ..., -9.3937e-05,\n",
       "                       -2.1973e-03,  1.2390e-02],\n",
       "                      [ 1.4526e-02,  6.6833e-03, -1.0223e-03,  ..., -6.3782e-03,\n",
       "                        1.2756e-02,  6.0120e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.up_proj.weight',\n",
       "              tensor([[-0.0161,  0.0125, -0.0369,  ...,  0.0437, -0.0056, -0.0344],\n",
       "                      [-0.0640,  0.0403, -0.0216,  ...,  0.0156,  0.0058, -0.0072],\n",
       "                      [-0.0366, -0.0172,  0.0048,  ...,  0.0894, -0.0194,  0.0026],\n",
       "                      ...,\n",
       "                      [ 0.0386,  0.0459, -0.0425,  ...,  0.0055, -0.0164, -0.0217],\n",
       "                      [-0.0452,  0.0308, -0.0447,  ...,  0.0029,  0.0045, -0.0271],\n",
       "                      [-0.0244,  0.0244, -0.0270,  ...,  0.0184,  0.0007,  0.0136]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0145,  0.0156, -0.0064,  ...,  0.0101, -0.0118, -0.0033],\n",
       "                      [-0.0055,  0.0034, -0.0124,  ..., -0.0087,  0.0107, -0.0039],\n",
       "                      [ 0.0089,  0.0052,  0.0012,  ...,  0.0116,  0.0145, -0.0040],\n",
       "                      ...,\n",
       "                      [ 0.0140,  0.0033, -0.0039,  ...,  0.0145, -0.0096,  0.0118],\n",
       "                      [ 0.0140,  0.0151,  0.0153,  ..., -0.0025,  0.0120,  0.0125],\n",
       "                      [-0.0142,  0.0041, -0.0101,  ...,  0.0150, -0.0009,  0.0047]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.down_proj.weight',\n",
       "              tensor([[-0.0189, -0.0178,  0.0292,  ...,  0.0469, -0.0369, -0.0125],\n",
       "                      [ 0.0525,  0.0033,  0.0056,  ..., -0.0148, -0.0178,  0.0129],\n",
       "                      [-0.0825, -0.0204,  0.0022,  ...,  0.0101, -0.0090, -0.0170],\n",
       "                      ...,\n",
       "                      [ 0.0371,  0.0115,  0.0248,  ..., -0.0106, -0.0060,  0.0134],\n",
       "                      [ 0.0208,  0.0469,  0.0020,  ...,  0.0420,  0.0400, -0.0079],\n",
       "                      [-0.0327,  0.0126, -0.0459,  ..., -0.0248,  0.0084,  0.0272]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0045, -0.0060,  0.0047,  ...,  0.0091, -0.0011,  0.0017],\n",
       "                      [ 0.0055, -0.0087,  0.0079,  ...,  0.0019,  0.0064,  0.0073],\n",
       "                      [-0.0081, -0.0087, -0.0010,  ..., -0.0068, -0.0063,  0.0012],\n",
       "                      ...,\n",
       "                      [ 0.0008, -0.0072, -0.0055,  ...,  0.0041, -0.0073,  0.0079],\n",
       "                      [-0.0024,  0.0091,  0.0017,  ..., -0.0025, -0.0050,  0.0084],\n",
       "                      [ 0.0034, -0.0087, -0.0037,  ...,  0.0038,  0.0069, -0.0030]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.input_layernorm.weight',\n",
       "              tensor([0.2891, 0.2773, 0.2412,  ..., 0.2773, 0.2734, 0.2715],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.10.post_attention_layernorm.weight',\n",
       "              tensor([0.2002, 0.1836, 0.1748,  ..., 0.1973, 0.1934, 0.1943],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0272,  0.0110, -0.0016,  ...,  0.0189, -0.0082, -0.0095],\n",
       "                      [ 0.0010,  0.0254,  0.0146,  ..., -0.0002, -0.0259, -0.0031],\n",
       "                      [-0.0024, -0.0074, -0.0123,  ..., -0.0242, -0.0005, -0.0085],\n",
       "                      ...,\n",
       "                      [-0.0022, -0.0201,  0.0583,  ...,  0.0154,  0.0493, -0.0591],\n",
       "                      [ 0.0513, -0.0222, -0.0110,  ..., -0.0255,  0.0179, -0.0613],\n",
       "                      [ 0.1357, -0.0254, -0.0297,  ..., -0.0161,  0.0114,  0.0134]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0123, -0.0127,  0.0125,  ...,  0.0068,  0.0047,  0.0140],\n",
       "                      [ 0.0140, -0.0099, -0.0020,  ..., -0.0023,  0.0133,  0.0148],\n",
       "                      [-0.0131, -0.0030, -0.0074,  ..., -0.0108,  0.0032,  0.0023],\n",
       "                      ...,\n",
       "                      [ 0.0013, -0.0147, -0.0012,  ...,  0.0008,  0.0105,  0.0120],\n",
       "                      [ 0.0050, -0.0120, -0.0081,  ...,  0.0027,  0.0056, -0.0141],\n",
       "                      [-0.0078, -0.0052, -0.0134,  ..., -0.0100,  0.0090,  0.0103]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0019,  0.0105,  0.0091,  ..., -0.0278,  0.0154, -0.0250],\n",
       "                      [-0.0107, -0.0176,  0.0080,  ..., -0.0137, -0.0096,  0.0204],\n",
       "                      [ 0.0099, -0.0339, -0.0055,  ...,  0.0063, -0.0221,  0.0154],\n",
       "                      ...,\n",
       "                      [ 0.0167,  0.0403, -0.0124,  ..., -0.0469, -0.0317,  0.0320],\n",
       "                      [ 0.0718, -0.0247, -0.0732,  ...,  0.0464,  0.0220,  0.0417],\n",
       "                      [-0.0057, -0.0723,  0.0214,  ...,  0.0025,  0.0786,  0.0024]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0141, -0.0120,  0.0016,  ...,  0.0011,  0.0095,  0.0146],\n",
       "                      [-0.0089,  0.0154, -0.0102,  ...,  0.0033,  0.0025,  0.0009],\n",
       "                      [ 0.0020,  0.0002,  0.0013,  ...,  0.0099, -0.0132, -0.0044],\n",
       "                      ...,\n",
       "                      [-0.0039, -0.0087,  0.0054,  ...,  0.0066, -0.0154, -0.0044],\n",
       "                      [ 0.0023,  0.0039,  0.0090,  ..., -0.0096,  0.0085, -0.0026],\n",
       "                      [-0.0132, -0.0007, -0.0102,  ...,  0.0029,  0.0005,  0.0056]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0035,  0.0076,  0.0242,  ..., -0.0006, -0.0251, -0.0087],\n",
       "                      [ 0.0017, -0.0132, -0.0275,  ..., -0.0059, -0.0112,  0.0172],\n",
       "                      [-0.0143, -0.0302,  0.0037,  ...,  0.0278, -0.0049, -0.0221],\n",
       "                      ...,\n",
       "                      [ 0.0240,  0.0036,  0.0205,  ...,  0.0186,  0.0229,  0.0304],\n",
       "                      [-0.0267,  0.0200, -0.0132,  ...,  0.0236,  0.0337, -0.0092],\n",
       "                      [ 0.0119, -0.0312,  0.0050,  ..., -0.0322, -0.0126, -0.0339]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0058, -0.0036, -0.0130,  ..., -0.0060,  0.0101, -0.0039],\n",
       "                      [-0.0129, -0.0117,  0.0150,  ..., -0.0010, -0.0054, -0.0028],\n",
       "                      [-0.0087,  0.0022, -0.0066,  ...,  0.0134,  0.0142, -0.0068],\n",
       "                      ...,\n",
       "                      [-0.0106,  0.0027, -0.0062,  ..., -0.0131,  0.0111, -0.0092],\n",
       "                      [-0.0067,  0.0084, -0.0041,  ..., -0.0038, -0.0114, -0.0014],\n",
       "                      [-0.0014, -0.0085, -0.0134,  ..., -0.0113, -0.0152, -0.0113]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0211,  0.0032, -0.0309,  ...,  0.0139,  0.0041,  0.0231],\n",
       "                      [ 0.0139, -0.0554, -0.0150,  ..., -0.0530, -0.0231, -0.0200],\n",
       "                      [ 0.0048,  0.0006, -0.0187,  ...,  0.0153, -0.0171,  0.0070],\n",
       "                      ...,\n",
       "                      [ 0.0289, -0.0073,  0.0359,  ...,  0.0520, -0.0211,  0.0036],\n",
       "                      [ 0.0041,  0.0137,  0.0299,  ...,  0.0015, -0.0238,  0.0075],\n",
       "                      [-0.0069, -0.0078,  0.0135,  ..., -0.0288,  0.0009, -0.0087]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-1.1780e-02, -1.0315e-02, -2.9449e-03,  ...,  6.0120e-03,\n",
       "                       -1.3428e-03,  1.2695e-02],\n",
       "                      [-9.3384e-03, -7.8125e-03, -4.3106e-04,  ..., -6.5918e-03,\n",
       "                        1.6098e-03,  2.5482e-03],\n",
       "                      [ 7.9956e-03,  3.1281e-03, -9.8267e-03,  ...,  5.8289e-03,\n",
       "                        3.6011e-03,  2.5635e-03],\n",
       "                      ...,\n",
       "                      [-1.2756e-02, -9.4604e-03, -5.9128e-05,  ..., -1.1673e-03,\n",
       "                       -1.3062e-02, -9.5825e-03],\n",
       "                      [-1.4587e-02, -1.4221e-02, -1.4465e-02,  ..., -3.0060e-03,\n",
       "                       -1.1902e-02,  3.3722e-03],\n",
       "                      [-1.3550e-02,  1.3428e-02,  1.1169e-02,  ...,  1.0376e-02,\n",
       "                       -3.0518e-03, -8.3618e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0415, -0.0129,  0.0107,  ..., -0.0277, -0.0197,  0.0260],\n",
       "                      [ 0.0070, -0.0024,  0.0152,  ...,  0.0035,  0.0015, -0.0265],\n",
       "                      [-0.0238, -0.0220, -0.0284,  ...,  0.0109,  0.0310, -0.0104],\n",
       "                      ...,\n",
       "                      [-0.0542,  0.0007, -0.0205,  ...,  0.0080, -0.0403, -0.0123],\n",
       "                      [ 0.0176,  0.0021,  0.0026,  ..., -0.0579, -0.0043,  0.0330],\n",
       "                      [ 0.0006, -0.0432, -0.0118,  ...,  0.0128, -0.0104,  0.0189]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 1.1047e-02, -7.5684e-03, -8.1787e-03,  ..., -1.5945e-03,\n",
       "                        7.8125e-03, -1.3000e-02],\n",
       "                      [ 1.3916e-02, -1.1169e-02, -6.9618e-05,  ..., -8.2397e-03,\n",
       "                       -1.1597e-02,  2.0599e-03],\n",
       "                      [-1.2695e-02,  1.5076e-02,  1.4404e-02,  ..., -2.0599e-03,\n",
       "                       -2.2736e-03,  5.7678e-03],\n",
       "                      ...,\n",
       "                      [ 3.1471e-04, -9.0332e-03,  8.3618e-03,  ...,  1.5320e-02,\n",
       "                       -1.2268e-02,  1.1047e-02],\n",
       "                      [-1.1597e-02, -1.0620e-02, -6.4850e-04,  ..., -4.0283e-03,\n",
       "                       -5.2795e-03,  7.2937e-03],\n",
       "                      [ 1.9302e-03,  6.8970e-03,  9.9487e-03,  ...,  2.8076e-03,\n",
       "                        6.8665e-03,  1.5564e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.up_proj.weight',\n",
       "              tensor([[-0.0270,  0.0286, -0.0361,  ..., -0.0035,  0.0510,  0.0306],\n",
       "                      [ 0.0608, -0.0007, -0.0229,  ...,  0.0210, -0.0864, -0.0034],\n",
       "                      [ 0.0187, -0.0304,  0.0079,  ..., -0.0183, -0.0003,  0.0466],\n",
       "                      ...,\n",
       "                      [ 0.0498, -0.0041,  0.0023,  ...,  0.0298,  0.0320,  0.0216],\n",
       "                      [-0.0253,  0.0053,  0.0168,  ...,  0.0265,  0.0136, -0.0030],\n",
       "                      [ 0.0119, -0.0248, -0.0049,  ..., -0.0015,  0.0081,  0.0249]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0003,  0.0034,  0.0075,  ...,  0.0060, -0.0117,  0.0053],\n",
       "                      [-0.0049,  0.0037, -0.0063,  ...,  0.0025,  0.0060,  0.0088],\n",
       "                      [ 0.0121,  0.0137, -0.0015,  ..., -0.0065,  0.0118,  0.0061],\n",
       "                      ...,\n",
       "                      [-0.0060,  0.0070,  0.0078,  ...,  0.0135, -0.0124,  0.0013],\n",
       "                      [-0.0135, -0.0097, -0.0126,  ..., -0.0005,  0.0107, -0.0110],\n",
       "                      [ 0.0015, -0.0039, -0.0122,  ...,  0.0123, -0.0106,  0.0108]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.down_proj.weight',\n",
       "              tensor([[-0.0527,  0.0339, -0.0051,  ...,  0.0109,  0.0253, -0.0339],\n",
       "                      [ 0.0084, -0.0073, -0.0413,  ..., -0.0229, -0.0037, -0.0112],\n",
       "                      [-0.0069, -0.0152,  0.0035,  ..., -0.0204, -0.0070, -0.0415],\n",
       "                      ...,\n",
       "                      [-0.0037, -0.0058, -0.0408,  ...,  0.0061,  0.0126, -0.0042],\n",
       "                      [-0.0094, -0.0593, -0.0381,  ...,  0.0073, -0.0103, -0.0016],\n",
       "                      [ 0.0164,  0.0267,  0.0260,  ...,  0.0018, -0.0013,  0.0179]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0092,  0.0039, -0.0082,  ..., -0.0057, -0.0032,  0.0073],\n",
       "                      [-0.0071,  0.0050,  0.0050,  ...,  0.0078, -0.0069,  0.0055],\n",
       "                      [ 0.0055,  0.0055, -0.0015,  ...,  0.0011, -0.0042,  0.0047],\n",
       "                      ...,\n",
       "                      [ 0.0080,  0.0075, -0.0034,  ...,  0.0089,  0.0081, -0.0082],\n",
       "                      [ 0.0047,  0.0070,  0.0046,  ...,  0.0028, -0.0001,  0.0086],\n",
       "                      [-0.0047,  0.0062, -0.0090,  ...,  0.0041,  0.0007,  0.0016]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.input_layernorm.weight',\n",
       "              tensor([0.3281, 0.3242, 0.2754,  ..., 0.3105, 0.2988, 0.3008],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.11.post_attention_layernorm.weight',\n",
       "              tensor([0.2139, 0.1934, 0.1875,  ..., 0.2148, 0.2061, 0.2051],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0041, -0.0095, -0.0024,  ...,  0.0255, -0.0047, -0.0044],\n",
       "                      [ 0.0071, -0.0276,  0.0019,  ..., -0.0104,  0.0070, -0.0165],\n",
       "                      [ 0.0066,  0.0121, -0.0359,  ..., -0.0117,  0.0295, -0.0129],\n",
       "                      ...,\n",
       "                      [-0.0334,  0.0405,  0.0087,  ..., -0.0403, -0.0063,  0.0272],\n",
       "                      [ 0.0283, -0.0393,  0.0137,  ...,  0.0854, -0.0144, -0.0352],\n",
       "                      [ 0.0334,  0.0127, -0.0479,  ..., -0.0016,  0.0287,  0.0698]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0061, -0.0093, -0.0098,  ..., -0.0013, -0.0098,  0.0071],\n",
       "                      [-0.0105,  0.0008,  0.0124,  ...,  0.0124,  0.0015, -0.0036],\n",
       "                      [ 0.0061,  0.0039,  0.0153,  ...,  0.0105,  0.0037, -0.0144],\n",
       "                      ...,\n",
       "                      [-0.0106, -0.0116,  0.0153,  ..., -0.0098, -0.0070,  0.0121],\n",
       "                      [ 0.0010, -0.0039, -0.0111,  ...,  0.0137,  0.0071,  0.0092],\n",
       "                      [ 0.0138, -0.0008,  0.0063,  ..., -0.0066, -0.0066, -0.0103]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0322, -0.0198, -0.0223,  ..., -0.0121, -0.0013, -0.0097],\n",
       "                      [ 0.0075,  0.0054, -0.0109,  ..., -0.0048, -0.0303,  0.0126],\n",
       "                      [-0.0089, -0.0135, -0.0029,  ...,  0.0019,  0.0088,  0.0286],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.0212, -0.0283,  ..., -0.0219,  0.0247, -0.0294],\n",
       "                      [-0.0381,  0.0066,  0.0486,  ..., -0.0630,  0.0187, -0.0483],\n",
       "                      [-0.0030,  0.0508,  0.0164,  ...,  0.0303, -0.0197, -0.0510]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0018,  0.0072,  0.0049,  ..., -0.0109, -0.0109, -0.0131],\n",
       "                      [-0.0114,  0.0114,  0.0145,  ..., -0.0053,  0.0087, -0.0060],\n",
       "                      [ 0.0064,  0.0009,  0.0031,  ..., -0.0089,  0.0019,  0.0144],\n",
       "                      ...,\n",
       "                      [-0.0020, -0.0057,  0.0032,  ...,  0.0082,  0.0156,  0.0023],\n",
       "                      [-0.0015,  0.0114,  0.0062,  ..., -0.0145, -0.0075,  0.0144],\n",
       "                      [ 0.0125,  0.0025,  0.0129,  ..., -0.0045, -0.0028,  0.0142]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0195,  0.0085, -0.0041,  ..., -0.0308,  0.0142, -0.0510],\n",
       "                      [-0.0557,  0.0173,  0.0082,  ..., -0.0708,  0.0134,  0.0085],\n",
       "                      [ 0.0036, -0.0295, -0.0118,  ..., -0.0134,  0.0530, -0.0113],\n",
       "                      ...,\n",
       "                      [ 0.0112,  0.0079, -0.0256,  ..., -0.0220, -0.0223, -0.0212],\n",
       "                      [-0.0151, -0.0127, -0.0070,  ..., -0.0077, -0.0042,  0.0056],\n",
       "                      [ 0.0179, -0.0130,  0.0151,  ...,  0.0188,  0.0250, -0.0205]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0147,  0.0024,  0.0103,  ...,  0.0113,  0.0070,  0.0087],\n",
       "                      [-0.0022, -0.0111, -0.0024,  ..., -0.0033, -0.0152,  0.0115],\n",
       "                      [-0.0020, -0.0084,  0.0139,  ...,  0.0151, -0.0120, -0.0075],\n",
       "                      ...,\n",
       "                      [-0.0139, -0.0131, -0.0129,  ...,  0.0104, -0.0050, -0.0071],\n",
       "                      [-0.0133,  0.0083,  0.0149,  ..., -0.0002,  0.0144, -0.0013],\n",
       "                      [-0.0109,  0.0014, -0.0015,  ..., -0.0087, -0.0014, -0.0069]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0286,  0.0299,  0.0027,  ...,  0.0464,  0.0120, -0.0043],\n",
       "                      [-0.0152, -0.0018,  0.0060,  ..., -0.0073,  0.0103, -0.0159],\n",
       "                      [-0.0559, -0.0160, -0.0194,  ...,  0.0107, -0.0229,  0.0312],\n",
       "                      ...,\n",
       "                      [ 0.0232, -0.0005,  0.0109,  ..., -0.0403,  0.0105,  0.0129],\n",
       "                      [-0.0182, -0.0300, -0.0217,  ..., -0.0135,  0.0142,  0.0072],\n",
       "                      [ 0.0374,  0.0096, -0.0010,  ...,  0.0256, -0.0127, -0.0283]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0150, -0.0122,  0.0028,  ..., -0.0143, -0.0149, -0.0021],\n",
       "                      [ 0.0131,  0.0106,  0.0067,  ..., -0.0082, -0.0041, -0.0088],\n",
       "                      [-0.0005,  0.0057,  0.0059,  ..., -0.0118, -0.0113, -0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0094,  0.0010, -0.0118,  ..., -0.0104, -0.0045,  0.0084],\n",
       "                      [-0.0070, -0.0052,  0.0092,  ..., -0.0137,  0.0093, -0.0117],\n",
       "                      [ 0.0105,  0.0093,  0.0024,  ..., -0.0150, -0.0117, -0.0017]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0070, -0.0330,  0.0150,  ..., -0.0474, -0.0371, -0.0143],\n",
       "                      [ 0.0061, -0.0145, -0.0079,  ..., -0.0145,  0.0396,  0.0012],\n",
       "                      [ 0.0153,  0.0427,  0.0182,  ..., -0.0051,  0.0103,  0.0238],\n",
       "                      ...,\n",
       "                      [-0.0289, -0.0060,  0.0209,  ..., -0.0176, -0.0054,  0.0364],\n",
       "                      [-0.0188,  0.0011, -0.0199,  ..., -0.0110, -0.0022,  0.0006],\n",
       "                      [ 0.0027, -0.0071, -0.0046,  ...,  0.0120, -0.0066, -0.0131]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0034, -0.0014, -0.0018,  ...,  0.0150,  0.0115, -0.0076],\n",
       "                      [-0.0049, -0.0151,  0.0098,  ..., -0.0102,  0.0027,  0.0112],\n",
       "                      [ 0.0126,  0.0136,  0.0065,  ..., -0.0154, -0.0043, -0.0039],\n",
       "                      ...,\n",
       "                      [ 0.0132,  0.0128,  0.0129,  ..., -0.0021,  0.0119,  0.0023],\n",
       "                      [ 0.0151, -0.0148,  0.0067,  ..., -0.0145,  0.0112,  0.0007],\n",
       "                      [ 0.0008, -0.0034,  0.0036,  ...,  0.0039,  0.0127,  0.0130]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.up_proj.weight',\n",
       "              tensor([[-0.0035, -0.0344, -0.0081,  ..., -0.0179,  0.0240,  0.0430],\n",
       "                      [-0.0237, -0.0005, -0.0469,  ...,  0.0124, -0.0417,  0.0347],\n",
       "                      [ 0.0193,  0.0229,  0.0009,  ..., -0.0248, -0.0182,  0.0197],\n",
       "                      ...,\n",
       "                      [-0.0014, -0.0187, -0.0264,  ...,  0.0210,  0.0047,  0.0112],\n",
       "                      [ 0.0057, -0.0269,  0.0210,  ...,  0.0061,  0.0332, -0.0295],\n",
       "                      [-0.0269, -0.0242, -0.0093,  ...,  0.0713, -0.0170, -0.0352]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0070,  0.0005, -0.0121,  ..., -0.0106, -0.0056, -0.0099],\n",
       "                      [-0.0082,  0.0082, -0.0040,  ..., -0.0037, -0.0023,  0.0027],\n",
       "                      [ 0.0083,  0.0135, -0.0032,  ...,  0.0033,  0.0070, -0.0145],\n",
       "                      ...,\n",
       "                      [-0.0077, -0.0077,  0.0020,  ..., -0.0150,  0.0040, -0.0151],\n",
       "                      [-0.0132,  0.0001,  0.0112,  ...,  0.0100,  0.0122, -0.0075],\n",
       "                      [-0.0096,  0.0052, -0.0059,  ...,  0.0034, -0.0140,  0.0082]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0281,  0.0091,  0.0107,  ...,  0.0267, -0.0176, -0.0040],\n",
       "                      [-0.0010, -0.0359,  0.0227,  ...,  0.0092, -0.0299,  0.0150],\n",
       "                      [-0.0002,  0.0075,  0.0254,  ..., -0.0233,  0.0087, -0.0154],\n",
       "                      ...,\n",
       "                      [ 0.0162,  0.0026, -0.0344,  ..., -0.0045,  0.0160,  0.0059],\n",
       "                      [-0.0214,  0.0095, -0.0104,  ...,  0.0133,  0.0325, -0.0197],\n",
       "                      [-0.0081,  0.0510,  0.0197,  ..., -0.0167,  0.0150, -0.0479]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0079,  0.0063, -0.0002,  ..., -0.0073, -0.0086,  0.0075],\n",
       "                      [ 0.0024,  0.0054, -0.0082,  ...,  0.0008, -0.0064, -0.0084],\n",
       "                      [-0.0012,  0.0078,  0.0003,  ..., -0.0004,  0.0085, -0.0035],\n",
       "                      ...,\n",
       "                      [-0.0024, -0.0076, -0.0059,  ..., -0.0077, -0.0023,  0.0062],\n",
       "                      [-0.0061,  0.0015, -0.0009,  ...,  0.0085, -0.0041, -0.0016],\n",
       "                      [-0.0023,  0.0004,  0.0032,  ..., -0.0051,  0.0083, -0.0052]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.input_layernorm.weight',\n",
       "              tensor([0.3418, 0.3145, 0.2793,  ..., 0.3047, 0.3105, 0.3184],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.12.post_attention_layernorm.weight',\n",
       "              tensor([0.2188, 0.1992, 0.1924,  ..., 0.2178, 0.2129, 0.2148],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0080, -0.0297, -0.0034,  ...,  0.0022,  0.0134, -0.0112],\n",
       "                      [-0.0231, -0.0101, -0.0327,  ...,  0.0184, -0.0064, -0.0115],\n",
       "                      [-0.0442,  0.0056,  0.0098,  ...,  0.0084, -0.0457,  0.0082],\n",
       "                      ...,\n",
       "                      [ 0.0688, -0.0286,  0.0275,  ...,  0.0378,  0.0146,  0.0122],\n",
       "                      [ 0.0254, -0.0052, -0.0193,  ..., -0.0113,  0.0270, -0.0240],\n",
       "                      [-0.0012, -0.0292,  0.0140,  ..., -0.0278, -0.0128, -0.0369]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0003, -0.0054,  0.0114,  ...,  0.0106, -0.0030, -0.0009],\n",
       "                      [ 0.0069, -0.0011,  0.0030,  ...,  0.0099, -0.0040,  0.0145],\n",
       "                      [-0.0134,  0.0080,  0.0117,  ...,  0.0134, -0.0022,  0.0078],\n",
       "                      ...,\n",
       "                      [-0.0047,  0.0105,  0.0134,  ...,  0.0009, -0.0032, -0.0023],\n",
       "                      [ 0.0068, -0.0148,  0.0047,  ...,  0.0125, -0.0085, -0.0039],\n",
       "                      [-0.0133, -0.0061, -0.0128,  ...,  0.0132, -0.0063,  0.0002]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0342,  0.0359, -0.0121,  ..., -0.0078,  0.0203,  0.0165],\n",
       "                      [ 0.0081, -0.0014, -0.0225,  ..., -0.0420,  0.0198,  0.0527],\n",
       "                      [-0.0171, -0.0074,  0.0243,  ..., -0.0081, -0.0226, -0.0280],\n",
       "                      ...,\n",
       "                      [ 0.0564,  0.0381,  0.0136,  ..., -0.0203, -0.0277, -0.0176],\n",
       "                      [ 0.0271,  0.0262,  0.0615,  ..., -0.0254,  0.0255,  0.0035],\n",
       "                      [-0.0508, -0.0508,  0.0167,  ..., -0.0101, -0.0515, -0.0193]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0032,  0.0139,  0.0085,  ..., -0.0027, -0.0022, -0.0153],\n",
       "                      [ 0.0021,  0.0064, -0.0112,  ..., -0.0141, -0.0120,  0.0038],\n",
       "                      [ 0.0123,  0.0109,  0.0148,  ..., -0.0122,  0.0043, -0.0056],\n",
       "                      ...,\n",
       "                      [-0.0069,  0.0045,  0.0081,  ...,  0.0141,  0.0072, -0.0048],\n",
       "                      [-0.0128, -0.0019, -0.0096,  ..., -0.0148,  0.0080, -0.0092],\n",
       "                      [-0.0053,  0.0143,  0.0145,  ...,  0.0137,  0.0123,  0.0076]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0024,  0.0334,  0.0209,  ..., -0.0080,  0.0077,  0.0442],\n",
       "                      [ 0.0024,  0.0091,  0.0320,  ..., -0.0104, -0.0095,  0.0255],\n",
       "                      [ 0.0569, -0.0062,  0.0221,  ..., -0.0306,  0.0267, -0.0243],\n",
       "                      ...,\n",
       "                      [-0.0065, -0.0111,  0.0178,  ..., -0.0623,  0.0564, -0.0400],\n",
       "                      [-0.0109,  0.0106,  0.0017,  ...,  0.0427,  0.0557, -0.0159],\n",
       "                      [ 0.0057, -0.0305,  0.0072,  ...,  0.0311, -0.0153,  0.0236]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0144,  0.0008, -0.0018,  ..., -0.0156, -0.0121, -0.0007],\n",
       "                      [-0.0099, -0.0065, -0.0093,  ...,  0.0117, -0.0039,  0.0115],\n",
       "                      [ 0.0140,  0.0129, -0.0072,  ...,  0.0003, -0.0150, -0.0126],\n",
       "                      ...,\n",
       "                      [-0.0131,  0.0072,  0.0082,  ...,  0.0064, -0.0104, -0.0062],\n",
       "                      [ 0.0013,  0.0149, -0.0118,  ...,  0.0026, -0.0070,  0.0019],\n",
       "                      [ 0.0110,  0.0053,  0.0131,  ..., -0.0134, -0.0052,  0.0133]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0061,  0.0089,  0.0040,  ..., -0.0160, -0.0251,  0.0118],\n",
       "                      [ 0.0388,  0.0135,  0.0166,  ...,  0.0091, -0.0277, -0.0304],\n",
       "                      [ 0.0233, -0.0079,  0.0254,  ...,  0.0016, -0.0220, -0.0232],\n",
       "                      ...,\n",
       "                      [-0.0053,  0.0085,  0.0209,  ..., -0.0339, -0.0071, -0.0286],\n",
       "                      [ 0.0217,  0.0093,  0.0530,  ..., -0.0102, -0.0250,  0.0005],\n",
       "                      [ 0.0120,  0.0461,  0.0248,  ...,  0.0051,  0.0006, -0.0095]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0109,  0.0128,  0.0014,  ...,  0.0067, -0.0022,  0.0074],\n",
       "                      [ 0.0049,  0.0134,  0.0113,  ...,  0.0054,  0.0106,  0.0031],\n",
       "                      [-0.0092, -0.0006, -0.0026,  ..., -0.0154,  0.0035, -0.0039],\n",
       "                      ...,\n",
       "                      [-0.0127, -0.0121, -0.0033,  ...,  0.0093, -0.0111,  0.0151],\n",
       "                      [ 0.0067, -0.0003,  0.0089,  ...,  0.0111,  0.0114, -0.0086],\n",
       "                      [-0.0095,  0.0136,  0.0036,  ...,  0.0085, -0.0045, -0.0074]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0091,  0.0027,  0.0026,  ...,  0.0056, -0.0049, -0.0432],\n",
       "                      [-0.0140,  0.0620,  0.0015,  ...,  0.0608, -0.0148, -0.0176],\n",
       "                      [ 0.0085, -0.0159,  0.0098,  ...,  0.0079, -0.0112,  0.0347],\n",
       "                      ...,\n",
       "                      [ 0.0366, -0.0383,  0.0325,  ..., -0.0003,  0.0259, -0.0352],\n",
       "                      [ 0.0457, -0.0189,  0.0330,  ...,  0.0386, -0.0261, -0.0374],\n",
       "                      [ 0.0021, -0.0065,  0.0439,  ...,  0.0080,  0.0591,  0.0349]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0095, -0.0078,  0.0025,  ...,  0.0044, -0.0140,  0.0036],\n",
       "                      [ 0.0029,  0.0145,  0.0070,  ..., -0.0073, -0.0142,  0.0069],\n",
       "                      [ 0.0022,  0.0030,  0.0103,  ...,  0.0125,  0.0110, -0.0128],\n",
       "                      ...,\n",
       "                      [ 0.0147,  0.0123, -0.0015,  ...,  0.0067, -0.0011,  0.0025],\n",
       "                      [ 0.0087, -0.0063,  0.0032,  ...,  0.0129,  0.0036,  0.0024],\n",
       "                      [ 0.0012,  0.0008,  0.0106,  ..., -0.0016,  0.0139,  0.0082]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.up_proj.weight',\n",
       "              tensor([[-0.0176,  0.0145,  0.0240,  ...,  0.0063, -0.0044, -0.0398],\n",
       "                      [ 0.0068,  0.0332,  0.0317,  ...,  0.0306,  0.0391, -0.0142],\n",
       "                      [ 0.0226,  0.0128,  0.0322,  ...,  0.0211,  0.0280, -0.0518],\n",
       "                      ...,\n",
       "                      [ 0.0209,  0.0219,  0.0393,  ..., -0.0304, -0.0354, -0.0240],\n",
       "                      [-0.0132, -0.0071,  0.0566,  ...,  0.0334,  0.0294, -0.0222],\n",
       "                      [ 0.0261, -0.0219,  0.0261,  ...,  0.0522,  0.0125,  0.0518]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0019, -0.0020,  0.0050,  ...,  0.0120,  0.0081, -0.0060],\n",
       "                      [-0.0075,  0.0085, -0.0031,  ...,  0.0118,  0.0128,  0.0067],\n",
       "                      [-0.0018, -0.0025, -0.0025,  ...,  0.0012, -0.0060, -0.0138],\n",
       "                      ...,\n",
       "                      [ 0.0146, -0.0087,  0.0078,  ...,  0.0025,  0.0135, -0.0002],\n",
       "                      [-0.0076,  0.0089,  0.0099,  ...,  0.0153,  0.0021, -0.0013],\n",
       "                      [ 0.0003,  0.0032, -0.0110,  ..., -0.0022, -0.0114,  0.0152]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0140, -0.0198,  0.0118,  ...,  0.0164, -0.0280, -0.0199],\n",
       "                      [-0.0356,  0.0537,  0.0167,  ...,  0.0118,  0.0099, -0.0081],\n",
       "                      [-0.0114,  0.0043,  0.0178,  ...,  0.0082, -0.0021,  0.0161],\n",
       "                      ...,\n",
       "                      [ 0.0115,  0.0231,  0.0225,  ..., -0.0337, -0.0405,  0.0303],\n",
       "                      [ 0.0076, -0.0156,  0.0090,  ...,  0.0013, -0.0148, -0.0133],\n",
       "                      [ 0.0036, -0.0334, -0.0041,  ...,  0.0189,  0.0444, -0.0036]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0003,  0.0068, -0.0035,  ..., -0.0054, -0.0081, -0.0019],\n",
       "                      [-0.0080,  0.0056, -0.0035,  ..., -0.0082, -0.0006, -0.0070],\n",
       "                      [-0.0006,  0.0062,  0.0019,  ..., -0.0035, -0.0009,  0.0066],\n",
       "                      ...,\n",
       "                      [-0.0074, -0.0005,  0.0047,  ..., -0.0081,  0.0092, -0.0060],\n",
       "                      [ 0.0006, -0.0068,  0.0002,  ..., -0.0045, -0.0035,  0.0022],\n",
       "                      [ 0.0031,  0.0002, -0.0066,  ..., -0.0085,  0.0008, -0.0092]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.input_layernorm.weight',\n",
       "              tensor([0.3418, 0.3184, 0.2930,  ..., 0.3164, 0.3086, 0.3125],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.13.post_attention_layernorm.weight',\n",
       "              tensor([0.2197, 0.2061, 0.2002,  ..., 0.2256, 0.2217, 0.2178],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0039, -0.0040,  0.0244,  ..., -0.0140,  0.0145,  0.0371],\n",
       "                      [ 0.0118,  0.0271,  0.0016,  ..., -0.0171,  0.0352, -0.0481],\n",
       "                      [-0.0021,  0.0153, -0.0011,  ...,  0.0601,  0.0327, -0.0309],\n",
       "                      ...,\n",
       "                      [-0.0184, -0.0229,  0.0243,  ...,  0.0171,  0.0132,  0.0084],\n",
       "                      [-0.0457, -0.0042,  0.0267,  ..., -0.0352,  0.0162, -0.0398],\n",
       "                      [ 0.0339, -0.0027, -0.0031,  ...,  0.0112, -0.0140, -0.0228]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0023,  0.0039, -0.0085,  ...,  0.0010, -0.0089,  0.0153],\n",
       "                      [-0.0137,  0.0151,  0.0004,  ...,  0.0018, -0.0107,  0.0068],\n",
       "                      [-0.0026,  0.0154,  0.0095,  ...,  0.0050, -0.0050,  0.0010],\n",
       "                      ...,\n",
       "                      [-0.0081, -0.0096,  0.0062,  ..., -0.0153, -0.0096, -0.0071],\n",
       "                      [ 0.0109, -0.0152,  0.0146,  ..., -0.0113, -0.0143, -0.0022],\n",
       "                      [ 0.0024, -0.0135, -0.0121,  ...,  0.0099, -0.0079,  0.0153]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0048, -0.0039,  0.0198,  ...,  0.0049,  0.0165,  0.0554],\n",
       "                      [ 0.0292,  0.0110, -0.0028,  ...,  0.0093, -0.0029, -0.0043],\n",
       "                      [ 0.0203, -0.0248,  0.0208,  ...,  0.0344,  0.0491, -0.0248],\n",
       "                      ...,\n",
       "                      [-0.0225, -0.0771, -0.0181,  ..., -0.0152, -0.0087, -0.0430],\n",
       "                      [-0.0598,  0.0206,  0.0160,  ..., -0.0400,  0.0183, -0.0168],\n",
       "                      [-0.0014,  0.0242,  0.0085,  ...,  0.0693,  0.0217,  0.0344]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0099,  0.0143, -0.0154,  ..., -0.0152,  0.0087, -0.0005],\n",
       "                      [ 0.0043,  0.0068, -0.0054,  ..., -0.0139, -0.0056, -0.0082],\n",
       "                      [-0.0042, -0.0142, -0.0145,  ...,  0.0105,  0.0143,  0.0052],\n",
       "                      ...,\n",
       "                      [-0.0020, -0.0153, -0.0079,  ...,  0.0052, -0.0132, -0.0018],\n",
       "                      [-0.0085, -0.0145, -0.0010,  ..., -0.0006,  0.0066,  0.0031],\n",
       "                      [-0.0015,  0.0150,  0.0121,  ..., -0.0092, -0.0023, -0.0017]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0022, -0.0004, -0.0527,  ..., -0.0320, -0.0063,  0.0334],\n",
       "                      [ 0.0067, -0.0177, -0.0108,  ..., -0.0240, -0.0089,  0.0101],\n",
       "                      [ 0.0171, -0.0140, -0.0063,  ..., -0.0100,  0.0439,  0.0315],\n",
       "                      ...,\n",
       "                      [ 0.0170, -0.0366, -0.0259,  ...,  0.0256, -0.0251,  0.0153],\n",
       "                      [-0.0192,  0.0151, -0.0249,  ...,  0.0114,  0.0182, -0.0732],\n",
       "                      [-0.0099,  0.0474, -0.0145,  ...,  0.0208, -0.0212, -0.0187]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0030, -0.0070, -0.0023,  ..., -0.0108, -0.0006,  0.0078],\n",
       "                      [-0.0008,  0.0051,  0.0128,  ..., -0.0153,  0.0085, -0.0004],\n",
       "                      [-0.0098, -0.0106, -0.0010,  ...,  0.0059,  0.0059, -0.0128],\n",
       "                      ...,\n",
       "                      [ 0.0077, -0.0090,  0.0093,  ..., -0.0087, -0.0029,  0.0093],\n",
       "                      [-0.0026,  0.0005,  0.0143,  ...,  0.0150, -0.0074, -0.0042],\n",
       "                      [ 0.0002, -0.0115, -0.0119,  ...,  0.0093, -0.0077, -0.0140]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0082, -0.0040, -0.0266,  ..., -0.0021,  0.0258, -0.0005],\n",
       "                      [ 0.0057,  0.0369,  0.0077,  ...,  0.0408, -0.0034, -0.0108],\n",
       "                      [ 0.0197,  0.0090, -0.0004,  ...,  0.0187,  0.0094,  0.0439],\n",
       "                      ...,\n",
       "                      [ 0.0229,  0.0109,  0.0118,  ...,  0.0002, -0.0055, -0.0087],\n",
       "                      [ 0.0113, -0.0067, -0.0422,  ...,  0.0212,  0.0139,  0.0376],\n",
       "                      [-0.0035, -0.0172, -0.0302,  ..., -0.0098,  0.0413,  0.0201]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0104,  0.0023, -0.0056,  ..., -0.0129, -0.0085,  0.0107],\n",
       "                      [-0.0080,  0.0057,  0.0079,  ...,  0.0005, -0.0020, -0.0105],\n",
       "                      [ 0.0150,  0.0073, -0.0043,  ..., -0.0068,  0.0085,  0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0012,  0.0075, -0.0148,  ...,  0.0080,  0.0110, -0.0081],\n",
       "                      [-0.0145, -0.0002, -0.0089,  ...,  0.0006, -0.0050, -0.0135],\n",
       "                      [ 0.0030, -0.0109, -0.0134,  ..., -0.0087,  0.0116,  0.0060]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0291,  0.0037,  0.0659,  ...,  0.0003, -0.0172,  0.0161],\n",
       "                      [ 0.0181,  0.0201, -0.0306,  ...,  0.0449,  0.0107,  0.0027],\n",
       "                      [-0.0243,  0.0432, -0.0038,  ...,  0.0006,  0.0452,  0.0110],\n",
       "                      ...,\n",
       "                      [ 0.0151,  0.0112, -0.0349,  ...,  0.0090, -0.0194, -0.0718],\n",
       "                      [-0.0002, -0.0176, -0.0064,  ...,  0.0157,  0.0303, -0.0076],\n",
       "                      [ 0.0086, -0.0040,  0.0201,  ..., -0.0310, -0.0147,  0.0020]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0064,  0.0051,  0.0115,  ...,  0.0149, -0.0120,  0.0130],\n",
       "                      [-0.0025, -0.0040, -0.0137,  ...,  0.0038,  0.0029,  0.0132],\n",
       "                      [-0.0034, -0.0118,  0.0126,  ...,  0.0086,  0.0087, -0.0040],\n",
       "                      ...,\n",
       "                      [-0.0030, -0.0095, -0.0005,  ..., -0.0156, -0.0024, -0.0153],\n",
       "                      [ 0.0027, -0.0131,  0.0003,  ...,  0.0040, -0.0005,  0.0022],\n",
       "                      [ 0.0067, -0.0004, -0.0109,  ...,  0.0065,  0.0011, -0.0071]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0271,  0.0176,  0.0052,  ..., -0.0085,  0.0052, -0.0471],\n",
       "                      [ 0.0077,  0.0200, -0.0215,  ..., -0.0286, -0.0306,  0.0752],\n",
       "                      [-0.0454, -0.0334,  0.0258,  ...,  0.0073, -0.0128, -0.0105],\n",
       "                      ...,\n",
       "                      [-0.0225,  0.0040, -0.0342,  ..., -0.0131, -0.0104, -0.0148],\n",
       "                      [-0.0452,  0.0356,  0.0018,  ..., -0.0420, -0.0162, -0.0033],\n",
       "                      [ 0.0146,  0.0013,  0.0001,  ...,  0.0145,  0.0072,  0.0091]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-1.2451e-02,  5.7373e-03,  1.1047e-02,  ...,  6.9580e-03,\n",
       "                       -1.5076e-02, -4.8161e-05],\n",
       "                      [-1.1658e-02, -1.9684e-03, -1.4801e-03,  ...,  1.4160e-02,\n",
       "                        6.0120e-03, -1.0376e-02],\n",
       "                      [-1.4160e-02, -1.5381e-02,  8.5449e-03,  ...,  3.9368e-03,\n",
       "                        8.6060e-03,  2.3499e-03],\n",
       "                      ...,\n",
       "                      [ 1.8387e-03,  1.3672e-02,  6.4392e-03,  ..., -1.5381e-02,\n",
       "                       -4.4861e-03,  4.2419e-03],\n",
       "                      [-1.0742e-02,  7.2327e-03, -2.3346e-03,  ...,  1.0864e-02,\n",
       "                        1.5564e-02,  7.9346e-03],\n",
       "                      [ 3.5706e-03,  9.2163e-03, -8.5449e-03,  ...,  1.0010e-02,\n",
       "                        1.2207e-02,  1.4954e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.down_proj.weight',\n",
       "              tensor([[-0.0168,  0.0161, -0.0103,  ...,  0.0042, -0.0383,  0.0032],\n",
       "                      [-0.0101,  0.0260, -0.0086,  ..., -0.0347,  0.0047, -0.0005],\n",
       "                      [ 0.0486, -0.0466,  0.0209,  ...,  0.0039, -0.0056,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0189,  0.0186, -0.0349,  ...,  0.0023, -0.0275, -0.0099],\n",
       "                      [ 0.0170,  0.0201,  0.0236,  ..., -0.0493, -0.0209,  0.0669],\n",
       "                      [-0.0471,  0.0008, -0.0010,  ..., -0.0454, -0.0134,  0.0299]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 8.7891e-03, -5.8289e-03, -9.3994e-03,  ..., -7.6904e-03,\n",
       "                        3.4027e-03, -5.1270e-03],\n",
       "                      [ 7.9956e-03,  3.1433e-03, -6.1951e-03,  ...,  3.2425e-05,\n",
       "                       -6.1035e-03,  2.7466e-03],\n",
       "                      [-8.6060e-03,  7.9346e-04,  2.9449e-03,  ..., -7.7209e-03,\n",
       "                       -8.3008e-03, -6.1035e-03],\n",
       "                      ...,\n",
       "                      [-2.5940e-03, -8.6670e-03, -5.0049e-03,  ...,  6.0272e-04,\n",
       "                        3.4332e-03, -8.3008e-03],\n",
       "                      [-4.3335e-03, -2.7466e-04, -2.5024e-03,  ..., -4.3335e-03,\n",
       "                        2.5940e-03, -1.1063e-03],\n",
       "                      [ 1.2665e-03, -4.2114e-03,  9.5215e-03,  ..., -3.0975e-03,\n",
       "                        3.8147e-03,  5.9814e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.input_layernorm.weight',\n",
       "              tensor([0.3594, 0.3516, 0.3047,  ..., 0.3320, 0.3301, 0.3262],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.14.post_attention_layernorm.weight',\n",
       "              tensor([0.2324, 0.2158, 0.2148,  ..., 0.2354, 0.2344, 0.2334],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0310, -0.0260,  0.0259,  ...,  0.0259, -0.0077,  0.0029],\n",
       "                      [-0.0136, -0.0062, -0.0221,  ..., -0.0078, -0.0076,  0.0089],\n",
       "                      [ 0.0103,  0.0098,  0.0031,  ..., -0.0162,  0.0078,  0.0121],\n",
       "                      ...,\n",
       "                      [-0.0085, -0.0408,  0.0098,  ..., -0.0119,  0.0244,  0.0148],\n",
       "                      [-0.0396, -0.0449, -0.0217,  ..., -0.0302, -0.0007,  0.0255],\n",
       "                      [ 0.0288, -0.0286,  0.0461,  ..., -0.0209, -0.0120,  0.0031]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0146,  0.0066,  0.0029,  ...,  0.0009,  0.0093, -0.0025],\n",
       "                      [ 0.0070,  0.0126,  0.0013,  ...,  0.0131, -0.0109, -0.0030],\n",
       "                      [-0.0144, -0.0046,  0.0113,  ...,  0.0127, -0.0131,  0.0067],\n",
       "                      ...,\n",
       "                      [ 0.0118,  0.0023,  0.0140,  ..., -0.0033, -0.0123,  0.0062],\n",
       "                      [ 0.0071,  0.0078,  0.0005,  ...,  0.0062,  0.0096,  0.0093],\n",
       "                      [-0.0068, -0.0046, -0.0139,  ...,  0.0034, -0.0094, -0.0156]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0002,  0.0070,  0.0131,  ..., -0.0005,  0.0060, -0.0004],\n",
       "                      [-0.0001,  0.0141, -0.0115,  ..., -0.0251,  0.0036,  0.0140],\n",
       "                      [-0.0011,  0.0153, -0.0120,  ...,  0.0062,  0.0023,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0466, -0.0491,  0.0349,  ...,  0.0339,  0.0095,  0.0184],\n",
       "                      [-0.0036, -0.0374,  0.0403,  ...,  0.0304,  0.0017,  0.0200],\n",
       "                      [-0.0425, -0.0255,  0.0557,  ..., -0.0337,  0.0007, -0.0015]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 5.7373e-03, -1.0864e-02,  8.7280e-03,  ...,  9.8267e-03,\n",
       "                        1.9073e-03,  3.9368e-03],\n",
       "                      [ 1.3733e-02,  1.1963e-02, -1.4038e-02,  ...,  6.3782e-03,\n",
       "                        1.1353e-02, -7.1106e-03],\n",
       "                      [ 8.1177e-03,  6.7902e-04,  5.0354e-03,  ..., -8.0872e-04,\n",
       "                       -4.2915e-05, -1.0071e-02],\n",
       "                      ...,\n",
       "                      [ 1.1169e-02,  1.5564e-02, -2.4736e-06,  ..., -4.2725e-03,\n",
       "                        9.5825e-03, -5.7373e-03],\n",
       "                      [ 4.7607e-03, -7.1411e-03, -1.0437e-02,  ...,  9.3384e-03,\n",
       "                       -1.3062e-02, -5.4932e-03],\n",
       "                      [ 1.4465e-02,  5.0964e-03,  2.4567e-03,  ..., -4.6997e-03,\n",
       "                       -4.7913e-03, -8.8501e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0269,  0.0254, -0.0142,  ...,  0.0088, -0.0143,  0.0139],\n",
       "                      [-0.0073,  0.0102, -0.0148,  ..., -0.0134, -0.0287,  0.0275],\n",
       "                      [-0.0135, -0.0237,  0.0003,  ..., -0.0210, -0.0052, -0.0021],\n",
       "                      ...,\n",
       "                      [-0.0272, -0.0405,  0.0214,  ...,  0.0544, -0.0013,  0.0206],\n",
       "                      [-0.0293,  0.0184, -0.0388,  ...,  0.0176,  0.0146,  0.0072],\n",
       "                      [ 0.0366, -0.0391, -0.0016,  ...,  0.0322,  0.0192,  0.0361]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0008,  0.0100,  0.0148,  ...,  0.0142, -0.0043,  0.0082],\n",
       "                      [ 0.0067, -0.0136,  0.0064,  ..., -0.0105, -0.0132, -0.0146],\n",
       "                      [-0.0016, -0.0114, -0.0142,  ..., -0.0070, -0.0039,  0.0119],\n",
       "                      ...,\n",
       "                      [-0.0027,  0.0018,  0.0015,  ...,  0.0045, -0.0085, -0.0044],\n",
       "                      [-0.0024, -0.0114, -0.0041,  ...,  0.0119, -0.0094,  0.0096],\n",
       "                      [-0.0087, -0.0092,  0.0108,  ...,  0.0063,  0.0003,  0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.o_proj.weight',\n",
       "              tensor([[ 4.9133e-03, -8.5449e-03, -1.9897e-02,  ...,  1.8799e-02,\n",
       "                       -1.2329e-02,  1.0986e-02],\n",
       "                      [ 4.6997e-03,  3.4668e-02, -5.2490e-02,  ...,  7.7515e-03,\n",
       "                       -2.2705e-02,  1.8433e-02],\n",
       "                      [ 1.5137e-02,  1.7524e-05, -8.8120e-04,  ..., -2.1484e-02,\n",
       "                       -1.5076e-02,  7.5378e-03],\n",
       "                      ...,\n",
       "                      [-7.7820e-03, -1.6113e-02,  4.3945e-03,  ...,  2.4658e-02,\n",
       "                        1.2360e-03, -2.2217e-02],\n",
       "                      [ 5.9814e-03,  2.0599e-03,  2.4048e-02,  ...,  1.3916e-02,\n",
       "                        2.3804e-03,  4.4678e-02],\n",
       "                      [-3.6621e-02, -1.6937e-03,  5.3223e-02,  ..., -2.6123e-02,\n",
       "                       -5.2185e-03, -3.2227e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0103, -0.0009,  0.0005,  ...,  0.0122,  0.0046,  0.0142],\n",
       "                      [ 0.0082,  0.0025, -0.0009,  ..., -0.0115, -0.0106,  0.0041],\n",
       "                      [ 0.0032, -0.0115, -0.0109,  ..., -0.0132,  0.0070,  0.0045],\n",
       "                      ...,\n",
       "                      [ 0.0143, -0.0112, -0.0131,  ..., -0.0104, -0.0015, -0.0005],\n",
       "                      [-0.0047, -0.0081,  0.0139,  ...,  0.0101, -0.0069,  0.0084],\n",
       "                      [ 0.0054,  0.0042, -0.0011,  ...,  0.0072,  0.0127,  0.0124]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0226,  0.0010, -0.0093,  ...,  0.0383, -0.0139,  0.0366],\n",
       "                      [ 0.0220,  0.0376,  0.0747,  ..., -0.0327,  0.0037,  0.0160],\n",
       "                      [-0.0100,  0.0067, -0.0317,  ...,  0.0413, -0.0273, -0.0063],\n",
       "                      ...,\n",
       "                      [-0.0459,  0.0166,  0.0457,  ..., -0.0228, -0.0047, -0.0164],\n",
       "                      [ 0.0209, -0.0011,  0.0435,  ...,  0.0081, -0.0222, -0.0206],\n",
       "                      [ 0.0267, -0.0198,  0.0119,  ...,  0.0047,  0.0084, -0.0013]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0092,  0.0051, -0.0064,  ..., -0.0053,  0.0117, -0.0074],\n",
       "                      [ 0.0150,  0.0031,  0.0106,  ...,  0.0123,  0.0096,  0.0031],\n",
       "                      [ 0.0101,  0.0106, -0.0052,  ..., -0.0028,  0.0115,  0.0028],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0092, -0.0062,  ...,  0.0122, -0.0145, -0.0136],\n",
       "                      [ 0.0127,  0.0140, -0.0002,  ...,  0.0123,  0.0076, -0.0083],\n",
       "                      [ 0.0090, -0.0101,  0.0091,  ..., -0.0084,  0.0104, -0.0073]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.up_proj.weight',\n",
       "              tensor([[-0.0190, -0.0108,  0.0259,  ...,  0.0192,  0.0410, -0.0674],\n",
       "                      [-0.0038,  0.0143,  0.0674,  ..., -0.0087, -0.0459, -0.0048],\n",
       "                      [-0.0256,  0.0103,  0.0160,  ..., -0.0200, -0.0061, -0.0045],\n",
       "                      ...,\n",
       "                      [-0.0505,  0.0078, -0.0192,  ...,  0.0234, -0.0461, -0.0033],\n",
       "                      [ 0.0238,  0.0101,  0.0168,  ..., -0.0266, -0.0041, -0.0072],\n",
       "                      [ 0.0008,  0.0067,  0.0097,  ..., -0.0040, -0.0515,  0.0277]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0091, -0.0070, -0.0079,  ..., -0.0106,  0.0067,  0.0075],\n",
       "                      [-0.0027, -0.0112, -0.0077,  ..., -0.0025, -0.0049,  0.0075],\n",
       "                      [-0.0074, -0.0113, -0.0136,  ..., -0.0017, -0.0129, -0.0115],\n",
       "                      ...,\n",
       "                      [-0.0151, -0.0023,  0.0132,  ..., -0.0104,  0.0070, -0.0117],\n",
       "                      [ 0.0115, -0.0079,  0.0138,  ...,  0.0114, -0.0104, -0.0082],\n",
       "                      [ 0.0058,  0.0009,  0.0134,  ..., -0.0072, -0.0078, -0.0148]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.down_proj.weight',\n",
       "              tensor([[ 2.0752e-02,  2.2461e-02, -1.6479e-02,  ...,  2.7100e-02,\n",
       "                        2.6733e-02, -4.2480e-02],\n",
       "                      [ 7.5684e-03, -8.2016e-05, -1.0498e-02,  ...,  5.1575e-03,\n",
       "                       -3.8574e-02,  2.3315e-02],\n",
       "                      [ 4.7363e-02,  4.5410e-02,  3.4912e-02,  ..., -5.5542e-03,\n",
       "                        7.3730e-02, -1.2436e-03],\n",
       "                      ...,\n",
       "                      [ 1.7822e-02, -1.3065e-04, -2.2217e-02,  ...,  1.8433e-02,\n",
       "                       -4.2725e-02,  1.7456e-02],\n",
       "                      [-1.8066e-02, -7.4707e-02, -1.2573e-02,  ..., -6.8665e-05,\n",
       "                       -5.8594e-03,  1.9165e-02],\n",
       "                      [ 1.6357e-02, -3.4790e-03, -2.5269e-02,  ..., -1.1292e-03,\n",
       "                       -1.9165e-02, -1.3184e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0009,  0.0061,  0.0048,  ..., -0.0010, -0.0095, -0.0016],\n",
       "                      [-0.0031,  0.0013, -0.0005,  ..., -0.0038, -0.0016, -0.0005],\n",
       "                      [ 0.0008,  0.0029,  0.0095,  ...,  0.0042,  0.0034, -0.0087],\n",
       "                      ...,\n",
       "                      [-0.0078,  0.0077, -0.0021,  ..., -0.0011, -0.0070,  0.0024],\n",
       "                      [-0.0047, -0.0016, -0.0032,  ...,  0.0076,  0.0030, -0.0034],\n",
       "                      [ 0.0040, -0.0041,  0.0025,  ..., -0.0049,  0.0060,  0.0085]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.input_layernorm.weight',\n",
       "              tensor([0.3438, 0.3281, 0.3066,  ..., 0.3223, 0.3145, 0.3223],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.15.post_attention_layernorm.weight',\n",
       "              tensor([0.2441, 0.2275, 0.2275,  ..., 0.2471, 0.2441, 0.2402],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0013, -0.0176, -0.0161,  ...,  0.0170, -0.0064,  0.0029],\n",
       "                      [ 0.0041, -0.0008,  0.0105,  ..., -0.0396,  0.0088,  0.0085],\n",
       "                      [-0.0096, -0.0186,  0.0067,  ...,  0.0040, -0.0099, -0.0267],\n",
       "                      ...,\n",
       "                      [ 0.0330, -0.0354, -0.0098,  ..., -0.0427,  0.0383, -0.0430],\n",
       "                      [ 0.0009, -0.0258,  0.0076,  ..., -0.0121,  0.0030, -0.0070],\n",
       "                      [-0.0471,  0.0337,  0.0240,  ...,  0.0084, -0.0139, -0.0493]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0021,  0.0098,  0.0048,  ..., -0.0071, -0.0106,  0.0084],\n",
       "                      [ 0.0137, -0.0142, -0.0060,  ..., -0.0118, -0.0103, -0.0115],\n",
       "                      [-0.0052,  0.0136,  0.0076,  ..., -0.0151, -0.0119,  0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0086,  0.0013,  0.0045,  ..., -0.0034,  0.0145,  0.0045],\n",
       "                      [-0.0140, -0.0121, -0.0141,  ...,  0.0036,  0.0015, -0.0120],\n",
       "                      [-0.0098, -0.0053,  0.0087,  ...,  0.0031,  0.0050, -0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.k_proj.weight',\n",
       "              tensor([[-1.0010e-02, -1.0147e-03, -1.7700e-02,  ...,  2.3804e-03,\n",
       "                        8.4839e-03, -7.7724e-05],\n",
       "                      [ 9.5215e-03, -1.5747e-02,  1.1230e-02,  ..., -9.0942e-03,\n",
       "                       -3.1281e-04, -1.3489e-02],\n",
       "                      [-1.4709e-02,  1.6724e-02, -1.8616e-03,  ..., -2.0386e-02,\n",
       "                       -1.6846e-02, -8.4839e-03],\n",
       "                      ...,\n",
       "                      [ 1.9897e-02, -3.0640e-02, -4.6082e-03,  ..., -7.1411e-03,\n",
       "                       -1.4587e-02, -2.9053e-02],\n",
       "                      [ 3.5156e-02,  1.5488e-03,  7.6599e-03,  ...,  7.4463e-03,\n",
       "                       -7.8583e-04,  9.0332e-03],\n",
       "                      [-4.6387e-02,  6.3477e-02,  4.7363e-02,  ..., -5.0354e-03,\n",
       "                       -2.5024e-02, -2.2217e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 1.0986e-02, -1.2451e-02,  8.2397e-03,  ...,  1.4221e-02,\n",
       "                        8.3447e-05, -9.3384e-03],\n",
       "                      [ 1.1719e-02,  1.0681e-02, -2.2888e-03,  ..., -1.0071e-02,\n",
       "                       -8.2397e-03,  6.7749e-03],\n",
       "                      [-1.2634e-02, -2.4872e-03, -1.4038e-03,  ..., -3.5858e-03,\n",
       "                       -2.8687e-03,  7.2937e-03],\n",
       "                      ...,\n",
       "                      [ 6.5308e-03, -1.5442e-02,  1.4404e-02,  ..., -9.4604e-03,\n",
       "                       -8.4839e-03, -2.4567e-03],\n",
       "                      [-7.6904e-03, -1.3611e-02, -4.8218e-03,  ..., -1.0109e-04,\n",
       "                       -6.0730e-03,  3.0975e-03],\n",
       "                      [ 1.0437e-02,  5.4016e-03,  7.9346e-03,  ..., -1.5198e-02,\n",
       "                        9.0790e-04, -1.5503e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.v_proj.weight',\n",
       "              tensor([[-1.3184e-02,  1.0803e-02, -8.1787e-03,  ..., -6.9580e-03,\n",
       "                       -9.8877e-03,  1.3245e-02],\n",
       "                      [-7.9346e-03, -9.3460e-05,  1.9897e-02,  ..., -1.3855e-02,\n",
       "                        8.3618e-03, -4.1260e-02],\n",
       "                      [-5.1514e-02, -6.9275e-03, -7.8125e-03,  ..., -2.5635e-02,\n",
       "                        2.5391e-02,  3.8818e-02],\n",
       "                      ...,\n",
       "                      [ 3.1738e-02,  2.1484e-02, -2.5146e-02,  ...,  2.2278e-03,\n",
       "                       -3.0640e-02,  1.8692e-03],\n",
       "                      [-2.2095e-02,  2.5269e-02,  1.4221e-02,  ...,  5.1514e-02,\n",
       "                       -1.3245e-02,  7.0190e-03],\n",
       "                      [ 9.1553e-03, -2.0752e-02,  7.1106e-03,  ..., -2.1606e-02,\n",
       "                       -1.5198e-02,  1.5030e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0131, -0.0070,  0.0117,  ...,  0.0130,  0.0068, -0.0099],\n",
       "                      [ 0.0110,  0.0131,  0.0091,  ..., -0.0135, -0.0075,  0.0099],\n",
       "                      [ 0.0053, -0.0155,  0.0090,  ..., -0.0145, -0.0131, -0.0085],\n",
       "                      ...,\n",
       "                      [-0.0131, -0.0139, -0.0029,  ..., -0.0056, -0.0135, -0.0021],\n",
       "                      [ 0.0096,  0.0016,  0.0079,  ...,  0.0152, -0.0090,  0.0123],\n",
       "                      [-0.0030, -0.0120, -0.0054,  ..., -0.0140, -0.0055,  0.0048]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0068, -0.0051,  0.0146,  ...,  0.0249,  0.0322,  0.0047],\n",
       "                      [ 0.0024,  0.0027, -0.0188,  ...,  0.0260,  0.0057,  0.0025],\n",
       "                      [-0.0194,  0.0074, -0.0272,  ..., -0.0115,  0.0138,  0.0147],\n",
       "                      ...,\n",
       "                      [-0.0159,  0.0052, -0.0065,  ...,  0.0138, -0.0275,  0.0212],\n",
       "                      [-0.0022,  0.0613,  0.0295,  ...,  0.0281,  0.0037,  0.0068],\n",
       "                      [-0.0102, -0.0036,  0.0483,  ...,  0.0256, -0.0135, -0.0200]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0041,  0.0052, -0.0019,  ...,  0.0049,  0.0099, -0.0100],\n",
       "                      [ 0.0033, -0.0112,  0.0079,  ...,  0.0019,  0.0117,  0.0081],\n",
       "                      [ 0.0141, -0.0137, -0.0142,  ..., -0.0023, -0.0029, -0.0003],\n",
       "                      ...,\n",
       "                      [ 0.0028, -0.0027,  0.0050,  ...,  0.0070, -0.0005,  0.0115],\n",
       "                      [-0.0070, -0.0010,  0.0063,  ...,  0.0058,  0.0036, -0.0081],\n",
       "                      [-0.0112,  0.0109,  0.0139,  ...,  0.0061, -0.0003,  0.0033]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0518, -0.0447, -0.0260,  ..., -0.0065, -0.0325,  0.0175],\n",
       "                      [ 0.0850,  0.0033,  0.0356,  ...,  0.0413,  0.0164,  0.0001],\n",
       "                      [ 0.0130,  0.0238,  0.0005,  ..., -0.0016, -0.0219, -0.0408],\n",
       "                      ...,\n",
       "                      [-0.0104,  0.0236, -0.0028,  ...,  0.0140,  0.0457,  0.0222],\n",
       "                      [-0.0219,  0.0361,  0.0133,  ...,  0.0566,  0.0141,  0.0564],\n",
       "                      [ 0.0194,  0.0300,  0.0359,  ..., -0.0011,  0.0128, -0.0101]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0033,  0.0062,  0.0058,  ...,  0.0153,  0.0110, -0.0131],\n",
       "                      [-0.0019,  0.0084, -0.0058,  ..., -0.0041,  0.0039,  0.0061],\n",
       "                      [-0.0024,  0.0117,  0.0051,  ...,  0.0117, -0.0044,  0.0101],\n",
       "                      ...,\n",
       "                      [-0.0035,  0.0092, -0.0107,  ...,  0.0073,  0.0039,  0.0143],\n",
       "                      [-0.0055,  0.0064, -0.0028,  ..., -0.0134,  0.0098,  0.0125],\n",
       "                      [ 0.0075,  0.0137, -0.0045,  ...,  0.0083, -0.0055, -0.0033]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.up_proj.weight',\n",
       "              tensor([[-0.0530, -0.0112,  0.0210,  ...,  0.0144, -0.0239,  0.0444],\n",
       "                      [ 0.0188, -0.0190, -0.0058,  ...,  0.0036, -0.0457,  0.0011],\n",
       "                      [-0.0220, -0.0040, -0.0030,  ..., -0.0203, -0.0298,  0.0076],\n",
       "                      ...,\n",
       "                      [ 0.0309, -0.0179, -0.0150,  ...,  0.0292,  0.0057, -0.0225],\n",
       "                      [-0.0122,  0.0284, -0.0352,  ...,  0.0074, -0.0258,  0.0153],\n",
       "                      [-0.0315,  0.0126,  0.0200,  ..., -0.0299, -0.0459,  0.0273]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0019, -0.0043, -0.0089,  ...,  0.0075,  0.0033,  0.0042],\n",
       "                      [-0.0048, -0.0061,  0.0139,  ..., -0.0139, -0.0109,  0.0019],\n",
       "                      [ 0.0059,  0.0109, -0.0101,  ...,  0.0078,  0.0115,  0.0146],\n",
       "                      ...,\n",
       "                      [-0.0048, -0.0026,  0.0012,  ..., -0.0126,  0.0039, -0.0103],\n",
       "                      [ 0.0103,  0.0137,  0.0005,  ..., -0.0014, -0.0103,  0.0036],\n",
       "                      [ 0.0008,  0.0004,  0.0042,  ..., -0.0003, -0.0025,  0.0025]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.down_proj.weight',\n",
       "              tensor([[-0.0437,  0.0030,  0.0354,  ..., -0.0057, -0.0356, -0.0153],\n",
       "                      [-0.0459, -0.0131, -0.0033,  ...,  0.0248,  0.0126,  0.0001],\n",
       "                      [-0.0386,  0.0156,  0.0150,  ...,  0.0344,  0.0016, -0.0101],\n",
       "                      ...,\n",
       "                      [-0.0076, -0.0168, -0.0197,  ...,  0.0137,  0.0211, -0.0052],\n",
       "                      [-0.0015,  0.0024, -0.0693,  ...,  0.0042,  0.0153, -0.0383],\n",
       "                      [-0.0065, -0.0049, -0.0309,  ...,  0.0092, -0.0125, -0.0232]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0090,  0.0009, -0.0015,  ...,  0.0092, -0.0093, -0.0051],\n",
       "                      [-0.0057,  0.0092,  0.0021,  ...,  0.0068,  0.0074,  0.0087],\n",
       "                      [ 0.0071, -0.0060, -0.0013,  ..., -0.0078, -0.0002, -0.0044],\n",
       "                      ...,\n",
       "                      [-0.0074,  0.0090,  0.0045,  ..., -0.0062,  0.0078,  0.0071],\n",
       "                      [-0.0072,  0.0093,  0.0008,  ..., -0.0004,  0.0033,  0.0030],\n",
       "                      [-0.0016, -0.0058,  0.0048,  ..., -0.0081, -0.0007,  0.0061]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.input_layernorm.weight',\n",
       "              tensor([0.3574, 0.3613, 0.3242,  ..., 0.3281, 0.3496, 0.3379],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.16.post_attention_layernorm.weight',\n",
       "              tensor([0.2637, 0.2412, 0.2432,  ..., 0.2559, 0.2617, 0.2539],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0007,  0.0072,  0.0041,  ..., -0.0010,  0.0175,  0.0041],\n",
       "                      [ 0.0094,  0.0015, -0.0146,  ..., -0.0063,  0.0187, -0.0015],\n",
       "                      [ 0.0113, -0.0179,  0.0043,  ..., -0.0068, -0.0182,  0.0064],\n",
       "                      ...,\n",
       "                      [ 0.0693, -0.0942,  0.0408,  ..., -0.0098, -0.0125,  0.0150],\n",
       "                      [ 0.0107, -0.0155,  0.0664,  ..., -0.0072,  0.0214, -0.0018],\n",
       "                      [ 0.0605, -0.0542,  0.0435,  ..., -0.0091, -0.0020,  0.0869]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0024,  0.0069,  0.0142,  ..., -0.0146, -0.0004, -0.0019],\n",
       "                      [-0.0016, -0.0044,  0.0078,  ...,  0.0043, -0.0081, -0.0100],\n",
       "                      [ 0.0018,  0.0061, -0.0084,  ...,  0.0106,  0.0095, -0.0009],\n",
       "                      ...,\n",
       "                      [-0.0078,  0.0095,  0.0154,  ..., -0.0055,  0.0123,  0.0125],\n",
       "                      [-0.0009,  0.0003, -0.0054,  ..., -0.0062, -0.0093, -0.0032],\n",
       "                      [ 0.0016,  0.0086,  0.0012,  ..., -0.0024, -0.0073, -0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0022,  0.0069, -0.0002,  ...,  0.0167,  0.0151, -0.0118],\n",
       "                      [ 0.0042, -0.0035,  0.0156,  ..., -0.0041, -0.0061, -0.0014],\n",
       "                      [-0.0067,  0.0123,  0.0105,  ...,  0.0020,  0.0121, -0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0588, -0.0703,  0.0190,  ...,  0.0332,  0.0830, -0.0381],\n",
       "                      [-0.0085,  0.0845,  0.0182,  ..., -0.0398, -0.0203, -0.0118],\n",
       "                      [ 0.0161, -0.0713,  0.0339,  ...,  0.0082,  0.0312,  0.0151]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0037, -0.0050, -0.0025,  ...,  0.0110, -0.0081, -0.0149],\n",
       "                      [ 0.0043,  0.0064, -0.0110,  ...,  0.0122,  0.0009,  0.0100],\n",
       "                      [-0.0051,  0.0125,  0.0107,  ..., -0.0009, -0.0021,  0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0025,  0.0004,  0.0033,  ...,  0.0003, -0.0071,  0.0054],\n",
       "                      [-0.0148,  0.0006,  0.0124,  ..., -0.0145,  0.0035, -0.0142],\n",
       "                      [-0.0086,  0.0107, -0.0143,  ..., -0.0051, -0.0008, -0.0015]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0214,  0.0312,  0.0025,  ...,  0.0126,  0.0074, -0.0198],\n",
       "                      [ 0.0005, -0.0063, -0.0332,  ...,  0.0309, -0.0415, -0.0630],\n",
       "                      [-0.0171,  0.0075,  0.0214,  ...,  0.0308,  0.0087,  0.0029],\n",
       "                      ...,\n",
       "                      [-0.0013, -0.0332,  0.0243,  ...,  0.0121, -0.0215,  0.0084],\n",
       "                      [-0.0396, -0.0325, -0.0170,  ...,  0.0065, -0.0025, -0.0334],\n",
       "                      [ 0.0076, -0.0070, -0.0161,  ...,  0.0178,  0.0361,  0.0059]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0088, -0.0008, -0.0034,  ..., -0.0137, -0.0052,  0.0112],\n",
       "                      [-0.0063,  0.0134,  0.0060,  ..., -0.0148,  0.0035,  0.0031],\n",
       "                      [-0.0056, -0.0068, -0.0084,  ...,  0.0081, -0.0080, -0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0078,  0.0110,  0.0149,  ...,  0.0027,  0.0003,  0.0110],\n",
       "                      [-0.0023,  0.0046,  0.0093,  ...,  0.0085,  0.0118, -0.0118],\n",
       "                      [-0.0070, -0.0045, -0.0024,  ..., -0.0115,  0.0014, -0.0148]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0085,  0.0060, -0.0113,  ...,  0.0074, -0.0092, -0.0095],\n",
       "                      [-0.0075, -0.0396,  0.0322,  ...,  0.0337, -0.0091,  0.0062],\n",
       "                      [ 0.0203,  0.0088, -0.0312,  ..., -0.0303, -0.0125, -0.0020],\n",
       "                      ...,\n",
       "                      [-0.0020,  0.0156, -0.0376,  ..., -0.0552, -0.0569, -0.0277],\n",
       "                      [-0.0096,  0.0053,  0.0204,  ..., -0.0645,  0.0025, -0.0047],\n",
       "                      [-0.0166, -0.0119,  0.0231,  ...,  0.0206,  0.0076,  0.0070]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0117, -0.0093,  0.0062,  ...,  0.0123,  0.0059,  0.0042],\n",
       "                      [-0.0030, -0.0114, -0.0115,  ...,  0.0045,  0.0098,  0.0048],\n",
       "                      [-0.0153,  0.0049, -0.0145,  ..., -0.0106, -0.0019, -0.0085],\n",
       "                      ...,\n",
       "                      [ 0.0110, -0.0092,  0.0104,  ..., -0.0074, -0.0111,  0.0064],\n",
       "                      [ 0.0121, -0.0150,  0.0070,  ..., -0.0037,  0.0125, -0.0115],\n",
       "                      [-0.0034, -0.0099,  0.0142,  ...,  0.0079, -0.0040,  0.0038]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0908,  0.0275,  0.0209,  ...,  0.0033, -0.0461,  0.0123],\n",
       "                      [-0.0439,  0.0297, -0.0067,  ...,  0.0258, -0.0366, -0.0352],\n",
       "                      [-0.0106,  0.0164, -0.0115,  ...,  0.0332,  0.0137,  0.0001],\n",
       "                      ...,\n",
       "                      [-0.0474,  0.0098, -0.0226,  ..., -0.0242,  0.0010, -0.0138],\n",
       "                      [-0.0116,  0.0050, -0.0075,  ...,  0.0145,  0.0835,  0.0129],\n",
       "                      [ 0.0199,  0.0170,  0.0200,  ...,  0.0052, -0.0013, -0.0021]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0074,  0.0153,  0.0073,  ...,  0.0133, -0.0047, -0.0032],\n",
       "                      [-0.0035,  0.0121,  0.0118,  ..., -0.0125,  0.0126,  0.0151],\n",
       "                      [-0.0093, -0.0146, -0.0060,  ..., -0.0109,  0.0011,  0.0001],\n",
       "                      ...,\n",
       "                      [-0.0006,  0.0125,  0.0155,  ..., -0.0098,  0.0145, -0.0028],\n",
       "                      [-0.0050,  0.0111,  0.0092,  ..., -0.0117,  0.0085, -0.0059],\n",
       "                      [ 0.0085, -0.0044,  0.0086,  ...,  0.0080, -0.0135,  0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.up_proj.weight',\n",
       "              tensor([[-0.0267, -0.0004,  0.0095,  ...,  0.0142, -0.0352, -0.0145],\n",
       "                      [-0.0238, -0.0229,  0.0023,  ..., -0.0293, -0.0173,  0.0339],\n",
       "                      [-0.0146,  0.0018,  0.0344,  ..., -0.0170, -0.0248, -0.0042],\n",
       "                      ...,\n",
       "                      [-0.0115,  0.0410,  0.0110,  ...,  0.0146, -0.0015,  0.0070],\n",
       "                      [ 0.0216, -0.0322,  0.0001,  ..., -0.0464, -0.0347,  0.0393],\n",
       "                      [ 0.0156, -0.0247,  0.0173,  ...,  0.0391, -0.0366,  0.0166]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0023,  0.0075,  0.0128,  ...,  0.0017,  0.0091,  0.0096],\n",
       "                      [-0.0002, -0.0105,  0.0154,  ...,  0.0152,  0.0061,  0.0129],\n",
       "                      [ 0.0034,  0.0153,  0.0015,  ..., -0.0155, -0.0064,  0.0121],\n",
       "                      ...,\n",
       "                      [-0.0058,  0.0143, -0.0117,  ..., -0.0113,  0.0053,  0.0107],\n",
       "                      [-0.0059,  0.0093,  0.0041,  ..., -0.0048, -0.0137, -0.0017],\n",
       "                      [-0.0145, -0.0001, -0.0027,  ..., -0.0060, -0.0125,  0.0117]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0544, -0.0306,  0.0435,  ..., -0.0054, -0.0253, -0.0413],\n",
       "                      [-0.0200,  0.0033, -0.0356,  ...,  0.0040, -0.0009,  0.0182],\n",
       "                      [-0.0320, -0.0018, -0.0240,  ..., -0.0259, -0.0332, -0.0173],\n",
       "                      ...,\n",
       "                      [-0.0262,  0.0038, -0.0077,  ..., -0.0193, -0.0312, -0.0203],\n",
       "                      [-0.0147, -0.0140, -0.0347,  ..., -0.0128, -0.0378, -0.0140],\n",
       "                      [ 0.0215, -0.0084, -0.0157,  ..., -0.0045,  0.0024,  0.0261]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-9.3384e-03, -1.5259e-03,  8.7280e-03,  ...,  2.0294e-03,\n",
       "                       -5.4932e-03, -7.1106e-03],\n",
       "                      [ 8.0566e-03, -1.0986e-03, -5.5847e-03,  ..., -9.1553e-03,\n",
       "                        7.1106e-03, -2.6398e-03],\n",
       "                      [-9.4604e-03,  1.2665e-03, -1.7929e-03,  ..., -6.6833e-03,\n",
       "                       -4.7913e-03, -6.3782e-03],\n",
       "                      ...,\n",
       "                      [-2.6093e-03,  1.2970e-03,  6.1035e-04,  ...,  6.2561e-03,\n",
       "                        4.2725e-03,  2.9564e-04],\n",
       "                      [ 5.9814e-03,  1.0910e-03,  6.1035e-03,  ...,  4.3030e-03,\n",
       "                       -1.8921e-03,  2.5392e-05],\n",
       "                      [ 4.3335e-03,  8.4229e-03, -6.3171e-03,  ...,  5.9204e-03,\n",
       "                       -3.8757e-03,  2.1210e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.input_layernorm.weight',\n",
       "              tensor([0.3594, 0.3633, 0.3418,  ..., 0.3574, 0.3594, 0.3379],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.17.post_attention_layernorm.weight',\n",
       "              tensor([0.2695, 0.2617, 0.2578,  ..., 0.2812, 0.2734, 0.2773],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0194, -0.0134,  0.0173,  ..., -0.0001,  0.0025,  0.0330],\n",
       "                      [ 0.0173,  0.0295,  0.0091,  ..., -0.0178, -0.0087,  0.0177],\n",
       "                      [-0.0087, -0.0292,  0.0037,  ..., -0.0007,  0.0300, -0.0089],\n",
       "                      ...,\n",
       "                      [ 0.0161,  0.0184, -0.0153,  ...,  0.0084,  0.0435,  0.0183],\n",
       "                      [ 0.0439, -0.0035,  0.0083,  ...,  0.0376,  0.0825,  0.0371],\n",
       "                      [ 0.0039,  0.0070, -0.0391,  ...,  0.0250,  0.0649, -0.0564]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0148,  0.0110, -0.0088,  ..., -0.0125, -0.0012,  0.0003],\n",
       "                      [ 0.0012,  0.0134, -0.0156,  ...,  0.0101,  0.0154,  0.0016],\n",
       "                      [ 0.0109,  0.0030,  0.0007,  ...,  0.0082, -0.0116,  0.0063],\n",
       "                      ...,\n",
       "                      [-0.0013, -0.0048,  0.0064,  ...,  0.0021,  0.0103,  0.0001],\n",
       "                      [-0.0098, -0.0002,  0.0094,  ..., -0.0053, -0.0088, -0.0062],\n",
       "                      [-0.0034,  0.0059, -0.0128,  ...,  0.0063, -0.0072,  0.0057]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0004,  0.0096, -0.0006,  ..., -0.0084, -0.0093, -0.0089],\n",
       "                      [ 0.0121, -0.0238,  0.0140,  ..., -0.0003,  0.0199, -0.0170],\n",
       "                      [ 0.0070, -0.0293, -0.0005,  ...,  0.0303, -0.0075, -0.0254],\n",
       "                      ...,\n",
       "                      [-0.0466, -0.0048, -0.0688,  ...,  0.0067,  0.0107, -0.0493],\n",
       "                      [ 0.0693,  0.0439,  0.0110,  ...,  0.0077,  0.0327, -0.0635],\n",
       "                      [-0.0542, -0.0102, -0.0806,  ...,  0.0623, -0.0045, -0.0212]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 3.7670e-05, -8.4229e-03, -1.3428e-02,  ..., -6.6223e-03,\n",
       "                       -1.4465e-02, -1.3550e-02],\n",
       "                      [ 4.8218e-03, -8.6670e-03, -5.8899e-03,  ..., -9.8877e-03,\n",
       "                       -3.6316e-03,  9.0942e-03],\n",
       "                      [ 8.6670e-03, -1.0254e-02, -1.1169e-02,  ..., -1.3794e-02,\n",
       "                       -1.4954e-02,  2.5024e-03],\n",
       "                      ...,\n",
       "                      [-1.1780e-02, -1.5625e-02, -9.5215e-03,  ..., -6.5918e-03,\n",
       "                        1.4526e-02, -7.8125e-03],\n",
       "                      [-1.3916e-02, -1.0498e-02,  5.0964e-03,  ..., -1.1658e-02,\n",
       "                       -1.4954e-02,  1.4587e-02],\n",
       "                      [ 1.1719e-02,  1.0498e-02, -8.4839e-03,  ..., -9.6436e-03,\n",
       "                        1.0986e-02, -5.9814e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0287,  0.0135, -0.0292,  ..., -0.0398,  0.0757,  0.0227],\n",
       "                      [ 0.0006, -0.0059, -0.0297,  ...,  0.0079,  0.0522,  0.0060],\n",
       "                      [-0.0522, -0.0005,  0.0007,  ...,  0.0134, -0.0225,  0.0126],\n",
       "                      ...,\n",
       "                      [-0.0056,  0.0098,  0.0022,  ..., -0.0166, -0.0156,  0.0337],\n",
       "                      [ 0.0420,  0.0120,  0.0019,  ...,  0.0034, -0.0342,  0.0190],\n",
       "                      [-0.0066,  0.0115, -0.0065,  ..., -0.0299, -0.0142,  0.0132]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-5.5237e-03,  1.8005e-03,  3.2806e-03,  ..., -1.1292e-02,\n",
       "                        2.0447e-03, -1.5381e-02],\n",
       "                      [ 8.6670e-03, -8.3160e-04,  3.9368e-03,  ..., -7.5817e-05,\n",
       "                       -6.3782e-03,  1.5564e-03],\n",
       "                      [-1.4572e-03, -2.6131e-04,  6.7444e-03,  ..., -6.7444e-03,\n",
       "                        1.2451e-02,  1.1520e-03],\n",
       "                      ...,\n",
       "                      [-1.0071e-02,  3.3875e-03,  7.2021e-03,  ..., -1.4038e-02,\n",
       "                       -8.5449e-03,  1.0498e-02],\n",
       "                      [-1.0132e-02, -7.2327e-03, -3.5706e-03,  ...,  4.6082e-03,\n",
       "                        1.1841e-02, -7.6904e-03],\n",
       "                      [ 1.0803e-02,  3.6926e-03,  5.5847e-03,  ...,  9.9487e-03,\n",
       "                        8.8501e-03, -1.1536e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0175, -0.0496, -0.0127,  ...,  0.0261,  0.0479,  0.0320],\n",
       "                      [-0.0182, -0.0527,  0.0251,  ..., -0.0063,  0.0006, -0.0244],\n",
       "                      [-0.0184, -0.0117,  0.0272,  ...,  0.0444,  0.0175,  0.0260],\n",
       "                      ...,\n",
       "                      [-0.0288, -0.0036,  0.0234,  ...,  0.0107,  0.0311, -0.0195],\n",
       "                      [ 0.0056,  0.0366,  0.0011,  ...,  0.0505,  0.0080, -0.0454],\n",
       "                      [ 0.0393, -0.0591, -0.0228,  ..., -0.0029, -0.0229, -0.0237]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-1.0498e-02,  5.1270e-03,  8.9111e-03,  ...,  1.4954e-02,\n",
       "                       -1.4709e-02,  1.2695e-02],\n",
       "                      [ 1.4160e-02, -1.2756e-02,  2.6093e-03,  ..., -2.6703e-03,\n",
       "                        4.8218e-03, -1.2024e-02],\n",
       "                      [-9.5844e-05, -3.9062e-03, -1.3733e-02,  ...,  3.7231e-03,\n",
       "                       -3.0823e-03, -1.1047e-02],\n",
       "                      ...,\n",
       "                      [ 1.6632e-03,  1.0376e-02, -1.9302e-03,  ...,  5.6152e-03,\n",
       "                        2.9907e-03, -7.9956e-03],\n",
       "                      [-5.1575e-03,  9.7656e-03,  1.5442e-02,  ..., -4.2915e-04,\n",
       "                       -8.3618e-03, -2.5177e-04],\n",
       "                      [ 1.1414e-02,  1.5717e-03,  1.5503e-02,  ..., -1.2634e-02,\n",
       "                        1.5503e-02, -1.2329e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0137, -0.0188,  0.0082,  ..., -0.0240,  0.0432, -0.0044],\n",
       "                      [-0.0170,  0.0068,  0.0028,  ...,  0.0122, -0.0036, -0.0131],\n",
       "                      [ 0.0038,  0.0070,  0.0420,  ..., -0.0437,  0.0089, -0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0188, -0.0060, -0.0200,  ..., -0.0176, -0.0454,  0.0280],\n",
       "                      [ 0.0188, -0.0061,  0.0272,  ..., -0.0435, -0.0071,  0.0144],\n",
       "                      [-0.0231, -0.0082, -0.0008,  ..., -0.0156, -0.0265,  0.0317]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0103,  0.0156,  0.0015,  ...,  0.0147,  0.0109, -0.0111],\n",
       "                      [-0.0050,  0.0034,  0.0038,  ..., -0.0146,  0.0062, -0.0029],\n",
       "                      [ 0.0114,  0.0090, -0.0029,  ..., -0.0055,  0.0008,  0.0028],\n",
       "                      ...,\n",
       "                      [ 0.0044, -0.0023, -0.0023,  ...,  0.0007,  0.0017,  0.0010],\n",
       "                      [-0.0095,  0.0084, -0.0100,  ...,  0.0010,  0.0026, -0.0140],\n",
       "                      [ 0.0064, -0.0101,  0.0095,  ...,  0.0109, -0.0074,  0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.up_proj.weight',\n",
       "              tensor([[-0.0106,  0.0125,  0.0113,  ..., -0.0034,  0.0137,  0.0359],\n",
       "                      [ 0.0114,  0.0206, -0.0581,  ...,  0.0400,  0.0043, -0.0176],\n",
       "                      [ 0.0256, -0.0195,  0.0219,  ...,  0.0184,  0.0181,  0.0272],\n",
       "                      ...,\n",
       "                      [ 0.0248,  0.0143,  0.0041,  ...,  0.0253,  0.0143,  0.0219],\n",
       "                      [ 0.0149, -0.0503, -0.0300,  ..., -0.0056,  0.0437,  0.0659],\n",
       "                      [ 0.0033, -0.0388, -0.0221,  ..., -0.0449,  0.0304,  0.0037]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0124,  0.0140,  0.0073,  ...,  0.0087,  0.0019,  0.0123],\n",
       "                      [ 0.0042,  0.0046,  0.0019,  ..., -0.0101,  0.0117,  0.0050],\n",
       "                      [-0.0027, -0.0124,  0.0081,  ..., -0.0035, -0.0054,  0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0014,  0.0105, -0.0048,  ...,  0.0028, -0.0099,  0.0116],\n",
       "                      [ 0.0005,  0.0120, -0.0044,  ...,  0.0131, -0.0056, -0.0100],\n",
       "                      [-0.0022, -0.0058,  0.0019,  ..., -0.0058,  0.0140, -0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0078, -0.0130, -0.0017,  ...,  0.0108,  0.0422,  0.0408],\n",
       "                      [ 0.0029,  0.0292,  0.0337,  ...,  0.0132, -0.0250, -0.0386],\n",
       "                      [-0.0113, -0.0038, -0.0067,  ...,  0.0214, -0.0063,  0.0044],\n",
       "                      ...,\n",
       "                      [-0.0535, -0.0042,  0.0283,  ...,  0.0493, -0.0204, -0.0393],\n",
       "                      [ 0.0096,  0.0068, -0.0234,  ..., -0.0359,  0.0120, -0.0128],\n",
       "                      [ 0.0099,  0.0356,  0.0092,  ..., -0.0081,  0.0383,  0.0261]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0057,  0.0047,  0.0085,  ...,  0.0027, -0.0022, -0.0091],\n",
       "                      [-0.0014,  0.0035, -0.0005,  ..., -0.0006,  0.0031, -0.0071],\n",
       "                      [-0.0052, -0.0008,  0.0094,  ..., -0.0084,  0.0072, -0.0056],\n",
       "                      ...,\n",
       "                      [-0.0092, -0.0079, -0.0045,  ..., -0.0026, -0.0002,  0.0091],\n",
       "                      [-0.0083,  0.0092,  0.0016,  ...,  0.0089, -0.0064, -0.0023],\n",
       "                      [-0.0071,  0.0034,  0.0070,  ...,  0.0024,  0.0004, -0.0095]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.input_layernorm.weight',\n",
       "              tensor([0.3672, 0.3691, 0.3535,  ..., 0.3613, 0.3633, 0.3516],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.18.post_attention_layernorm.weight',\n",
       "              tensor([0.2832, 0.2754, 0.2715,  ..., 0.2910, 0.2871, 0.2852],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.4648e-02, -8.9722e-03,  1.9043e-02,  ..., -1.4038e-02,\n",
       "                        6.1035e-03,  4.4250e-03],\n",
       "                      [ 2.4872e-03, -1.3504e-03, -1.1292e-02,  ...,  2.6489e-02,\n",
       "                       -1.5320e-02,  4.7302e-03],\n",
       "                      [-1.0376e-02, -2.2278e-03, -1.9043e-02,  ..., -1.9409e-02,\n",
       "                        3.5156e-02,  8.1177e-03],\n",
       "                      ...,\n",
       "                      [-2.6398e-03, -2.0508e-02, -8.9645e-05,  ..., -5.0293e-02,\n",
       "                       -5.4016e-03,  9.3994e-03],\n",
       "                      [ 2.2705e-02,  6.1646e-03,  4.1260e-02,  ...,  4.9072e-02,\n",
       "                        4.6387e-02, -5.5420e-02],\n",
       "                      [-1.7334e-02, -6.6528e-03,  3.4637e-03,  ...,  1.3245e-02,\n",
       "                       -7.6172e-02, -7.2266e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0040,  0.0046, -0.0036,  ..., -0.0090, -0.0153, -0.0060],\n",
       "                      [-0.0082,  0.0037, -0.0009,  ...,  0.0106,  0.0091, -0.0009],\n",
       "                      [-0.0145, -0.0035, -0.0090,  ...,  0.0102,  0.0096, -0.0008],\n",
       "                      ...,\n",
       "                      [-0.0010,  0.0103,  0.0046,  ...,  0.0079,  0.0139, -0.0150],\n",
       "                      [-0.0084, -0.0151,  0.0143,  ...,  0.0050, -0.0059, -0.0133],\n",
       "                      [-0.0129,  0.0014,  0.0104,  ..., -0.0084, -0.0035, -0.0121]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0187,  0.0096, -0.0103,  ..., -0.0108,  0.0038, -0.0098],\n",
       "                      [-0.0105,  0.0104, -0.0317,  ...,  0.0091,  0.0204, -0.0091],\n",
       "                      [ 0.0022, -0.0011, -0.0420,  ...,  0.0156,  0.0125,  0.0227],\n",
       "                      ...,\n",
       "                      [ 0.0413,  0.0016,  0.0029,  ..., -0.0334, -0.0115, -0.0233],\n",
       "                      [-0.0148, -0.0437,  0.0089,  ...,  0.0352, -0.0050, -0.0228],\n",
       "                      [-0.0513,  0.0157,  0.0006,  ..., -0.0383, -0.0043,  0.0219]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0112,  0.0046,  0.0011,  ..., -0.0016,  0.0092, -0.0054],\n",
       "                      [-0.0145, -0.0116, -0.0119,  ..., -0.0156, -0.0027,  0.0020],\n",
       "                      [ 0.0109, -0.0011, -0.0090,  ..., -0.0103,  0.0018, -0.0053],\n",
       "                      ...,\n",
       "                      [-0.0071, -0.0101, -0.0112,  ...,  0.0031, -0.0008, -0.0134],\n",
       "                      [ 0.0044, -0.0126,  0.0115,  ...,  0.0039, -0.0026, -0.0122],\n",
       "                      [-0.0154,  0.0052,  0.0106,  ..., -0.0121, -0.0052,  0.0016]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0371, -0.0027, -0.0275,  ..., -0.0009,  0.0194,  0.0050],\n",
       "                      [-0.0019,  0.0359, -0.0449,  ...,  0.0342,  0.0011,  0.0159],\n",
       "                      [-0.0640, -0.0344, -0.0243,  ...,  0.0243, -0.0540, -0.0100],\n",
       "                      ...,\n",
       "                      [-0.0051,  0.0176,  0.0405,  ..., -0.0079,  0.0114, -0.0190],\n",
       "                      [-0.0044, -0.0194, -0.0121,  ..., -0.0101,  0.0013, -0.0095],\n",
       "                      [-0.0134,  0.0098, -0.0186,  ..., -0.0045, -0.0039,  0.0106]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0132,  0.0134, -0.0015,  ...,  0.0077, -0.0136,  0.0145],\n",
       "                      [-0.0021,  0.0042, -0.0073,  ...,  0.0046,  0.0017,  0.0085],\n",
       "                      [ 0.0113,  0.0054,  0.0115,  ...,  0.0106, -0.0108,  0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0054,  0.0038,  0.0124,  ..., -0.0121,  0.0029,  0.0057],\n",
       "                      [ 0.0055, -0.0049, -0.0102,  ..., -0.0150, -0.0067,  0.0038],\n",
       "                      [-0.0014,  0.0137,  0.0118,  ..., -0.0150, -0.0019, -0.0051]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0664, -0.0435,  0.0028,  ..., -0.0330, -0.0019, -0.0258],\n",
       "                      [ 0.0869, -0.0297,  0.0025,  ...,  0.0175,  0.0161,  0.0078],\n",
       "                      [ 0.0461, -0.0305, -0.0586,  ...,  0.0098,  0.0116, -0.0122],\n",
       "                      ...,\n",
       "                      [-0.0145, -0.0256, -0.0077,  ...,  0.0231, -0.0361, -0.0256],\n",
       "                      [ 0.0206, -0.0011,  0.0244,  ...,  0.0190,  0.0121, -0.0167],\n",
       "                      [ 0.0220,  0.0109, -0.0048,  ..., -0.0001, -0.0420,  0.0019]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0031,  0.0109,  0.0087,  ..., -0.0024, -0.0009,  0.0028],\n",
       "                      [ 0.0057, -0.0127, -0.0063,  ...,  0.0114,  0.0108, -0.0001],\n",
       "                      [ 0.0131, -0.0144, -0.0021,  ..., -0.0056,  0.0148,  0.0156],\n",
       "                      ...,\n",
       "                      [ 0.0114,  0.0129, -0.0095,  ..., -0.0038, -0.0100, -0.0071],\n",
       "                      [ 0.0118,  0.0120, -0.0131,  ..., -0.0005,  0.0129, -0.0132],\n",
       "                      [-0.0150, -0.0151, -0.0037,  ...,  0.0153,  0.0059, -0.0070]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0118,  0.0084,  0.0151,  ..., -0.0132, -0.0065,  0.0131],\n",
       "                      [ 0.0119,  0.0356, -0.0227,  ..., -0.0608,  0.0082, -0.0378],\n",
       "                      [-0.0109,  0.0159, -0.0309,  ..., -0.0322,  0.0269,  0.0011],\n",
       "                      ...,\n",
       "                      [-0.0312,  0.0016, -0.0183,  ..., -0.0063, -0.0134,  0.0059],\n",
       "                      [-0.0603, -0.0114,  0.0255,  ...,  0.0084, -0.0247, -0.0308],\n",
       "                      [-0.0206,  0.0031,  0.0374,  ..., -0.0121,  0.0044, -0.0159]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0092,  0.0104,  0.0090,  ...,  0.0084,  0.0041,  0.0066],\n",
       "                      [-0.0065, -0.0047, -0.0125,  ...,  0.0126, -0.0075, -0.0021],\n",
       "                      [-0.0015, -0.0084,  0.0081,  ...,  0.0041, -0.0008, -0.0004],\n",
       "                      ...,\n",
       "                      [-0.0075, -0.0010, -0.0008,  ..., -0.0121, -0.0051,  0.0059],\n",
       "                      [-0.0101,  0.0021,  0.0089,  ...,  0.0018, -0.0090,  0.0010],\n",
       "                      [-0.0156,  0.0091, -0.0142,  ..., -0.0138,  0.0114, -0.0070]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0053, -0.0127,  0.0016,  ...,  0.0091,  0.0171,  0.0052],\n",
       "                      [-0.0259,  0.0136,  0.0028,  ..., -0.0052, -0.0432,  0.0083],\n",
       "                      [-0.0034,  0.0126,  0.0080,  ...,  0.0060,  0.0146,  0.0378],\n",
       "                      ...,\n",
       "                      [ 0.0092,  0.0189,  0.0457,  ...,  0.0233, -0.0008,  0.0349],\n",
       "                      [-0.0111,  0.0142,  0.0452,  ..., -0.0359,  0.0267, -0.0219],\n",
       "                      [ 0.0137, -0.0182, -0.0105,  ..., -0.0041, -0.0053, -0.0135]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0150, -0.0120,  0.0110,  ...,  0.0121, -0.0056,  0.0063],\n",
       "                      [ 0.0020, -0.0003, -0.0148,  ..., -0.0143, -0.0002, -0.0123],\n",
       "                      [ 0.0090,  0.0099,  0.0009,  ...,  0.0134,  0.0054,  0.0071],\n",
       "                      ...,\n",
       "                      [ 0.0062, -0.0146, -0.0095,  ...,  0.0065,  0.0039, -0.0032],\n",
       "                      [ 0.0107,  0.0006, -0.0092,  ..., -0.0120, -0.0149, -0.0032],\n",
       "                      [-0.0148, -0.0099,  0.0039,  ...,  0.0101,  0.0071,  0.0061]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0031, -0.0234,  0.0042,  ...,  0.0141, -0.0074,  0.0071],\n",
       "                      [-0.0204, -0.0292, -0.0320,  ...,  0.0089,  0.0259,  0.0154],\n",
       "                      [-0.0106, -0.0166,  0.0071,  ..., -0.0161, -0.0167,  0.0039],\n",
       "                      ...,\n",
       "                      [-0.0136,  0.0078, -0.0297,  ...,  0.0212,  0.0292,  0.0371],\n",
       "                      [ 0.0015,  0.0018, -0.0205,  ...,  0.0228,  0.0184, -0.0157],\n",
       "                      [-0.0009,  0.0075, -0.0310,  ...,  0.0254, -0.0068, -0.0154]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0047, -0.0039,  0.0092,  ...,  0.0059, -0.0070, -0.0052],\n",
       "                      [ 0.0090, -0.0037, -0.0041,  ...,  0.0031,  0.0074,  0.0014],\n",
       "                      [ 0.0051,  0.0025, -0.0061,  ...,  0.0095, -0.0054, -0.0076],\n",
       "                      ...,\n",
       "                      [ 0.0028,  0.0066,  0.0020,  ..., -0.0091, -0.0094,  0.0088],\n",
       "                      [ 0.0058,  0.0041,  0.0009,  ...,  0.0056, -0.0095,  0.0018],\n",
       "                      [ 0.0002, -0.0038, -0.0030,  ...,  0.0011,  0.0064,  0.0052]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.input_layernorm.weight',\n",
       "              tensor([0.3711, 0.3691, 0.3613,  ..., 0.3477, 0.3535, 0.3555],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.19.post_attention_layernorm.weight',\n",
       "              tensor([0.3027, 0.2910, 0.2852,  ..., 0.3008, 0.3027, 0.2949],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0008,  0.0058, -0.0016,  ...,  0.0199,  0.0069,  0.0173],\n",
       "                      [-0.0160,  0.0061, -0.0035,  ...,  0.0211,  0.0125,  0.0198],\n",
       "                      [ 0.0066,  0.0037, -0.0016,  ...,  0.0078, -0.0096,  0.0143],\n",
       "                      ...,\n",
       "                      [-0.0513, -0.0752,  0.0206,  ..., -0.0386,  0.0481, -0.0042],\n",
       "                      [-0.1045,  0.0054,  0.0527,  ..., -0.0107, -0.0186,  0.0011],\n",
       "                      [ 0.0400,  0.0107, -0.0305,  ..., -0.0167,  0.0408,  0.0153]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0050, -0.0026,  0.0056,  ...,  0.0041, -0.0025, -0.0132],\n",
       "                      [ 0.0045, -0.0037, -0.0034,  ...,  0.0045, -0.0107,  0.0023],\n",
       "                      [ 0.0075, -0.0063, -0.0073,  ...,  0.0013, -0.0113, -0.0141],\n",
       "                      ...,\n",
       "                      [ 0.0060, -0.0072,  0.0143,  ...,  0.0030, -0.0006,  0.0090],\n",
       "                      [-0.0059,  0.0153, -0.0125,  ..., -0.0025,  0.0060,  0.0028],\n",
       "                      [-0.0096,  0.0023, -0.0047,  ...,  0.0058, -0.0012,  0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0168, -0.0036,  0.0391,  ..., -0.0110, -0.0166, -0.0007],\n",
       "                      [ 0.0084,  0.0020,  0.0090,  ...,  0.0042,  0.0057, -0.0012],\n",
       "                      [ 0.0085,  0.0244, -0.0070,  ..., -0.0123, -0.0028, -0.0135],\n",
       "                      ...,\n",
       "                      [-0.0194, -0.0215, -0.0535,  ..., -0.0157, -0.0141,  0.0059],\n",
       "                      [-0.0130,  0.0403,  0.0306,  ..., -0.0139,  0.0144,  0.0304],\n",
       "                      [-0.0718, -0.0258, -0.0292,  ...,  0.0280, -0.0259,  0.0732]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0064, -0.0092,  0.0114,  ..., -0.0060,  0.0077,  0.0145],\n",
       "                      [-0.0116, -0.0146,  0.0150,  ...,  0.0008,  0.0093,  0.0067],\n",
       "                      [ 0.0136, -0.0070, -0.0022,  ..., -0.0069,  0.0145, -0.0142],\n",
       "                      ...,\n",
       "                      [-0.0019, -0.0140,  0.0149,  ..., -0.0003, -0.0137,  0.0047],\n",
       "                      [ 0.0131, -0.0053,  0.0029,  ..., -0.0001, -0.0092,  0.0106],\n",
       "                      [ 0.0098, -0.0026, -0.0084,  ...,  0.0078,  0.0024,  0.0133]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0474, -0.0291, -0.0063,  ..., -0.0203, -0.0084,  0.0308],\n",
       "                      [-0.0077, -0.0047, -0.0035,  ..., -0.0080, -0.0264,  0.0023],\n",
       "                      [-0.0275,  0.0168, -0.0153,  ..., -0.0266,  0.0059,  0.0109],\n",
       "                      ...,\n",
       "                      [-0.0005, -0.0439, -0.0267,  ...,  0.0308, -0.0056, -0.0396],\n",
       "                      [-0.0168, -0.0010, -0.0591,  ..., -0.0066,  0.0216, -0.0156],\n",
       "                      [ 0.0422,  0.0304,  0.0054,  ..., -0.0053,  0.0410, -0.0260]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0044, -0.0040,  0.0146,  ...,  0.0092, -0.0146,  0.0081],\n",
       "                      [-0.0017, -0.0135, -0.0114,  ..., -0.0130, -0.0057,  0.0131],\n",
       "                      [-0.0023,  0.0005, -0.0141,  ..., -0.0064,  0.0043,  0.0055],\n",
       "                      ...,\n",
       "                      [ 0.0079,  0.0059,  0.0115,  ..., -0.0097, -0.0024,  0.0137],\n",
       "                      [ 0.0028, -0.0094, -0.0060,  ...,  0.0035, -0.0056, -0.0152],\n",
       "                      [-0.0031,  0.0107, -0.0092,  ...,  0.0029,  0.0104, -0.0145]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0308,  0.0050,  0.0693,  ...,  0.0117, -0.0244, -0.0137],\n",
       "                      [ 0.0008, -0.0069,  0.0079,  ...,  0.0038, -0.0249, -0.0210],\n",
       "                      [-0.0024,  0.0176, -0.0206,  ..., -0.0270, -0.0004, -0.0133],\n",
       "                      ...,\n",
       "                      [-0.0146,  0.0308, -0.0133,  ...,  0.0108, -0.0376,  0.0515],\n",
       "                      [-0.0219,  0.0177, -0.0199,  ..., -0.0442, -0.0076, -0.0037],\n",
       "                      [ 0.0087,  0.0231,  0.0016,  ..., -0.0214, -0.0275,  0.0469]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-1.2329e-02,  9.5215e-03,  1.7090e-03,  ...,  1.3306e-02,\n",
       "                        4.6387e-03, -2.0142e-03],\n",
       "                      [ 1.2390e-02, -3.6774e-03,  1.5442e-02,  ...,  8.4229e-03,\n",
       "                        3.4332e-03,  8.4400e-05],\n",
       "                      [-7.2632e-03, -1.4771e-02,  1.1047e-02,  ..., -4.2725e-03,\n",
       "                       -1.4954e-02,  2.1973e-03],\n",
       "                      ...,\n",
       "                      [-4.2725e-03, -4.7607e-03,  9.3994e-03,  ..., -9.9659e-05,\n",
       "                       -1.0254e-02, -8.3160e-04],\n",
       "                      [ 7.0190e-03,  3.8757e-03,  1.3123e-02,  ...,  1.1780e-02,\n",
       "                       -4.2419e-03, -1.4893e-02],\n",
       "                      [-5.6763e-03,  8.4229e-03,  1.3062e-02,  ...,  8.5449e-03,\n",
       "                       -4.9744e-03,  3.0212e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0102, -0.0266,  0.0190,  ...,  0.0391, -0.0439,  0.0058],\n",
       "                      [ 0.0092, -0.0186,  0.0017,  ..., -0.0177, -0.0183, -0.0088],\n",
       "                      [ 0.0096, -0.0126,  0.0383,  ...,  0.0122,  0.0540, -0.0172],\n",
       "                      ...,\n",
       "                      [ 0.0170, -0.0220,  0.0466,  ...,  0.0023, -0.0306,  0.0045],\n",
       "                      [-0.0192,  0.0083, -0.0122,  ...,  0.0361, -0.0299,  0.0262],\n",
       "                      [-0.0074, -0.0153, -0.0344,  ..., -0.0006, -0.0092, -0.0271]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0077, -0.0041, -0.0090,  ..., -0.0077,  0.0042, -0.0050],\n",
       "                      [-0.0098,  0.0024, -0.0101,  ..., -0.0039, -0.0007,  0.0005],\n",
       "                      [-0.0036,  0.0007,  0.0033,  ...,  0.0069,  0.0075,  0.0133],\n",
       "                      ...,\n",
       "                      [-0.0004, -0.0100, -0.0137,  ..., -0.0140,  0.0104, -0.0085],\n",
       "                      [-0.0084,  0.0015, -0.0079,  ..., -0.0033,  0.0069, -0.0063],\n",
       "                      [-0.0032,  0.0027,  0.0085,  ..., -0.0035,  0.0121,  0.0082]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.up_proj.weight',\n",
       "              tensor([[-5.2979e-02,  2.9541e-02,  2.3804e-02,  ...,  9.9487e-03,\n",
       "                       -3.7109e-02,  2.0599e-03],\n",
       "                      [ 8.5449e-03,  6.0730e-03, -1.6937e-03,  ..., -5.2979e-02,\n",
       "                       -5.2795e-03,  3.3203e-02],\n",
       "                      [ 4.9072e-02,  4.1748e-02,  1.5991e-02,  ...,  2.8931e-02,\n",
       "                       -2.5177e-03,  3.8818e-02],\n",
       "                      ...,\n",
       "                      [-3.9368e-03,  4.0527e-02,  1.0315e-02,  ...,  9.5825e-03,\n",
       "                       -1.6724e-02,  2.9541e-02],\n",
       "                      [-3.4668e-02, -6.0730e-03, -3.0762e-02,  ..., -1.8677e-02,\n",
       "                       -6.5308e-03, -3.9307e-02],\n",
       "                      [ 6.4697e-03, -1.8311e-03,  1.3733e-02,  ..., -6.1951e-03,\n",
       "                        3.1128e-03, -8.3923e-05]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0052,  0.0130,  0.0084,  ..., -0.0081, -0.0045, -0.0143],\n",
       "                      [ 0.0089, -0.0120,  0.0047,  ..., -0.0112, -0.0064, -0.0089],\n",
       "                      [-0.0066,  0.0032,  0.0081,  ..., -0.0154,  0.0107, -0.0022],\n",
       "                      ...,\n",
       "                      [ 0.0151, -0.0093,  0.0034,  ...,  0.0110,  0.0072,  0.0110],\n",
       "                      [ 0.0108, -0.0076,  0.0048,  ...,  0.0021,  0.0156, -0.0014],\n",
       "                      [ 0.0134, -0.0103, -0.0134,  ..., -0.0108, -0.0134, -0.0079]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.down_proj.weight',\n",
       "              tensor([[-0.0273,  0.0242,  0.0518,  ..., -0.0061, -0.0369, -0.0130],\n",
       "                      [ 0.0267,  0.0352,  0.0071,  ...,  0.0016, -0.0261,  0.0427],\n",
       "                      [-0.0134, -0.0030, -0.0067,  ..., -0.0050,  0.0112, -0.0157],\n",
       "                      ...,\n",
       "                      [-0.0369, -0.0087,  0.0031,  ...,  0.0067,  0.0198, -0.0103],\n",
       "                      [-0.0240, -0.0518,  0.0211,  ..., -0.0003,  0.0052, -0.0305],\n",
       "                      [ 0.0347, -0.0417,  0.0255,  ...,  0.0126, -0.0325, -0.0342]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-7.5989e-03, -6.8665e-03,  3.7384e-03,  ...,  7.5989e-03,\n",
       "                       -5.4626e-03, -5.6763e-03],\n",
       "                      [ 3.8147e-03,  5.0964e-03,  6.6833e-03,  ..., -2.1667e-03,\n",
       "                        3.1090e-04, -4.2677e-05],\n",
       "                      [-6.2866e-03, -4.1504e-03, -9.2773e-03,  ...,  7.3242e-03,\n",
       "                       -1.8768e-03,  5.2185e-03],\n",
       "                      ...,\n",
       "                      [ 4.3335e-03, -3.2196e-03, -6.0425e-03,  ..., -8.9111e-03,\n",
       "                       -9.0942e-03,  8.6060e-03],\n",
       "                      [ 7.9956e-03,  5.5237e-03,  2.3499e-03,  ..., -8.3618e-03,\n",
       "                       -3.4790e-03,  8.5449e-03],\n",
       "                      [ 3.4180e-03,  9.3384e-03,  6.0120e-03,  ..., -2.1362e-04,\n",
       "                        7.0496e-03,  5.8289e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.input_layernorm.weight',\n",
       "              tensor([0.3789, 0.3691, 0.3633,  ..., 0.3652, 0.3652, 0.3652],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.20.post_attention_layernorm.weight',\n",
       "              tensor([0.3184, 0.3008, 0.2949,  ..., 0.3066, 0.3125, 0.3027],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0059, -0.0352,  0.0236,  ...,  0.0016, -0.0125,  0.0042],\n",
       "                      [ 0.0123, -0.0103,  0.0082,  ...,  0.0089, -0.0138,  0.0056],\n",
       "                      [-0.0006,  0.0069, -0.0025,  ...,  0.0352, -0.0123, -0.0060],\n",
       "                      ...,\n",
       "                      [-0.0077,  0.0128,  0.0058,  ..., -0.0256, -0.0815,  0.0109],\n",
       "                      [-0.0505,  0.0005,  0.0048,  ...,  0.0182,  0.0136, -0.0001],\n",
       "                      [-0.0693, -0.0510, -0.0457,  ..., -0.0422, -0.0018, -0.0598]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-7.5073e-03,  2.1820e-03,  5.7678e-03,  ...,  1.3794e-02,\n",
       "                       -5.0354e-03, -1.5137e-02],\n",
       "                      [ 1.0498e-02,  6.0425e-03,  3.1948e-05,  ..., -7.9346e-03,\n",
       "                       -1.2085e-02,  2.3937e-04],\n",
       "                      [-7.7209e-03,  7.2937e-03,  1.1841e-02,  ...,  3.3264e-03,\n",
       "                        6.9885e-03, -1.5137e-02],\n",
       "                      ...,\n",
       "                      [-4.3335e-03, -1.3000e-02,  6.7902e-04,  ..., -5.5847e-03,\n",
       "                        9.1553e-03, -4.3640e-03],\n",
       "                      [ 1.3000e-02, -5.1880e-03,  7.1335e-04,  ...,  6.2561e-03,\n",
       "                       -8.4839e-03,  5.6458e-03],\n",
       "                      [ 8.6784e-05, -1.2451e-02, -1.3306e-02,  ..., -8.3008e-03,\n",
       "                        1.3733e-02,  7.5378e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0029,  0.0078,  0.0116,  ..., -0.0145, -0.0020,  0.0061],\n",
       "                      [ 0.0075, -0.0138,  0.0189,  ...,  0.0225,  0.0012,  0.0038],\n",
       "                      [ 0.0175,  0.0183,  0.0121,  ..., -0.0142,  0.0074,  0.0067],\n",
       "                      ...,\n",
       "                      [-0.0596,  0.0228,  0.0050,  ...,  0.0571, -0.0972, -0.0466],\n",
       "                      [ 0.0500,  0.0522, -0.0532,  ..., -0.0129,  0.0479, -0.0378],\n",
       "                      [-0.0084, -0.0403, -0.0082,  ..., -0.0065,  0.0052,  0.1138]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 9.8877e-03, -1.3794e-02,  2.1820e-03,  ..., -1.3550e-02,\n",
       "                       -2.0599e-03, -6.3782e-03],\n",
       "                      [ 3.4027e-03,  8.0566e-03,  1.0742e-02,  ..., -1.7319e-03,\n",
       "                        2.7313e-03,  1.4709e-02],\n",
       "                      [-1.1719e-02,  1.0620e-02, -1.4832e-02,  ..., -1.0864e-02,\n",
       "                       -3.9673e-03, -6.2561e-03],\n",
       "                      ...,\n",
       "                      [ 2.8534e-03, -2.6855e-03, -3.3875e-03,  ...,  8.8501e-03,\n",
       "                       -1.5015e-02,  1.3367e-02],\n",
       "                      [-5.0735e-04, -3.0708e-04,  2.8038e-04,  ..., -9.4604e-03,\n",
       "                       -7.6294e-03,  2.1515e-03],\n",
       "                      [ 1.4496e-03,  8.0566e-03, -9.3994e-03,  ..., -1.3611e-02,\n",
       "                        1.0431e-05,  2.9449e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0454, -0.0022, -0.0093,  ..., -0.0002,  0.0251, -0.0510],\n",
       "                      [-0.0141,  0.0269, -0.0139,  ...,  0.0074,  0.0110,  0.0259],\n",
       "                      [ 0.0132,  0.0121,  0.0053,  ..., -0.0219,  0.0659,  0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0057,  0.0081,  0.0190,  ...,  0.0139,  0.0124,  0.0016],\n",
       "                      [-0.0088, -0.0452,  0.0002,  ...,  0.0089, -0.0096,  0.0164],\n",
       "                      [-0.0417,  0.0325,  0.0081,  ..., -0.0255, -0.0131, -0.0086]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0059, -0.0038,  0.0081,  ..., -0.0013, -0.0021,  0.0099],\n",
       "                      [-0.0007,  0.0042,  0.0141,  ...,  0.0139, -0.0027,  0.0033],\n",
       "                      [-0.0135, -0.0154,  0.0116,  ..., -0.0141,  0.0095, -0.0091],\n",
       "                      ...,\n",
       "                      [-0.0078, -0.0154,  0.0114,  ..., -0.0051, -0.0054,  0.0039],\n",
       "                      [-0.0129, -0.0154,  0.0136,  ..., -0.0075,  0.0074,  0.0050],\n",
       "                      [ 0.0120,  0.0092,  0.0012,  ..., -0.0042,  0.0056,  0.0036]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0167,  0.0221,  0.0488,  ...,  0.0187,  0.0427, -0.0030],\n",
       "                      [-0.0288, -0.0280,  0.0139,  ..., -0.0098,  0.0132,  0.0078],\n",
       "                      [ 0.0045, -0.0344,  0.0116,  ..., -0.0100,  0.0189,  0.0415],\n",
       "                      ...,\n",
       "                      [ 0.0574, -0.0066,  0.0022,  ...,  0.0325,  0.0405,  0.0264],\n",
       "                      [-0.0118,  0.0079,  0.0349,  ...,  0.0162, -0.0147, -0.0018],\n",
       "                      [-0.0248, -0.0088, -0.0364,  ...,  0.0459, -0.0045,  0.0177]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-9.8267e-03, -1.2878e-02, -5.7678e-03,  ..., -1.4771e-02,\n",
       "                       -5.3101e-03, -1.2939e-02],\n",
       "                      [-4.4250e-03, -1.1536e-02, -1.4404e-02,  ...,  4.5471e-03,\n",
       "                        1.1047e-02,  1.5137e-02],\n",
       "                      [ 1.0452e-03,  1.4343e-02,  2.3346e-03,  ...,  1.1047e-02,\n",
       "                        8.3008e-03, -1.0742e-02],\n",
       "                      ...,\n",
       "                      [ 3.5095e-03,  7.6904e-03, -8.2397e-03,  ...,  5.7678e-03,\n",
       "                       -3.1128e-03, -1.0254e-02],\n",
       "                      [-1.3733e-02, -8.1177e-03,  3.7384e-03,  ..., -1.1353e-02,\n",
       "                       -4.3335e-03,  1.1902e-02],\n",
       "                      [-5.1270e-03, -1.3733e-02, -1.2390e-02,  ...,  2.4080e-05,\n",
       "                       -9.3384e-03,  3.6774e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0155, -0.0105,  0.0292,  ...,  0.0109,  0.0388,  0.0081],\n",
       "                      [ 0.0087, -0.0219, -0.0259,  ..., -0.0469,  0.0454, -0.0209],\n",
       "                      [ 0.0315,  0.0231, -0.0112,  ..., -0.0204, -0.0300, -0.0308],\n",
       "                      ...,\n",
       "                      [ 0.0315, -0.0649,  0.0199,  ...,  0.0065, -0.0557, -0.0077],\n",
       "                      [-0.0089,  0.0052,  0.0139,  ...,  0.0095, -0.0383, -0.0286],\n",
       "                      [-0.0430,  0.0386,  0.0315,  ..., -0.0153,  0.0267, -0.0117]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0098, -0.0153,  0.0079,  ..., -0.0092,  0.0153,  0.0142],\n",
       "                      [ 0.0028,  0.0132,  0.0121,  ..., -0.0151,  0.0077, -0.0096],\n",
       "                      [-0.0126,  0.0081, -0.0140,  ..., -0.0156, -0.0081, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0129, -0.0047,  0.0064,  ..., -0.0006,  0.0128, -0.0142],\n",
       "                      [-0.0099, -0.0054, -0.0050,  ..., -0.0082,  0.0035,  0.0091],\n",
       "                      [ 0.0001,  0.0005,  0.0141,  ..., -0.0033, -0.0034,  0.0075]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0098, -0.0157,  0.0090,  ...,  0.0271,  0.0124,  0.0118],\n",
       "                      [-0.0155,  0.0203,  0.0167,  ...,  0.0352,  0.0339, -0.0267],\n",
       "                      [-0.0073, -0.0251,  0.0006,  ...,  0.0179,  0.0051, -0.0601],\n",
       "                      ...,\n",
       "                      [-0.0371,  0.0071, -0.0132,  ...,  0.0097, -0.0006,  0.0188],\n",
       "                      [ 0.0131, -0.0001, -0.0361,  ..., -0.0140,  0.0210, -0.0072],\n",
       "                      [-0.0447,  0.0041, -0.0583,  ...,  0.0011, -0.0354, -0.0114]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 9.1553e-03,  1.0300e-03, -1.3306e-02,  ..., -1.2589e-03,\n",
       "                       -9.8267e-03,  2.3041e-03],\n",
       "                      [-1.0071e-02, -1.1902e-02, -2.2583e-03,  ...,  1.3977e-02,\n",
       "                       -3.7231e-03,  3.3722e-03],\n",
       "                      [-1.2695e-02,  1.0254e-02,  5.6152e-03,  ...,  1.2695e-02,\n",
       "                       -2.7924e-03, -9.2163e-03],\n",
       "                      ...,\n",
       "                      [ 9.9487e-03,  1.1475e-02, -3.6774e-03,  ...,  1.5015e-02,\n",
       "                       -1.4038e-02, -1.9989e-03],\n",
       "                      [-1.2451e-02,  9.5825e-03, -3.2196e-03,  ...,  1.7700e-03,\n",
       "                       -3.3569e-03,  1.2451e-02],\n",
       "                      [-1.3062e-02,  7.9956e-03, -4.8828e-04,  ..., -1.1719e-02,\n",
       "                       -1.1475e-02, -1.3590e-05]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0349,  0.0459,  0.0129,  ...,  0.0206, -0.0096, -0.0310],\n",
       "                      [ 0.0070, -0.0061, -0.0070,  ..., -0.0288,  0.0369, -0.0352],\n",
       "                      [-0.0259,  0.0270, -0.0025,  ...,  0.0109,  0.0205,  0.0457],\n",
       "                      ...,\n",
       "                      [-0.0337,  0.0649,  0.0093,  ..., -0.0111, -0.0288, -0.0210],\n",
       "                      [-0.0134,  0.0298, -0.0261,  ..., -0.0359, -0.0170,  0.0114],\n",
       "                      [ 0.0006,  0.0062,  0.0454,  ..., -0.0297,  0.0029,  0.0025]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0023, -0.0062,  0.0066,  ..., -0.0014, -0.0051,  0.0090],\n",
       "                      [ 0.0024,  0.0093, -0.0057,  ..., -0.0045, -0.0032,  0.0083],\n",
       "                      [-0.0082,  0.0081, -0.0006,  ...,  0.0071, -0.0082, -0.0080],\n",
       "                      ...,\n",
       "                      [-0.0065, -0.0020, -0.0073,  ...,  0.0081, -0.0023, -0.0005],\n",
       "                      [ 0.0075,  0.0040,  0.0071,  ...,  0.0078,  0.0017,  0.0070],\n",
       "                      [ 0.0025,  0.0077,  0.0025,  ...,  0.0028,  0.0018,  0.0026]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.input_layernorm.weight',\n",
       "              tensor([0.3691, 0.3887, 0.3789,  ..., 0.3691, 0.3887, 0.3750],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.21.post_attention_layernorm.weight',\n",
       "              tensor([0.3242, 0.3164, 0.3125,  ..., 0.3242, 0.3184, 0.3184],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0040, -0.0361, -0.0145,  ...,  0.0287, -0.0177, -0.0087],\n",
       "                      [-0.0019, -0.0094, -0.0237,  ...,  0.0014, -0.0275,  0.0003],\n",
       "                      [-0.0175, -0.0413,  0.0027,  ..., -0.0179,  0.0266,  0.0320],\n",
       "                      ...,\n",
       "                      [ 0.0051,  0.0403,  0.0078,  ...,  0.0286,  0.0189, -0.0001],\n",
       "                      [-0.0266, -0.0027, -0.0544,  ...,  0.0898, -0.0148, -0.0299],\n",
       "                      [-0.0079, -0.0288,  0.0011,  ..., -0.0182,  0.0009,  0.0219]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0142, -0.0155, -0.0140,  ..., -0.0095, -0.0063,  0.0078],\n",
       "                      [ 0.0150,  0.0068, -0.0040,  ...,  0.0140, -0.0145, -0.0040],\n",
       "                      [-0.0112, -0.0037,  0.0089,  ...,  0.0095, -0.0034, -0.0139],\n",
       "                      ...,\n",
       "                      [-0.0101, -0.0099, -0.0039,  ...,  0.0105, -0.0042, -0.0153],\n",
       "                      [ 0.0146, -0.0138, -0.0055,  ..., -0.0131,  0.0042,  0.0046],\n",
       "                      [ 0.0075, -0.0051,  0.0128,  ...,  0.0027, -0.0042,  0.0079]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0015, -0.0237, -0.0019,  ...,  0.0063, -0.0322, -0.0344],\n",
       "                      [ 0.0156, -0.0184,  0.0070,  ..., -0.0352, -0.0008,  0.0051],\n",
       "                      [ 0.0063, -0.0144,  0.0201,  ...,  0.0134,  0.0051,  0.0537],\n",
       "                      ...,\n",
       "                      [ 0.0286,  0.0430, -0.0417,  ..., -0.0405, -0.0193, -0.0201],\n",
       "                      [-0.0344, -0.0569, -0.0289,  ..., -0.0042, -0.0271, -0.0210],\n",
       "                      [ 0.0525,  0.0244,  0.0205,  ...,  0.0304, -0.0281,  0.0229]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 1.5137e-02,  6.9885e-03,  1.2634e-02,  ...,  1.2817e-02,\n",
       "                        3.7537e-03, -9.2773e-03],\n",
       "                      [-3.2959e-03,  6.4087e-03, -4.6387e-03,  ..., -8.4229e-03,\n",
       "                       -1.0681e-02,  1.4587e-02],\n",
       "                      [-1.2573e-02, -1.9836e-03,  1.3428e-02,  ..., -1.1597e-03,\n",
       "                       -1.4465e-02, -6.1035e-03],\n",
       "                      ...,\n",
       "                      [-1.3489e-02,  2.5787e-03,  4.0283e-03,  ..., -2.0752e-03,\n",
       "                       -1.4404e-02,  9.1553e-03],\n",
       "                      [-1.5182e-03,  5.9204e-03, -7.8735e-03,  ..., -1.5442e-02,\n",
       "                        2.7008e-03, -1.4038e-02],\n",
       "                      [-2.1515e-03,  4.9353e-05,  1.4648e-02,  ..., -1.3916e-02,\n",
       "                        8.8501e-03,  1.1658e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0325, -0.0299,  0.0109,  ..., -0.0400,  0.0190,  0.0085],\n",
       "                      [-0.0226,  0.0344, -0.0013,  ..., -0.0908, -0.0396,  0.0073],\n",
       "                      [ 0.0239,  0.0141,  0.0188,  ...,  0.0084, -0.0605, -0.0265],\n",
       "                      ...,\n",
       "                      [-0.0422, -0.0544, -0.0020,  ..., -0.0200, -0.0192,  0.0178],\n",
       "                      [-0.0220,  0.0435,  0.0574,  ..., -0.0182,  0.0120, -0.0243],\n",
       "                      [ 0.0209,  0.0267, -0.0250,  ..., -0.0289, -0.0108,  0.0620]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 3.2196e-03, -4.6692e-03,  3.3417e-03,  ...,  9.7656e-03,\n",
       "                       -8.3618e-03,  1.4526e-02],\n",
       "                      [ 9.2163e-03, -9.7046e-03, -8.3618e-03,  ..., -4.0283e-03,\n",
       "                       -1.5259e-02, -6.3171e-03],\n",
       "                      [-1.1658e-02, -2.3556e-04, -6.4087e-03,  ...,  6.1035e-03,\n",
       "                        7.9956e-03, -3.0518e-05],\n",
       "                      ...,\n",
       "                      [-1.4404e-02, -2.7657e-04, -9.9487e-03,  ..., -1.3184e-02,\n",
       "                        8.0566e-03,  8.8501e-03],\n",
       "                      [ 8.3618e-03,  5.4016e-03,  2.4414e-04,  ..., -3.0975e-03,\n",
       "                       -9.7656e-04,  1.5320e-02],\n",
       "                      [-1.2085e-02, -5.6458e-03,  3.4027e-03,  ...,  1.3611e-02,\n",
       "                       -1.1292e-02,  8.6594e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0001,  0.0552, -0.0520,  ..., -0.0123, -0.0072,  0.0166],\n",
       "                      [-0.0164, -0.0025,  0.0237,  ..., -0.0063, -0.0454, -0.0359],\n",
       "                      [ 0.0021, -0.0129, -0.0160,  ..., -0.0134, -0.0167,  0.0269],\n",
       "                      ...,\n",
       "                      [-0.0058,  0.0198, -0.0347,  ..., -0.0253,  0.0276, -0.0004],\n",
       "                      [-0.0320,  0.0123,  0.0002,  ...,  0.0096,  0.0393, -0.0214],\n",
       "                      [ 0.0113,  0.0127,  0.0442,  ...,  0.0016, -0.0139,  0.0085]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0029, -0.0016, -0.0116,  ..., -0.0141,  0.0143, -0.0120],\n",
       "                      [ 0.0061,  0.0064, -0.0031,  ..., -0.0092,  0.0080, -0.0123],\n",
       "                      [-0.0060, -0.0156,  0.0125,  ..., -0.0075, -0.0104,  0.0115],\n",
       "                      ...,\n",
       "                      [ 0.0116,  0.0135,  0.0145,  ..., -0.0029, -0.0057,  0.0043],\n",
       "                      [ 0.0045,  0.0020, -0.0109,  ...,  0.0014,  0.0140, -0.0145],\n",
       "                      [ 0.0098, -0.0046,  0.0079,  ...,  0.0148,  0.0027,  0.0154]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0186, -0.0311,  0.0026,  ..., -0.0388,  0.0388, -0.0156],\n",
       "                      [-0.0026, -0.0094, -0.0179,  ...,  0.0242,  0.0047,  0.0160],\n",
       "                      [-0.0447,  0.0154,  0.0258,  ...,  0.0243,  0.0073, -0.0042],\n",
       "                      ...,\n",
       "                      [-0.0309,  0.0053, -0.0075,  ...,  0.0155,  0.0070, -0.0215],\n",
       "                      [-0.0308,  0.0256,  0.0178,  ...,  0.0066,  0.0347, -0.0162],\n",
       "                      [-0.0020,  0.0082, -0.0159,  ..., -0.0086,  0.0199,  0.0237]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-1.0254e-02,  6.7444e-03,  1.1414e-02,  ...,  7.9346e-03,\n",
       "                        1.2970e-03,  5.2490e-03],\n",
       "                      [ 8.6975e-04, -8.6060e-03, -1.1292e-02,  ..., -2.5787e-03,\n",
       "                        1.1826e-03, -4.3335e-03],\n",
       "                      [-1.5564e-02, -4.3030e-03,  4.3869e-04,  ..., -2.1219e-05,\n",
       "                       -1.3123e-02, -1.1536e-02],\n",
       "                      ...,\n",
       "                      [-6.1951e-03, -1.3672e-02, -6.2256e-03,  ...,  7.8735e-03,\n",
       "                       -4.0283e-03,  9.2773e-03],\n",
       "                      [ 5.8899e-03,  1.5335e-03,  6.3782e-03,  ...,  1.4954e-02,\n",
       "                       -9.2773e-03,  2.8229e-03],\n",
       "                      [ 1.3245e-02,  1.3184e-02, -3.9368e-03,  ...,  7.3242e-03,\n",
       "                       -1.2573e-02,  3.5858e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.up_proj.weight',\n",
       "              tensor([[-0.0161, -0.0040, -0.0093,  ..., -0.0266,  0.0221,  0.0415],\n",
       "                      [ 0.0317, -0.0160,  0.0177,  ..., -0.0034,  0.0291,  0.0073],\n",
       "                      [ 0.0004,  0.0212, -0.0452,  ...,  0.0322,  0.0012,  0.0073],\n",
       "                      ...,\n",
       "                      [-0.0327,  0.0076, -0.0222,  ..., -0.0094,  0.0145,  0.0391],\n",
       "                      [ 0.0410, -0.0396, -0.0133,  ...,  0.0084,  0.0034, -0.0581],\n",
       "                      [-0.0141,  0.0210,  0.0303,  ..., -0.0381, -0.0806,  0.0608]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0023, -0.0026, -0.0047,  ..., -0.0064,  0.0140,  0.0059],\n",
       "                      [ 0.0051,  0.0131,  0.0142,  ...,  0.0041,  0.0132, -0.0146],\n",
       "                      [-0.0129, -0.0095, -0.0121,  ...,  0.0005, -0.0017,  0.0096],\n",
       "                      ...,\n",
       "                      [-0.0085, -0.0011,  0.0098,  ..., -0.0126,  0.0078,  0.0150],\n",
       "                      [-0.0039, -0.0125, -0.0079,  ...,  0.0077, -0.0021,  0.0096],\n",
       "                      [ 0.0155,  0.0135, -0.0036,  ...,  0.0088,  0.0115, -0.0109]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.down_proj.weight',\n",
       "              tensor([[-0.0210, -0.0192,  0.0051,  ...,  0.0255,  0.0352, -0.0635],\n",
       "                      [ 0.0297, -0.0032,  0.0325,  ...,  0.0075, -0.0060, -0.0143],\n",
       "                      [ 0.0168, -0.0219, -0.0374,  ..., -0.0031,  0.0276,  0.0170],\n",
       "                      ...,\n",
       "                      [ 0.0625,  0.0292, -0.0562,  ..., -0.0176,  0.0157, -0.0220],\n",
       "                      [-0.0018, -0.0231,  0.0258,  ..., -0.0070,  0.0104,  0.0222],\n",
       "                      [ 0.0461,  0.0315, -0.0172,  ...,  0.0103, -0.0168, -0.0009]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0058,  0.0022,  0.0073,  ..., -0.0025, -0.0084,  0.0016],\n",
       "                      [-0.0072, -0.0090,  0.0052,  ...,  0.0050, -0.0012,  0.0012],\n",
       "                      [ 0.0039, -0.0070, -0.0035,  ...,  0.0007, -0.0002,  0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0061, -0.0008, -0.0075,  ...,  0.0008,  0.0009, -0.0045],\n",
       "                      [ 0.0029, -0.0017,  0.0085,  ...,  0.0029,  0.0082, -0.0058],\n",
       "                      [-0.0042, -0.0012, -0.0010,  ...,  0.0068,  0.0084,  0.0058]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.input_layernorm.weight',\n",
       "              tensor([0.4004, 0.4023, 0.4023,  ..., 0.3906, 0.4023, 0.3965],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.22.post_attention_layernorm.weight',\n",
       "              tensor([0.3340, 0.3320, 0.3262,  ..., 0.3438, 0.3398, 0.3398],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0010, -0.0067, -0.0073,  ...,  0.0209, -0.0212,  0.0081],\n",
       "                      [ 0.0081, -0.0264, -0.0047,  ..., -0.0071, -0.0039, -0.0008],\n",
       "                      [ 0.0194, -0.0010,  0.0023,  ..., -0.0003, -0.0026,  0.0130],\n",
       "                      ...,\n",
       "                      [-0.0559,  0.0486,  0.0161,  ..., -0.0311, -0.0564, -0.0132],\n",
       "                      [-0.0564, -0.0195,  0.0410,  ..., -0.0137, -0.0003,  0.0192],\n",
       "                      [-0.0019, -0.0435, -0.0199,  ...,  0.0215,  0.0352,  0.0068]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 1.0498e-02,  1.5717e-03,  1.2024e-02,  ...,  1.0315e-02,\n",
       "                       -1.2634e-02, -5.7373e-03],\n",
       "                      [-9.3384e-03, -5.7068e-03, -4.5471e-03,  ..., -1.5503e-02,\n",
       "                        1.4404e-02,  9.6436e-03],\n",
       "                      [ 6.3171e-03,  7.9346e-03,  2.6894e-04,  ...,  1.1658e-02,\n",
       "                        6.1035e-03,  8.6060e-03],\n",
       "                      ...,\n",
       "                      [-1.3123e-02, -7.6599e-03, -3.8910e-04,  ..., -3.7079e-03,\n",
       "                       -9.9487e-03,  1.3611e-02],\n",
       "                      [-4.8218e-03, -1.1536e-02,  2.5024e-03,  ..., -1.2268e-02,\n",
       "                        1.1658e-02,  1.4954e-02],\n",
       "                      [ 3.6011e-03, -3.2902e-05,  8.3160e-04,  ...,  4.0817e-04,\n",
       "                       -4.6387e-03, -8.0566e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0071, -0.0173,  0.0109,  ..., -0.0132, -0.0124,  0.0233],\n",
       "                      [ 0.0089,  0.0001, -0.0005,  ...,  0.0052, -0.0059,  0.0078],\n",
       "                      [ 0.0020, -0.0036,  0.0093,  ..., -0.0093,  0.0023, -0.0040],\n",
       "                      ...,\n",
       "                      [ 0.0016,  0.0151,  0.0325,  ..., -0.0371,  0.0017, -0.0082],\n",
       "                      [ 0.0214, -0.0251, -0.0166,  ...,  0.0045, -0.0303,  0.0299],\n",
       "                      [-0.0171, -0.0227, -0.0058,  ...,  0.0542, -0.0037, -0.0019]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 4.3335e-03,  7.8125e-03,  7.6599e-03,  ...,  1.2573e-02,\n",
       "                       -1.0254e-02,  5.7678e-03],\n",
       "                      [ 4.1199e-04, -4.2677e-05,  3.0823e-03,  ...,  9.0942e-03,\n",
       "                       -5.2185e-03,  1.2756e-02],\n",
       "                      [ 6.1340e-03,  1.0742e-02, -1.3962e-03,  ...,  1.2817e-02,\n",
       "                       -1.1902e-02,  8.2397e-03],\n",
       "                      ...,\n",
       "                      [-5.4016e-03,  7.9346e-03, -1.0071e-02,  ..., -1.2207e-02,\n",
       "                       -6.6528e-03, -3.4790e-03],\n",
       "                      [-5.4016e-03, -2.9144e-03, -9.3384e-03,  ..., -9.4604e-03,\n",
       "                       -9.2773e-03,  6.5918e-03],\n",
       "                      [ 2.3804e-03,  7.5073e-03, -1.0986e-02,  ..., -3.6163e-03,\n",
       "                       -6.3477e-03,  7.5912e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0072,  0.0144, -0.0172,  ..., -0.0383,  0.0439, -0.0027],\n",
       "                      [-0.0068,  0.0211, -0.0427,  ...,  0.0123,  0.0095,  0.0153],\n",
       "                      [ 0.0139, -0.0544, -0.0391,  ...,  0.0194, -0.0630, -0.0208],\n",
       "                      ...,\n",
       "                      [ 0.0078,  0.0187,  0.0199,  ..., -0.0065,  0.0688,  0.0374],\n",
       "                      [ 0.0259,  0.0288,  0.0229,  ...,  0.0023,  0.0148, -0.0190],\n",
       "                      [-0.0087, -0.0287, -0.0400,  ...,  0.0104, -0.0310,  0.0115]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0073, -0.0103,  0.0141,  ...,  0.0153,  0.0043, -0.0138],\n",
       "                      [ 0.0023, -0.0077,  0.0073,  ..., -0.0075, -0.0153, -0.0078],\n",
       "                      [ 0.0084, -0.0101,  0.0060,  ..., -0.0155, -0.0053,  0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0042,  0.0087,  0.0153,  ...,  0.0120, -0.0038,  0.0074],\n",
       "                      [ 0.0140, -0.0078, -0.0131,  ...,  0.0131,  0.0132,  0.0017],\n",
       "                      [ 0.0121, -0.0013, -0.0125,  ..., -0.0067, -0.0031, -0.0124]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0057, -0.0493,  0.0190,  ..., -0.0352,  0.0217, -0.0508],\n",
       "                      [ 0.0099,  0.0161,  0.0153,  ..., -0.0036, -0.0005, -0.0237],\n",
       "                      [-0.0128, -0.0049,  0.0249,  ..., -0.0400,  0.0222, -0.0391],\n",
       "                      ...,\n",
       "                      [ 0.0141,  0.0070,  0.0361,  ...,  0.0098,  0.0153,  0.0151],\n",
       "                      [ 0.0031, -0.0008,  0.0013,  ..., -0.0089,  0.0033, -0.0243],\n",
       "                      [-0.0201,  0.0115,  0.0262,  ...,  0.0069, -0.0029, -0.0430]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0040, -0.0101, -0.0129,  ..., -0.0039, -0.0007,  0.0069],\n",
       "                      [ 0.0073,  0.0115,  0.0078,  ..., -0.0092, -0.0105, -0.0132],\n",
       "                      [ 0.0089, -0.0005,  0.0069,  ..., -0.0114,  0.0081, -0.0057],\n",
       "                      ...,\n",
       "                      [-0.0128, -0.0156, -0.0098,  ..., -0.0144, -0.0082,  0.0078],\n",
       "                      [ 0.0129, -0.0121,  0.0057,  ..., -0.0114,  0.0079, -0.0098],\n",
       "                      [ 0.0015, -0.0108, -0.0117,  ...,  0.0067,  0.0117,  0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0025,  0.0281,  0.0175,  ..., -0.0447, -0.0098, -0.0205],\n",
       "                      [ 0.0090,  0.0613,  0.0013,  ..., -0.0018, -0.0251,  0.0698],\n",
       "                      [-0.0237,  0.0199,  0.0070,  ...,  0.0344, -0.0449,  0.0620],\n",
       "                      ...,\n",
       "                      [-0.0310, -0.0172, -0.0255,  ..., -0.0143,  0.0187, -0.0030],\n",
       "                      [-0.0253,  0.0139,  0.0203,  ..., -0.0289,  0.0012,  0.0020],\n",
       "                      [ 0.0032, -0.0271, -0.0339,  ..., -0.0393,  0.0334,  0.0315]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0121,  0.0028,  0.0034,  ..., -0.0060,  0.0062,  0.0143],\n",
       "                      [-0.0144,  0.0098,  0.0044,  ..., -0.0034,  0.0076, -0.0114],\n",
       "                      [ 0.0120, -0.0049,  0.0081,  ...,  0.0128, -0.0089, -0.0007],\n",
       "                      ...,\n",
       "                      [-0.0052,  0.0007, -0.0099,  ...,  0.0150, -0.0151,  0.0058],\n",
       "                      [ 0.0052,  0.0019,  0.0081,  ..., -0.0023,  0.0050, -0.0065],\n",
       "                      [ 0.0031,  0.0025,  0.0078,  ...,  0.0072, -0.0115, -0.0132]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0251, -0.0043,  0.0189,  ...,  0.0033, -0.0092,  0.0013],\n",
       "                      [-0.0118,  0.0325, -0.0208,  ..., -0.0095,  0.0156,  0.0004],\n",
       "                      [ 0.0058, -0.0068, -0.0576,  ...,  0.0255, -0.0405, -0.0172],\n",
       "                      ...,\n",
       "                      [-0.0151, -0.0320, -0.0233,  ...,  0.0097,  0.0292, -0.0312],\n",
       "                      [ 0.0153,  0.0194, -0.0493,  ...,  0.0273,  0.0147,  0.0071],\n",
       "                      [ 0.0021, -0.0135,  0.0012,  ..., -0.0752,  0.0031, -0.0011]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-6.8665e-03, -3.5400e-03,  5.9814e-03,  ..., -7.2632e-03,\n",
       "                        5.5075e-05, -2.3956e-03],\n",
       "                      [ 4.6730e-04, -2.8229e-03,  8.9111e-03,  ..., -1.4648e-02,\n",
       "                        4.5471e-03, -5.4932e-03],\n",
       "                      [-5.1498e-04, -1.2085e-02,  1.8997e-03,  ..., -1.3275e-03,\n",
       "                       -4.4861e-03, -5.3406e-03],\n",
       "                      ...,\n",
       "                      [-2.4872e-03, -1.1414e-02, -1.5442e-02,  ..., -2.6512e-04,\n",
       "                        8.3618e-03, -9.7275e-04],\n",
       "                      [ 7.8125e-03,  1.1108e-02,  1.4587e-02,  ...,  1.0452e-03,\n",
       "                       -2.9907e-03,  3.7842e-03],\n",
       "                      [-1.4343e-02, -1.5015e-02,  1.0437e-02,  ..., -1.0925e-02,\n",
       "                        2.3956e-03, -9.7046e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.down_proj.weight',\n",
       "              tensor([[ 2.2736e-03,  2.8687e-02, -4.0039e-02,  ..., -1.1475e-02,\n",
       "                       -2.1240e-02, -2.6489e-02],\n",
       "                      [ 3.2471e-02,  6.4453e-02, -2.6489e-02,  ...,  2.9297e-02,\n",
       "                        1.1902e-02, -4.0527e-02],\n",
       "                      [ 1.2878e-02,  3.0100e-06, -2.6550e-03,  ..., -1.8677e-02,\n",
       "                        2.5269e-02,  1.7822e-02],\n",
       "                      ...,\n",
       "                      [ 3.0823e-03, -1.7700e-02, -4.1260e-02,  ..., -1.3367e-02,\n",
       "                        4.2114e-03, -1.0681e-02],\n",
       "                      [ 4.5654e-02, -6.8970e-03, -4.4434e-02,  ..., -8.6670e-03,\n",
       "                        1.2054e-03,  4.9072e-02],\n",
       "                      [-7.5073e-03, -2.9297e-02, -2.1667e-03,  ..., -5.1758e-02,\n",
       "                        1.5625e-02, -1.7090e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 6.6833e-03, -9.2163e-03,  2.3346e-03,  ..., -5.9814e-03,\n",
       "                       -5.4932e-03, -2.9602e-03],\n",
       "                      [ 8.1787e-03, -6.2561e-03, -2.0294e-03,  ..., -8.5449e-03,\n",
       "                        5.3101e-03, -3.8757e-03],\n",
       "                      [ 3.2806e-03, -3.9978e-03, -4.5471e-03,  ...,  9.0332e-03,\n",
       "                        8.2397e-03,  2.4796e-05],\n",
       "                      ...,\n",
       "                      [-1.4114e-03, -2.1820e-03, -1.3580e-03,  ...,  7.4463e-03,\n",
       "                       -8.0566e-03, -5.9814e-03],\n",
       "                      [-4.8828e-03,  4.7684e-04,  2.5940e-03,  ..., -7.4768e-03,\n",
       "                       -4.2725e-03, -3.7231e-03],\n",
       "                      [-1.3885e-03,  4.2725e-03,  1.5945e-03,  ..., -1.1063e-03,\n",
       "                        7.3853e-03,  3.8605e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.input_layernorm.weight',\n",
       "              tensor([0.4336, 0.4238, 0.4434,  ..., 0.4316, 0.4375, 0.4395],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.23.post_attention_layernorm.weight',\n",
       "              tensor([0.3516, 0.3457, 0.3340,  ..., 0.3457, 0.3496, 0.3496],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0050,  0.0117, -0.0245,  ...,  0.0073,  0.0016,  0.0002],\n",
       "                      [-0.0308, -0.0047, -0.0113,  ..., -0.0095,  0.0056, -0.0742],\n",
       "                      [-0.0267, -0.0371, -0.0437,  ...,  0.0084, -0.0064,  0.0070],\n",
       "                      ...,\n",
       "                      [-0.0148,  0.0128, -0.0014,  ...,  0.0029,  0.0442, -0.0295],\n",
       "                      [-0.0146, -0.0095,  0.0003,  ...,  0.0264, -0.0408,  0.0195],\n",
       "                      [-0.0198, -0.0025, -0.0281,  ...,  0.0237,  0.0096,  0.0267]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0032, -0.0117, -0.0055,  ..., -0.0153,  0.0023, -0.0107],\n",
       "                      [-0.0051,  0.0069, -0.0018,  ..., -0.0088,  0.0084,  0.0027],\n",
       "                      [-0.0013,  0.0102, -0.0090,  ..., -0.0138,  0.0081,  0.0012],\n",
       "                      ...,\n",
       "                      [-0.0071, -0.0106, -0.0149,  ...,  0.0027,  0.0142,  0.0070],\n",
       "                      [ 0.0036, -0.0010, -0.0094,  ...,  0.0153, -0.0032, -0.0013],\n",
       "                      [ 0.0065, -0.0012, -0.0018,  ...,  0.0129, -0.0112,  0.0148]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.k_proj.weight',\n",
       "              tensor([[ 3.0151e-02,  2.2583e-02, -5.2795e-03,  ...,  1.8311e-02,\n",
       "                       -2.6733e-02,  9.5215e-03],\n",
       "                      [-3.2959e-02,  9.3994e-03,  3.3936e-02,  ..., -1.6724e-02,\n",
       "                        2.4080e-05, -4.3457e-02],\n",
       "                      [-2.3346e-03, -1.6357e-02, -2.3438e-02,  ..., -2.1118e-02,\n",
       "                       -2.0264e-02, -6.1340e-03],\n",
       "                      ...,\n",
       "                      [ 9.7656e-03,  4.4434e-02,  1.2512e-03,  ...,  2.9297e-02,\n",
       "                        6.5918e-02, -5.6152e-02],\n",
       "                      [ 2.7313e-03, -1.2817e-02,  1.0315e-02,  ..., -8.3008e-03,\n",
       "                       -1.7212e-02, -1.8463e-03],\n",
       "                      [-1.0223e-03,  1.4160e-02, -1.9409e-02,  ..., -8.2397e-03,\n",
       "                       -4.1992e-02,  5.8899e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-5.1575e-03,  4.7302e-04,  8.4229e-03,  ..., -6.6223e-03,\n",
       "                        1.0010e-02, -3.8910e-03],\n",
       "                      [-6.7139e-04,  1.7776e-03,  3.5400e-03,  ...,  1.1902e-02,\n",
       "                        4.2114e-03, -3.2959e-03],\n",
       "                      [ 8.5449e-03, -1.4771e-02,  1.0071e-02,  ...,  9.8877e-03,\n",
       "                       -7.8735e-03,  9.8267e-03],\n",
       "                      ...,\n",
       "                      [-1.4526e-02, -1.3428e-03, -1.1047e-02,  ...,  9.3994e-03,\n",
       "                        8.4839e-03, -7.2956e-05],\n",
       "                      [ 9.3994e-03, -1.2665e-03, -1.6251e-03,  ..., -7.2937e-03,\n",
       "                        1.2146e-02, -1.5442e-02],\n",
       "                      [ 3.6316e-03,  8.6670e-03, -1.6708e-03,  ...,  1.5640e-03,\n",
       "                       -1.5503e-02, -7.2937e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0137, -0.0299, -0.0121,  ...,  0.0212, -0.0160,  0.0190],\n",
       "                      [ 0.0195, -0.0040, -0.0417,  ..., -0.0535, -0.0879, -0.0015],\n",
       "                      [-0.0845, -0.0137, -0.0024,  ...,  0.0503,  0.0098, -0.0160],\n",
       "                      ...,\n",
       "                      [ 0.0284,  0.0152,  0.0152,  ..., -0.0239,  0.0435,  0.0559],\n",
       "                      [-0.0310, -0.0308, -0.0396,  ..., -0.0126, -0.0014, -0.0063],\n",
       "                      [ 0.0025,  0.0265, -0.0315,  ..., -0.0083, -0.0040,  0.0173]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-3.6469e-03, -1.1520e-03, -2.7924e-03,  ..., -8.6670e-03,\n",
       "                       -1.5015e-02, -3.5858e-04],\n",
       "                      [-3.9291e-04,  7.0801e-03, -1.2512e-02,  ..., -2.1973e-03,\n",
       "                       -2.1362e-03,  3.7994e-03],\n",
       "                      [-8.6670e-03, -5.0354e-03,  7.1716e-03,  ...,  7.9346e-03,\n",
       "                        1.0681e-02,  1.2207e-02],\n",
       "                      ...,\n",
       "                      [ 8.5449e-03,  2.9325e-05,  1.4465e-02,  ..., -1.0620e-02,\n",
       "                       -7.1411e-03, -2.7313e-03],\n",
       "                      [-4.3335e-03, -1.4221e-02,  4.9591e-04,  ...,  1.2939e-02,\n",
       "                        1.5259e-02,  1.1169e-02],\n",
       "                      [-5.0354e-03,  4.1199e-03, -5.0964e-03,  ...,  1.2695e-02,\n",
       "                       -2.4261e-03,  8.5449e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0361, -0.0133, -0.0090,  ..., -0.0090,  0.0137,  0.0244],\n",
       "                      [-0.0240,  0.0201, -0.0255,  ...,  0.0126,  0.0014,  0.0021],\n",
       "                      [ 0.0057,  0.0251,  0.0219,  ...,  0.0006,  0.0312, -0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0113,  0.0396, -0.0178,  ...,  0.0181,  0.0107,  0.0240],\n",
       "                      [ 0.0347,  0.0077,  0.0192,  ..., -0.0082, -0.0374,  0.0039],\n",
       "                      [ 0.0078, -0.0017, -0.0021,  ...,  0.0046, -0.0237,  0.0234]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0003, -0.0155,  0.0006,  ..., -0.0047, -0.0092,  0.0156],\n",
       "                      [ 0.0039,  0.0015,  0.0024,  ..., -0.0085, -0.0003,  0.0107],\n",
       "                      [ 0.0049,  0.0107,  0.0120,  ..., -0.0120, -0.0038,  0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0135, -0.0084, -0.0027,  ...,  0.0037,  0.0044,  0.0099],\n",
       "                      [-0.0044, -0.0067,  0.0052,  ..., -0.0067, -0.0109,  0.0068],\n",
       "                      [ 0.0114, -0.0076, -0.0125,  ...,  0.0058,  0.0042,  0.0101]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0376, -0.0096,  0.0366,  ..., -0.0190,  0.0045, -0.0126],\n",
       "                      [ 0.0073,  0.0142, -0.0084,  ...,  0.0476, -0.0090,  0.0239],\n",
       "                      [ 0.0293, -0.0162,  0.0089,  ..., -0.0237, -0.0417,  0.0049],\n",
       "                      ...,\n",
       "                      [-0.0259, -0.0181, -0.0143,  ..., -0.0295, -0.0019, -0.0352],\n",
       "                      [-0.0043,  0.0104,  0.0106,  ...,  0.0019, -0.0291, -0.0130],\n",
       "                      [-0.0311, -0.0001, -0.0111,  ...,  0.0356,  0.0029, -0.0417]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0025,  0.0104,  0.0080,  ...,  0.0134,  0.0013, -0.0006],\n",
       "                      [ 0.0028,  0.0085, -0.0010,  ..., -0.0155, -0.0021, -0.0143],\n",
       "                      [ 0.0136, -0.0034,  0.0124,  ...,  0.0118, -0.0101, -0.0144],\n",
       "                      ...,\n",
       "                      [ 0.0093, -0.0148,  0.0079,  ..., -0.0115, -0.0061, -0.0117],\n",
       "                      [-0.0127, -0.0151, -0.0053,  ..., -0.0064, -0.0118,  0.0097],\n",
       "                      [ 0.0060,  0.0052, -0.0144,  ..., -0.0123,  0.0078,  0.0012]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.up_proj.weight',\n",
       "              tensor([[-0.0166, -0.0240,  0.0498,  ...,  0.0203,  0.0371,  0.0359],\n",
       "                      [-0.0342,  0.0166, -0.0054,  ..., -0.0140, -0.0150,  0.0079],\n",
       "                      [-0.0381, -0.0239,  0.0109,  ...,  0.0457,  0.0093, -0.0093],\n",
       "                      ...,\n",
       "                      [ 0.0156, -0.0544,  0.0469,  ..., -0.0095,  0.0320, -0.0157],\n",
       "                      [ 0.0010, -0.0598,  0.0151,  ..., -0.0261,  0.0160, -0.0221],\n",
       "                      [ 0.0476, -0.0294,  0.0161,  ...,  0.0065,  0.0131, -0.0097]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0137,  0.0047, -0.0150,  ..., -0.0123,  0.0118, -0.0042],\n",
       "                      [-0.0111, -0.0001,  0.0010,  ...,  0.0087,  0.0011,  0.0070],\n",
       "                      [ 0.0015,  0.0003, -0.0051,  ...,  0.0009,  0.0108,  0.0145],\n",
       "                      ...,\n",
       "                      [-0.0069, -0.0132, -0.0034,  ..., -0.0067,  0.0128, -0.0014],\n",
       "                      [ 0.0103,  0.0086,  0.0065,  ..., -0.0048, -0.0092, -0.0061],\n",
       "                      [-0.0033,  0.0011, -0.0027,  ..., -0.0062,  0.0032, -0.0082]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.down_proj.weight',\n",
       "              tensor([[-0.0295, -0.0201, -0.0164,  ...,  0.0537,  0.0227, -0.0444],\n",
       "                      [-0.0045, -0.0008,  0.0278,  ..., -0.0130, -0.0014, -0.0437],\n",
       "                      [ 0.0099,  0.0146, -0.0236,  ..., -0.0101, -0.0055, -0.0008],\n",
       "                      ...,\n",
       "                      [-0.0220, -0.0272, -0.0099,  ..., -0.0278,  0.0093, -0.0049],\n",
       "                      [-0.0222,  0.0052, -0.0044,  ...,  0.0364,  0.0018,  0.0320],\n",
       "                      [ 0.0067, -0.0090, -0.0120,  ...,  0.0322,  0.0018, -0.0317]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-7.7438e-04, -7.4463e-03, -2.6855e-03,  ..., -9.0942e-03,\n",
       "                       -1.6632e-03, -7.2937e-03],\n",
       "                      [ 7.7209e-03, -7.7209e-03,  8.6670e-03,  ..., -6.7139e-03,\n",
       "                       -1.2894e-03, -7.3853e-03],\n",
       "                      [ 8.3008e-03,  7.0801e-03, -8.9722e-03,  ...,  5.8899e-03,\n",
       "                        2.2125e-03, -8.1177e-03],\n",
       "                      ...,\n",
       "                      [ 4.5967e-04,  6.9275e-03,  3.0823e-03,  ..., -4.4250e-03,\n",
       "                       -9.9182e-04,  9.3842e-04],\n",
       "                      [ 6.4087e-04, -3.4523e-04,  9.4175e-06,  ..., -7.1716e-03,\n",
       "                        1.2589e-03, -3.7689e-03],\n",
       "                      [-8.8120e-04,  8.3618e-03,  5.7373e-03,  ..., -1.4572e-03,\n",
       "                        7.5684e-03, -4.9744e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.input_layernorm.weight',\n",
       "              tensor([0.4082, 0.4180, 0.4375,  ..., 0.4141, 0.4297, 0.4141],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.24.post_attention_layernorm.weight',\n",
       "              tensor([0.3555, 0.3574, 0.3535,  ..., 0.3613, 0.3633, 0.3574],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0320, -0.0289, -0.0092,  ...,  0.0165,  0.0028, -0.0014],\n",
       "                      [-0.0123, -0.0286,  0.0087,  ..., -0.0072, -0.0160,  0.0083],\n",
       "                      [ 0.0071, -0.0037,  0.0120,  ..., -0.0038,  0.0054,  0.0070],\n",
       "                      ...,\n",
       "                      [ 0.0520, -0.0123, -0.0220,  ..., -0.0051, -0.0148, -0.0267],\n",
       "                      [-0.0505, -0.0620,  0.0052,  ..., -0.0464, -0.0276,  0.0015],\n",
       "                      [ 0.0376, -0.0449, -0.0223,  ...,  0.0537,  0.0116, -0.0272]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-7.2327e-03,  1.3977e-02, -1.3245e-02,  ..., -1.5015e-02,\n",
       "                       -1.1841e-02,  3.3875e-03],\n",
       "                      [ 1.3123e-02,  1.4404e-02,  1.9431e-05,  ...,  1.0071e-03,\n",
       "                        2.6245e-03,  5.6763e-03],\n",
       "                      [-1.2756e-02, -1.2878e-02, -1.5564e-03,  ..., -5.6458e-03,\n",
       "                       -1.4221e-02, -8.9111e-03],\n",
       "                      ...,\n",
       "                      [-8.7280e-03,  1.5198e-02,  2.9602e-03,  ...,  2.3041e-03,\n",
       "                       -8.6670e-03, -5.5847e-03],\n",
       "                      [ 1.1047e-02, -1.3000e-02, -1.8997e-03,  ..., -3.9978e-03,\n",
       "                       -1.2573e-02, -4.0283e-03],\n",
       "                      [-1.1597e-02, -3.0365e-03,  1.0132e-02,  ..., -1.4832e-02,\n",
       "                       -1.3123e-02, -6.5918e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0012,  0.0064, -0.0004,  ..., -0.0054,  0.0026,  0.0013],\n",
       "                      [ 0.0073, -0.0266, -0.0037,  ...,  0.0061,  0.0101,  0.0025],\n",
       "                      [-0.0033, -0.0098,  0.0059,  ...,  0.0058, -0.0063, -0.0130],\n",
       "                      ...,\n",
       "                      [-0.0092,  0.0327, -0.0430,  ..., -0.0179,  0.0049,  0.0027],\n",
       "                      [ 0.0292, -0.0505, -0.0342,  ..., -0.0176,  0.0145, -0.0125],\n",
       "                      [-0.0020,  0.0140, -0.0581,  ...,  0.0698, -0.0269, -0.0093]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0022, -0.0101, -0.0073,  ..., -0.0109, -0.0131, -0.0121],\n",
       "                      [-0.0016, -0.0150,  0.0129,  ..., -0.0114,  0.0024,  0.0079],\n",
       "                      [-0.0156, -0.0025, -0.0124,  ...,  0.0095,  0.0067,  0.0030],\n",
       "                      ...,\n",
       "                      [-0.0123, -0.0074,  0.0075,  ...,  0.0090,  0.0118,  0.0083],\n",
       "                      [-0.0087, -0.0081, -0.0021,  ..., -0.0084, -0.0146, -0.0070],\n",
       "                      [ 0.0055,  0.0120,  0.0135,  ..., -0.0010, -0.0087,  0.0070]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0309,  0.0288, -0.0186,  ..., -0.0092, -0.0104, -0.0114],\n",
       "                      [-0.0076, -0.0165,  0.0004,  ...,  0.0742, -0.0449,  0.0354],\n",
       "                      [-0.0258,  0.0152, -0.0515,  ..., -0.0203, -0.0312,  0.0243],\n",
       "                      ...,\n",
       "                      [-0.0552,  0.0046,  0.0334,  ...,  0.0388,  0.0542,  0.0374],\n",
       "                      [-0.0366,  0.0381, -0.0530,  ...,  0.0211,  0.0593,  0.0306],\n",
       "                      [-0.0376,  0.0248,  0.0398,  ...,  0.0157,  0.0284,  0.0366]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0134,  0.0086, -0.0053,  ..., -0.0109,  0.0139,  0.0007],\n",
       "                      [ 0.0132, -0.0094,  0.0094,  ...,  0.0043, -0.0129, -0.0003],\n",
       "                      [-0.0097,  0.0043,  0.0113,  ..., -0.0050,  0.0142, -0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0123,  0.0099, -0.0155,  ..., -0.0108, -0.0074,  0.0002],\n",
       "                      [ 0.0029,  0.0081, -0.0093,  ..., -0.0016,  0.0095,  0.0112],\n",
       "                      [ 0.0030,  0.0148, -0.0096,  ..., -0.0095, -0.0106,  0.0134]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0114,  0.0189, -0.0222,  ...,  0.0417, -0.0220, -0.0237],\n",
       "                      [-0.0110, -0.0142, -0.0028,  ...,  0.0359, -0.0515,  0.0231],\n",
       "                      [ 0.0047, -0.0287,  0.0236,  ..., -0.0215,  0.0014, -0.0168],\n",
       "                      ...,\n",
       "                      [-0.0228,  0.0028,  0.0049,  ...,  0.0129, -0.0031,  0.0486],\n",
       "                      [-0.0339, -0.0244, -0.0148,  ..., -0.0479,  0.0537, -0.0299],\n",
       "                      [-0.0306,  0.0250, -0.0684,  ..., -0.0233, -0.0299, -0.0146]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0009, -0.0072, -0.0135,  ..., -0.0137,  0.0046,  0.0126],\n",
       "                      [ 0.0140,  0.0061,  0.0078,  ..., -0.0087, -0.0060, -0.0084],\n",
       "                      [-0.0031,  0.0016,  0.0142,  ...,  0.0121, -0.0071,  0.0078],\n",
       "                      ...,\n",
       "                      [-0.0022,  0.0060,  0.0103,  ...,  0.0054, -0.0073,  0.0150],\n",
       "                      [-0.0100,  0.0087, -0.0062,  ..., -0.0148, -0.0103,  0.0137],\n",
       "                      [-0.0104, -0.0097, -0.0054,  ..., -0.0079,  0.0028,  0.0101]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0160, -0.0271,  0.0055,  ...,  0.0087,  0.0110,  0.0172],\n",
       "                      [-0.0249,  0.0039,  0.0145,  ...,  0.0168,  0.0164,  0.0491],\n",
       "                      [-0.0271,  0.0400, -0.0082,  ..., -0.0170, -0.0035,  0.0188],\n",
       "                      ...,\n",
       "                      [-0.0439, -0.0013,  0.0082,  ..., -0.0061, -0.0417,  0.0201],\n",
       "                      [ 0.0056, -0.0181,  0.0273,  ...,  0.0408,  0.0457,  0.0093],\n",
       "                      [ 0.0334, -0.0053, -0.0297,  ...,  0.0557,  0.0295,  0.0011]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 5.0964e-03, -3.7994e-03,  1.2695e-02,  ..., -2.1515e-03,\n",
       "                        8.3618e-03, -1.2085e-02],\n",
       "                      [-2.4109e-03, -8.8501e-03,  1.3428e-02,  ..., -1.0925e-02,\n",
       "                       -1.0681e-02, -4.2915e-05],\n",
       "                      [ 1.2634e-02, -7.2479e-04, -1.3245e-02,  ...,  7.5684e-03,\n",
       "                       -2.1973e-03, -1.3733e-02],\n",
       "                      ...,\n",
       "                      [ 8.7738e-04, -4.0588e-03,  9.2163e-03,  ..., -2.6512e-04,\n",
       "                        4.1199e-04,  6.9885e-03],\n",
       "                      [-5.6152e-03, -9.7046e-03, -1.2207e-02,  ...,  1.1658e-02,\n",
       "                       -1.4038e-02,  1.3855e-02],\n",
       "                      [ 1.0254e-02, -8.7891e-03,  1.4099e-02,  ...,  6.1951e-03,\n",
       "                       -1.5137e-02,  5.7373e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.up_proj.weight',\n",
       "              tensor([[-1.0620e-02, -3.2349e-03,  2.9419e-02,  ...,  7.3853e-03,\n",
       "                       -6.2561e-03,  2.0981e-05],\n",
       "                      [-3.7109e-02, -1.1063e-03,  6.6223e-03,  ..., -1.3916e-02,\n",
       "                       -3.9062e-02,  9.9487e-03],\n",
       "                      [ 3.2471e-02, -1.3672e-02, -1.3000e-02,  ..., -1.0864e-02,\n",
       "                       -2.1851e-02,  1.8921e-02],\n",
       "                      ...,\n",
       "                      [-6.7444e-03, -2.9907e-03,  4.4434e-02,  ..., -1.9165e-02,\n",
       "                       -1.4709e-02, -4.9316e-02],\n",
       "                      [ 1.9226e-03,  1.8463e-03, -4.3701e-02,  ..., -3.2959e-02,\n",
       "                       -3.4027e-03,  2.3193e-03],\n",
       "                      [ 3.3936e-02,  1.9897e-02,  1.6113e-02,  ...,  4.0283e-03,\n",
       "                       -1.5625e-02,  4.5410e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-1.2634e-02,  4.2114e-03,  2.6584e-05,  ...,  1.2146e-02,\n",
       "                        1.0437e-02, -9.2773e-03],\n",
       "                      [ 3.2349e-03, -7.7515e-03,  1.1597e-02,  ..., -6.1951e-03,\n",
       "                       -1.6479e-03,  5.7068e-03],\n",
       "                      [ 1.2451e-02,  1.1902e-02,  9.0332e-03,  ..., -1.3306e-02,\n",
       "                        1.4465e-02, -7.7820e-03],\n",
       "                      ...,\n",
       "                      [ 6.2561e-03,  1.0803e-02,  2.2278e-03,  ...,  1.4038e-02,\n",
       "                        1.0193e-02, -1.3062e-02],\n",
       "                      [ 4.9438e-03, -2.6398e-03,  5.7983e-03,  ...,  1.5564e-02,\n",
       "                       -1.1169e-02, -2.4414e-04],\n",
       "                      [-1.3184e-02, -8.7280e-03,  1.1902e-02,  ...,  4.2725e-03,\n",
       "                       -1.0559e-02,  9.7656e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.down_proj.weight',\n",
       "              tensor([[-0.0113, -0.0081, -0.0118,  ..., -0.0206, -0.0361,  0.0144],\n",
       "                      [ 0.0026, -0.0047, -0.0092,  ...,  0.0010,  0.0076,  0.0186],\n",
       "                      [ 0.0371, -0.0215, -0.0228,  ..., -0.0175,  0.0011,  0.0112],\n",
       "                      ...,\n",
       "                      [-0.0087, -0.0265, -0.0325,  ..., -0.0415, -0.0033,  0.0354],\n",
       "                      [-0.0256,  0.0099,  0.0078,  ...,  0.0104,  0.0410, -0.0084],\n",
       "                      [-0.0352, -0.0074, -0.0157,  ..., -0.0104,  0.0094,  0.0091]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-2.3346e-03, -3.6926e-03,  4.6387e-03,  ..., -5.4932e-03,\n",
       "                       -1.6708e-03,  4.7913e-03],\n",
       "                      [ 8.6670e-03,  5.4626e-03,  8.0490e-04,  ...,  4.4556e-03,\n",
       "                        1.4725e-03, -8.3008e-03],\n",
       "                      [-5.5542e-03,  2.1210e-03,  9.3460e-05,  ..., -8.9722e-03,\n",
       "                        1.1215e-03, -1.7624e-03],\n",
       "                      ...,\n",
       "                      [ 8.9111e-03, -3.0670e-03, -1.9169e-04,  ..., -8.3008e-03,\n",
       "                        8.8501e-03, -4.7913e-03],\n",
       "                      [ 3.2959e-03, -8.1177e-03,  1.1826e-03,  ...,  6.8665e-03,\n",
       "                       -7.0801e-03,  4.5166e-03],\n",
       "                      [ 1.6632e-03, -6.8359e-03, -4.9591e-04,  ...,  5.4016e-03,\n",
       "                        9.3994e-03, -6.2180e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.input_layernorm.weight',\n",
       "              tensor([0.4355, 0.4512, 0.4551,  ..., 0.4375, 0.4473, 0.4512],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.25.post_attention_layernorm.weight',\n",
       "              tensor([0.3652, 0.3613, 0.3613,  ..., 0.3730, 0.3730, 0.3652],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0135, -0.0214, -0.0058,  ..., -0.0391,  0.0078, -0.0457],\n",
       "                      [-0.0186,  0.0040, -0.0046,  ..., -0.0106, -0.0151,  0.0027],\n",
       "                      [-0.0121, -0.0131,  0.0271,  ..., -0.0147, -0.0086, -0.0232],\n",
       "                      ...,\n",
       "                      [-0.0092, -0.0027, -0.0136,  ...,  0.0204,  0.0161, -0.0018],\n",
       "                      [-0.0120,  0.0137, -0.0110,  ..., -0.0374,  0.0073, -0.0369],\n",
       "                      [ 0.0356,  0.0415,  0.0420,  ..., -0.0264, -0.0081, -0.0415]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-9.5825e-03, -9.5825e-03, -6.6223e-03,  ..., -1.3489e-02,\n",
       "                       -3.8757e-03,  1.1902e-02],\n",
       "                      [-8.1787e-03, -9.3994e-03, -1.3489e-02,  ..., -2.8839e-03,\n",
       "                       -6.0730e-03,  9.3460e-04],\n",
       "                      [-5.4016e-03, -4.8828e-03, -1.1292e-02,  ...,  5.8594e-03,\n",
       "                       -7.3242e-03, -1.0315e-02],\n",
       "                      ...,\n",
       "                      [ 1.1353e-02,  9.5367e-05,  1.5320e-02,  ...,  7.4463e-03,\n",
       "                       -1.2146e-02,  1.1841e-02],\n",
       "                      [-1.4282e-02,  2.6550e-03, -1.8921e-03,  ...,  5.7068e-03,\n",
       "                       -1.4648e-02, -5.4932e-03],\n",
       "                      [-4.0894e-03,  3.2806e-03,  9.8877e-03,  ...,  1.0071e-02,\n",
       "                       -7.2632e-03,  1.4221e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0139,  0.0027, -0.0229,  ..., -0.0259,  0.0088, -0.0381],\n",
       "                      [-0.0120, -0.0078,  0.0009,  ..., -0.0298,  0.0168,  0.0153],\n",
       "                      [ 0.0203,  0.0026,  0.0449,  ...,  0.0048,  0.0156, -0.0040],\n",
       "                      ...,\n",
       "                      [ 0.0330,  0.0141, -0.0140,  ..., -0.0305, -0.0410,  0.0009],\n",
       "                      [ 0.0010,  0.0172,  0.0388,  ...,  0.0388, -0.0128, -0.0649],\n",
       "                      [-0.0278,  0.0306,  0.0349,  ...,  0.0195, -0.0256, -0.0557]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 1.1108e-02, -1.1230e-02,  9.0332e-03,  ...,  1.1414e-02,\n",
       "                       -8.7891e-03,  1.5137e-02],\n",
       "                      [-1.3855e-02,  2.0142e-03, -3.1662e-04,  ..., -7.4768e-03,\n",
       "                       -6.6223e-03, -1.3916e-02],\n",
       "                      [-7.0190e-04, -1.5320e-02,  1.3550e-02,  ..., -1.4221e-02,\n",
       "                       -3.5858e-03,  7.3242e-03],\n",
       "                      ...,\n",
       "                      [ 9.5215e-03, -1.2024e-02, -8.6670e-03,  ..., -1.5198e-02,\n",
       "                        1.1368e-03, -2.7537e-05],\n",
       "                      [-5.1270e-03, -9.3994e-03,  2.7771e-03,  ..., -6.7444e-03,\n",
       "                        6.4087e-04, -3.0670e-03],\n",
       "                      [-3.9978e-03,  2.6550e-03, -1.0132e-02,  ..., -2.1667e-03,\n",
       "                       -3.2043e-03,  1.4160e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0008, -0.0469,  0.0325,  ..., -0.0087,  0.0728, -0.0121],\n",
       "                      [ 0.0009, -0.0493,  0.0214,  ..., -0.0168, -0.0193, -0.0156],\n",
       "                      [-0.0125,  0.0374, -0.0398,  ...,  0.0031,  0.0161, -0.0366],\n",
       "                      ...,\n",
       "                      [-0.0166, -0.0315, -0.0028,  ...,  0.0405,  0.0055, -0.0330],\n",
       "                      [ 0.0153, -0.0413, -0.0221,  ..., -0.0498,  0.0195, -0.0151],\n",
       "                      [ 0.0097, -0.0270,  0.0061,  ...,  0.0107,  0.0386,  0.0698]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-8.3923e-05, -1.7242e-03, -1.1719e-02,  ...,  1.5259e-02,\n",
       "                        1.4343e-02,  1.3184e-02],\n",
       "                      [ 9.9487e-03, -9.5825e-03,  4.6387e-03,  ..., -1.3733e-02,\n",
       "                        1.2329e-02,  2.0905e-03],\n",
       "                      [ 7.2937e-03, -1.0864e-02, -7.5378e-03,  ...,  9.5825e-03,\n",
       "                       -1.4267e-03,  8.1177e-03],\n",
       "                      ...,\n",
       "                      [ 9.7046e-03, -9.2773e-03,  1.1597e-02,  ...,  6.0120e-03,\n",
       "                       -1.0681e-02, -6.3782e-03],\n",
       "                      [-5.8594e-03, -1.2329e-02, -2.9144e-03,  ...,  1.4160e-02,\n",
       "                        2.5482e-03,  8.3618e-03],\n",
       "                      [ 5.2185e-03, -1.0071e-03, -1.3062e-02,  ...,  1.1169e-02,\n",
       "                        1.1108e-02,  1.0132e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0249, -0.0532, -0.0247,  ...,  0.0053,  0.0142,  0.0500],\n",
       "                      [ 0.0298,  0.0119,  0.0134,  ...,  0.0182,  0.0432, -0.0095],\n",
       "                      [ 0.0007,  0.0168, -0.0427,  ...,  0.0024, -0.0005,  0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0261,  0.0299,  0.0016,  ..., -0.0072,  0.0168,  0.0513],\n",
       "                      [-0.0334,  0.0737, -0.0275,  ..., -0.0040, -0.0222, -0.0442],\n",
       "                      [-0.0107,  0.0130, -0.0023,  ..., -0.0148,  0.0079,  0.0112]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0138,  0.0021,  0.0046,  ..., -0.0008, -0.0034,  0.0064],\n",
       "                      [ 0.0154, -0.0042,  0.0003,  ..., -0.0154,  0.0012,  0.0090],\n",
       "                      [-0.0065, -0.0051, -0.0010,  ...,  0.0103, -0.0018,  0.0081],\n",
       "                      ...,\n",
       "                      [ 0.0027,  0.0026,  0.0137,  ...,  0.0047, -0.0135, -0.0033],\n",
       "                      [-0.0153, -0.0015,  0.0118,  ..., -0.0114,  0.0112,  0.0063],\n",
       "                      [ 0.0120,  0.0110, -0.0030,  ...,  0.0025,  0.0024, -0.0108]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0337, -0.0190,  0.0422,  ...,  0.0322,  0.0535, -0.0189],\n",
       "                      [ 0.0161,  0.0001, -0.0203,  ..., -0.0292, -0.0122,  0.0072],\n",
       "                      [ 0.0120, -0.0109, -0.0305,  ..., -0.0024, -0.0204, -0.0334],\n",
       "                      ...,\n",
       "                      [-0.0322,  0.0157, -0.0476,  ..., -0.0049,  0.0105, -0.0219],\n",
       "                      [ 0.0048,  0.0079,  0.0317,  ..., -0.0260, -0.0197, -0.0131],\n",
       "                      [-0.0223,  0.0549,  0.0371,  ..., -0.0344,  0.0210, -0.0166]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0022, -0.0133,  0.0048,  ..., -0.0091,  0.0114,  0.0110],\n",
       "                      [ 0.0146, -0.0070,  0.0133,  ..., -0.0065, -0.0093, -0.0059],\n",
       "                      [-0.0137, -0.0039, -0.0015,  ...,  0.0061, -0.0142,  0.0023],\n",
       "                      ...,\n",
       "                      [ 0.0046,  0.0137,  0.0143,  ..., -0.0080, -0.0117,  0.0139],\n",
       "                      [ 0.0002, -0.0053,  0.0123,  ..., -0.0049,  0.0064,  0.0089],\n",
       "                      [-0.0102,  0.0022,  0.0128,  ..., -0.0012, -0.0028, -0.0001]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0002,  0.0559, -0.0630,  ...,  0.0410,  0.0074, -0.0153],\n",
       "                      [-0.0231, -0.0110,  0.0141,  ...,  0.0569,  0.0272,  0.0267],\n",
       "                      [-0.0212,  0.0027, -0.0247,  ..., -0.0104, -0.0243, -0.0374],\n",
       "                      ...,\n",
       "                      [-0.0010, -0.0310,  0.0200,  ..., -0.0032, -0.0216,  0.0199],\n",
       "                      [ 0.0291,  0.0140,  0.0486,  ..., -0.0162, -0.0038,  0.0034],\n",
       "                      [-0.0410, -0.0084, -0.0187,  ..., -0.0243,  0.0125, -0.0259]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0013,  0.0053, -0.0098,  ..., -0.0138, -0.0127, -0.0089],\n",
       "                      [ 0.0114, -0.0001, -0.0078,  ...,  0.0154,  0.0025, -0.0108],\n",
       "                      [-0.0094,  0.0109,  0.0045,  ...,  0.0130,  0.0023,  0.0117],\n",
       "                      ...,\n",
       "                      [-0.0038,  0.0070, -0.0043,  ..., -0.0042,  0.0042,  0.0053],\n",
       "                      [-0.0115,  0.0090,  0.0119,  ...,  0.0043, -0.0086, -0.0068],\n",
       "                      [-0.0067, -0.0147, -0.0027,  ..., -0.0068,  0.0110,  0.0003]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0036, -0.0148, -0.0457,  ...,  0.0117, -0.0143,  0.0272],\n",
       "                      [ 0.0153,  0.0033,  0.0142,  ...,  0.0549,  0.0466, -0.0640],\n",
       "                      [ 0.0229, -0.0049, -0.0503,  ..., -0.0153,  0.0703, -0.0023],\n",
       "                      ...,\n",
       "                      [-0.0459,  0.0055,  0.0201,  ..., -0.0141,  0.0249,  0.0376],\n",
       "                      [-0.0088,  0.0054, -0.0378,  ...,  0.0026,  0.0264,  0.0361],\n",
       "                      [ 0.0398, -0.0085,  0.0223,  ...,  0.0080, -0.0146, -0.0129]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[ 6.4087e-03, -7.5684e-03,  9.3994e-03,  ...,  6.3782e-03,\n",
       "                        8.9722e-03,  7.0190e-03],\n",
       "                      [ 1.2517e-05,  1.9379e-03,  9.0332e-03,  ...,  2.1515e-03,\n",
       "                       -4.2725e-03, -3.0212e-03],\n",
       "                      [-8.8501e-03, -3.1853e-04, -3.3112e-03,  ...,  3.9062e-03,\n",
       "                       -5.8289e-03, -5.9509e-03],\n",
       "                      ...,\n",
       "                      [-3.6926e-03, -2.5177e-03,  4.2319e-06,  ..., -5.7373e-03,\n",
       "                        6.1035e-04, -5.4016e-03],\n",
       "                      [-2.3346e-03, -3.4904e-04, -6.7444e-03,  ...,  4.2419e-03,\n",
       "                       -6.3782e-03, -1.9836e-03],\n",
       "                      [ 3.4332e-03,  2.6550e-03, -6.1035e-03,  ...,  1.3504e-03,\n",
       "                        7.9956e-03,  8.8501e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.input_layernorm.weight',\n",
       "              tensor([0.4219, 0.4297, 0.4551,  ..., 0.4277, 0.4414, 0.4355],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.26.post_attention_layernorm.weight',\n",
       "              tensor([0.3848, 0.3789, 0.3711,  ..., 0.3887, 0.3867, 0.3887],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0040,  0.0067, -0.0110,  ..., -0.0272,  0.0129,  0.0015],\n",
       "                      [ 0.0243,  0.0079,  0.0050,  ...,  0.0287, -0.0209, -0.0021],\n",
       "                      [-0.0063, -0.0063, -0.0116,  ..., -0.0063, -0.0034, -0.0106],\n",
       "                      ...,\n",
       "                      [-0.0154,  0.0498,  0.0199,  ...,  0.0108, -0.0615, -0.0447],\n",
       "                      [ 0.0386,  0.0359,  0.0211,  ..., -0.0261, -0.0308,  0.0012],\n",
       "                      [-0.0256, -0.0027,  0.0040,  ...,  0.0267,  0.0269, -0.0045]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0132,  0.0031, -0.0044,  ...,  0.0145, -0.0028, -0.0144],\n",
       "                      [ 0.0078, -0.0110, -0.0135,  ...,  0.0081,  0.0015, -0.0092],\n",
       "                      [ 0.0140, -0.0042, -0.0110,  ...,  0.0080,  0.0135, -0.0057],\n",
       "                      ...,\n",
       "                      [-0.0044, -0.0057, -0.0026,  ..., -0.0048,  0.0114, -0.0095],\n",
       "                      [-0.0142, -0.0019,  0.0036,  ..., -0.0043, -0.0082,  0.0133],\n",
       "                      [ 0.0135,  0.0113, -0.0087,  ..., -0.0140, -0.0140,  0.0014]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0153, -0.0203,  0.0228,  ..., -0.0260,  0.0142,  0.0175],\n",
       "                      [-0.0065, -0.0035, -0.0052,  ..., -0.0206,  0.0312, -0.0381],\n",
       "                      [ 0.0025, -0.0337, -0.0143,  ..., -0.0223, -0.0272, -0.0233],\n",
       "                      ...,\n",
       "                      [-0.0148, -0.0452,  0.0300,  ...,  0.0160, -0.0474,  0.0098],\n",
       "                      [-0.0186, -0.0330, -0.0204,  ..., -0.0103, -0.0476,  0.0061],\n",
       "                      [ 0.0469,  0.0272,  0.0255,  ..., -0.0165, -0.0449, -0.0142]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0010, -0.0125,  0.0026,  ...,  0.0156, -0.0028,  0.0126],\n",
       "                      [ 0.0047, -0.0133, -0.0134,  ..., -0.0015,  0.0026, -0.0029],\n",
       "                      [ 0.0099, -0.0115,  0.0150,  ..., -0.0155, -0.0023, -0.0005],\n",
       "                      ...,\n",
       "                      [-0.0100,  0.0131, -0.0156,  ..., -0.0080,  0.0048, -0.0057],\n",
       "                      [-0.0026,  0.0039,  0.0016,  ...,  0.0115, -0.0039, -0.0128],\n",
       "                      [-0.0125, -0.0007,  0.0007,  ...,  0.0104, -0.0128,  0.0115]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0215,  0.0282, -0.0081,  ...,  0.0317, -0.0222, -0.0378],\n",
       "                      [ 0.0557, -0.0059,  0.0352,  ...,  0.0195,  0.0193, -0.0140],\n",
       "                      [-0.0219,  0.0247, -0.0063,  ...,  0.0283,  0.0134, -0.0082],\n",
       "                      ...,\n",
       "                      [ 0.0131,  0.0055, -0.0522,  ..., -0.0103, -0.0608,  0.0043],\n",
       "                      [ 0.0229,  0.0086, -0.0344,  ..., -0.0175,  0.0036, -0.0052],\n",
       "                      [ 0.0189, -0.0464,  0.0045,  ..., -0.0155, -0.0153, -0.0203]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[-1.0132e-02,  5.7068e-03, -1.7700e-03,  ..., -1.0376e-02,\n",
       "                       -1.0925e-02,  1.2695e-02],\n",
       "                      [-5.2795e-03,  8.3008e-03, -7.6599e-03,  ..., -3.6011e-03,\n",
       "                       -1.2573e-02, -1.3977e-02],\n",
       "                      [ 1.4648e-02, -6.8665e-03, -1.3367e-02,  ..., -9.6130e-04,\n",
       "                       -9.2773e-03, -1.5442e-02],\n",
       "                      ...,\n",
       "                      [-5.4169e-04,  1.3794e-02, -1.0864e-02,  ...,  1.2589e-04,\n",
       "                        1.5198e-02, -2.6855e-03],\n",
       "                      [-1.2268e-02, -1.0681e-02, -7.8735e-03,  ..., -5.9605e-05,\n",
       "                        1.5564e-02,  1.2512e-03],\n",
       "                      [ 1.1047e-02, -1.9684e-03, -1.3046e-03,  ..., -1.1658e-02,\n",
       "                        5.5542e-03,  3.3112e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.o_proj.weight',\n",
       "              tensor([[ 4.0039e-02, -4.2236e-02,  3.3203e-02,  ...,  1.5991e-02,\n",
       "                       -3.1006e-02, -1.9897e-02],\n",
       "                      [ 2.2949e-02, -9.4891e-05,  1.3428e-02,  ..., -6.2256e-03,\n",
       "                        2.3438e-02,  7.3242e-02],\n",
       "                      [-2.0874e-02, -9.0332e-03, -2.1118e-02,  ...,  8.5449e-03,\n",
       "                        3.2227e-02,  1.0315e-02],\n",
       "                      ...,\n",
       "                      [ 8.4229e-03, -8.3618e-03, -1.6479e-03,  ..., -2.0508e-02,\n",
       "                        5.4932e-02,  1.4465e-02],\n",
       "                      [ 3.1006e-02, -3.9062e-02,  4.3701e-02,  ...,  1.6235e-02,\n",
       "                        2.9663e-02,  3.1494e-02],\n",
       "                      [ 4.7119e-02,  5.0735e-04, -1.7944e-02,  ...,  3.1250e-02,\n",
       "                        1.0620e-02,  2.6367e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0056, -0.0012,  0.0133,  ...,  0.0156, -0.0006,  0.0045],\n",
       "                      [ 0.0112, -0.0001, -0.0014,  ..., -0.0068,  0.0026,  0.0003],\n",
       "                      [-0.0065, -0.0153, -0.0008,  ...,  0.0058, -0.0112, -0.0126],\n",
       "                      ...,\n",
       "                      [ 0.0106,  0.0101,  0.0069,  ...,  0.0153,  0.0126, -0.0082],\n",
       "                      [-0.0087, -0.0035,  0.0069,  ..., -0.0143,  0.0002, -0.0064],\n",
       "                      [-0.0015,  0.0047,  0.0086,  ...,  0.0053, -0.0015,  0.0002]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0371,  0.0153,  0.0192,  ...,  0.0056, -0.0344, -0.0036],\n",
       "                      [ 0.0258,  0.0186, -0.0347,  ...,  0.0210,  0.0352,  0.0337],\n",
       "                      [ 0.0052,  0.0065, -0.0112,  ...,  0.0022, -0.0571,  0.0398],\n",
       "                      ...,\n",
       "                      [ 0.0391,  0.0376,  0.0183,  ..., -0.0067, -0.0023, -0.0173],\n",
       "                      [-0.0001, -0.0034, -0.0045,  ...,  0.0084, -0.0179,  0.0060],\n",
       "                      [-0.0161, -0.0354,  0.0043,  ...,  0.0114,  0.0388, -0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0048,  0.0058, -0.0131,  ...,  0.0064,  0.0065,  0.0011],\n",
       "                      [ 0.0007,  0.0107,  0.0057,  ..., -0.0151, -0.0005,  0.0056],\n",
       "                      [-0.0097,  0.0008, -0.0133,  ..., -0.0023, -0.0073,  0.0047],\n",
       "                      ...,\n",
       "                      [ 0.0153, -0.0087, -0.0084,  ..., -0.0125, -0.0107, -0.0094],\n",
       "                      [-0.0053, -0.0099, -0.0023,  ...,  0.0148,  0.0125,  0.0084],\n",
       "                      [-0.0148,  0.0011,  0.0112,  ..., -0.0125,  0.0118, -0.0077]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0070,  0.0037,  0.0251,  ...,  0.0081,  0.0364, -0.0352],\n",
       "                      [-0.0060,  0.0033, -0.0366,  ..., -0.0052, -0.0364, -0.0021],\n",
       "                      [-0.0227,  0.0300,  0.0172,  ...,  0.0281, -0.0659, -0.0017],\n",
       "                      ...,\n",
       "                      [-0.0106, -0.0334,  0.0065,  ..., -0.0508,  0.0248,  0.0115],\n",
       "                      [-0.0109,  0.0302, -0.0025,  ...,  0.0079,  0.0095, -0.0101],\n",
       "                      [-0.0145,  0.0264,  0.0040,  ..., -0.0342, -0.0068,  0.0176]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 1.1597e-02,  7.8125e-03,  7.3242e-03,  ...,  4.3945e-03,\n",
       "                        1.3428e-02, -1.9836e-03],\n",
       "                      [ 1.3367e-02, -1.3672e-02, -5.4932e-04,  ..., -5.5552e-05,\n",
       "                        9.8419e-04, -1.2878e-02],\n",
       "                      [ 2.1057e-03, -2.3956e-03,  1.0223e-03,  ...,  8.7357e-04,\n",
       "                        3.5553e-03, -1.0925e-02],\n",
       "                      ...,\n",
       "                      [ 5.4016e-03, -1.4832e-02, -1.4771e-02,  ...,  7.9956e-03,\n",
       "                        1.3428e-02, -1.3977e-02],\n",
       "                      [-8.3542e-04,  1.4099e-02, -6.4087e-03,  ...,  1.1215e-03,\n",
       "                        3.9368e-03,  2.7776e-05],\n",
       "                      [ 1.0376e-02, -7.0496e-03,  3.0975e-03,  ..., -3.8910e-03,\n",
       "                        6.6223e-03,  8.6670e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0255, -0.0164, -0.0117,  ..., -0.0269,  0.0232,  0.0062],\n",
       "                      [-0.0277,  0.0073, -0.0173,  ..., -0.0236, -0.0276, -0.0133],\n",
       "                      [ 0.0170,  0.0674, -0.0153,  ..., -0.0474,  0.0598,  0.0106],\n",
       "                      ...,\n",
       "                      [ 0.0265, -0.0055, -0.0219,  ..., -0.0143, -0.0330,  0.0206],\n",
       "                      [ 0.0144,  0.0018,  0.0332,  ...,  0.0004,  0.0767,  0.0417],\n",
       "                      [ 0.0160,  0.0135, -0.0515,  ...,  0.0361, -0.0133,  0.0194]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0013, -0.0020, -0.0089,  ...,  0.0072, -0.0056,  0.0091],\n",
       "                      [-0.0006,  0.0033, -0.0013,  ...,  0.0007, -0.0063,  0.0020],\n",
       "                      [-0.0019,  0.0089,  0.0011,  ..., -0.0081,  0.0032,  0.0054],\n",
       "                      ...,\n",
       "                      [ 0.0083,  0.0019, -0.0068,  ...,  0.0008,  0.0072,  0.0085],\n",
       "                      [ 0.0065,  0.0016, -0.0060,  ..., -0.0032, -0.0019,  0.0028],\n",
       "                      [ 0.0002, -0.0084, -0.0011,  ...,  0.0014,  0.0074, -0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.input_layernorm.weight',\n",
       "              tensor([0.4609, 0.4629, 0.4727,  ..., 0.4746, 0.4590, 0.4629],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.27.post_attention_layernorm.weight',\n",
       "              tensor([0.4004, 0.3848, 0.3887,  ..., 0.3984, 0.4082, 0.3926],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0103, -0.0214, -0.0234,  ..., -0.0036, -0.0280, -0.0132],\n",
       "                      [ 0.0007,  0.0258, -0.0097,  ..., -0.0057, -0.0036, -0.0079],\n",
       "                      [ 0.0317,  0.0107,  0.0280,  ...,  0.0121, -0.0126, -0.0065],\n",
       "                      ...,\n",
       "                      [-0.0129,  0.0072, -0.0216,  ..., -0.0781,  0.0289, -0.0518],\n",
       "                      [-0.0024,  0.0176, -0.0245,  ...,  0.0283,  0.0432,  0.0097],\n",
       "                      [ 0.0165, -0.0238, -0.0272,  ...,  0.0347,  0.0581, -0.0124]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0067, -0.0030, -0.0107,  ..., -0.0029, -0.0151,  0.0090],\n",
       "                      [-0.0118,  0.0147,  0.0088,  ...,  0.0040,  0.0042,  0.0075],\n",
       "                      [ 0.0031, -0.0144,  0.0120,  ...,  0.0017, -0.0076,  0.0060],\n",
       "                      ...,\n",
       "                      [-0.0085,  0.0068,  0.0042,  ..., -0.0016, -0.0072,  0.0058],\n",
       "                      [ 0.0101,  0.0120,  0.0111,  ..., -0.0154, -0.0149, -0.0134],\n",
       "                      [-0.0145,  0.0023,  0.0078,  ..., -0.0064, -0.0153, -0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0077,  0.0114, -0.0023,  ..., -0.0095,  0.0025,  0.0001],\n",
       "                      [-0.0064,  0.0150,  0.0270,  ...,  0.0052,  0.0150,  0.0012],\n",
       "                      [ 0.0151,  0.0148,  0.0047,  ..., -0.0091, -0.0099,  0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0071,  0.0427,  0.0415,  ..., -0.0287, -0.0215,  0.0374],\n",
       "                      [ 0.0276, -0.0115,  0.0269,  ...,  0.0070, -0.0305, -0.0425],\n",
       "                      [-0.0029,  0.0312, -0.0057,  ..., -0.0304, -0.0713,  0.0483]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0017,  0.0140, -0.0128,  ...,  0.0086, -0.0027,  0.0120],\n",
       "                      [ 0.0126,  0.0068, -0.0023,  ..., -0.0118,  0.0066,  0.0082],\n",
       "                      [ 0.0017, -0.0131,  0.0141,  ...,  0.0088,  0.0121, -0.0025],\n",
       "                      ...,\n",
       "                      [-0.0017, -0.0129, -0.0016,  ..., -0.0003,  0.0098, -0.0084],\n",
       "                      [ 0.0118, -0.0060,  0.0097,  ...,  0.0061,  0.0137,  0.0022],\n",
       "                      [-0.0148,  0.0008,  0.0009,  ...,  0.0006, -0.0069,  0.0116]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0260,  0.0103, -0.0085,  ..., -0.0306, -0.0378, -0.0199],\n",
       "                      [ 0.0175, -0.0153,  0.0332,  ..., -0.0057,  0.0206,  0.0238],\n",
       "                      [ 0.0210,  0.0349, -0.0168,  ...,  0.0052, -0.0737,  0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0437,  0.0203,  0.0003,  ...,  0.0177,  0.0364,  0.0386],\n",
       "                      [-0.0042, -0.0040, -0.0046,  ..., -0.0031,  0.0014, -0.0062],\n",
       "                      [ 0.0192,  0.0182, -0.0051,  ...,  0.0342, -0.0276,  0.0278]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 7.5378e-03, -1.2390e-02,  1.1536e-02,  ...,  9.5215e-03,\n",
       "                       -1.8406e-04, -4.1504e-03],\n",
       "                      [ 7.8735e-03, -1.2329e-02, -4.3335e-03,  ..., -2.3346e-03,\n",
       "                       -7.5912e-04, -1.2451e-02],\n",
       "                      [ 5.4626e-03,  6.8665e-03, -5.0964e-03,  ...,  5.3101e-03,\n",
       "                        1.2207e-02,  1.2756e-02],\n",
       "                      ...,\n",
       "                      [ 1.3885e-03, -1.1780e-02,  1.1902e-02,  ...,  6.4697e-03,\n",
       "                        1.0498e-02, -7.0496e-03],\n",
       "                      [ 7.9956e-03,  6.9275e-03, -1.4099e-02,  ...,  3.9577e-05,\n",
       "                        1.2634e-02, -1.5442e-02],\n",
       "                      [ 1.3367e-02,  6.5002e-03,  4.7607e-03,  ...,  3.7231e-03,\n",
       "                       -1.3733e-02, -2.2583e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0356, -0.0184, -0.0530,  ...,  0.0033,  0.0571,  0.0442],\n",
       "                      [-0.0001,  0.0305, -0.0036,  ..., -0.0203, -0.0320,  0.0356],\n",
       "                      [ 0.0093,  0.0293,  0.0012,  ...,  0.0236,  0.0138, -0.0123],\n",
       "                      ...,\n",
       "                      [ 0.0571,  0.0223,  0.0320,  ...,  0.0119,  0.0059,  0.0009],\n",
       "                      [ 0.0135, -0.0654,  0.0107,  ..., -0.0236, -0.0100,  0.0226],\n",
       "                      [-0.0044, -0.0221, -0.0299,  ...,  0.0228, -0.0021,  0.0153]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0066, -0.0090,  0.0139,  ..., -0.0032, -0.0073, -0.0026],\n",
       "                      [-0.0056,  0.0055,  0.0107,  ...,  0.0049,  0.0066, -0.0053],\n",
       "                      [ 0.0101, -0.0021, -0.0137,  ..., -0.0067,  0.0085, -0.0066],\n",
       "                      ...,\n",
       "                      [-0.0100,  0.0155, -0.0111,  ..., -0.0094, -0.0074, -0.0104],\n",
       "                      [-0.0104,  0.0043,  0.0081,  ..., -0.0082, -0.0147,  0.0137],\n",
       "                      [-0.0145, -0.0122,  0.0027,  ...,  0.0004, -0.0035,  0.0132]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0245, -0.0064,  0.0048,  ...,  0.0297, -0.0381,  0.0525],\n",
       "                      [-0.0220,  0.0300, -0.0061,  ...,  0.0175,  0.0366,  0.0320],\n",
       "                      [-0.0034,  0.0058,  0.0374,  ...,  0.0051,  0.0187,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0179,  0.0825,  0.0112,  ..., -0.0015,  0.0237,  0.0515],\n",
       "                      [-0.0266, -0.0684,  0.0310,  ...,  0.0339, -0.0077,  0.0060],\n",
       "                      [-0.0090, -0.0515,  0.0211,  ...,  0.0381, -0.0017,  0.0449]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0084, -0.0043, -0.0011,  ..., -0.0078, -0.0016,  0.0084],\n",
       "                      [ 0.0012, -0.0068, -0.0061,  ...,  0.0085, -0.0128, -0.0154],\n",
       "                      [-0.0056,  0.0010,  0.0067,  ..., -0.0150,  0.0140,  0.0045],\n",
       "                      ...,\n",
       "                      [ 0.0017,  0.0086,  0.0014,  ..., -0.0140,  0.0087,  0.0097],\n",
       "                      [-0.0156, -0.0051,  0.0069,  ..., -0.0090,  0.0119,  0.0065],\n",
       "                      [ 0.0142,  0.0132,  0.0116,  ..., -0.0107, -0.0103, -0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.up_proj.weight',\n",
       "              tensor([[-0.0064,  0.0476, -0.0020,  ...,  0.0518,  0.0469,  0.0347],\n",
       "                      [-0.0398, -0.0432, -0.0322,  ...,  0.0118, -0.0354,  0.0002],\n",
       "                      [ 0.0021,  0.0017, -0.0240,  ...,  0.0027,  0.0182,  0.0098],\n",
       "                      ...,\n",
       "                      [-0.0014,  0.0649,  0.0247,  ...,  0.0258,  0.0031,  0.0201],\n",
       "                      [-0.0098,  0.0249,  0.0211,  ...,  0.0184,  0.0220, -0.0728],\n",
       "                      [ 0.0211, -0.0481, -0.0361,  ..., -0.0024,  0.0167,  0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0002,  0.0141,  0.0039,  ..., -0.0054, -0.0052, -0.0135],\n",
       "                      [ 0.0113,  0.0142, -0.0053,  ...,  0.0028,  0.0011, -0.0124],\n",
       "                      [ 0.0117, -0.0045,  0.0069,  ..., -0.0120, -0.0063,  0.0092],\n",
       "                      ...,\n",
       "                      [ 0.0087, -0.0114,  0.0133,  ..., -0.0025, -0.0108, -0.0033],\n",
       "                      [-0.0099,  0.0049, -0.0042,  ...,  0.0033,  0.0146, -0.0046],\n",
       "                      [-0.0014,  0.0059, -0.0151,  ...,  0.0037,  0.0101,  0.0058]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0118,  0.0369, -0.0435,  ...,  0.0205, -0.0089, -0.0101],\n",
       "                      [ 0.0439, -0.0371, -0.0222,  ..., -0.0197, -0.0264, -0.0312],\n",
       "                      [-0.0540,  0.0520, -0.0092,  ...,  0.0141, -0.0223, -0.0243],\n",
       "                      ...,\n",
       "                      [ 0.0186,  0.0132, -0.0286,  ..., -0.0036,  0.0157,  0.0359],\n",
       "                      [ 0.0247, -0.0312,  0.0742,  ...,  0.0334, -0.0398,  0.0238],\n",
       "                      [-0.0215,  0.0266, -0.0199,  ...,  0.0417,  0.0267, -0.0608]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0066,  0.0056,  0.0023,  ..., -0.0028,  0.0055, -0.0057],\n",
       "                      [-0.0009, -0.0065,  0.0060,  ..., -0.0088, -0.0093,  0.0085],\n",
       "                      [-0.0039,  0.0064,  0.0029,  ..., -0.0009, -0.0003, -0.0058],\n",
       "                      ...,\n",
       "                      [-0.0061,  0.0001,  0.0008,  ..., -0.0004,  0.0087, -0.0020],\n",
       "                      [-0.0027, -0.0040, -0.0072,  ...,  0.0008, -0.0090,  0.0009],\n",
       "                      [-0.0067, -0.0049,  0.0085,  ...,  0.0062,  0.0017,  0.0063]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.input_layernorm.weight',\n",
       "              tensor([0.4629, 0.4648, 0.4805,  ..., 0.4609, 0.4707, 0.4805],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.28.post_attention_layernorm.weight',\n",
       "              tensor([0.4160, 0.4082, 0.4062,  ..., 0.4062, 0.4102, 0.4102],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0032, -0.0091, -0.0140,  ...,  0.0189,  0.0057,  0.0366],\n",
       "                      [-0.0620,  0.0006,  0.0148,  ..., -0.0286,  0.0160, -0.0016],\n",
       "                      [-0.0017, -0.0052,  0.0010,  ...,  0.0364, -0.0187,  0.0284],\n",
       "                      ...,\n",
       "                      [-0.0131,  0.0518, -0.0017,  ..., -0.0181, -0.0061, -0.0236],\n",
       "                      [ 0.0011,  0.0064,  0.0005,  ..., -0.0400, -0.0221, -0.0244],\n",
       "                      [-0.0277, -0.0601, -0.0298,  ...,  0.0249, -0.0002, -0.0236]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0078, -0.0039, -0.0149,  ...,  0.0028, -0.0052,  0.0069],\n",
       "                      [ 0.0018, -0.0065,  0.0094,  ...,  0.0024,  0.0061,  0.0096],\n",
       "                      [ 0.0016,  0.0101, -0.0025,  ..., -0.0128, -0.0114, -0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0093,  0.0082,  0.0021,  ...,  0.0141,  0.0132, -0.0129],\n",
       "                      [-0.0031, -0.0128,  0.0043,  ...,  0.0041, -0.0011, -0.0133],\n",
       "                      [ 0.0028, -0.0151,  0.0106,  ..., -0.0058, -0.0085,  0.0135]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0037, -0.0273, -0.0079,  ...,  0.0060,  0.0029,  0.0233],\n",
       "                      [-0.0060,  0.0175,  0.0085,  ..., -0.0008, -0.0315, -0.0003],\n",
       "                      [-0.0038, -0.0056,  0.0063,  ...,  0.0327,  0.0035,  0.0238],\n",
       "                      ...,\n",
       "                      [-0.0010, -0.0059, -0.0012,  ..., -0.0137,  0.0198,  0.0170],\n",
       "                      [ 0.0400,  0.0054, -0.0500,  ...,  0.0255, -0.0104,  0.0299],\n",
       "                      [ 0.0046, -0.0236,  0.0010,  ..., -0.0413, -0.0112,  0.0113]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 1.6174e-03,  3.2043e-03, -6.3477e-03,  ...,  7.5912e-04,\n",
       "                        5.8289e-03, -1.4038e-03],\n",
       "                      [-3.3264e-03, -1.2573e-02,  6.3477e-03,  ..., -4.0588e-03,\n",
       "                        8.3447e-06,  3.3264e-03],\n",
       "                      [-4.6082e-03, -1.0254e-02, -7.7209e-03,  ...,  1.0986e-02,\n",
       "                        1.4587e-02,  1.1597e-03],\n",
       "                      ...,\n",
       "                      [ 1.2360e-03,  9.5215e-03, -8.9722e-03,  ..., -6.1646e-03,\n",
       "                       -8.9111e-03,  4.1809e-03],\n",
       "                      [ 1.3855e-02, -1.8768e-03,  2.0599e-03,  ...,  5.7373e-03,\n",
       "                       -1.3184e-02,  2.3193e-03],\n",
       "                      [-9.1934e-04,  1.9989e-03,  1.5198e-02,  ..., -8.5449e-03,\n",
       "                       -1.2512e-02, -8.1787e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0540,  0.0172,  0.0464,  ...,  0.0088, -0.0142,  0.0107],\n",
       "                      [ 0.0386,  0.0449,  0.0157,  ..., -0.0437, -0.0106,  0.0264],\n",
       "                      [-0.0308, -0.0630,  0.0041,  ...,  0.0153,  0.0354, -0.0037],\n",
       "                      ...,\n",
       "                      [ 0.0099, -0.0075,  0.0277,  ..., -0.0094, -0.0287, -0.0242],\n",
       "                      [-0.0077,  0.0064, -0.0272,  ...,  0.0004,  0.0562, -0.0153],\n",
       "                      [ 0.0422, -0.0016, -0.0098,  ...,  0.0038,  0.0288,  0.0041]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0132, -0.0007, -0.0019,  ..., -0.0075, -0.0088,  0.0066],\n",
       "                      [ 0.0031,  0.0114,  0.0017,  ..., -0.0118,  0.0146,  0.0149],\n",
       "                      [ 0.0019, -0.0106,  0.0070,  ..., -0.0047,  0.0093,  0.0134],\n",
       "                      ...,\n",
       "                      [-0.0134, -0.0101,  0.0143,  ...,  0.0105,  0.0112, -0.0095],\n",
       "                      [-0.0098, -0.0112, -0.0018,  ..., -0.0038,  0.0118, -0.0129],\n",
       "                      [-0.0070,  0.0040, -0.0002,  ...,  0.0117, -0.0048, -0.0085]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0154,  0.0476,  0.0510,  ..., -0.0064, -0.0223, -0.0137],\n",
       "                      [-0.0233,  0.0117,  0.0216,  ...,  0.0055, -0.0294,  0.0413],\n",
       "                      [ 0.0378,  0.0361,  0.0036,  ...,  0.0234,  0.0139, -0.0024],\n",
       "                      ...,\n",
       "                      [-0.0630, -0.0094,  0.0654,  ..., -0.0010, -0.0302, -0.0615],\n",
       "                      [-0.0205, -0.0378, -0.0146,  ..., -0.0391,  0.0302, -0.0311],\n",
       "                      [-0.0245,  0.0087, -0.0244,  ..., -0.0208,  0.0236, -0.0552]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-1.2695e-02,  1.5564e-02,  1.0610e-05,  ..., -1.5320e-02,\n",
       "                       -2.1057e-03, -1.0681e-02],\n",
       "                      [ 1.5564e-02,  5.4321e-03,  1.4771e-02,  ..., -9.8267e-03,\n",
       "                        3.1738e-03, -2.6398e-03],\n",
       "                      [ 1.1292e-02,  1.4038e-03,  1.8387e-03,  ...,  2.2278e-03,\n",
       "                        3.8452e-03, -4.1199e-03],\n",
       "                      ...,\n",
       "                      [ 7.2327e-03,  4.4861e-03,  2.8839e-03,  ..., -1.3611e-02,\n",
       "                       -2.0752e-03, -5.2185e-03],\n",
       "                      [-1.3794e-02,  4.1809e-03, -1.3550e-02,  ..., -1.2665e-03,\n",
       "                       -6.5918e-03,  1.1215e-03],\n",
       "                      [-9.3994e-03, -2.1362e-03, -1.2146e-02,  ...,  3.7994e-03,\n",
       "                        5.6076e-04,  1.3794e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0618,  0.0096,  0.0061,  ..., -0.0150,  0.0032, -0.0027],\n",
       "                      [ 0.0197,  0.0121, -0.0034,  ..., -0.0295,  0.0222,  0.0093],\n",
       "                      [ 0.0082, -0.0383,  0.0175,  ..., -0.0400,  0.0288,  0.0019],\n",
       "                      ...,\n",
       "                      [ 0.0557,  0.0243, -0.0068,  ..., -0.0200, -0.0090,  0.0183],\n",
       "                      [ 0.0269,  0.0466, -0.0476,  ..., -0.0008,  0.0156,  0.0138],\n",
       "                      [ 0.0062,  0.0493, -0.0258,  ...,  0.0123,  0.0099, -0.0142]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 4.5166e-03, -1.2512e-02, -3.7253e-06,  ...,  6.8359e-03,\n",
       "                       -5.6763e-03,  1.2817e-02],\n",
       "                      [ 1.4099e-02,  1.9932e-04,  3.8338e-04,  ...,  1.1658e-02,\n",
       "                       -1.0010e-02,  1.1597e-02],\n",
       "                      [-4.0627e-04,  5.7068e-03, -6.2256e-03,  ..., -1.5381e-02,\n",
       "                        9.8877e-03,  5.2185e-03],\n",
       "                      ...,\n",
       "                      [ 6.5918e-03, -1.1719e-02, -1.3245e-02,  ...,  1.1902e-03,\n",
       "                       -3.7384e-03, -9.3994e-03],\n",
       "                      [ 7.7438e-04, -1.3428e-02,  6.4392e-03,  ...,  1.4221e-02,\n",
       "                        1.2451e-02, -6.7902e-04],\n",
       "                      [-1.1108e-02, -1.5137e-02, -1.1292e-02,  ..., -7.2021e-03,\n",
       "                       -1.4343e-02,  5.0964e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0277,  0.0264,  0.0417,  ...,  0.0302, -0.0303,  0.0157],\n",
       "                      [-0.0325, -0.0012, -0.0269,  ...,  0.0243, -0.0080,  0.0457],\n",
       "                      [-0.0041,  0.0137,  0.0011,  ..., -0.0347, -0.0209,  0.0564],\n",
       "                      ...,\n",
       "                      [ 0.0449,  0.0124, -0.0006,  ..., -0.0620, -0.0359,  0.0003],\n",
       "                      [ 0.0087, -0.0068,  0.0015,  ...,  0.0505, -0.0199, -0.0172],\n",
       "                      [ 0.0219,  0.0250,  0.0250,  ..., -0.0481,  0.0035, -0.0106]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-1.4709e-02,  5.8594e-03,  5.5237e-03,  ..., -1.2131e-03,\n",
       "                       -1.4404e-02, -2.1515e-03],\n",
       "                      [-1.2695e-02, -3.1433e-03, -5.2490e-03,  ...,  2.7618e-03,\n",
       "                        1.0681e-02, -1.1902e-02],\n",
       "                      [-1.5320e-02, -9.9487e-03,  1.3977e-02,  ...,  7.8201e-04,\n",
       "                        1.3367e-02, -9.8877e-03],\n",
       "                      ...,\n",
       "                      [ 4.7607e-03,  4.4861e-03,  1.5564e-02,  ...,  4.9744e-03,\n",
       "                        4.5967e-04, -1.2146e-02],\n",
       "                      [-1.4160e-02,  9.2163e-03, -1.5503e-02,  ..., -1.2512e-02,\n",
       "                       -8.5449e-04, -7.8201e-05],\n",
       "                      [-1.1597e-02,  2.8381e-03,  5.6152e-03,  ...,  6.4468e-04,\n",
       "                       -9.5215e-03, -6.6528e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0347,  0.0019,  0.0002,  ..., -0.0117,  0.0334, -0.0089],\n",
       "                      [ 0.0552, -0.0444,  0.0332,  ..., -0.0308, -0.0302, -0.0047],\n",
       "                      [ 0.0156, -0.0410, -0.0210,  ...,  0.0347, -0.0288, -0.0471],\n",
       "                      ...,\n",
       "                      [ 0.0070, -0.0181,  0.0186,  ..., -0.0095, -0.0044,  0.0020],\n",
       "                      [ 0.0427,  0.0347,  0.0981,  ..., -0.0157, -0.0669, -0.0347],\n",
       "                      [ 0.0212,  0.0095, -0.0106,  ...,  0.0079, -0.0228,  0.0024]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0051,  0.0038, -0.0064,  ...,  0.0025,  0.0015,  0.0044],\n",
       "                      [-0.0053,  0.0084,  0.0085,  ...,  0.0042, -0.0011, -0.0022],\n",
       "                      [-0.0030,  0.0029,  0.0002,  ...,  0.0092,  0.0050, -0.0074],\n",
       "                      ...,\n",
       "                      [-0.0090,  0.0065,  0.0094,  ..., -0.0050,  0.0095,  0.0074],\n",
       "                      [-0.0074, -0.0071, -0.0022,  ..., -0.0089,  0.0089, -0.0022],\n",
       "                      [-0.0040,  0.0095, -0.0091,  ..., -0.0051, -0.0019,  0.0012]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.input_layernorm.weight',\n",
       "              tensor([0.4316, 0.4434, 0.4453,  ..., 0.4258, 0.4492, 0.4375],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.29.post_attention_layernorm.weight',\n",
       "              tensor([0.4258, 0.4277, 0.4141,  ..., 0.4238, 0.4355, 0.4238],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0254, -0.0205,  0.0121,  ..., -0.0212, -0.0082,  0.0018],\n",
       "                      [ 0.0299,  0.0188,  0.0032,  ...,  0.0177, -0.0056,  0.0227],\n",
       "                      [ 0.0058, -0.0008,  0.0164,  ..., -0.0067, -0.0183, -0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0225,  0.0320,  0.0022,  ...,  0.0549,  0.0186, -0.0131],\n",
       "                      [-0.0220, -0.0344,  0.0002,  ...,  0.0097,  0.0327, -0.0260],\n",
       "                      [-0.0461, -0.0273,  0.0398,  ..., -0.0085, -0.0125, -0.0064]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0015, -0.0044,  0.0013,  ..., -0.0064, -0.0050, -0.0104],\n",
       "                      [-0.0100,  0.0147,  0.0066,  ..., -0.0137,  0.0089,  0.0101],\n",
       "                      [ 0.0080, -0.0059, -0.0146,  ...,  0.0155,  0.0107, -0.0061],\n",
       "                      ...,\n",
       "                      [ 0.0155,  0.0142,  0.0057,  ...,  0.0047,  0.0051,  0.0128],\n",
       "                      [-0.0111,  0.0116, -0.0043,  ..., -0.0039, -0.0141,  0.0060],\n",
       "                      [-0.0016, -0.0150,  0.0049,  ..., -0.0026, -0.0006,  0.0015]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0024, -0.0108,  0.0059,  ..., -0.0014, -0.0059,  0.0061],\n",
       "                      [ 0.0277,  0.0033,  0.0160,  ..., -0.0109, -0.0114,  0.0095],\n",
       "                      [ 0.0129, -0.0142,  0.0260,  ...,  0.0065, -0.0111,  0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0315,  0.0737,  0.0554,  ...,  0.0388,  0.0121, -0.0405],\n",
       "                      [-0.0214, -0.0601, -0.0060,  ...,  0.0378,  0.0270, -0.0508],\n",
       "                      [ 0.0405, -0.0569,  0.0228,  ..., -0.0127, -0.0059,  0.0127]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0069, -0.0012,  0.0001,  ..., -0.0082,  0.0103, -0.0107],\n",
       "                      [ 0.0068, -0.0129,  0.0146,  ..., -0.0034,  0.0026, -0.0089],\n",
       "                      [-0.0111, -0.0113, -0.0085,  ...,  0.0136, -0.0156, -0.0120],\n",
       "                      ...,\n",
       "                      [ 0.0032, -0.0074,  0.0126,  ..., -0.0065,  0.0029, -0.0002],\n",
       "                      [ 0.0112, -0.0135,  0.0019,  ...,  0.0005, -0.0055, -0.0051],\n",
       "                      [-0.0124,  0.0125,  0.0098,  ..., -0.0126,  0.0038, -0.0067]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0188, -0.0289, -0.0096,  ..., -0.0094, -0.0713,  0.0330],\n",
       "                      [ 0.0281,  0.0317,  0.0168,  ..., -0.0012,  0.0240,  0.0576],\n",
       "                      [-0.0134,  0.0344,  0.0041,  ..., -0.0128, -0.0276, -0.0003],\n",
       "                      ...,\n",
       "                      [-0.0004,  0.0039, -0.0415,  ..., -0.0074,  0.0114,  0.0466],\n",
       "                      [-0.0171, -0.0272,  0.0239,  ...,  0.0052, -0.0019,  0.0309],\n",
       "                      [-0.0183, -0.0339,  0.0181,  ...,  0.0014, -0.0118,  0.0092]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0104, -0.0074,  0.0033,  ..., -0.0086,  0.0145,  0.0079],\n",
       "                      [ 0.0054,  0.0089,  0.0150,  ..., -0.0057,  0.0064,  0.0006],\n",
       "                      [-0.0116, -0.0030, -0.0131,  ...,  0.0003, -0.0054, -0.0079],\n",
       "                      ...,\n",
       "                      [-0.0112,  0.0112,  0.0118,  ...,  0.0084, -0.0133,  0.0136],\n",
       "                      [ 0.0080, -0.0109,  0.0075,  ..., -0.0106, -0.0132, -0.0081],\n",
       "                      [ 0.0152,  0.0029,  0.0037,  ..., -0.0021,  0.0090, -0.0075]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0231, -0.0051,  0.0057,  ..., -0.0048, -0.0486, -0.0654],\n",
       "                      [ 0.0148,  0.0136, -0.0211,  ..., -0.0150, -0.0258, -0.0086],\n",
       "                      [-0.0376,  0.0120, -0.0240,  ..., -0.0096,  0.0388,  0.0102],\n",
       "                      ...,\n",
       "                      [ 0.0212,  0.0058,  0.0369,  ..., -0.0383,  0.0087,  0.0552],\n",
       "                      [ 0.0099, -0.0457,  0.0583,  ..., -0.0004,  0.0791,  0.0229],\n",
       "                      [-0.0117,  0.0194, -0.0386,  ..., -0.0110,  0.0393, -0.0173]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0126,  0.0142, -0.0140,  ..., -0.0016,  0.0032,  0.0119],\n",
       "                      [ 0.0135,  0.0072, -0.0036,  ..., -0.0013,  0.0050, -0.0014],\n",
       "                      [-0.0107, -0.0062,  0.0098,  ...,  0.0062,  0.0153, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0049,  0.0153,  0.0087,  ...,  0.0093, -0.0109,  0.0082],\n",
       "                      [-0.0068,  0.0009, -0.0065,  ...,  0.0072,  0.0027, -0.0051],\n",
       "                      [-0.0133,  0.0091,  0.0153,  ..., -0.0004, -0.0107,  0.0143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0466, -0.0105,  0.0537,  ..., -0.0206, -0.0166, -0.0046],\n",
       "                      [-0.0005,  0.0120,  0.0552,  ...,  0.0168,  0.0039,  0.0193],\n",
       "                      [-0.0532,  0.0005, -0.0101,  ...,  0.0012, -0.0669,  0.0008],\n",
       "                      ...,\n",
       "                      [ 0.0081, -0.0107,  0.0115,  ...,  0.0239, -0.0022, -0.0187],\n",
       "                      [-0.0239,  0.0209, -0.0383,  ...,  0.0315, -0.0361,  0.0708],\n",
       "                      [-0.0006, -0.0593, -0.0143,  ...,  0.0065, -0.0239,  0.0066]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0115,  0.0032, -0.0057,  ...,  0.0063, -0.0053,  0.0105],\n",
       "                      [ 0.0106, -0.0037, -0.0113,  ..., -0.0023, -0.0031, -0.0077],\n",
       "                      [-0.0070,  0.0016, -0.0010,  ...,  0.0137,  0.0006,  0.0117],\n",
       "                      ...,\n",
       "                      [ 0.0043, -0.0068,  0.0087,  ..., -0.0062, -0.0077,  0.0077],\n",
       "                      [-0.0085, -0.0090, -0.0123,  ..., -0.0067, -0.0075, -0.0022],\n",
       "                      [ 0.0110,  0.0105,  0.0090,  ..., -0.0068, -0.0126, -0.0070]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0011,  0.0023,  0.0220,  ...,  0.0334,  0.0374, -0.0082],\n",
       "                      [ 0.0022, -0.0026, -0.0094,  ...,  0.0039,  0.0117,  0.0027],\n",
       "                      [-0.0078,  0.0193,  0.0253,  ...,  0.0200, -0.0210,  0.0056],\n",
       "                      ...,\n",
       "                      [ 0.0013, -0.0378,  0.0170,  ...,  0.0199, -0.0288, -0.0500],\n",
       "                      [-0.0068, -0.0103, -0.0349,  ...,  0.0028, -0.0216,  0.0172],\n",
       "                      [ 0.0153,  0.0091, -0.0093,  ...,  0.0297,  0.0282,  0.0554]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0084, -0.0003,  0.0078,  ...,  0.0017, -0.0100,  0.0113],\n",
       "                      [ 0.0076,  0.0117,  0.0120,  ...,  0.0081,  0.0089,  0.0023],\n",
       "                      [-0.0082,  0.0146,  0.0071,  ..., -0.0016,  0.0045, -0.0150],\n",
       "                      ...,\n",
       "                      [-0.0101,  0.0100,  0.0089,  ...,  0.0109,  0.0060,  0.0124],\n",
       "                      [-0.0007,  0.0003, -0.0052,  ..., -0.0025, -0.0150, -0.0053],\n",
       "                      [ 0.0110,  0.0041,  0.0082,  ..., -0.0097, -0.0031,  0.0113]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.down_proj.weight',\n",
       "              tensor([[-0.0332,  0.0479, -0.0479,  ...,  0.0243, -0.0284,  0.0280],\n",
       "                      [ 0.0176, -0.0005,  0.0065,  ...,  0.0141,  0.0106,  0.0079],\n",
       "                      [ 0.0129, -0.0245, -0.0236,  ..., -0.0175,  0.0311,  0.0179],\n",
       "                      ...,\n",
       "                      [-0.0547,  0.0266, -0.0422,  ...,  0.0110, -0.0337,  0.0034],\n",
       "                      [-0.0378, -0.0125, -0.0067,  ...,  0.0315,  0.0532,  0.0208],\n",
       "                      [ 0.0037, -0.0364,  0.0146,  ...,  0.0449,  0.0029,  0.0139]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0077,  0.0017, -0.0084,  ...,  0.0079,  0.0023, -0.0024],\n",
       "                      [-0.0057, -0.0062, -0.0090,  ...,  0.0027,  0.0074, -0.0070],\n",
       "                      [-0.0006,  0.0030, -0.0032,  ..., -0.0036, -0.0020,  0.0089],\n",
       "                      ...,\n",
       "                      [-0.0021,  0.0028,  0.0077,  ..., -0.0059, -0.0007,  0.0059],\n",
       "                      [ 0.0004, -0.0024,  0.0028,  ..., -0.0092,  0.0024,  0.0006],\n",
       "                      [ 0.0040,  0.0030, -0.0069,  ..., -0.0071,  0.0025, -0.0065]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.input_layernorm.weight',\n",
       "              tensor([0.4727, 0.4766, 0.4863,  ..., 0.4727, 0.4863, 0.4941],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.30.post_attention_layernorm.weight',\n",
       "              tensor([0.4336, 0.4453, 0.4375,  ..., 0.4453, 0.4453, 0.4316],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0022,  0.0082, -0.0017,  ...,  0.0046,  0.0124, -0.0102],\n",
       "                      [ 0.0198, -0.0177, -0.0327,  ..., -0.0023, -0.0143,  0.0227],\n",
       "                      [-0.0098, -0.0019,  0.0118,  ...,  0.0086,  0.0420,  0.0193],\n",
       "                      ...,\n",
       "                      [ 0.0262,  0.0215, -0.0022,  ..., -0.0061,  0.0186, -0.0476],\n",
       "                      [ 0.0134, -0.0115,  0.0159,  ..., -0.0527,  0.0072,  0.0115],\n",
       "                      [-0.0038,  0.0085,  0.0199,  ...,  0.0522,  0.0591,  0.0649]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0031, -0.0143, -0.0123,  ..., -0.0038,  0.0107, -0.0143],\n",
       "                      [-0.0079,  0.0120, -0.0143,  ...,  0.0092,  0.0081,  0.0066],\n",
       "                      [ 0.0060, -0.0138, -0.0030,  ..., -0.0074, -0.0082,  0.0115],\n",
       "                      ...,\n",
       "                      [ 0.0104, -0.0098, -0.0001,  ...,  0.0081, -0.0122, -0.0125],\n",
       "                      [ 0.0140, -0.0117,  0.0051,  ...,  0.0086,  0.0009, -0.0109],\n",
       "                      [-0.0063,  0.0007,  0.0006,  ...,  0.0041,  0.0038,  0.0117]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0103, -0.0061,  0.0175,  ..., -0.0035, -0.0256, -0.0339],\n",
       "                      [ 0.0258,  0.0109, -0.0016,  ...,  0.0491, -0.0086,  0.0137],\n",
       "                      [ 0.0197, -0.0095, -0.0086,  ...,  0.0120, -0.0085, -0.0356],\n",
       "                      ...,\n",
       "                      [-0.0090, -0.0757, -0.0674,  ..., -0.0092, -0.0210, -0.0530],\n",
       "                      [ 0.0203,  0.0293,  0.0356,  ...,  0.0130, -0.0576, -0.0027],\n",
       "                      [-0.0342, -0.0347, -0.0118,  ..., -0.0356, -0.0184,  0.0266]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0099, -0.0006, -0.0037,  ...,  0.0086, -0.0009,  0.0090],\n",
       "                      [ 0.0095, -0.0004,  0.0047,  ..., -0.0114,  0.0035, -0.0098],\n",
       "                      [-0.0052,  0.0064,  0.0096,  ...,  0.0093,  0.0109, -0.0104],\n",
       "                      ...,\n",
       "                      [-0.0081, -0.0114, -0.0150,  ...,  0.0151,  0.0119,  0.0098],\n",
       "                      [-0.0054, -0.0055,  0.0106,  ..., -0.0043,  0.0018, -0.0089],\n",
       "                      [ 0.0121, -0.0043,  0.0139,  ..., -0.0008, -0.0118,  0.0098]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0231, -0.0226,  0.0173,  ...,  0.0143,  0.0388,  0.0096],\n",
       "                      [-0.0071,  0.0017,  0.0272,  ...,  0.0225,  0.0250, -0.0142],\n",
       "                      [ 0.0013, -0.0070, -0.0160,  ...,  0.0109,  0.0684,  0.0182],\n",
       "                      ...,\n",
       "                      [ 0.0153,  0.0295,  0.0031,  ..., -0.0249,  0.0124,  0.0110],\n",
       "                      [ 0.0276, -0.0376, -0.0325,  ..., -0.0106, -0.0452,  0.0079],\n",
       "                      [ 0.0009,  0.0182,  0.0476,  ..., -0.0005, -0.0054,  0.0144]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0068,  0.0100,  0.0025,  ...,  0.0065,  0.0134, -0.0031],\n",
       "                      [-0.0034, -0.0024, -0.0145,  ..., -0.0134,  0.0002, -0.0005],\n",
       "                      [ 0.0092,  0.0006,  0.0105,  ...,  0.0151,  0.0037, -0.0093],\n",
       "                      ...,\n",
       "                      [ 0.0090, -0.0003, -0.0061,  ..., -0.0019,  0.0104, -0.0115],\n",
       "                      [-0.0092, -0.0022, -0.0074,  ...,  0.0136, -0.0126,  0.0148],\n",
       "                      [-0.0062,  0.0080,  0.0053,  ..., -0.0129, -0.0132, -0.0145]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0537,  0.0270,  0.0576,  ...,  0.0200, -0.0056, -0.0093],\n",
       "                      [ 0.0625,  0.0060, -0.0356,  ...,  0.0228, -0.0151, -0.0175],\n",
       "                      [ 0.0051, -0.0173,  0.0099,  ..., -0.0039,  0.0265,  0.0150],\n",
       "                      ...,\n",
       "                      [-0.0102, -0.0082,  0.0330,  ...,  0.0016,  0.0178, -0.0164],\n",
       "                      [ 0.0022,  0.0052, -0.0271,  ..., -0.0130, -0.0052, -0.0181],\n",
       "                      [-0.0123,  0.0297,  0.0151,  ...,  0.0223, -0.0277,  0.0088]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0121,  0.0084,  0.0106,  ..., -0.0095, -0.0036, -0.0034],\n",
       "                      [-0.0100,  0.0096,  0.0022,  ...,  0.0131,  0.0109,  0.0085],\n",
       "                      [ 0.0116, -0.0012, -0.0104,  ..., -0.0095, -0.0056, -0.0034],\n",
       "                      ...,\n",
       "                      [ 0.0004, -0.0006, -0.0137,  ..., -0.0111, -0.0092,  0.0096],\n",
       "                      [ 0.0036,  0.0081,  0.0104,  ..., -0.0090, -0.0115, -0.0002],\n",
       "                      [-0.0015,  0.0074,  0.0155,  ..., -0.0145,  0.0057,  0.0013]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0369, -0.0051,  0.0432,  ...,  0.0164, -0.0201,  0.0374],\n",
       "                      [-0.0703,  0.0267,  0.0126,  ..., -0.0063, -0.0095,  0.0090],\n",
       "                      [-0.0037,  0.0297,  0.0103,  ..., -0.0278, -0.0282, -0.0054],\n",
       "                      ...,\n",
       "                      [-0.0036, -0.0269, -0.0109,  ...,  0.0064,  0.0231,  0.0056],\n",
       "                      [ 0.0574, -0.0033, -0.0219,  ...,  0.0378, -0.0393, -0.0374],\n",
       "                      [-0.0115, -0.0349,  0.0043,  ..., -0.0337, -0.0053, -0.0094]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0057,  0.0046,  0.0049,  ...,  0.0009, -0.0006, -0.0029],\n",
       "                      [-0.0014, -0.0027,  0.0006,  ...,  0.0129, -0.0128, -0.0037],\n",
       "                      [ 0.0026, -0.0006, -0.0097,  ..., -0.0082,  0.0070, -0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0016, -0.0156, -0.0151,  ..., -0.0021, -0.0147,  0.0148],\n",
       "                      [-0.0048,  0.0118, -0.0147,  ...,  0.0042,  0.0045,  0.0016],\n",
       "                      [-0.0135, -0.0055,  0.0148,  ...,  0.0139,  0.0055, -0.0030]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.up_proj.weight',\n",
       "              tensor([[-0.0197, -0.0238, -0.0491,  ...,  0.0061,  0.0167, -0.0107],\n",
       "                      [-0.0278,  0.0043,  0.0413,  ...,  0.0291,  0.0092, -0.0178],\n",
       "                      [ 0.0294, -0.0080, -0.0125,  ...,  0.0189, -0.0408, -0.0097],\n",
       "                      ...,\n",
       "                      [-0.0003, -0.0459, -0.0010,  ...,  0.0337,  0.0623,  0.0043],\n",
       "                      [ 0.0113,  0.0289, -0.0211,  ...,  0.0166,  0.0098,  0.0181],\n",
       "                      [ 0.0021, -0.0066,  0.0085,  ..., -0.0107,  0.0085,  0.0302]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight',\n",
       "              tensor([[ 0.0038,  0.0044, -0.0153,  ..., -0.0073,  0.0060,  0.0009],\n",
       "                      [ 0.0019, -0.0059, -0.0102,  ...,  0.0101,  0.0058,  0.0038],\n",
       "                      [ 0.0106, -0.0066,  0.0154,  ...,  0.0146,  0.0151,  0.0143],\n",
       "                      ...,\n",
       "                      [-0.0139, -0.0002, -0.0135,  ..., -0.0107,  0.0064,  0.0089],\n",
       "                      [-0.0049, -0.0146, -0.0044,  ..., -0.0142,  0.0056,  0.0056],\n",
       "                      [ 0.0125, -0.0132,  0.0050,  ...,  0.0012,  0.0071,  0.0121]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0015,  0.0093,  0.0630,  ...,  0.0148, -0.0415,  0.0076],\n",
       "                      [ 0.0383,  0.0016,  0.0114,  ..., -0.0349, -0.0320, -0.0535],\n",
       "                      [-0.0325, -0.0176,  0.0271,  ...,  0.0811,  0.0033, -0.0052],\n",
       "                      ...,\n",
       "                      [-0.0012,  0.0415, -0.0131,  ...,  0.0208,  0.0037,  0.0077],\n",
       "                      [ 0.0183,  0.0552,  0.0137,  ..., -0.0265,  0.0160,  0.0266],\n",
       "                      [ 0.0359, -0.0108,  0.0046,  ...,  0.0014,  0.0032,  0.0439]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight',\n",
       "              tensor([[-0.0047, -0.0036,  0.0079,  ...,  0.0089, -0.0083,  0.0088],\n",
       "                      [ 0.0093, -0.0088, -0.0008,  ..., -0.0035, -0.0003, -0.0034],\n",
       "                      [ 0.0020, -0.0003, -0.0090,  ...,  0.0087,  0.0020,  0.0082],\n",
       "                      ...,\n",
       "                      [ 0.0034, -0.0002, -0.0050,  ..., -0.0031, -0.0049, -0.0076],\n",
       "                      [ 0.0017,  0.0002, -0.0019,  ...,  0.0070,  0.0071, -0.0054],\n",
       "                      [-0.0058,  0.0093, -0.0054,  ..., -0.0023, -0.0065,  0.0017]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight',\n",
       "              tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.input_layernorm.weight',\n",
       "              tensor([0.4180, 0.4082, 0.4277,  ..., 0.3848, 0.4004, 0.4199],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.layers.31.post_attention_layernorm.weight',\n",
       "              tensor([0.4199, 0.4160, 0.4277,  ..., 0.4297, 0.4277, 0.4082],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.norm.weight',\n",
       "              tensor([1.0234, 1.0312, 1.0078,  ..., 1.0078, 1.0312, 0.9648],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.W_pos',\n",
       "              tensor([[ 5.9814e-03,  1.3916e-02, -1.9646e-04, -1.7944e-02,  9.3384e-03,\n",
       "                       -3.8910e-03, -2.8229e-03,  7.7248e-05,  1.6235e-02,  1.1444e-03,\n",
       "                       -7.6294e-03,  1.0681e-02, -1.3489e-02,  5.1880e-03,  1.4832e-02,\n",
       "                       -4.4250e-03, -4.6997e-03, -3.6926e-03, -1.5991e-02,  4.8828e-03,\n",
       "                       -9.7046e-03, -1.2939e-02, -1.2756e-02,  9.9487e-03, -1.5991e-02,\n",
       "                        8.7280e-03,  6.8970e-03,  1.2207e-03,  5.7678e-03, -3.0060e-03,\n",
       "                       -1.0132e-02,  9.7656e-03,  1.2329e-02, -1.4191e-03, -7.6294e-03,\n",
       "                       -1.1169e-02,  1.8433e-02,  8.1787e-03, -8.2397e-04,  7.7515e-03,\n",
       "                       -7.3547e-03, -1.9653e-02, -5.3711e-03,  1.9165e-02,  5.9509e-03,\n",
       "                       -1.6968e-02, -2.8992e-03, -1.0437e-02,  1.3000e-02,  4.8828e-03,\n",
       "                       -1.4343e-03, -1.4526e-02,  4.3945e-03, -4.6082e-03,  5.9307e-06,\n",
       "                       -5.9814e-03, -1.3428e-02, -7.8125e-03, -2.1667e-03,  1.1658e-02,\n",
       "                       -1.1658e-02, -4.0588e-03, -1.1108e-02, -1.6251e-03, -3.1128e-03,\n",
       "                        1.4709e-02,  3.4790e-03, -1.0315e-02, -1.0620e-02, -3.6163e-03,\n",
       "                       -1.3489e-02, -9.7046e-03,  1.5869e-02, -1.2451e-02, -4.7913e-03,\n",
       "                        1.0254e-02, -1.7166e-03, -5.1880e-04, -7.5378e-03, -4.6387e-03,\n",
       "                       -1.0437e-02,  1.1475e-02,  5.9509e-03,  1.1536e-02,  1.0757e-03,\n",
       "                       -1.0315e-02, -6.5613e-03,  1.1780e-02,  1.8799e-02,  1.6113e-02,\n",
       "                       -1.9531e-02, -1.9775e-02, -2.4872e-03, -1.0223e-03,  1.8921e-02,\n",
       "                       -8.5449e-03,  1.2573e-02,  3.6774e-03,  1.9043e-02,  6.2561e-04,\n",
       "                        9.8267e-03, -4.2725e-03,  5.9814e-03,  3.2349e-03, -8.6670e-03,\n",
       "                       -1.6357e-02,  4.4556e-03,  7.8201e-04,  6.4392e-03,  3.1090e-04,\n",
       "                        1.5625e-02, -2.8839e-03, -1.7944e-02,  5.5237e-03,  1.8311e-02,\n",
       "                        5.3406e-03,  1.9775e-02, -1.0071e-02, -1.1963e-02,  8.1177e-03,\n",
       "                        8.6060e-03, -8.4229e-03, -5.0545e-05, -4.0283e-03, -1.6846e-02,\n",
       "                       -3.1891e-03, -2.0020e-02, -3.3722e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.0.qTrans',\n",
       "              tensor([[ 0.0742, -0.0776, -0.0239,  ..., -0.0859, -0.1396, -0.0260],\n",
       "                      [ 0.0894,  0.1543, -0.1025,  ...,  0.0713,  0.0160, -0.0737],\n",
       "                      [ 0.0889,  0.0918, -0.0063,  ..., -0.1260,  0.1191, -0.0393],\n",
       "                      ...,\n",
       "                      [-0.0344, -0.0132, -0.0894,  ..., -0.0237, -0.0309,  0.0127],\n",
       "                      [-0.0967, -0.1064,  0.0728,  ..., -0.1152, -0.0287, -0.1240],\n",
       "                      [-0.1289, -0.0771,  0.0175,  ..., -0.1426, -0.0280, -0.1143]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.0.kTrans',\n",
       "              tensor([[-0.0396, -0.0688, -0.1387,  ...,  0.1060,  0.0972,  0.0222],\n",
       "                      [ 0.0149,  0.1416, -0.0295,  ..., -0.0767,  0.1157, -0.0850],\n",
       "                      [ 0.1099, -0.1201, -0.0228,  ..., -0.1494, -0.0791, -0.0869],\n",
       "                      ...,\n",
       "                      [ 0.0654,  0.1455, -0.1069,  ...,  0.1377, -0.0967,  0.1484],\n",
       "                      [-0.0598, -0.0913,  0.1094,  ...,  0.1357, -0.0295, -0.1309],\n",
       "                      [ 0.0237, -0.0295,  0.1079,  ...,  0.0654, -0.1162, -0.0118]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.0.vTrans',\n",
       "              tensor([[ 0.0383,  0.0674, -0.0410,  ...,  0.0179,  0.0723,  0.0728],\n",
       "                      [-0.1260, -0.0996,  0.0752,  ..., -0.0820,  0.0574, -0.0486],\n",
       "                      [ 0.0674,  0.0067,  0.0408,  ...,  0.1484,  0.1216,  0.0884],\n",
       "                      ...,\n",
       "                      [-0.0972,  0.1475,  0.1377,  ..., -0.0299,  0.0554, -0.1250],\n",
       "                      [ 0.0422,  0.1455, -0.0454,  ..., -0.0933, -0.0317, -0.1357],\n",
       "                      [ 0.1318,  0.0586, -0.1074,  ...,  0.0476,  0.0283, -0.0266]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.0.norm.weight',\n",
       "              tensor([1.0000, 0.9922, 1.0000, 0.9961, 0.9961, 1.0000, 1.0078, 1.0000, 1.0078,\n",
       "                      0.9922, 0.9922, 1.0000, 1.0000, 1.0078, 1.0000, 0.9961, 1.0000, 0.9961,\n",
       "                      0.9922, 0.9961, 1.0078, 0.9961, 0.9961, 1.0000, 0.9922, 0.9883, 0.9922,\n",
       "                      1.0000, 0.9961, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 0.9883, 0.9922,\n",
       "                      1.0000, 1.0000, 1.0000, 1.0000, 0.9922, 1.0000, 1.0000, 1.0000, 0.9922,\n",
       "                      1.0000, 0.9883, 0.9961, 1.0000, 1.0000, 0.9961, 0.9844, 1.0078, 0.9961,\n",
       "                      0.9961, 1.0000, 1.0000, 0.9922, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "                      0.9961, 0.9922, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 0.9961, 0.9961,\n",
       "                      1.0000, 1.0000, 0.9922, 0.9961, 1.0000, 0.9961, 1.0078, 1.0000, 0.9922,\n",
       "                      0.9961, 1.0000, 0.9883, 0.9961, 0.9883, 1.0000, 0.9922, 0.9922, 1.0000,\n",
       "                      1.0078, 0.9922, 0.9961, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 1.0078,\n",
       "                      0.9922, 1.0078, 0.9961, 0.9961, 1.0078, 1.0000, 0.9961, 0.9922, 1.0078,\n",
       "                      1.0000, 1.0000, 1.0078, 0.9922, 0.9961, 0.9961, 0.9961, 0.9961, 1.0078,\n",
       "                      0.9961, 0.9922, 0.9961, 0.9961, 0.9883, 1.0078, 0.9961, 0.9961, 0.9961,\n",
       "                      1.0000, 1.0000], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.0.norm.bias',\n",
       "              tensor([ 9.5367e-04,  3.6621e-02, -5.7678e-03,  1.6479e-02,  6.0425e-03,\n",
       "                      -1.1047e-02,  4.6143e-02,  1.7090e-02,  1.4893e-02, -2.8198e-02,\n",
       "                       1.3916e-02,  5.3406e-03,  2.5146e-02,  3.3203e-02,  6.1951e-03,\n",
       "                       1.9409e-02, -1.7822e-02, -9.2163e-03, -1.1292e-02, -9.5215e-03,\n",
       "                      -3.8330e-02,  2.8687e-02, -8.6060e-03,  8.0109e-05, -2.2583e-02,\n",
       "                       2.3041e-03, -7.8964e-04,  2.5879e-02,  2.4292e-02,  3.1891e-03,\n",
       "                       1.4160e-02, -1.9531e-02, -6.8283e-04,  3.7079e-03,  1.0193e-02,\n",
       "                      -2.1210e-03,  1.7853e-03,  2.5269e-02, -5.8289e-03, -2.1362e-02,\n",
       "                      -1.3977e-02,  6.7139e-03, -2.8198e-02,  6.5002e-03,  6.8359e-03,\n",
       "                       3.2471e-02, -1.0681e-03, -1.0605e-03,  1.6846e-02, -1.1597e-02,\n",
       "                      -1.2329e-02, -3.3112e-03, -2.7954e-02,  1.7700e-02, -2.1973e-02,\n",
       "                      -2.6733e-02, -2.3193e-02,  1.6235e-02,  1.3809e-03, -1.7456e-02,\n",
       "                      -2.4170e-02,  2.3560e-02, -1.8066e-02, -7.9956e-03,  5.3101e-03,\n",
       "                      -7.8125e-03,  3.1738e-02,  1.3123e-02, -1.6235e-02, -2.2095e-02,\n",
       "                      -1.4709e-02, -2.2949e-02,  1.2024e-02,  9.0942e-03,  4.1260e-02,\n",
       "                       1.0864e-02, -1.4420e-03,  3.3447e-02,  2.0264e-02, -1.6632e-03,\n",
       "                       7.8125e-03,  2.6703e-03,  1.2756e-02,  1.2024e-02,  7.9956e-03,\n",
       "                       1.9531e-02,  1.3245e-02,  2.6123e-02, -4.7607e-03, -1.2207e-02,\n",
       "                      -8.8501e-03,  2.0264e-02,  3.6774e-03, -1.2085e-02,  2.5330e-03,\n",
       "                      -1.7700e-02, -6.8054e-03, -2.6855e-02,  2.5635e-02, -1.4420e-03,\n",
       "                      -1.3000e-02, -3.7079e-03,  3.2471e-02, -2.7466e-03,  3.2654e-03,\n",
       "                       2.0630e-02,  2.5940e-03,  1.0925e-02,  4.6387e-03,  8.2397e-03,\n",
       "                      -3.0396e-02, -3.1982e-02,  1.9165e-02,  1.5381e-02, -4.1809e-03,\n",
       "                      -3.5095e-03, -4.7363e-02,  6.7749e-03, -2.5269e-02,  1.4526e-02,\n",
       "                      -1.4420e-03,  1.0300e-03, -2.8076e-02,  3.0273e-02,  3.5095e-03,\n",
       "                       6.6833e-03, -2.0386e-02,  3.1006e-02], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.1.qTrans',\n",
       "              tensor([[ 0.0649, -0.0177, -0.1367,  ..., -0.1396,  0.0273, -0.0659],\n",
       "                      [ 0.1187, -0.0059,  0.1250,  ...,  0.0204, -0.1196,  0.1089],\n",
       "                      [-0.0008, -0.1367,  0.0503,  ..., -0.0698, -0.1426,  0.0522],\n",
       "                      ...,\n",
       "                      [-0.0928, -0.0547,  0.1064,  ...,  0.1328,  0.0498,  0.0742],\n",
       "                      [-0.0117, -0.0571,  0.0289,  ..., -0.0011, -0.0233,  0.0378],\n",
       "                      [ 0.1260, -0.1465,  0.0476,  ...,  0.0303, -0.0527,  0.1025]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.1.kTrans',\n",
       "              tensor([[-0.1245, -0.0564, -0.1416,  ..., -0.0996, -0.0781, -0.0005],\n",
       "                      [-0.0106, -0.0354, -0.1050,  ...,  0.0187, -0.1055, -0.0864],\n",
       "                      [ 0.0986, -0.1309, -0.1001,  ...,  0.0182, -0.0220, -0.0913],\n",
       "                      ...,\n",
       "                      [ 0.0957, -0.0884,  0.0583,  ..., -0.1377, -0.0820,  0.0615],\n",
       "                      [-0.1357,  0.1260, -0.0097,  ..., -0.1216,  0.0525, -0.0376],\n",
       "                      [-0.0518,  0.1396, -0.1021,  ...,  0.0601, -0.1128, -0.0410]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.1.vTrans',\n",
       "              tensor([[-0.0469,  0.1328,  0.0249,  ...,  0.0139,  0.0835,  0.1118],\n",
       "                      [ 0.1406, -0.0442, -0.1318,  ...,  0.0056, -0.1045,  0.0060],\n",
       "                      [ 0.1016,  0.0036, -0.0771,  ..., -0.0099, -0.1514, -0.0469],\n",
       "                      ...,\n",
       "                      [-0.0547, -0.0114, -0.0576,  ..., -0.1279,  0.0383,  0.0830],\n",
       "                      [ 0.1260,  0.0352, -0.0574,  ..., -0.0972, -0.0064,  0.0132],\n",
       "                      [-0.0245,  0.1084, -0.0479,  ..., -0.1465, -0.0977, -0.0503]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.1.norm.weight',\n",
       "              tensor([1.0000, 0.9961, 0.9922, 1.0000, 0.9961, 1.0000, 0.9922, 0.9922, 1.0078,\n",
       "                      1.0000, 1.0078, 1.0078, 0.9961, 0.9922, 1.0000, 1.0078, 0.9961, 0.9961,\n",
       "                      0.9922, 0.9883, 1.0000, 0.9961, 0.9961, 0.9922, 0.9961, 1.0000, 0.9961,\n",
       "                      0.9883, 0.9961, 0.9961, 1.0000, 0.9922, 1.0000, 0.9883, 0.9922, 1.0000,\n",
       "                      1.0000, 1.0000, 1.0156, 1.0000, 1.0000, 1.0000, 1.0000, 0.9922, 0.9883,\n",
       "                      1.0078, 0.9922, 0.9961, 1.0000, 1.0078, 0.9961, 0.9883, 1.0156, 1.0078,\n",
       "                      0.9961, 0.9961, 1.0156, 0.9883, 1.0078, 1.0156, 1.0000, 1.0078, 1.0000,\n",
       "                      1.0000, 0.9961, 1.0000, 0.9961, 0.9961, 1.0000, 0.9961, 0.9922, 1.0000,\n",
       "                      1.0000, 1.0000, 0.9922, 0.9922, 0.9961, 0.9922, 1.0000, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 0.9883, 0.9883, 1.0078, 1.0078, 0.9844, 1.0000, 0.9961,\n",
       "                      1.0000, 0.9922, 0.9922, 1.0078, 0.9961, 1.0000, 0.9883, 0.9922, 1.0000,\n",
       "                      0.9922, 0.9922, 1.0000, 0.9961, 1.0000, 1.0000, 0.9961, 0.9922, 0.9844,\n",
       "                      1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 1.0000, 1.0078, 1.0000, 1.0078,\n",
       "                      1.0000, 0.9922, 1.0078, 0.9961, 0.9922, 0.9922, 0.9922, 0.9922, 1.0078,\n",
       "                      1.0000, 0.9961], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.1.norm.bias',\n",
       "              tensor([ 0.0291,  0.0420,  0.0050,  0.0471, -0.0005, -0.0025,  0.0332,  0.0452,\n",
       "                       0.0001, -0.0374,  0.0076,  0.0284,  0.0087,  0.0085,  0.0102,  0.0166,\n",
       "                      -0.0107, -0.0051, -0.0188, -0.0027, -0.0378,  0.0282, -0.0197,  0.0141,\n",
       "                      -0.0036, -0.0302,  0.0078,  0.0054,  0.0192, -0.0127,  0.0176, -0.0206,\n",
       "                      -0.0054, -0.0082, -0.0090,  0.0050, -0.0128,  0.0317, -0.0078, -0.0198,\n",
       "                      -0.0073,  0.0222, -0.0322,  0.0134,  0.0013,  0.0228, -0.0028, -0.0013,\n",
       "                       0.0136, -0.0024, -0.0265, -0.0057, -0.0222, -0.0002, -0.0251,  0.0034,\n",
       "                      -0.0217, -0.0039, -0.0349, -0.0129,  0.0022,  0.0078, -0.0320,  0.0010,\n",
       "                      -0.0060, -0.0082,  0.0209,  0.0046,  0.0011, -0.0188, -0.0040,  0.0095,\n",
       "                      -0.0188, -0.0018,  0.0344, -0.0010, -0.0085,  0.0222,  0.0222, -0.0184,\n",
       "                       0.0154, -0.0148,  0.0239, -0.0035,  0.0059,  0.0454,  0.0060,  0.0217,\n",
       "                      -0.0104,  0.0085, -0.0146,  0.0320,  0.0017, -0.0244,  0.0087, -0.0096,\n",
       "                      -0.0075,  0.0038,  0.0152,  0.0031, -0.0115, -0.0101,  0.0427,  0.0107,\n",
       "                       0.0011, -0.0134, -0.0024, -0.0209,  0.0201,  0.0002, -0.0339, -0.0435,\n",
       "                       0.0220, -0.0039,  0.0025,  0.0042, -0.0454,  0.0136, -0.0140,  0.0320,\n",
       "                       0.0134, -0.0093, -0.0150,  0.0205,  0.0007,  0.0151, -0.0184,  0.0249],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.2.qTrans',\n",
       "              tensor([[-0.0190, -0.0386,  0.0077,  ..., -0.0413,  0.0496,  0.0381],\n",
       "                      [-0.0684, -0.1260, -0.0287,  ...,  0.0840, -0.1445, -0.1406],\n",
       "                      [-0.0845,  0.0457,  0.0393,  ...,  0.0006, -0.0981, -0.0554],\n",
       "                      ...,\n",
       "                      [-0.0170, -0.1484, -0.0200,  ..., -0.0461,  0.1133, -0.0796],\n",
       "                      [ 0.0356, -0.0786, -0.1289,  ...,  0.0359, -0.0179,  0.0771],\n",
       "                      [ 0.0476, -0.0500,  0.1123,  ..., -0.0359, -0.0349,  0.0767]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.2.kTrans',\n",
       "              tensor([[-0.1396,  0.1270,  0.0160,  ..., -0.0518, -0.0854,  0.0294],\n",
       "                      [-0.0830,  0.0574, -0.0262,  ..., -0.1187, -0.0869, -0.0508],\n",
       "                      [ 0.0608, -0.0408,  0.0977,  ..., -0.0227, -0.0208, -0.0192],\n",
       "                      ...,\n",
       "                      [-0.0732, -0.1328,  0.1260,  ..., -0.0432, -0.0304,  0.0219],\n",
       "                      [-0.1377, -0.1328, -0.1279,  ...,  0.0466, -0.0352,  0.0500],\n",
       "                      [-0.0354,  0.0889,  0.0747,  ...,  0.0786,  0.0459, -0.1494]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.2.vTrans',\n",
       "              tensor([[-0.0408,  0.1455,  0.0830,  ...,  0.0664, -0.0801, -0.1025],\n",
       "                      [ 0.0173,  0.0371, -0.0439,  ...,  0.0182,  0.0017,  0.0674],\n",
       "                      [-0.0588,  0.0535,  0.0229,  ...,  0.1445,  0.0869, -0.0325],\n",
       "                      ...,\n",
       "                      [-0.1182, -0.0325, -0.1572,  ...,  0.0630,  0.0781, -0.1406],\n",
       "                      [-0.0947, -0.0042, -0.0049,  ...,  0.1289,  0.0206, -0.0894],\n",
       "                      [-0.1533, -0.0815,  0.1030,  ..., -0.0342,  0.0747,  0.0273]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.2.norm.weight',\n",
       "              tensor([1.0000, 1.0000, 0.9922, 1.0000, 0.9961, 0.9961, 1.0078, 1.0000, 0.9922,\n",
       "                      1.0000, 1.0000, 1.0000, 0.9922, 0.9961, 1.0000, 1.0000, 0.9961, 0.9961,\n",
       "                      1.0000, 0.9922, 1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 0.9961, 1.0000,\n",
       "                      0.9961, 0.9961, 1.0078, 0.9961, 0.9922, 1.0078, 1.0000, 0.9961, 0.9961,\n",
       "                      1.0000, 0.9961, 1.0000, 0.9961, 1.0000, 1.0078, 1.0000, 0.9961, 0.9922,\n",
       "                      0.9961, 0.9922, 1.0078, 1.0000, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000,\n",
       "                      0.9922, 0.9961, 0.9961, 0.9961, 1.0078, 1.0078, 0.9961, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 0.9922, 1.0078, 0.9961, 1.0000, 1.0000, 1.0000, 0.9961,\n",
       "                      1.0000, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 0.9961, 1.0000, 1.0000, 0.9961, 1.0078, 1.0000, 0.9922,\n",
       "                      1.0000, 1.0000, 1.0000, 1.0078, 0.9961, 1.0000, 0.9961, 1.0000, 0.9961,\n",
       "                      0.9961, 0.9961, 0.9961, 1.0000, 0.9961, 1.0078, 0.9961, 1.0078, 0.9961,\n",
       "                      0.9961, 1.0000, 1.0000, 1.0000, 0.9961, 1.0000, 1.0000, 1.0000, 1.0078,\n",
       "                      0.9922, 0.9961, 1.0078, 0.9922, 1.0078, 1.0000, 1.0000, 0.9883, 1.0000,\n",
       "                      1.0000, 1.0000], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.gtLayers.2.norm.bias',\n",
       "              tensor([ 3.1494e-02,  4.0771e-02, -8.9722e-03,  5.4932e-02, -1.8799e-02,\n",
       "                      -6.2866e-03,  4.2236e-02,  2.9907e-02,  1.6235e-02, -6.0059e-02,\n",
       "                       1.5625e-02,  1.2756e-02,  2.7832e-02,  1.6357e-02,  2.6611e-02,\n",
       "                      -1.0193e-02, -1.8799e-02,  2.1484e-02,  5.2490e-03,  2.5482e-03,\n",
       "                      -2.4780e-02,  2.7222e-02, -2.6611e-02,  1.5259e-02, -1.4496e-03,\n",
       "                      -3.9551e-02, -3.5400e-02,  2.3926e-02,  1.9287e-02, -6.2866e-03,\n",
       "                       2.7954e-02, -1.9897e-02,  1.9653e-02, -2.0862e-05, -5.7068e-03,\n",
       "                      -1.7395e-03, -3.5645e-02,  2.2705e-02, -2.4414e-03, -2.0996e-02,\n",
       "                      -1.4038e-02,  1.6479e-02, -3.0396e-02, -6.5308e-03,  1.4038e-02,\n",
       "                       3.5645e-02, -3.6133e-02, -2.9785e-02,  3.7842e-02, -1.1780e-02,\n",
       "                      -4.5166e-02, -1.3062e-02, -4.4434e-02,  2.9053e-02, -1.4465e-02,\n",
       "                       1.6602e-02, -3.1738e-02, -3.1982e-02, -4.0771e-02, -1.5991e-02,\n",
       "                       3.2349e-03,  1.7822e-02, -4.7119e-02,  5.1575e-03,  9.7656e-03,\n",
       "                      -3.3417e-03,  3.4912e-02,  1.6251e-03, -1.6251e-03,  3.4943e-03,\n",
       "                       4.2236e-02, -5.3711e-03, -1.4893e-02, -2.7832e-02,  5.2490e-02,\n",
       "                       5.6152e-03, -3.4912e-02, -1.6632e-03,  1.4465e-02, -3.1128e-02,\n",
       "                       1.9653e-02, -3.4180e-02,  2.3438e-02, -2.4902e-02,  9.7656e-03,\n",
       "                       4.6631e-02, -2.6512e-04,  4.6143e-02, -6.1340e-03,  3.1006e-02,\n",
       "                       5.5432e-06,  2.5635e-02,  1.4587e-02, -2.8564e-02,  3.3447e-02,\n",
       "                      -1.3855e-02, -6.4697e-03,  1.2695e-02,  2.1851e-02,  8.7891e-03,\n",
       "                      -2.5269e-02, -2.3560e-02,  3.5645e-02,  8.4839e-03,  8.4229e-03,\n",
       "                      -3.8330e-02, -4.6143e-02, -5.1270e-02,  3.6133e-02, -4.5471e-03,\n",
       "                      -3.0518e-02, -6.4941e-02,  1.6968e-02, -3.5400e-02,  3.4027e-03,\n",
       "                       1.4709e-02, -6.2012e-02,  3.6133e-02, -4.6387e-02,  1.6357e-02,\n",
       "                       5.0659e-03,  4.5204e-04,  1.6937e-03,  1.2451e-02,  2.1851e-02,\n",
       "                       3.9307e-02, -3.2715e-02,  1.8883e-04], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.W_P.weight',\n",
       "              tensor([[-0.0854, -0.0825, -0.0147,  ..., -0.0304,  0.0898,  0.0181],\n",
       "                      [-0.0767,  0.0165,  0.0664,  ..., -0.0110, -0.0918, -0.0205],\n",
       "                      [-0.0227,  0.0610,  0.0139,  ...,  0.0815,  0.0718, -0.0205],\n",
       "                      ...,\n",
       "                      [-0.0172, -0.0145, -0.0515,  ...,  0.0708, -0.0229,  0.0664],\n",
       "                      [-0.0305,  0.0131,  0.0615,  ...,  0.0293,  0.0498,  0.0129],\n",
       "                      [-0.0752,  0.0850, -0.0542,  ..., -0.0176,  0.0693,  0.0864]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.W_P.bias',\n",
       "              tensor([-8.7891e-02,  7.8613e-02,  1.3916e-02, -5.8594e-02,  9.4727e-02,\n",
       "                      -9.4727e-02,  1.1328e-01, -6.5430e-02,  7.2266e-02, -4.8096e-02,\n",
       "                       3.3691e-02, -2.7618e-03, -4.1246e-05,  9.1309e-02, -1.8005e-03,\n",
       "                      -5.6396e-02, -3.0151e-02, -1.0156e-01, -8.7891e-02,  2.5635e-03,\n",
       "                      -1.2598e-01, -6.4941e-02, -1.0352e-01,  1.0010e-01, -1.2939e-02,\n",
       "                       4.2152e-04,  1.4648e-02,  8.3496e-02, -8.6060e-03, -5.7617e-02,\n",
       "                       7.5378e-03, -3.0273e-02,  5.1758e-02,  3.4424e-02, -7.0801e-02,\n",
       "                      -3.9307e-02, -7.0312e-02,  6.3965e-02, -6.1279e-02, -9.1309e-02,\n",
       "                       5.2734e-02, -3.0151e-02, -1.0986e-01,  5.7373e-02, -6.1035e-02,\n",
       "                       5.9326e-02, -3.7354e-02,  8.1787e-03,  5.0781e-02, -4.7363e-02,\n",
       "                      -6.6406e-02,  8.8867e-02, -1.5137e-02,  7.8613e-02, -7.2754e-02,\n",
       "                      -3.4424e-02, -6.3965e-02, -4.8096e-02,  4.6692e-03,  7.1777e-02,\n",
       "                      -9.1309e-02,  3.8086e-02, -3.3936e-02, -5.3711e-02, -1.2634e-02,\n",
       "                      -4.7119e-02,  4.7852e-02,  4.2725e-02, -1.0645e-01, -8.3008e-02,\n",
       "                       2.0142e-03, -4.2419e-03, -2.1362e-03,  6.0547e-02,  8.1543e-02,\n",
       "                       5.6152e-02,  1.9897e-02, -9.7656e-03, -4.4434e-02, -5.4932e-02,\n",
       "                      -6.4453e-02,  4.2480e-02, -1.0376e-02,  5.9570e-02, -6.0059e-02,\n",
       "                       4.7363e-02, -3.0396e-02,  5.7617e-02,  3.4912e-02, -8.5449e-02,\n",
       "                      -8.4961e-02,  1.0693e-01,  2.9755e-03, -5.5176e-02, -9.4727e-02,\n",
       "                      -7.5195e-02, -3.5156e-02,  6.4453e-02,  1.2793e-01, -2.6611e-02,\n",
       "                       1.4709e-02, -4.5898e-02,  9.4238e-02, -8.2397e-03,  9.2773e-03,\n",
       "                       4.3945e-02, -1.4893e-02, -4.2236e-02,  7.6172e-02,  1.2207e-02,\n",
       "                      -9.7168e-02, -4.6875e-02,  6.0303e-02, -7.6172e-02, -6.4453e-02,\n",
       "                       1.7700e-02, -3.7109e-02, -3.3936e-02, -1.1169e-02,  8.3496e-02,\n",
       "                       4.1016e-02,  8.6426e-02, -1.1523e-01, -2.0142e-02, -1.0864e-02,\n",
       "                      -1.7212e-02, -9.9609e-02,  1.1963e-01], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.inverW_P.weight',\n",
       "              tensor([[ 7.3853e-03, -5.8105e-02,  5.9814e-02,  ...,  4.9072e-02,\n",
       "                       -6.9336e-02,  1.1902e-02],\n",
       "                      [ 6.3477e-02, -8.1055e-02,  3.7354e-02,  ..., -8.1787e-03,\n",
       "                       -3.3112e-03, -6.7871e-02],\n",
       "                      [-5.7220e-05,  6.9824e-02,  7.9590e-02,  ..., -3.2715e-02,\n",
       "                       -8.5938e-02,  8.4961e-02],\n",
       "                      ...,\n",
       "                      [-2.4902e-02, -6.7383e-02,  8.1543e-02,  ...,  6.3477e-02,\n",
       "                        1.2390e-02,  3.0273e-02],\n",
       "                      [ 4.8584e-02, -1.2024e-02,  4.3335e-03,  ...,  6.5430e-02,\n",
       "                       -6.1279e-02, -1.8921e-02],\n",
       "                      [ 5.8838e-02, -5.8838e-02, -3.0884e-02,  ..., -5.8105e-02,\n",
       "                       -6.7871e-02, -3.1494e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_tower.inverW_P.bias',\n",
       "              tensor([-0.1079,  0.0271, -0.0525, -0.0500,  0.1035,  0.0649, -0.0278,  0.0166,\n",
       "                       0.0352, -0.0596,  0.0659, -0.0410,  0.0133, -0.0272,  0.0830,  0.0386,\n",
       "                      -0.0957,  0.0889,  0.0845, -0.0933,  0.0364,  0.0664, -0.0679,  0.0688,\n",
       "                       0.0918, -0.0603,  0.0640,  0.0197,  0.0884, -0.0608,  0.0264,  0.0498,\n",
       "                      -0.0425, -0.0194,  0.0371,  0.0225,  0.0549, -0.0986,  0.0160,  0.0830,\n",
       "                      -0.1221,  0.0554, -0.0298, -0.0244,  0.0422, -0.0586,  0.0747,  0.0791,\n",
       "                       0.0393,  0.0260,  0.0498, -0.0645, -0.0378,  0.0991,  0.0417,  0.0518,\n",
       "                      -0.0280,  0.0175, -0.1094,  0.0493, -0.1001, -0.0698, -0.0488, -0.0269,\n",
       "                       0.0176, -0.0251,  0.0503,  0.0349,  0.0437,  0.0008, -0.0889,  0.0840,\n",
       "                      -0.0806, -0.0300, -0.0781, -0.0806,  0.0079, -0.0718, -0.0189,  0.0776,\n",
       "                      -0.0496,  0.1001, -0.1055, -0.0182,  0.0137, -0.0359,  0.0491, -0.0325,\n",
       "                      -0.0393, -0.0918,  0.0908, -0.0205, -0.0093,  0.0317,  0.0742, -0.0601,\n",
       "                       0.0898,  0.0217, -0.0549, -0.0245,  0.0393,  0.0002,  0.0002, -0.0298,\n",
       "                       0.0096, -0.0078, -0.0737, -0.1074, -0.0014, -0.1011,  0.0967,  0.0737,\n",
       "                      -0.0874,  0.0361, -0.0618, -0.0240, -0.0349,  0.0947,  0.0981,  0.0398,\n",
       "                       0.0664, -0.0923,  0.0884, -0.0262,  0.0869, -0.0260,  0.0221, -0.0742],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_projector.weight',\n",
       "              tensor([[-0.0262, -0.0601,  0.0413,  ...,  0.0515, -0.0386,  0.0248],\n",
       "                      [-0.0359,  0.0381, -0.0762,  ...,  0.0571,  0.0101, -0.0342],\n",
       "                      [-0.0430, -0.0243, -0.0491,  ...,  0.0068, -0.0530,  0.0237],\n",
       "                      ...,\n",
       "                      [-0.0757, -0.0393,  0.0393,  ..., -0.0206,  0.0820,  0.0703],\n",
       "                      [ 0.0569,  0.0144, -0.0085,  ...,  0.0596, -0.0669,  0.0236],\n",
       "                      [ 0.0664,  0.0845, -0.0284,  ...,  0.0410, -0.0010,  0.0255]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.model.graph_projector.bias',\n",
       "              tensor([ 0.0830,  0.0486,  0.0576,  ...,  0.0208, -0.0243, -0.0011],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.base_model.model.lm_head.weight',\n",
       "              tensor([[-0.0459,  0.0026, -0.0171,  ...,  0.0273, -0.0415,  0.0229],\n",
       "                      [-0.0381,  0.0688,  0.0625,  ...,  0.0259, -0.0210,  0.0356],\n",
       "                      [-0.0085,  0.0077,  0.0194,  ..., -0.0674,  0.0327, -0.0574],\n",
       "                      ...,\n",
       "                      [-0.0121,  0.0075, -0.0320,  ...,  0.0125, -0.0011, -0.0249],\n",
       "                      [-0.0086, -0.0051, -0.0036,  ...,  0.0081, -0.0079,  0.0035],\n",
       "                      [-0.0086, -0.0051, -0.0036,  ...,  0.0081, -0.0079,  0.0035]],\n",
       "                     dtype=torch.bfloat16))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['state_dict']['model.base_model.model.lm_head.weight']\n",
    "ckpt['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['global_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0.post0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['pytorch-lightning_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_loop': {'state_dict': {},\n",
       "  'epoch_loop.state_dict': {'_batches_that_stepped': 537},\n",
       "  'epoch_loop.batch_progress': {'total': {'ready': 537,\n",
       "    'completed': 537,\n",
       "    'started': 537,\n",
       "    'processed': 537},\n",
       "   'current': {'ready': 537,\n",
       "    'completed': 537,\n",
       "    'started': 537,\n",
       "    'processed': 537},\n",
       "   'is_last_batch': True},\n",
       "  'epoch_loop.scheduler_progress': {'total': {'ready': 537, 'completed': 537},\n",
       "   'current': {'ready': 537, 'completed': 537}},\n",
       "  'epoch_loop.automatic_optimization.state_dict': {},\n",
       "  'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 537,\n",
       "      'completed': 537},\n",
       "     'current': {'ready': 537, 'completed': 537}},\n",
       "    'zero_grad': {'total': {'ready': 537, 'completed': 537, 'started': 537},\n",
       "     'current': {'ready': 537, 'completed': 537, 'started': 537}}}},\n",
       "  'epoch_loop.manual_optimization.state_dict': {},\n",
       "  'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 0,\n",
       "    'completed': 0},\n",
       "   'current': {'ready': 0, 'completed': 0}},\n",
       "  'epoch_loop.val_loop.state_dict': {},\n",
       "  'epoch_loop.val_loop.batch_progress': {'total': {'ready': 0,\n",
       "    'completed': 0,\n",
       "    'started': 0,\n",
       "    'processed': 0},\n",
       "   'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "   'is_last_batch': False},\n",
       "  'epoch_progress': {'total': {'ready': 1,\n",
       "    'completed': 0,\n",
       "    'started': 1,\n",
       "    'processed': 1},\n",
       "   'current': {'ready': 1, 'completed': 0, 'started': 1, 'processed': 1}}},\n",
       " 'validate_loop': {'state_dict': {},\n",
       "  'batch_progress': {'total': {'ready': 0,\n",
       "    'completed': 0,\n",
       "    'started': 0,\n",
       "    'processed': 0},\n",
       "   'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "   'is_last_batch': False}},\n",
       " 'test_loop': {'state_dict': {},\n",
       "  'batch_progress': {'total': {'ready': 0,\n",
       "    'completed': 0,\n",
       "    'started': 0,\n",
       "    'processed': 0},\n",
       "   'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "   'is_last_batch': False}},\n",
       " 'predict_loop': {'state_dict': {},\n",
       "  'batch_progress': {'total': {'ready': 0,\n",
       "    'completed': 0,\n",
       "    'started': 0,\n",
       "    'processed': 0},\n",
       "   'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['loops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"ModelCheckpoint{'monitor': 'train_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\": {'monitor': 'train_loss',\n",
       "  'best_model_score': tensor(2.5536, device='cuda:0'),\n",
       "  'best_model_path': '/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2/model_epoch=0-step=537.ckpt',\n",
       "  'current_score': tensor(2.5536, device='cuda:0'),\n",
       "  'dirpath': '/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2',\n",
       "  'best_k_models': {'/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2/model_epoch=0-step=537.ckpt': tensor(2.5536, device='cuda:0')},\n",
       "  'kth_best_model_path': '/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2/model_epoch=0-step=537.ckpt',\n",
       "  'kth_value': tensor(2.5536, device='cuda:0'),\n",
       "  'last_model_path': '/data/LPJ/ICML25/GraphGPT/checkpoints/fine_tuning/v2/last.ckpt'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['callbacks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': {0: {'exp_avg': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "            [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "            [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "            ...,\n",
       "            [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "            [ 0.1768, -0.0269,  0.1143,  ...,  0.1021, -0.1055, -0.1157],\n",
       "            [ 0.2041, -0.0938,  0.1328,  ...,  0.1157, -0.1128, -0.1318]],\n",
       "           dtype=torch.bfloat16),\n",
       "    'exp_avg_sq': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            ...,\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0403, 0.0015, 0.0055,  ..., 0.0037, 0.0057, 0.0043],\n",
       "            [0.0354, 0.0041, 0.0073,  ..., 0.0057, 0.0036, 0.0056]],\n",
       "           dtype=torch.bfloat16),\n",
       "    'step': tensor(537.)},\n",
       "   759: {'exp_avg': tensor([ 0.0020,  0.0081,  0.0114,  ...,  0.0009,  0.0027, -0.0050],\n",
       "           dtype=torch.bfloat16),\n",
       "    'exp_avg_sq': tensor([2.3603e-05, 9.7752e-05, 4.7684e-05,  ..., 9.8348e-06, 1.0252e-05,\n",
       "            3.3855e-05], dtype=torch.bfloat16),\n",
       "    'step': tensor(537.)},\n",
       "   758: {'exp_avg': tensor([[-0.0013, -0.0016, -0.0013,  ...,  0.0009, -0.0004, -0.0021],\n",
       "            [-0.0045, -0.0055, -0.0049,  ...,  0.0036, -0.0010, -0.0081],\n",
       "            [-0.0068, -0.0089, -0.0068,  ...,  0.0052, -0.0020, -0.0110],\n",
       "            ...,\n",
       "            [-0.0003, -0.0009, -0.0010,  ...,  0.0006, -0.0003, -0.0005],\n",
       "            [-0.0015, -0.0022, -0.0018,  ...,  0.0013, -0.0002, -0.0023],\n",
       "            [ 0.0030,  0.0043,  0.0029,  ..., -0.0023,  0.0009,  0.0046]],\n",
       "           dtype=torch.bfloat16),\n",
       "    'exp_avg_sq': tensor([[7.9274e-06, 1.5736e-05, 7.4804e-06,  ..., 6.0499e-06, 4.3213e-07,\n",
       "             1.8597e-05],\n",
       "            [3.1948e-05, 6.2466e-05, 3.0994e-05,  ..., 2.0862e-05, 1.7881e-06,\n",
       "             7.5817e-05],\n",
       "            [1.7524e-05, 2.9445e-05, 1.5140e-05,  ..., 1.0014e-05, 1.1697e-06,\n",
       "             4.7684e-05],\n",
       "            ...,\n",
       "            [3.3975e-06, 7.6890e-06, 2.8908e-06,  ..., 2.3991e-06, 2.7195e-07,\n",
       "             8.0466e-06],\n",
       "            [3.2932e-06, 8.5831e-06, 3.3975e-06,  ..., 2.5183e-06, 3.2037e-07,\n",
       "             7.3910e-06],\n",
       "            [1.1086e-05, 2.5868e-05, 1.1623e-05,  ..., 8.5831e-06, 7.0035e-07,\n",
       "             2.5630e-05]], dtype=torch.bfloat16),\n",
       "    'step': tensor(537.)}},\n",
       "  'param_groups': [{'lr_scale': [1e-05, 0.0001],\n",
       "    'lr': 1.0740000000000002e-05,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0.01,\n",
       "    'amsgrad': False,\n",
       "    'foreach': None,\n",
       "    'maximize': False,\n",
       "    'capturable': False,\n",
       "    'differentiable': False,\n",
       "    'fused': None,\n",
       "    'initial_lr': 2e-05,\n",
       "    'params': [0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4,\n",
       "     5,\n",
       "     6,\n",
       "     7,\n",
       "     8,\n",
       "     9,\n",
       "     10,\n",
       "     11,\n",
       "     12,\n",
       "     13,\n",
       "     14,\n",
       "     15,\n",
       "     16,\n",
       "     17,\n",
       "     18,\n",
       "     19,\n",
       "     20,\n",
       "     21,\n",
       "     22,\n",
       "     23,\n",
       "     24,\n",
       "     25,\n",
       "     26,\n",
       "     27,\n",
       "     28,\n",
       "     29,\n",
       "     30,\n",
       "     31,\n",
       "     32,\n",
       "     33,\n",
       "     34,\n",
       "     35,\n",
       "     36,\n",
       "     37,\n",
       "     38,\n",
       "     39,\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44,\n",
       "     45,\n",
       "     46,\n",
       "     47,\n",
       "     48,\n",
       "     49,\n",
       "     50,\n",
       "     51,\n",
       "     52,\n",
       "     53,\n",
       "     54,\n",
       "     55,\n",
       "     56,\n",
       "     57,\n",
       "     58,\n",
       "     59,\n",
       "     60,\n",
       "     61,\n",
       "     62,\n",
       "     63,\n",
       "     64,\n",
       "     65,\n",
       "     66,\n",
       "     67,\n",
       "     68,\n",
       "     69,\n",
       "     70,\n",
       "     71,\n",
       "     72,\n",
       "     73,\n",
       "     74,\n",
       "     75,\n",
       "     76,\n",
       "     77,\n",
       "     78,\n",
       "     79,\n",
       "     80,\n",
       "     81,\n",
       "     82,\n",
       "     83,\n",
       "     84,\n",
       "     85,\n",
       "     86,\n",
       "     87,\n",
       "     88,\n",
       "     89,\n",
       "     90,\n",
       "     91,\n",
       "     92,\n",
       "     93,\n",
       "     94,\n",
       "     95,\n",
       "     96,\n",
       "     97,\n",
       "     98,\n",
       "     99,\n",
       "     100,\n",
       "     101,\n",
       "     102,\n",
       "     103,\n",
       "     104,\n",
       "     105,\n",
       "     106,\n",
       "     107,\n",
       "     108,\n",
       "     109,\n",
       "     110,\n",
       "     111,\n",
       "     112,\n",
       "     113,\n",
       "     114,\n",
       "     115,\n",
       "     116,\n",
       "     117,\n",
       "     118,\n",
       "     119,\n",
       "     120,\n",
       "     121,\n",
       "     122,\n",
       "     123,\n",
       "     124,\n",
       "     125,\n",
       "     126,\n",
       "     127,\n",
       "     128,\n",
       "     129,\n",
       "     130,\n",
       "     131,\n",
       "     132,\n",
       "     133,\n",
       "     134,\n",
       "     135,\n",
       "     136,\n",
       "     137,\n",
       "     138,\n",
       "     139,\n",
       "     140,\n",
       "     141,\n",
       "     142,\n",
       "     143,\n",
       "     144,\n",
       "     145,\n",
       "     146,\n",
       "     147,\n",
       "     148,\n",
       "     149,\n",
       "     150,\n",
       "     151,\n",
       "     152,\n",
       "     153,\n",
       "     154,\n",
       "     155,\n",
       "     156,\n",
       "     157,\n",
       "     158,\n",
       "     159,\n",
       "     160,\n",
       "     161,\n",
       "     162,\n",
       "     163,\n",
       "     164,\n",
       "     165,\n",
       "     166,\n",
       "     167,\n",
       "     168,\n",
       "     169,\n",
       "     170,\n",
       "     171,\n",
       "     172,\n",
       "     173,\n",
       "     174,\n",
       "     175,\n",
       "     176,\n",
       "     177,\n",
       "     178,\n",
       "     179,\n",
       "     180,\n",
       "     181,\n",
       "     182,\n",
       "     183,\n",
       "     184,\n",
       "     185,\n",
       "     186,\n",
       "     187,\n",
       "     188,\n",
       "     189,\n",
       "     190,\n",
       "     191,\n",
       "     192,\n",
       "     193,\n",
       "     194,\n",
       "     195,\n",
       "     196,\n",
       "     197,\n",
       "     198,\n",
       "     199,\n",
       "     200,\n",
       "     201,\n",
       "     202,\n",
       "     203,\n",
       "     204,\n",
       "     205,\n",
       "     206,\n",
       "     207,\n",
       "     208,\n",
       "     209,\n",
       "     210,\n",
       "     211,\n",
       "     212,\n",
       "     213,\n",
       "     214,\n",
       "     215,\n",
       "     216,\n",
       "     217,\n",
       "     218,\n",
       "     219,\n",
       "     220,\n",
       "     221,\n",
       "     222,\n",
       "     223,\n",
       "     224,\n",
       "     225,\n",
       "     226,\n",
       "     227,\n",
       "     228,\n",
       "     229,\n",
       "     230,\n",
       "     231,\n",
       "     232,\n",
       "     233,\n",
       "     234,\n",
       "     235,\n",
       "     236,\n",
       "     237,\n",
       "     238,\n",
       "     239,\n",
       "     240,\n",
       "     241,\n",
       "     242,\n",
       "     243,\n",
       "     244,\n",
       "     245,\n",
       "     246,\n",
       "     247,\n",
       "     248,\n",
       "     249,\n",
       "     250,\n",
       "     251,\n",
       "     252,\n",
       "     253,\n",
       "     254,\n",
       "     255,\n",
       "     256,\n",
       "     257,\n",
       "     258,\n",
       "     259,\n",
       "     260,\n",
       "     261,\n",
       "     262,\n",
       "     263,\n",
       "     264,\n",
       "     265,\n",
       "     266,\n",
       "     267,\n",
       "     268,\n",
       "     269,\n",
       "     270,\n",
       "     271,\n",
       "     272,\n",
       "     273,\n",
       "     274,\n",
       "     275,\n",
       "     276,\n",
       "     277,\n",
       "     278,\n",
       "     279,\n",
       "     280,\n",
       "     281,\n",
       "     282,\n",
       "     283,\n",
       "     284,\n",
       "     285,\n",
       "     286,\n",
       "     287,\n",
       "     288,\n",
       "     289,\n",
       "     290,\n",
       "     291,\n",
       "     292,\n",
       "     293,\n",
       "     294,\n",
       "     295,\n",
       "     296,\n",
       "     297,\n",
       "     298,\n",
       "     299,\n",
       "     300,\n",
       "     301,\n",
       "     302,\n",
       "     303,\n",
       "     304,\n",
       "     305,\n",
       "     306,\n",
       "     307,\n",
       "     308,\n",
       "     309,\n",
       "     310,\n",
       "     311,\n",
       "     312,\n",
       "     313,\n",
       "     314,\n",
       "     315,\n",
       "     316,\n",
       "     317,\n",
       "     318,\n",
       "     319,\n",
       "     320,\n",
       "     321,\n",
       "     322,\n",
       "     323,\n",
       "     324,\n",
       "     325,\n",
       "     326,\n",
       "     327,\n",
       "     328,\n",
       "     329,\n",
       "     330,\n",
       "     331,\n",
       "     332,\n",
       "     333,\n",
       "     334,\n",
       "     335,\n",
       "     336,\n",
       "     337,\n",
       "     338,\n",
       "     339,\n",
       "     340,\n",
       "     341,\n",
       "     342,\n",
       "     343,\n",
       "     344,\n",
       "     345,\n",
       "     346,\n",
       "     347,\n",
       "     348,\n",
       "     349,\n",
       "     350,\n",
       "     351,\n",
       "     352,\n",
       "     353,\n",
       "     354,\n",
       "     355,\n",
       "     356,\n",
       "     357,\n",
       "     358,\n",
       "     359,\n",
       "     360,\n",
       "     361,\n",
       "     362,\n",
       "     363,\n",
       "     364,\n",
       "     365,\n",
       "     366,\n",
       "     367,\n",
       "     368,\n",
       "     369,\n",
       "     370,\n",
       "     371,\n",
       "     372,\n",
       "     373,\n",
       "     374,\n",
       "     375,\n",
       "     376,\n",
       "     377,\n",
       "     378,\n",
       "     379,\n",
       "     380,\n",
       "     381,\n",
       "     382,\n",
       "     383,\n",
       "     384,\n",
       "     385,\n",
       "     386,\n",
       "     387,\n",
       "     388,\n",
       "     389,\n",
       "     390,\n",
       "     391,\n",
       "     392,\n",
       "     393,\n",
       "     394,\n",
       "     395,\n",
       "     396,\n",
       "     397,\n",
       "     398,\n",
       "     399,\n",
       "     400,\n",
       "     401,\n",
       "     402,\n",
       "     403,\n",
       "     404,\n",
       "     405,\n",
       "     406,\n",
       "     407,\n",
       "     408,\n",
       "     409,\n",
       "     410,\n",
       "     411,\n",
       "     412,\n",
       "     413,\n",
       "     414,\n",
       "     415,\n",
       "     416,\n",
       "     417,\n",
       "     418,\n",
       "     419,\n",
       "     420,\n",
       "     421,\n",
       "     422,\n",
       "     423,\n",
       "     424,\n",
       "     425,\n",
       "     426,\n",
       "     427,\n",
       "     428,\n",
       "     429,\n",
       "     430,\n",
       "     431,\n",
       "     432,\n",
       "     433,\n",
       "     434,\n",
       "     435,\n",
       "     436,\n",
       "     437,\n",
       "     438,\n",
       "     439,\n",
       "     440,\n",
       "     441,\n",
       "     442,\n",
       "     443,\n",
       "     444,\n",
       "     445,\n",
       "     446,\n",
       "     447,\n",
       "     448,\n",
       "     449,\n",
       "     450,\n",
       "     451,\n",
       "     452,\n",
       "     453,\n",
       "     454,\n",
       "     455,\n",
       "     456,\n",
       "     457,\n",
       "     458,\n",
       "     459,\n",
       "     460,\n",
       "     461,\n",
       "     462,\n",
       "     463,\n",
       "     464,\n",
       "     465,\n",
       "     466,\n",
       "     467,\n",
       "     468,\n",
       "     469,\n",
       "     470,\n",
       "     471,\n",
       "     472,\n",
       "     473,\n",
       "     474,\n",
       "     475,\n",
       "     476,\n",
       "     477,\n",
       "     478,\n",
       "     479,\n",
       "     480,\n",
       "     481,\n",
       "     482,\n",
       "     483,\n",
       "     484,\n",
       "     485,\n",
       "     486,\n",
       "     487,\n",
       "     488,\n",
       "     489,\n",
       "     490,\n",
       "     491,\n",
       "     492,\n",
       "     493,\n",
       "     494,\n",
       "     495,\n",
       "     496,\n",
       "     497,\n",
       "     498,\n",
       "     499,\n",
       "     500,\n",
       "     501,\n",
       "     502,\n",
       "     503,\n",
       "     504,\n",
       "     505,\n",
       "     506,\n",
       "     507,\n",
       "     508,\n",
       "     509,\n",
       "     510,\n",
       "     511,\n",
       "     512,\n",
       "     513,\n",
       "     514,\n",
       "     515,\n",
       "     516,\n",
       "     517,\n",
       "     518,\n",
       "     519,\n",
       "     520,\n",
       "     521,\n",
       "     522,\n",
       "     523,\n",
       "     524,\n",
       "     525,\n",
       "     526,\n",
       "     527,\n",
       "     528,\n",
       "     529,\n",
       "     530,\n",
       "     531,\n",
       "     532,\n",
       "     533,\n",
       "     534,\n",
       "     535,\n",
       "     536,\n",
       "     537,\n",
       "     538,\n",
       "     539,\n",
       "     540,\n",
       "     541,\n",
       "     542,\n",
       "     543,\n",
       "     544,\n",
       "     545,\n",
       "     546,\n",
       "     547,\n",
       "     548,\n",
       "     549,\n",
       "     550,\n",
       "     551,\n",
       "     552,\n",
       "     553,\n",
       "     554,\n",
       "     555,\n",
       "     556,\n",
       "     557,\n",
       "     558,\n",
       "     559,\n",
       "     560,\n",
       "     561,\n",
       "     562,\n",
       "     563,\n",
       "     564,\n",
       "     565,\n",
       "     566,\n",
       "     567,\n",
       "     568,\n",
       "     569,\n",
       "     570,\n",
       "     571,\n",
       "     572,\n",
       "     573,\n",
       "     574,\n",
       "     575,\n",
       "     576,\n",
       "     577,\n",
       "     578,\n",
       "     579,\n",
       "     580,\n",
       "     581,\n",
       "     582,\n",
       "     583,\n",
       "     584,\n",
       "     585,\n",
       "     586,\n",
       "     587,\n",
       "     588,\n",
       "     589,\n",
       "     590,\n",
       "     591,\n",
       "     592,\n",
       "     593,\n",
       "     594,\n",
       "     595,\n",
       "     596,\n",
       "     597,\n",
       "     598,\n",
       "     599,\n",
       "     600,\n",
       "     601,\n",
       "     602,\n",
       "     603,\n",
       "     604,\n",
       "     605,\n",
       "     606,\n",
       "     607,\n",
       "     608,\n",
       "     609,\n",
       "     610,\n",
       "     611,\n",
       "     612,\n",
       "     613,\n",
       "     614,\n",
       "     615,\n",
       "     616,\n",
       "     617,\n",
       "     618,\n",
       "     619,\n",
       "     620,\n",
       "     621,\n",
       "     622,\n",
       "     623,\n",
       "     624,\n",
       "     625,\n",
       "     626,\n",
       "     627,\n",
       "     628,\n",
       "     629,\n",
       "     630,\n",
       "     631,\n",
       "     632,\n",
       "     633,\n",
       "     634,\n",
       "     635,\n",
       "     636,\n",
       "     637,\n",
       "     638,\n",
       "     639,\n",
       "     640,\n",
       "     641,\n",
       "     642,\n",
       "     643,\n",
       "     644,\n",
       "     645,\n",
       "     646,\n",
       "     647,\n",
       "     648,\n",
       "     649,\n",
       "     650,\n",
       "     651,\n",
       "     652,\n",
       "     653,\n",
       "     654,\n",
       "     655,\n",
       "     656,\n",
       "     657,\n",
       "     658,\n",
       "     659,\n",
       "     660,\n",
       "     661,\n",
       "     662,\n",
       "     663,\n",
       "     664,\n",
       "     665,\n",
       "     666,\n",
       "     667,\n",
       "     668,\n",
       "     669,\n",
       "     670,\n",
       "     671,\n",
       "     672,\n",
       "     673,\n",
       "     674,\n",
       "     675,\n",
       "     676,\n",
       "     677,\n",
       "     678,\n",
       "     679,\n",
       "     680,\n",
       "     681,\n",
       "     682,\n",
       "     683,\n",
       "     684,\n",
       "     685,\n",
       "     686,\n",
       "     687,\n",
       "     688,\n",
       "     689,\n",
       "     690,\n",
       "     691,\n",
       "     692,\n",
       "     693,\n",
       "     694,\n",
       "     695,\n",
       "     696,\n",
       "     697,\n",
       "     698,\n",
       "     699,\n",
       "     700,\n",
       "     701,\n",
       "     702,\n",
       "     703,\n",
       "     704,\n",
       "     705,\n",
       "     706,\n",
       "     707,\n",
       "     708,\n",
       "     709,\n",
       "     710,\n",
       "     711,\n",
       "     712,\n",
       "     713,\n",
       "     714,\n",
       "     715,\n",
       "     716,\n",
       "     717,\n",
       "     718,\n",
       "     719,\n",
       "     720,\n",
       "     721,\n",
       "     722,\n",
       "     723,\n",
       "     724,\n",
       "     725,\n",
       "     726,\n",
       "     727,\n",
       "     728,\n",
       "     729,\n",
       "     730,\n",
       "     731,\n",
       "     732,\n",
       "     733,\n",
       "     734,\n",
       "     735,\n",
       "     736,\n",
       "     737,\n",
       "     738,\n",
       "     739,\n",
       "     740,\n",
       "     741,\n",
       "     742,\n",
       "     743,\n",
       "     744,\n",
       "     745,\n",
       "     746,\n",
       "     747,\n",
       "     748,\n",
       "     749,\n",
       "     750,\n",
       "     751,\n",
       "     752,\n",
       "     753,\n",
       "     754,\n",
       "     755,\n",
       "     756,\n",
       "     757,\n",
       "     758,\n",
       "     759,\n",
       "     760]}]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['optimizer_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'base_lrs': [2e-05],\n",
       "  'last_epoch': 537,\n",
       "  'verbose': False,\n",
       "  '_step_count': 538,\n",
       "  '_get_lr_called_within_step': False,\n",
       "  '_last_lr': [1.0740000000000002e-05],\n",
       "  'lr_lambdas': [{}]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['lr_schedulers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath='/data/LPJ/ICML25/graphgpt_dataset/HiVerilog_Eval/availiabe_for_graphcoder/conversations.json'\n",
    "graph_data_path='/data/LPJ/ICML25/graphgpt_dataset/HiVerilog_Eval/availiabe_for_graphcoder/graph.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pd = pd.read_json(graph_data_path, lines=True)\n",
    "with open(datapath, 'r') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'conversations': [{'from': 'human', 'value': \"Given a submodules interconnection graph: \\n<graph>\\n, \\nnodes: [{'id': 0, 'content': 'clk', 'type': 'input port'}, {'id': 1, 'content': 'reset', 'type': 'input port'}, {'id': 2, 'content': 'up_down', 'type': 'input port'}, {'id': 3, 'content': 'count', 'type': 'output port'}, {'id': 4, 'content': 'u_counter_logic', 'type': 'submodule'}, {'id': 5, 'content': 'u_counter_register', 'type': 'submodule'}], \\nedge_attrs: [], \\nconnectivity: [[1, 2, 0, 5, 1, 0, 4, 5], [4, 4, 4, 4, 5, 5, 5, 3]]\\n, Module name:\\n    up_down_counter\\n\\nFunction:\\nA 16-bit counter that can increment or decrement based on control signals, implemented in a modular fashion.\\n\\nInput ports:\\n    - clk: Clock signal (1-bit), used to synchronize the counting process.\\n    - reset: Reset signal (1-bit), used to reset the counter to zero.\\n    - up_down: Control signal (1-bit), determines the counting direction.\\n    If up_down = 1, the counter increments; if up_down = 0, it decrements.\\n\\nOutput ports:\\n    - count [15:0]: 16-bit output representing the current counter value.\\n\\nImplementation:\\nThe module consists of two submodules: counter_logic and counter_register.\\n- The counter_logic module calculates the next count value based on the current count and control signals.\\n- The counter_register module updates the current count value on the rising edge of the clock signal or resets it based on the reset condition.\\nThe count output reflects the current value of the counter, which can range from 0 to 65535.\\n\\nGive me the complete code.\"}], 'task_id': 'up_down_counter'}\n",
      "nodes           [{'id': 0, 'content': 'clk', 'type': 'input po...\n",
      "edge_attrs                                                     []\n",
      "connectivity    [[1, 2, 0, 5, 1, 0, 4, 5], [4, 4, 4, 4, 5, 5, ...\n",
      "Name: 0, dtype: object\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for idx, instruct_item, graph in tqdm(enumerate(zip(data, graph.iterrows()))):\n",
    "#     print(idx)\n",
    "for idx, (instruct_item, (graph_index, graph)) in tqdm(enumerate(zip(data, graph_pd.iterrows())), total=len(data)):\n",
    "    print(idx)\n",
    "    print(instruct_item)\n",
    "    print(graph)\n",
    "    print(graph_index)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
