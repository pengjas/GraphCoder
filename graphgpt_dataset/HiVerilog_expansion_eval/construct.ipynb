{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_path = '/data/LPJ/ICML25/GraphCoder/ablation_study/fine_tune_text_LLM_with_original_graph/1989_stage2_400/verified_conversations.jsonl'\n",
    "graph_path = '/data/LPJ/ICML25/GraphCoder/ablation_study/fine_tune_text_LLM_with_original_graph/1989_stage2_400/verified_graphs.jsonl'\n",
    "conv = pd.read_json(conv_path, lines=True)\n",
    "graph = pd.read_json(graph_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled indices: [782, 142, 631, 1223, 769, 847, 1943, 1322, 1021, 1794, 1749, 188, 1192, 1414, 585, 823, 340, 453, 949, 751, 390, 649, 763, 639, 280, 468, 421, 1743, 1175, 836, 1245, 122, 739, 1452, 50, 787, 686, 1438, 1317, 750, 3, 76, 1648, 876, 403, 1564, 1839, 14, 210, 12, 8, 18, 674, 1721, 1496, 90, 1681]\n",
      "Sampled counts per task_id:\n",
      "task_id: pe, sampled count: 5\n",
      "task_id: adder_8bit, sampled count: 5\n",
      "task_id: up_down_counter, sampled count: 4\n",
      "task_id: sub_8bit, sampled count: 4\n",
      "task_id: stagepipe3, sampled count: 4\n",
      "task_id: barrel_shifter, sampled count: 4\n",
      "task_id: sub_16bit, sampled count: 4\n",
      "task_id: systolic1x4, sampled count: 3\n",
      "task_id: systolic1x2, sampled count: 3\n",
      "task_id: comparator_32bit, sampled count: 3\n",
      "task_id: instr_reg, sampled count: 2\n",
      "task_id: square_wave, sampled count: 2\n",
      "task_id: adder_16bit, sampled count: 2\n",
      "task_id: shift8, sampled count: 2\n",
      "task_id: freq_div, sampled count: 2\n",
      "task_id: freq_divbyodd, sampled count: 1\n",
      "task_id: alu, sampled count: 1\n",
      "task_id: accu, sampled count: 1\n",
      "task_id: stagepipe5, sampled count: 1\n",
      "task_id: adder_16bit_csa, sampled count: 1\n",
      "task_id: adder_32bit, sampled count: 1\n",
      "task_id: shift_reg, sampled count: 1\n",
      "task_id: ring_counter, sampled count: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 统计不同的task_id有多少个\n",
    "task_id_counts = conv['task_id'].value_counts()\n",
    "\n",
    "# 对于每个task_id随机抽取其中1/30，若无法整除则向下取整，但至少取一条数据\n",
    "sampled_indices = []\n",
    "sampled_counts = {}  # 用于记录每个task_id实际抽取的数据量\n",
    "\n",
    "for task_id, count in task_id_counts.items():\n",
    "    sample_size = max(1, count // 30)  # 至少取一条数据\n",
    "    sampled_data = conv[conv['task_id'] == task_id].sample(n=sample_size, random_state=42)\n",
    "    sampled_indices.extend(sampled_data.index)\n",
    "    sampled_counts[task_id] = len(sampled_data)  # 记录实际抽取的数据量\n",
    "\n",
    "# 输出整个index\n",
    "print(\"Sampled indices:\", sampled_indices)\n",
    "\n",
    "# 输出每个task_id实际抽取的数据量\n",
    "print(\"Sampled counts per task_id:\")\n",
    "for task_id, count in sampled_counts.items():\n",
    "    print(f\"task_id: {task_id}, sampled count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_eval = conv.loc[sampled_indices]\n",
    "graph_eval = graph.loc[sampled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb_path = '/data/LPJ/ICML25/GraphCoder/ablation_study/fine_tune_text_LLM_with_original_graph/1989_stage2_400/verified_testbenches.jsonl'\n",
    "# tb = pd.read_json(tb_path, lines=True)\n",
    "# tb_eval = tb.loc[sampled_indices]\n",
    "# new_tb_path = '/data/LPJ/ICML25/GraphCoder/graphgpt_dataset/HiVerilog_expansion_eval/testbenches.jsonl'\n",
    "# tb_eval.to_json(new_tb_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in conv_eval.iterrows():\n",
    "#     conv_eval.at[index, 'task_id'] = f\"v_{index}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conv_path = '/data/LPJ/ICML25/GraphCoder/graphgpt_dataset/HiVerilog_expansion_eval/conversations.jsonl'\n",
    "new_graph_path = '/data/LPJ/ICML25/GraphCoder/graphgpt_dataset/HiVerilog_expansion_eval/graphs.jsonl'\n",
    "conv_eval.to_json(new_conv_path, orient='records', lines=True)\n",
    "graph_eval.to_json(new_graph_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conv = pd.read_json(new_conv_path, lines=True)\n",
    "for index, row in new_conv.iterrows():\n",
    "    new_conv.at[index, 'task_id'] = f\"{new_conv.at[index, 'task_id']}_v_{index}\"\n",
    "\n",
    "new_conv_path = '/data/LPJ/ICML25/GraphCoder/graphgpt_dataset/HiVerilog_expansion_eval/new_conversations.jsonl'\n",
    "new_conv.to_json(new_conv_path, orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
